{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size = 2, activation = nn.GELU):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # linear layers wich reduce the dimensionality of the input\n",
    "        # and aim in campturing the most important features\n",
    "        self.fc1_text = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2_text = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # transformer encoder layers\n",
    "        # applied to the image and text features\n",
    "        self.t_encoder_text = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=3)\n",
    "        self.t_encoder_image = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=3)\n",
    "        \n",
    "        # linear layer which combines the image and text features\n",
    "        # encoded and classify them\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # default activation function\n",
    "        self.activation = activation()\n",
    "        \n",
    "        # out activation function\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x =\n",
    "        x = self.softmax(self.fc(x))\n",
    "        return x\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# data loader\n",
    "cap = dset.CocoCaptions()\n",
    "\n",
    "print('Number of samples: ', len(cap))\n",
    "img, target = cap[3]\n",
    "\n",
    "print(\"Image Size: \", img.size())\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3163cfb8aa3549ad3f5400bc3427ee7a4002d2a0d6d7ead52f641c6a7636395"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
