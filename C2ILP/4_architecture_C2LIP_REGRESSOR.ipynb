{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6fg3iKf3xJ9",
        "outputId": "65262812-0140-4842-e34e-2bf2fd63f865"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.0.2-py3-none-any.whl (719 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.0/719.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.65.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.4.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Collecting lightning-utilities>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.27.1)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (16.0.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->pytorch-lightning) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->pytorch-lightning) (1.3.0)\n",
            "Installing collected packages: multidict, lightning-utilities, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torchmetrics, pytorch-lightning\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 lightning-utilities-0.8.0 multidict-6.0.4 pytorch-lightning-2.0.2 torchmetrics-0.11.4 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uzLq1mb4A3p",
        "outputId": "ec4c2866-d65e-4d28-909d-937e33016214"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "LhVPHRqJ4PyU",
        "outputId": "6d2b1c0f-547c-4bed-b916-1af3dd779890"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2573/2573 [00:00<00:00, 620976.13it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b97b56d93c40>\u001b[0m in \u001b[0;36m<cell line: 75>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# text_encoding_test, box_encoding_test, box_coords_test, target_boxes_test = get_data_single_prompt(data_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mtext_encoding_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_encoding_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_coords_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_boxes_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_single_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mdata_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mtext_encoding_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_encoding_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_coords_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_boxes_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_single_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-b97b56d93c40>\u001b[0m in \u001b[0;36mget_data_single_prompt\u001b[0;34m(full_data)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m            \u001b[0;31m# (number of samples, 1, 512, 1)          padd the tensors to stack them togheter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m48\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcat_encoding_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m48\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbox_coords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0...\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import torch\n",
        "\n",
        "max_training_samples = 20000\n",
        "\n",
        "# load test dtaset\n",
        "file_name = '/content/drive/MyDrive/refCOCOg Visual Grounding/yolov8x+clip/yolo_v8x_1_dictionary_full_train.p'\n",
        "with open(file_name, 'rb') as f:\n",
        "    data_train = pickle.load(f)[:max_training_samples]\n",
        "\n",
        "# load test dataset\n",
        "# file_name = '/content/drive/MyDrive/refCOCOg Visual Grounding/yolov8x+clip/yolo_v8x_1_dictionary_full_test.p'\n",
        "# with open(file_name, 'rb') as f:\n",
        "#     data_test = pickle.load(f)\n",
        "\n",
        "# load test dataset\n",
        "file_name = '/content/drive/MyDrive/refCOCOg Visual Grounding/yolov8x+clip/yolo_v8x_1_dictionary_full_val.p'\n",
        "with open(file_name, 'rb') as f:\n",
        "    data_val = pickle.load(f)\n",
        "\n",
        "\n",
        "# load the dataset\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# load the dataset\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_data_single_prompt(full_data):\n",
        "\n",
        "    text_encoding, cat_encoding_text, box_coords, target_boxes = [], [], [], []\n",
        "\n",
        "    for idx in tqdm(list(full_data)):\n",
        "        # for _ in range(data['image_emb'].shape[0]):\n",
        "        for idx_text in range(full_data[idx]['text_emb'].shape[0]):\n",
        "\n",
        "            # number of available crops\n",
        "            number_of_crop = min(full_data[idx]['image_emb'].shape[0], len(full_data[idx]['df_boxes']))\n",
        "\n",
        "            if number_of_crop == 0:\n",
        "                break\n",
        "\n",
        "            text_enc = full_data[idx]['text_emb'][idx_text].unsqueeze(1)\n",
        "            box_enc = full_data[idx]['image_emb'][:number_of_crop,:].permute(1, 0)\n",
        "\n",
        "            # shape: (number of samples, 512, 1)\n",
        "            text_encoding.append(text_enc)\n",
        "\n",
        "            # shape: (number of samples, 512, number of crop embeddings)\n",
        "            # box_encoding.append(box_enc)\n",
        "\n",
        "            # shape: (number of samples, number of boxes, 4)\n",
        "            box_coords.append(torch.stack([torch.tensor(full_data[idx]['df_boxes'].iloc[i][:4]).type(torch.float16)\n",
        "                                                    for i in range(number_of_crop)]))\n",
        "\n",
        "            # shape: torch.Size(number of samples, 2, 512, number of boxes])\n",
        "            cat_encoding_text.append(torch.cat([box_enc.unsqueeze(0), torch.stack([text_enc.squeeze(1) for _ in range(number_of_crop)], dim=1).unsqueeze(0)], dim=0))\n",
        "\n",
        "            # shape: (number of samples, 1, 4)\n",
        "            target_boxes.append(torch.tensor(full_data[idx]['bbox_target']).type(torch.float16).unsqueeze(0))\n",
        "\n",
        "           # (number of samples, 1, 512, 1)          padd the tensors to stack them togheter\n",
        "    return torch.stack(text_encoding).unsqueeze(1), torch.stack([torch.nn.functional.pad(b.permute(0, 2, 1), (0, 0, 0, 48 - b.shape[2])).permute(0, 2, 1) for b in cat_encoding_text]), torch.stack([torch.nn.functional.pad(b, (0, 0, 0, 48 - b.shape[0])) for b in box_coords]), torch.stack(target_boxes)\n",
        "\n",
        "\n",
        "\n",
        "# text_encoding_test, box_encoding_test, box_coords_test, target_boxes_test = get_data_single_prompt(data_test)\n",
        "text_encoding_val, box_encoding_val, box_coords_val, target_boxes_val = get_data_single_prompt(data_val)\n",
        "del data_val\n",
        "text_encoding_train, box_encoding_train, box_coords_train, target_boxes_train = get_data_single_prompt(data_train)\n",
        "\n",
        "del data_train\n",
        "\n",
        "# text_encoding_train, box_encoding_train, box_coords_train, target_boxes_train = text_encoding_train[:20000], box_encoding_train[:20000], box_coords_train[:20000], target_boxes_train[:20000]\n",
        "\n",
        "def box_norm_rescale(box_target):\n",
        "    \"\"\" Rescale the box_target\n",
        "    Args:\n",
        "        box_target: (number of samples, 1, 4)\n",
        "\n",
        "    Returns:\n",
        "        box_target: (number of samples, 1, 4)\n",
        "\n",
        "    \"\"\"\n",
        "    # convert the box_pred to x1, y1, x2, y2\n",
        "    box_target[:, 0, 2] = box_target[:, 0, 0] + box_target[:, 0, 2]\n",
        "    box_target[:, 0, 3] = box_target[:, 0, 1] + box_target[:, 0, 3]\n",
        "\n",
        "    return box_target\n",
        "\n",
        "# box rescaling\n",
        "# target_boxes_test = box_norm_rescale(target_boxes_test)\n",
        "target_boxes_val = box_norm_rescale(target_boxes_val)\n",
        "target_boxes_train = box_norm_rescale(target_boxes_train)\n",
        "\n",
        "\n",
        "# print(text_encoding_test.shape)\n",
        "# print(box_encoding_test.shape)\n",
        "# print(box_coords_test.shape)\n",
        "# print(target_boxes_test.shape)\n",
        "\n",
        "# target_boxes_test = target_boxes_test.squeeze(1)\n",
        "target_boxes_val = target_boxes_val.squeeze(1)\n",
        "target_boxes_train = target_boxes_train.squeeze(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qm8HfTJt3OET"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torchmetrics\n",
        "import torchvision\n",
        "\n",
        "#############################################################\n",
        "# with multiple textual prompts\n",
        "#############################################################\n",
        "\n",
        "############################ 1. MLP\n",
        "\n",
        "from typing import Any, Optional\n",
        "\n",
        "\n",
        "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, out_dim=512):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(4, 64)\n",
        "        self.l2 = nn.Linear(64, 32)\n",
        "        self.l3 = nn.Linear(32, out_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.gelu(self.l1(x))\n",
        "        x = self.gelu(self.l2(x))\n",
        "        return self.l3(x)\n",
        "\n",
        "\n",
        "############################ 2. BoxEncoder\n",
        "\n",
        "class LinearBoxWeightingBlock(nn.Module):\n",
        "    def __init__(self, hidden_dim=64, n_head=4, number_transformer_layers=3, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear_1 = nn.Linear(3, hidden_dim)\n",
        "        self.layernorm_1 = nn.LayerNorm(hidden_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "        # In our architecture the transformer encoder has the layer norm first.\n",
        "        # According with the results of the paper 'On Layer Normalization in \n",
        "        # the Transformer Architecture' the convergence is faster with the layer \n",
        "        # norm first.\n",
        "        # source: https://arxiv.org/pdf/2002.04745.pdf\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "                nn.TransformerEncoderLayer(d_model=3, nhead=n_head, norm_first=True), num_layers=number_transformer_layers\n",
        "            )\n",
        "\n",
        "        # This layer is used to combine the output of the transformer encoder\n",
        "        # with the initial concatenated embedding. The result of this layer is\n",
        "        # to get a single vector for each box.\n",
        "        self.bilinear_sum = nn.Bilinear(hidden_dim, hidden_dim, 1)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, box, text_emb):\n",
        "        ##################################################   BOX ENCODER BLOCK\n",
        "        # x: (batch_size, 4, n_boxes) -> (batch_size, 1, 512, n_boxes)\n",
        "        box_emb = self.mlp(box).permute(0, 2, 1).unsqueeze(1)\n",
        "\n",
        "        # x: (batch_size, 3, 512, n_boxes)\n",
        "        x = torch.cat([x, box_emb], dim=1)              # concat EMB and box\n",
        "        # del box_emb                                     # del box_emb\n",
        "\n",
        "        ##########################################   WEIGHTED COMBINATION BLOCK\n",
        "        # x: (batch_size, n_boxes, 512, 3)\n",
        "        x_1 = x.permute(0, 3, 2, 1)                 # permute\n",
        "        x = self.linear_1(x_1)                         # linear transformation\n",
        "        x = self.gelu(x)                            # non-linearity\n",
        "        x = self.layernorm_1(x)                     # add & norm\n",
        "        x = self.transformer_encoder(x)             # transformer encoder\n",
        "\n",
        "        # here we apply dropout only during training\n",
        "        # over the embedding in output of the transformer \n",
        "        # encoder (regularyzation) ADD & DROP\n",
        "        x = self.bilinear_sum(\n",
        "            x + F.dropout(x, self.dropout, training=self.training), \n",
        "            x_1\n",
        "        )                                           # get a weight for each box\n",
        "        x = self.gelu(x)                            # non-linearity\n",
        "        x = self.layernorm_2(x)                     # norm\n",
        "\n",
        "\n",
        "        # x: torch.Size([BATCH_SIZE, N_BOXES, 512, 1]) -> torch.Size([BATCH_SIZE, N_BOXES, 1, 512])\n",
        "        x = x.permute(0, 1, 3, 2)                   # permute\n",
        "\n",
        "        return x, box_emb\n",
        "    \n",
        "class CLIPscoreBlock(nn.Module):\n",
        "        ''' This block compute the bached clip score for each box.\n",
        "            The input is the text embedding and the box embedding.\n",
        "            The output is a tensor of shape (batch_size, n_boxes, 1)\n",
        "            where each element is the score of the box.\n",
        "        \n",
        "        Args:\n",
        "            @params text_emb: tensor of shape (batch_size, 512)\n",
        "            @params box_emb: tensor of shape (batch_size, n_boxes, 512)\n",
        "\n",
        "        Returns:\n",
        "            @params returns x: tensor of shape (batch_size, n_boxes, 1)\n",
        "        \n",
        "        x:  [tex_0 @ box_1.T, tex_0 @ box_2.T, ..., tex_0 @ box_j]\n",
        "            [tex_2 @ box_1.T, tex_2 @ box_2.T, ..., tex_2 @ box_j]\n",
        "            [tex_3 @ box_1.T, tex_3 @ box_2.T, ..., tex_3 @ box_j]\n",
        "                                    ...\n",
        "                                    ...\n",
        "            [tex_i @ box_1.T, tex_i @ box_2.T, ..., tex_i @ box_j]\n",
        "\n",
        "        I am normalizing over the rows and summing over the columns\n",
        "        to get the overall score for each box.\n",
        "\n",
        "        x summarize the matching in between the sentence and the boxes\n",
        "        x: [sum(tex_[:] @ box_1.T), sum(tex_[:] @ box_2.T), ..., sum(tex_[:] @ box_j)]\n",
        "    '''\n",
        "    def __init__(self, n_box=48):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_box = n_box\n",
        "\n",
        "\n",
        "    def forward(self, box_encoding, text_emb):\n",
        "\n",
        "        # torch.Size([256, 1, 1, 512]), torch.Size([256, 48, 1, 512])\n",
        "        # torch.Size([256, 1, 512]), torch.Size([256, 48, 512]) \n",
        "        # torch.Size([256, 1, 512]) @ torch.Size([256, 512, 48]) ->                     # torch.Size([256, 10, 48])\n",
        "        x = torch.bmm(text_emb.squeeze(2), box_encoding.squeeze(1).permute(0, 2, 1))\n",
        "        # I should sum over the row to get the overall score for each box\n",
        "        # specifically, I am summing the probabilities of each box being\n",
        "        # the target box according to the matching score of the text encoding\n",
        "        x = x.sum(dim=-2)                                                               # torch.Size([64, 10, 10]) -> torch.Size([64, 10])\n",
        "        x_score = x_score.unsqueeze(2)                                                  # box: (batch_size, n_boxes) -> (batch_size, n_boxes, 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BoxEncoder(pl.LightningModule):\n",
        "    def __init__(self, out_dim_box=512, latent_dim=240, hidden_dim_regressor=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linWeighting = LinearBoxWeightingBlock()\n",
        "        self.clipScoreBlock = CLIPscoreBlock()\n",
        "\n",
        "        self.box_regressor = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim_regressor),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(hidden_dim_regressor),\n",
        "            nn.Linear(hidden_dim_regressor, hidden_dim_regressor//2),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(hidden_dim_regressor//2),\n",
        "            nn.Linear(hidden_dim_regressor//2, 4)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, box, text_emb):\n",
        "\n",
        "        x, box_emb = self.linWeighting(x, box, text_emb)\n",
        "\n",
        "\n",
        "        x = self.clipScoreBlock(x, text_emb)\n",
        "\n",
        "        # box: (batch_size, 4) -> (batch_size, 5)\n",
        "        box = torch.cat([x, box_emb], dim=-1)\n",
        "\n",
        "\n",
        "        # box: (batch_size, n_boxes, 512) -> (batch_size, 4)\n",
        "        box =  self.box_regressor(box.reshape(box.shape[0], box.shape[1]*box.shape[2]))\n",
        "\n",
        "        return box\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        cat_emb_text, box, t_emb, target = batch\n",
        "        # print(cat_emb_text.shape, box.shape, y.shape)\n",
        "        pred = self(cat_emb_text, box, t_emb)\n",
        "        # https://arxiv.org/pdf/2108.12627\n",
        "        # loss = F.huber_loss(y_hat, y)\n",
        "        # https://arxiv.org/abs/1911.08287\n",
        "        # loss = torchvision.ops.distance_box_iou_loss(y_hat, y)\n",
        "        # https://arxiv.org/abs/1902.09630\n",
        "        # loss = torchvision.ops.generalized_box_iou_loss(y_hat, y)\n",
        "\n",
        "        mse_loss = self.MSE(pred, target)\n",
        "\n",
        "        mae_loss = self.MAE(pred, target)\n",
        "\n",
        "        huber_loss = self.HUBER(pred, target)\n",
        "\n",
        "        g_box_iou_loss = self.generalized_box_iou_loss(pred, target, reduction = 'mean')\n",
        "\n",
        "        d_box_iou_loss = self.distance_box_iou_loss(pred, target, reduction = 'mean')\n",
        "\n",
        "        c_box_iou_loss = self.complete_box_iou_loss(pred, target, reduction = 'mean')\n",
        "\n",
        "        self.log('val_mse_loss', mse_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
        "        self.log('val_mae_loss', mae_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
        "        self.log('val_huber_loss', huber_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
        "        self.log('val_g_box_iou_loss', g_box_iou_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
        "        self.log('val_d_box_iou_loss', d_box_iou_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
        "        self.log('val_c_box_iou_loss', c_box_iou_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
        "\n",
        "        return huber_loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        cat_emb_text, box, t_emb, target = batch\n",
        "        # print(cat_emb_text.shape, box.shape, y.shape)\n",
        "        pred = self(cat_emb_text, box, t_emb)\n",
        "        # https://arxiv.org/pdf/2108.12627\n",
        "        # loss = F.huber_loss(y_hat, y)\n",
        "        # https://arxiv.org/abs/1911.08287\n",
        "        # loss = torchvision.ops.distance_box_iou_loss(y_hat, y)\n",
        "        # https://arxiv.org/abs/1902.09630\n",
        "        # loss = torchvision.ops.generalized_box_iou_loss(y_hat, y)\n",
        "\n",
        "        mse_loss = self.MSE(pred, target)\n",
        "\n",
        "        mae_loss = self.MAE(pred, target)\n",
        "\n",
        "        huber_loss = self.HUBER(pred, target)\n",
        "\n",
        "        g_box_iou_loss = self.generalized_box_iou_loss(pred, target, reduction = 'mean')\n",
        "\n",
        "        d_box_iou_loss = self.distance_box_iou_loss(pred, target, reduction = 'mean')\n",
        "\n",
        "        c_box_iou_loss = self.complete_box_iou_loss(pred, target, reduction = 'mean')\n",
        "\n",
        "        self.log('val_mse_loss', mse_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
        "        self.log('val_mae_loss', mae_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
        "        self.log('val_huber_loss', huber_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
        "        self.log('val_g_box_iou_loss', g_box_iou_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
        "        self.log('val_d_box_iou_loss', d_box_iou_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
        "        self.log('val_c_box_iou_loss', c_box_iou_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
        "\n",
        "        return huber_loss\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=0.05)\n",
        "        # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10)\n",
        "        return [optimizer], [scheduler]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "DvHvZvid4-sF",
        "outputId": "8eba8954-3e4b-40e7-becb-4218c08e559e"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-41ab3cf8dd0a>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# dataset_test = TensorDataset(box_encoding_test.type(torch.float32), box_coords_test.type(torch.float32), text_encoding_test.type(torch.float32).permute(0, 1, 3, 2), target_boxes_test.type(torch.float32))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdataset_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_encoding_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_coords_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_encoding_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_boxes_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_encoding_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_encoding_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_coords_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_boxes_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4"
          ]
        }
      ],
      "source": [
        "# dataloaders\n",
        "\n",
        "# architecture number 4\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "# dataset_test = TensorDataset(box_encoding_test.type(torch.float16), box_coords_test.type(torch.float16), text_encoding_test.type(torch.float16).permute(0, 1, 3, 2), target_boxes_test.type(torch.float16))\n",
        "# dataset_val = TensorDataset(box_encoding_val.type(torch.float16), box_coords_val.type(torch.float16), text_encoding_val.type(torch.float16).permute(0, 1, 3, 2), target_boxes_val.type(torch.float16))\n",
        "\n",
        "# dataset_test = TensorDataset(box_encoding_test.type(torch.float32), box_coords_test.type(torch.float32), text_encoding_test.type(torch.float32).permute(0, 1, 3, 2), target_boxes_test.type(torch.float32))\n",
        "\n",
        "dataset_val = TensorDataset(box_encoding_val.type(torch.float32), box_coords_val.type(torch.float32), text_encoding_val.type(torch.float32).permute(0, 1, 3, 2), target_boxes_val.type(torch.float32))\n",
        "dataset_train = TensorDataset(text_encoding_train.type(torch.float32), box_encoding_train.type(torch.float32), box_coords_train.type(torch.float32), target_boxes_train.type(torch.float32))\n",
        "\n",
        "test_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
        "# train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "# most basic trainer, uses good defaults\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=3, verbose=True, mode=\"min\")\n",
        "checkpoint_callback = ModelCheckpoint(dirpath='/content/drive/MyDrive/Colab Notebooks/dl')\n",
        "\n",
        "\n",
        "# train the model\n",
        "model = BoxEncoder().cuda()\n",
        "\n",
        "print(model)\n",
        "\n",
        "# print number of parameters\n",
        "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)/1000000.0} M\")\n",
        "\n",
        "trainer = pl.Trainer(accelerator='auto', max_epochs=5, callbacks=[early_stop_callback, checkpoint_callback])\n",
        "\n",
        "# train the model\n",
        "trainer.fit(model, test_loader, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k_1-5x_6L0r"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "dataset_val = TensorDataset(box_encoding_val.type(torch.float32), box_coords_val.type(torch.float32), text_encoding_val.type(torch.float32).permute(0, 1, 3, 2), target_boxes_val.type(torch.float32))\n",
        "dataset_train = TensorDataset(text_encoding_train.type(torch.float32), box_encoding_train.type(torch.float32), box_coords_train.type(torch.float32), target_boxes_train.type(torch.float32))\n",
        "\n",
        "test_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "# most basic trainer, uses good defaults\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=10, verbose=True, mode=\"min\")\n",
        "checkpoint_callback = ModelCheckpoint(dirpath='/content/drive/MyDrive/Colab Notebooks/dl/', monitor='val_loss',save_top_k=2)\n",
        "\n",
        "\n",
        "# train the model\n",
        "model = BoxEncoder().cuda()\n",
        "\n",
        "print(model)\n",
        "\n",
        "\n",
        "# print number of parameters\n",
        "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)/1000000.0} M\")\n",
        "\n",
        "trainer = pl.Trainer(accelerator='auto', max_epochs=20, callbacks=[early_stop_callback, checkpoint_callback])\n",
        "\n",
        "# train the model\n",
        "trainer.fit(model, test_loader, val_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
