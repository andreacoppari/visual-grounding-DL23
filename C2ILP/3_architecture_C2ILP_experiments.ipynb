{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning in /home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.1 in /home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages (from pytorch-lightning) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/rickbook/.local/lib/python3.10/site-packages (from pytorch-lightning) (1.24.2)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages (from pytorch-lightning) (2023.5.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages (from pytorch-lightning) (0.11.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/rickbook/.local/lib/python3.10/site-packages (from pytorch-lightning) (2.0.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages (from pytorch-lightning) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/rickbook/.local/lib/python3.10/site-packages (from pytorch-lightning) (4.5.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages (from pytorch-lightning) (4.65.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.7.0 in /home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages (from pytorch-lightning) (0.8.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.4)\n",
      "Requirement already satisfied: requests in /home/rickbook/.local/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.28.2)\n",
      "Requirement already satisfied: networkx in /home/rickbook/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (3.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/rickbook/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/rickbook/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/rickbook/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.7.101)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/rickbook/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (2.0.0)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/rickbook/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (10.2.10.91)\n",
      "Requirement already satisfied: jinja2 in /home/rickbook/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/rickbook/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (8.5.0.96)\n",
      "Requirement already satisfied: sympy in /home/rickbook/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (1.11.1)\n",
      "Requirement already satisfied: filelock in /home/rickbook/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (3.10.4)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/rickbook/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/rickbook/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/rickbook/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/rickbook/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/rickbook/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/rickbook/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.10.3.66)\n",
      "Requirement already satisfied: setuptools in /home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->pytorch-lightning) (67.6.0)\n",
      "Requirement already satisfied: wheel in /home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->pytorch-lightning) (0.40.0)\n",
      "Requirement already satisfied: cmake in /home/rickbook/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (3.26.1)\n",
      "Requirement already satisfied: lit in /home/rickbook/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (16.0.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/rickbook/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (3.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (22.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/rickbook/.local/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->pytorch-lightning) (2.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/rickbook/.local/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/rickbook/.local/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/rickbook/.local/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/rickbook/.local/lib/python3.10/site-packages (from sympy->torch>=1.11.0->pytorch-lightning) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# with multiple textual prompts\n",
    "#############################################################\n",
    "\n",
    "############################ 1. MLP\n",
    "\n",
    "from typing import Any, Optional\n",
    "\n",
    "\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, out_dim=512):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(4, 128)\n",
    "        self.l2 = nn.Linear(128, 256)\n",
    "        self.l3 = nn.Linear(256, out_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.gelu(self.l1(x))\n",
    "        x = self.gelu(self.l2(x))\n",
    "        return self.l3(x)\n",
    "\n",
    "\n",
    "############################ 2. BoxEncoder\n",
    "\n",
    "class BoxEncoder(pl.LightningModule):\n",
    "    def __init__(self, out_dim_box=512, latent_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mlp = MLP(out_dim_box)\n",
    "        \n",
    "        # self.linear_1 = nn.Linear(3, 768)\n",
    "        # self.layernorm_1 = nn.LayerNorm(768)\n",
    "        # self.linear_2 = nn.Linear(768, 256)\n",
    "        # self.layernorm_2 = nn.LayerNorm(256)\n",
    "        # self.weighted_sum = nn.Linear(256, 1)\n",
    "\n",
    "        self.linear_1 = nn.Linear(3, 3)\n",
    "        self.layernorm_1 = nn.LayerNorm(3)\n",
    "        self.linear_2 = nn.Linear(3, 3)\n",
    "        self.layernorm_2 = nn.LayerNorm(3)\n",
    "        self.weighted_sum = nn.Linear(3, 1)\n",
    "        \n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "    def forward(self, x, box, text_emb):\n",
    "        \n",
    "        ##################################################   BOX ENCODER BLOCK\n",
    "        # x: (batch_size, 512, n_boxes)\n",
    "        box_emb = self.mlp(box).permute(0, 2, 1).unsqueeze(1)\n",
    "        \n",
    "        # x: (batch_size, 3, 512, n_boxes)\n",
    "        x = torch.cat([x, box_emb], dim=1)              # concat EMB and box\n",
    "        del box_emb                                     # del box_emb\n",
    "        \n",
    "        ##########################################   WEIGHTED COMBINATION BLOCK\n",
    "        # x: (batch_size, n_boxes, 512, 3)\n",
    "        x_1 = self.linear_1(x.permute(0, 3, 2, 1))  # linear transformation\n",
    "        x = self.gelu(x_1)                          # non-linearity\n",
    "        x = self.layernorm_1(x_1 + x)               # add & norm\n",
    "        del x_1                                     # del x_1\n",
    "        x_2 = self.linear_2(x)                      # linear transformation\n",
    "        x = self.gelu(x_2)                          # non-linearity\n",
    "        x_2 = self.layernorm_2(x_2 + x)             # add & norm\n",
    "        del x_2                                     # del x_2\n",
    "        x = self.weighted_sum(x)                    # get a weight for each box\n",
    "\n",
    "\n",
    "        ##########################################   CONTRASTIVE SCORE BLOCK\n",
    "        # x: torch.Size([BATCH_SIZE, N_BOXES, 512, 1]) -> torch.Size([BATCH_SIZE, N_BOXES, 1, 512])\n",
    "        box_encoding = x.permute(0, 1, 3, 2)\n",
    "        del x\n",
    "\n",
    "        # normalize\n",
    "        box_encoding.norm(dim=-1, keepdim=True)\n",
    "        text_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "        # Get the batched text encoding\n",
    "        x = torch.stack([\n",
    "                torch.stack([\n",
    "                    torch.stack([\n",
    "                    # (torch.Size([1, 512]) @ torch.Size([512, 1])) -> torch.Size([1, 1])\n",
    "                    # [tex_i @ box_1.T, tex_i @ box_2.T,              ..., tex_i @ box_j]\n",
    "                    text_emb[i][j_tex] @ box_encoding[i][j_box].T \n",
    "                            for j_box in range(box_encoding.shape[1])])                             # get the matching score for each box\n",
    "                        for j_tex in range(text_emb.shape[1])])                                     # stack the matching scores in a matrix\n",
    "                    for i in range(box_encoding.shape[0])]).squeeze(4).squeeze(3).softmax(dim=-1)   # torch.Size([BATCH_SIZE, N_BOXES, N_IMAGES, 1, 1]) -> torch.Size([BATCH_SIZE, N_BOXES, N_IMAGES])\n",
    "            \n",
    "                                                                                                    # with the softmax we are making sure that the scores for\n",
    "                                                                                                    # each piece of text sum to 1\n",
    "        # I should sum over the row to get the overall score for each box\n",
    "        # specifically, I am summing the probabilities of each box being \n",
    "        # the target box according to the matching score of the text encoding\n",
    "\n",
    "\n",
    "        x = x.sum(dim=-2)                                                                           # torch.Size([64, 10, 10]) -> torch.Size([64, 10])\n",
    "\n",
    "        \n",
    "        ''' output matrix\n",
    "            x:  [tex_0 @ box_1.T, tex_0 @ box_2.T, ..., tex_0 @ box_j] \n",
    "                [tex_2 @ box_1.T, tex_2 @ box_2.T, ..., tex_2 @ box_j]\n",
    "                [tex_3 @ box_1.T, tex_3 @ box_2.T, ..., tex_3 @ box_j]\n",
    "                                        ...\n",
    "                                        ...\n",
    "                [tex_i @ box_1.T, tex_i @ box_2.T, ..., tex_i @ box_j]\n",
    "\n",
    "            I am normalizing over the rows and summing over the columns\n",
    "            to get the overall score for each box.\n",
    "\n",
    "            x summarize the matching in between the sentence and the boxes\n",
    "            x: [sum(tex_[:] @ box_1.T), sum(tex_[:] @ box_2.T), ..., sum(tex_[:] @ box_j)]\n",
    "        '''\n",
    "\n",
    "        # get the index of the box with the highest score\n",
    "        x = torch.argmax(x, dim=-1)                                                                # torch.Size([BATCH_SIZE, N_BOXES]) -> torch.Size([BATCH_SIZE])\n",
    "\n",
    "        #x = torch.tensor(x, requires_grad=True)\n",
    "\n",
    "        # get the box with the highest score\n",
    "        # box = box[torch.arange(box_encoding.shape[0]), x]\n",
    "        # box = torch.tensor(box[torch.arange(box_encoding.shape[0]), x], requires_grad=True)\n",
    "\n",
    "        box = torch.autograd.Variable(box[torch.arange(box_encoding.shape[0]), x], requires_grad=True)\n",
    "    \n",
    "        return box\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        cat_emb_text, box, t_emb, y = batch\n",
    "        # print(cat_emb_text.shape, box.shape, y.shape)\n",
    "        y_hat = self(cat_emb_text, box, t_emb)\n",
    "        # https://arxiv.org/pdf/2108.12627\n",
    "        # loss = F.huber_loss(y_hat, y)\n",
    "        # https://arxiv.org/abs/1911.08287\n",
    "        # loss = torchvision.ops.distance_box_iou_loss(y_hat, y)\n",
    "        # https://arxiv.org/abs/1902.09630\n",
    "        # loss = torchvision.ops.generalized_box_iou_loss(y_hat, y)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "\n",
    "        # loss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        cat_emb_text, box, t_emb, y = batch\n",
    "        # print(cat_emb_text.shape, box.shape, y.shape)\n",
    "        y_hat = self(cat_emb_text, box, t_emb)\n",
    "        # https://arxiv.org/pdf/2108.12627\n",
    "        # loss = F.huber_loss(y_hat, y)\n",
    "        # https://arxiv.org/abs/1911.08287\n",
    "        # loss = torchvision.ops.distance_box_iou_loss(y_hat, y)\n",
    "        # https://arxiv.org/abs/1902.09630\n",
    "        # loss = torchvision.ops.generalized_box_iou_loss(y_hat, y)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "\n",
    "        # loss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### junk code\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "# text_encoding = torch.randn(400, 512, 1)\n",
    "# box_encoding = torch.randn(400, 512, 10)\n",
    "\n",
    "\n",
    "\n",
    "# cat_encoding_text = torch.cat([\n",
    "#                         box_encoding.unsqueeze(1), \n",
    "#                         torch.stack([\n",
    "#                             text_encoding.squeeze(2) for _ in range(10)], dim=1).permute(0, 2, 1).unsqueeze(1)\n",
    "#                             ], dim=1)\n",
    "\n",
    "# text_encoding = torch.randn(400, 1, 512, 1)\n",
    "\n",
    "# # text_encoding = torch.stack([text_encoding.squeeze(2) for _ in range(10)], dim=1).permute(0, 2, 1).unsqueeze(1).permute(0, 3, 2, 1)\n",
    "\n",
    "# box_coords = torch.randn(400, 10, 4)\n",
    "# box_coords_target = box_coords[:, 0, :]\n",
    "\n",
    "# print(cat_encoding_text.shape, box_coords.shape, box_coords_target.shape, text_encoding.shape)\n",
    "\n",
    "# # dataloader\n",
    "\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# # get dataset\n",
    "# dataset = TensorDataset(cat_encoding_text, box_coords, text_encoding.permute(0, 1, 3, 2), box_coords_target) # .permute(0, 1, 3, 2)\n",
    "\n",
    "# # get dataloader\n",
    "# dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# # split with stratified kfold\n",
    "# # from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# # skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# # kfold = []\n",
    "\n",
    "# # for fold, (train_idx, val_idx) in enumerate(skf.split(cat_encoding_text, torch.zeros(4000), torch.zeros(4000))):\n",
    "    \n",
    "# #     print(f\"Fold: {fold}\")\n",
    "    \n",
    "# #     # get dataloader\n",
    "# #     train_loader = DataLoader(dataset[train_idx], batch_size=64, shuffle=True)\n",
    "# #     val_loader = DataLoader(dataset[val_idx], batch_size=64, shuffle=False)\n",
    "    \n",
    "# #     # print(train_idx, val_idx)\n",
    "    \n",
    "# #     # append the fold\n",
    "# #     kfold.append((train_loader, val_loader))\n",
    "\n",
    "\n",
    "\n",
    "# # train the model\n",
    "# model = BoxEncoder()\n",
    "\n",
    "# print(model)\n",
    "\n",
    "# # print number of parameters\n",
    "# print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# trainer = pl.Trainer(max_epochs=10, accelerator='auto')\n",
    "\n",
    "# trainer.fit(model, dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# load test dataset\n",
    "file_name = './data/yolo_v8x/yolo_v8x_1_dictionary_full_test.p'\n",
    "with open(file_name, 'rb') as f:\n",
    "    data_test = pickle.load(f)\n",
    "\n",
    "# load val dataset\n",
    "file_name = './data/yolo_v8x/yolo_v8x_1_dictionary_full_val.p'\n",
    "with open(file_name, 'rb') as f:\n",
    "    data_val = pickle.load(f)\n",
    "\n",
    "# load train dataset\n",
    "# file_name = './data/yolo_v8x/yolo_v8x_1_dictionary_full_train.p'\n",
    "# with open(file_name, 'rb') as f:\n",
    "#     data_train = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1977629505.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    cat_encoding_text =\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### TODO: make the experiment using single prompt \n",
    "### TODO: make the experiment using multiple prompts whenever available to make the model more robust\n",
    "### TODO: do the experiment rephrasing the content in the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5023/5023 [00:10<00:00, 457.92it/s]\n",
      "100%|██████████| 2573/2573 [00:05<00:00, 469.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9598, 1, 512, 1])\n",
      "torch.Size([9598, 2, 512, 48])\n",
      "torch.Size([9598, 48, 4])\n",
      "torch.Size([9598, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_data_single_prompt(full_data):\n",
    "\n",
    "    text_encoding, cat_encoding_text, box_coords, target_boxes = [], [], [], []\n",
    "\n",
    "    for idx in tqdm(list(full_data)):\n",
    "        # for _ in range(data['image_emb'].shape[0]):\n",
    "        for idx_text in range(full_data[idx]['text_emb'].shape[0]):\n",
    "            \n",
    "            # number of available crops\n",
    "            number_of_crop = min(full_data[idx]['image_emb'].shape[0], len(full_data[idx]['df_boxes']))\n",
    "\n",
    "            if number_of_crop == 0:\n",
    "                break\n",
    "            \n",
    "            text_enc = full_data[idx]['text_emb'][idx_text].unsqueeze(1)\n",
    "            box_enc = full_data[idx]['image_emb'][:number_of_crop,:].permute(1, 0)\n",
    "\n",
    "            # shape: (number of samples, 512, 1)\n",
    "            text_encoding.append(text_enc)\n",
    "\n",
    "            # shape: (number of samples, 512, number of crop embeddings)\n",
    "            # box_encoding.append(box_enc)\n",
    "\n",
    "            # shape: (number of samples, number of boxes, 4)\n",
    "            box_coords.append(torch.stack([torch.tensor(full_data[idx]['df_boxes'].iloc[i][:4]).type(torch.float16) \n",
    "                                                    for i in range(number_of_crop)]))\n",
    "                        \n",
    "            # shape: torch.Size(number of samples, 2, 512, number of boxes])\n",
    "            cat_encoding_text.append(torch.cat([box_enc.unsqueeze(0), torch.stack([text_enc.squeeze(1) for _ in range(number_of_crop)], dim=1).unsqueeze(0)], dim=0))\n",
    "            \n",
    "            # shape: (number of samples, 1, 4)\n",
    "            target_boxes.append(torch.tensor(full_data[idx]['bbox_target']).type(torch.float16).unsqueeze(0))\n",
    "\n",
    "           # (number of samples, 1, 512, 1)          padd the tensors to stack them togheter\n",
    "    return torch.stack(text_encoding).unsqueeze(1), torch.stack([torch.nn.functional.pad(b.permute(0, 2, 1), (0, 0, 0, 48 - b.shape[2])).permute(0, 2, 1) for b in cat_encoding_text]), torch.stack([torch.nn.functional.pad(b, (0, 0, 0, 48 - b.shape[0])) for b in box_coords]), torch.stack(target_boxes)\n",
    "\n",
    "\n",
    "\n",
    "text_encoding_test, box_encoding_test, box_coords_test, target_boxes_test = get_data_single_prompt(data_test)\n",
    "text_encoding_val, box_encoding_val, box_coords_val, target_boxes_val = get_data_single_prompt(data_val)\n",
    "# text_encoding_train, box_encoding_train, box_coords_train, target_boxes_train = get_data_single_prompt(data_train)\n",
    "\n",
    "def box_norm_rescale(box_target):\n",
    "    \"\"\" Rescale the box_target \n",
    "    Args:\n",
    "        box_target: (number of samples, 1, 4)\n",
    "\n",
    "    Returns:\n",
    "        box_target: (number of samples, 1, 4)\n",
    "\n",
    "    \"\"\"\n",
    "    # convert the box_pred to x1, y1, x2, y2\n",
    "    box_target[:, 0, 2] = box_target[:, 0, 0] + box_target[:, 0, 2]\n",
    "    box_target[:, 0, 3] = box_target[:, 0, 1] + box_target[:, 0, 3]\n",
    "\n",
    "    return box_target\n",
    "\n",
    "# box rescaling\n",
    "target_boxes_test = box_norm_rescale(target_boxes_test)\n",
    "target_boxes_val = box_norm_rescale(target_boxes_val)\n",
    "# target_boxes_train = box_norm_rescale(target_boxes_train)\n",
    "\n",
    "\n",
    "\n",
    "print(text_encoding_test.shape)\n",
    "print(box_encoding_test.shape)\n",
    "print(box_coords_test.shape)\n",
    "print(target_boxes_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_boxes_test = target_boxes_test.squeeze(1)\n",
    "target_boxes_val = target_boxes_val.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type      | Params\n",
      "-------------------------------------------\n",
      "0 | mlp          | MLP       | 165 K \n",
      "1 | linear_1     | Linear    | 12    \n",
      "2 | layernorm_1  | LayerNorm | 6     \n",
      "3 | linear_2     | Linear    | 12    \n",
      "4 | layernorm_2  | LayerNorm | 6     \n",
      "5 | weighted_sum | Linear    | 4     \n",
      "6 | gelu         | GELU      | 0     \n",
      "-------------------------------------------\n",
      "165 K     Trainable params\n",
      "0         Non-trainable params\n",
      "165 K     Total params\n",
      "0.661     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoxEncoder(\n",
      "  (mlp): MLP(\n",
      "    (l1): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (l2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (l3): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (gelu): GELU(approximate='none')\n",
      "  )\n",
      "  (linear_1): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (layernorm_1): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear_2): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (layernorm_2): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
      "  (weighted_sum): Linear(in_features=3, out_features=1, bias=True)\n",
      "  (gelu): GELU(approximate='none')\n",
      ")\n",
      "Number of parameters: 0.165288 M\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 19/19 [00:58<00:00,  3.09s/it, v_num=77]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 70670.633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  26%|██▋       | 5/19 [00:10<00:30,  2.20s/it, v_num=77] "
     ]
    }
   ],
   "source": [
    "# dataloaders\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "dataset_test = TensorDataset(box_encoding_test.type(torch.float32), box_coords_test.type(torch.float32), text_encoding_test.type(torch.float32).permute(0, 1, 3, 2), target_boxes_test.type(torch.float32))\n",
    "dataset_val = TensorDataset(box_encoding_val.type(torch.float32), box_coords_val.type(torch.float32), text_encoding_val.type(torch.float32).permute(0, 1, 3, 2), target_boxes_val.type(torch.float32))\n",
    "# dataset_train = TensorDataset(text_encoding_train.type(torch.float32), box_encoding_train.type(torch.float32), box_coords_train.type(torch.float32), target_boxes_train.type(torch.float32))\n",
    "\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
    "# train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# most basic trainer, uses good defaults\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=3, verbose=True, mode=\"min\")\n",
    "\n",
    "\n",
    "# train the model\n",
    "model = BoxEncoder().cuda()\n",
    "\n",
    "print(model)\n",
    "\n",
    "# print number of parameters\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)/1000000.0} M\")\n",
    "\n",
    "trainer = pl.Trainer(accelerator='auto', max_epochs=5, callbacks=[early_stop_callback])\n",
    "\n",
    "# train the model\n",
    "trainer.fit(model, test_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BATCH_SIZE = 64\n",
    "\n",
    "# box_encoding = torch.randn(BATCH_SIZE, 10, 512, 1).permute(0, 1, 3, 2)\n",
    "# text_encoding = torch.randn(BATCH_SIZE, 10, 1, 512)\n",
    "\n",
    "# box_encoding.norm(dim=-1, keepdim=True)\n",
    "# text_encoding.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "# # Get the batched text encoding\n",
    "# x = torch.stack([\n",
    "#         torch.stack([\n",
    "#             torch.stack([\n",
    "#             # (torch.Size([1, 512]) @ torch.Size([512, 1])) -> torch.Size([1, 1])\n",
    "#             # [tex_i @ box_1.T, tex_i @ box_2.T, ..., tex_i @ box_j]\n",
    "#             text_encoding[i][j_tex] @ box_encoding[i][j_box].T \n",
    "#                     for j_box in range(box_encoding.shape[1])])                            # get the matching score for each box\n",
    "#                 for j_tex in range(text_encoding.shape[1])])                                # stack the matching scores in a matrix\n",
    "#             for i in range(BATCH_SIZE)]).squeeze(4).squeeze(3).softmax(dim=-1)  # torch.Size([BATCH_SIZE, N_BOXES, N_IMAGES, 1, 1]) -> torch.Size([BATCH_SIZE, N_BOXES, N_IMAGES])\n",
    "    \n",
    "#                                                                         # with the softmax we are making sure that the scores for\n",
    "#                                                                         # each piece of text sum to 1\n",
    "\n",
    "# # I should sum over the row to get the overall score for each box\n",
    "# # specifically, I am summing the probabilities of each box being \n",
    "# # the target box according to the matching score of the text encoding\n",
    "\n",
    "# x = x.sum(dim=-2) # torch.Size([64, 10, 10]) -> torch.Size([64, 10])\n",
    "\n",
    "\n",
    "# ''' output matrix\n",
    "#     x:  [tex_0 @ box_1.T, tex_0 @ box_2.T, ..., tex_0 @ box_j] \n",
    "#         [tex_2 @ box_1.T, tex_2 @ box_2.T, ..., tex_2 @ box_j]\n",
    "#         [tex_3 @ box_1.T, tex_3 @ box_2.T, ..., tex_3 @ box_j]\n",
    "#                                 ...\n",
    "#                                 ...\n",
    "#         [tex_i @ box_1.T, tex_i @ box_2.T, ..., tex_i @ box_j]\n",
    "\n",
    "#     I am normalizing over the rows and summing over the columns\n",
    "#     to get the overall score for each box.\n",
    "\n",
    "#     x summarize the matching in between the sentence and the boxes\n",
    "#     x: [sum(tex_[:] @ box_1.T), sum(tex_[:] @ box_2.T), ..., sum(tex_[:] @ box_j)]\n",
    "# '''\n",
    "\n",
    "# # get the index of the box with the highest score\n",
    "# x = torch.argmax(x, dim=-1) # torch.Size([BATCH_SIZE, N_BOXES]) -> torch.Size([BATCH_SIZE])\n",
    "\n",
    "# box = torch.randn(BATCH_SIZE, 10, 4)\n",
    "\n",
    "# # get the box with the highest score\n",
    "# box[torch.arange(BATCH_SIZE), x].shape # torch.Size([BATCH_SIZE, N_BOXES, 4]) -> torch.Size([BATCH_SIZE, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "batch1 must be a 3D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m box_encoding\u001b[39m.\u001b[39mnorm(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m text_encoding\u001b[39m.\u001b[39mnorm(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m torch\u001b[39m.\u001b[39;49mbmm(box_encoding, text_encoding)\u001b[39m.\u001b[39msqueeze(\u001b[39m3\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: batch1 must be a 3D tensor"
     ]
    }
   ],
   "source": [
    "# batched mat mul\n",
    "torch.bmm(box_encoding, text_encoding).squeeze(3).squeeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "torch.Size([1, 512])\n",
      "torch.Size([100, 512])\n",
      "torch.Size([1, 100])\n",
      "\n",
      "Top predictions:\n",
      "\n",
      "           snake: 65.48%\n",
      "          turtle: 12.30%\n",
      "    sweet_pepper: 3.87%\n",
      "          lizard: 1.86%\n",
      "       crocodile: 1.69%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "# Download the dataset\n",
    "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
    "\n",
    "# Prepare the inputs\n",
    "image, class_id = cifar100[3637]\n",
    "image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
    "\n",
    "# Calculate features\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "    print(image_features.shape) # torch.Size([1, 512])\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "    print(text_features.shape) # torch.Size([100, 512])\n",
    "\n",
    "# Pick the top 5 most similar labels for the image\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "print(( image_features @ text_features.T).shape)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "values, indices = similarity[0].topk(5)\n",
    "\n",
    "# Print the result\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: torchvision in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from sentence-transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from sentence-transformers) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from sentence-transformers) (1.2.1)\n",
      "Requirement already satisfied: scipy in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from sentence-transformers) (1.9.3)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from sentence-transformers) (0.13.0)\n",
      "Requirement already satisfied: requests in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
      "Requirement already satisfied: filelock in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-macosx_12_0_arm64.whl (3.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2023.5.5-cp310-cp310-macosx_11_0_arm64.whl (288 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.9/288.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2022.11.0)\n",
      "Requirement already satisfied: joblib in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.1.1)\n",
      "Requirement already satisfied: click in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=9de7c737ba6fa652abb9f3b0ec537a88f0edfcb6adaa794805b31454f062abd3\n",
      "  Stored in directory: /Users/riccardotedoldi/Library/Caches/pip/wheels/0a/f5/dd/9d00836c4e9e279c2a59d5b0ab72dafa66cbc626a327c550dd\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: tokenizers, sentencepiece, regex, nltk, huggingface-hub, transformers, sentence-transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.13.0\n",
      "    Uninstalling huggingface-hub-0.13.0:\n",
      "      Successfully uninstalled huggingface-hub-0.13.0\n",
      "Successfully installed huggingface-hub-0.14.1 nltk-3.8.1 regex-2023.5.5 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.29.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with resnet\n",
    "\n",
    "The aim of this task is to determine the categories of objects present in the image. Without being so specific..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /Users/riccardotedoldi/.cache/torch/hub/v0.10.0.zip\n",
      "/Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /Users/riccardotedoldi/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff607052653e4b4f8ff4c7ede996abb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-9.4537e-02, -1.4596e+00, -1.1330e+00, -1.6520e+00, -2.8367e+00,\n",
       "          2.5034e-01, -1.0562e+00,  3.0290e+00,  4.8499e+00,  1.4322e-01,\n",
       "         -3.0074e+00, -1.2253e+00, -1.7812e+00, -2.6095e+00, -2.7430e+00,\n",
       "         -1.6470e+00, -6.3207e-01,  1.2096e+00, -1.2136e-01, -1.0899e+00,\n",
       "         -2.0130e+00, -6.3550e-01, -5.5107e-01,  1.1292e+00, -1.7935e+00,\n",
       "         -4.5523e-01, -6.1320e-01, -6.3687e-01, -1.0281e+00,  1.1140e+00,\n",
       "         -1.1821e+00, -9.3301e-01, -6.6413e-01, -2.8717e+00, -2.2735e+00,\n",
       "         -1.1740e+00, -1.2014e+00, -1.5959e+00, -2.8250e+00, -5.8934e-01,\n",
       "         -3.9307e-01, -2.6001e+00, -1.5767e+00, -2.5568e+00,  5.8025e-01,\n",
       "         -2.1373e+00, -5.2948e-01, -1.5152e+00, -2.4959e+00, -1.1388e+00,\n",
       "          1.6644e-01, -4.5050e-01,  1.6418e-01,  2.7721e-01, -1.1867e+00,\n",
       "         -9.4941e-01, -1.2710e+00, -9.4497e-01, -5.2795e-01, -5.3808e-01,\n",
       "          1.5079e+00, -1.5966e+00, -2.4035e+00, -1.5511e+00, -3.0469e+00,\n",
       "         -2.0768e+00, -9.0381e-01, -9.8416e-01, -1.1183e+00, -4.4446e+00,\n",
       "         -9.1346e-01,  2.0901e-01, -9.2836e-01, -1.3642e+00,  1.9494e-01,\n",
       "         -1.9500e+00, -1.6371e+00, -2.8247e-01,  1.1704e+00, -6.6097e-01,\n",
       "         -1.3871e+00, -1.8538e-01, -1.5085e-01, -1.1430e+00,  1.1163e+00,\n",
       "         -1.6534e+00, -1.1215e+00, -1.8885e+00, -2.3102e+00,  1.8395e+00,\n",
       "         -1.4072e+00, -2.8119e+00, -3.3014e+00, -5.4583e-01,  5.3575e-01,\n",
       "         -3.3550e+00, -1.9394e+00, -1.4369e+00, -2.0799e+00,  1.0298e+00,\n",
       "          7.6606e-01, -1.7189e+00, -4.3532e-02, -2.6271e+00,  7.4904e+00,\n",
       "          8.7529e-01,  1.5082e+00, -3.0841e+00, -1.3444e+00, -3.1554e+00,\n",
       "         -1.4641e+00, -4.1890e+00,  2.8284e-01, -4.0142e-01,  5.8519e-01,\n",
       "         -9.6308e-02, -1.5788e+00, -2.4649e+00,  7.9057e-01,  6.7724e-01,\n",
       "         -1.0712e+00, -6.7547e-01,  4.1097e-01, -1.3468e-01,  3.1050e-01,\n",
       "         -3.1523e-01, -9.3057e-01,  2.4811e+00, -4.2663e-01,  1.2861e+00,\n",
       "         -3.9774e-01, -1.6067e+00,  7.3882e-02, -2.9237e+00, -5.5623e-01,\n",
       "         -2.0783e-01, -7.8810e-01, -2.7009e+00, -2.7574e+00, -3.4834e+00,\n",
       "         -2.5592e+00, -1.6587e+00, -2.7456e+00, -1.3155e+00,  4.6671e-01,\n",
       "         -1.1093e+00, -6.6570e-01, -2.4061e+00, -2.7735e+00, -1.9285e+00,\n",
       "         -2.2034e+00,  6.0853e+00,  5.6581e+00,  2.8655e+00,  6.4100e+00,\n",
       "          1.2974e+00,  1.3589e+00,  5.7452e+00,  2.7705e+00,  1.2982e+00,\n",
       "          5.8214e-01,  5.3879e-02, -2.9359e-01, -4.7368e-01, -4.2329e-01,\n",
       "         -1.7802e+00, -3.9527e-01, -1.4163e+00, -3.1357e-01,  4.3152e+00,\n",
       "          2.3927e+00, -2.8116e-01, -7.6243e-01,  2.4532e+00,  4.4332e+00,\n",
       "          2.7162e-01,  1.5354e+00,  8.6683e-01, -1.3464e+00,  1.6023e+00,\n",
       "          1.1207e+00, -5.5432e-01,  1.2071e+00, -5.2843e-01,  1.0541e+00,\n",
       "          4.0926e+00,  4.5496e+00,  5.9371e-01,  2.2555e+00,  1.2266e-01,\n",
       "          1.4803e+00, -1.6930e+00,  4.2137e+00,  3.8770e+00,  8.9980e-01,\n",
       "          1.6686e+00, -4.2923e-01, -8.7888e-02, -9.0798e-01,  5.3565e+00,\n",
       "          2.6556e+00,  1.4274e+00,  2.3591e-01,  6.5906e+00,  2.1124e+00,\n",
       "          1.3018e+00, -8.8185e-01,  4.6874e+00,  2.4550e+00, -1.7623e-01,\n",
       "         -2.0454e+00,  1.9276e-01,  2.9354e+00,  1.1430e+00,  8.7655e-01,\n",
       "          2.1812e+00,  3.7016e+00,  2.3891e+00,  2.0640e+00,  5.9695e-01,\n",
       "          1.7928e+00, -1.5818e+00,  7.0992e+00,  5.5427e+00,  7.2015e+00,\n",
       "          2.1539e+00,  2.3155e+00,  4.3141e+00,  3.6316e+00,  2.9590e+00,\n",
       "          5.3014e+00,  7.2519e+00,  6.8848e+00,  6.9810e-01,  6.9717e-01,\n",
       "          6.2982e+00,  1.9783e+00,  8.9455e-01,  4.3719e-01,  1.6732e+00,\n",
       "          2.6122e+00,  1.3918e+00,  9.9787e-01, -3.1328e-01,  3.3182e+00,\n",
       "          9.5835e-01,  6.0793e-01,  3.4678e+00,  8.2230e+00,  6.4206e+00,\n",
       "          6.4243e+00,  1.7845e+00,  2.4306e-01,  1.7204e+00,  1.1965e+00,\n",
       "          2.8093e+00,  4.3926e+00,  8.2000e+00,  1.2776e+01,  9.4135e+00,\n",
       "          6.8797e+00,  8.4079e+00,  1.8012e+00,  4.6655e+00,  4.5297e+00,\n",
       "          2.6417e+00,  1.2802e+00,  2.7984e+00, -5.8052e-01,  4.8921e+00,\n",
       "          8.9825e+00,  1.5337e+00,  2.3591e+00,  4.2308e+00,  5.0354e+00,\n",
       "         -8.8467e-01,  4.4582e-01,  3.0863e+00,  2.4934e+00,  7.1393e+00,\n",
       "          2.6510e+00,  2.9643e+00,  1.3999e+00,  6.5207e+00,  3.1288e+00,\n",
       "          3.5729e+00,  2.3105e-01,  3.2757e+00,  4.3981e-01,  1.1972e+00,\n",
       "         -1.5733e-01,  2.3810e+00,  1.7232e+00,  1.4009e+00, -7.5995e-02,\n",
       "         -1.6510e-01,  1.0521e+00, -1.0927e+00, -1.2375e+00, -1.2289e-01,\n",
       "         -2.1614e+00, -2.8592e+00, -1.3245e+00, -1.5155e+00, -1.6582e+00,\n",
       "         -2.1023e+00,  3.8612e-01, -9.3121e-01, -3.2139e+00, -7.1650e-01,\n",
       "         -3.7237e-01, -2.6560e+00, -1.4305e+00, -6.7720e-01,  6.4707e-01,\n",
       "         -2.5702e+00, -2.6687e+00, -2.4140e+00, -2.9279e+00, -3.4107e+00,\n",
       "         -1.8254e+00, -1.7090e+00, -3.2740e+00, -1.9872e+00, -8.2992e-01,\n",
       "         -4.7261e-01, -1.6200e+00, -1.1185e-01, -6.9124e-01, -1.8316e+00,\n",
       "          2.5827e+00,  5.3910e+00,  6.3939e+00,  2.8635e+00,  6.2335e-02,\n",
       "          9.9880e-01,  1.5260e-01, -7.5345e-01,  9.0768e-01,  1.6450e+00,\n",
       "         -7.2268e-02,  4.1942e-01, -6.7981e-01, -1.1968e+00, -1.8703e+00,\n",
       "          5.5377e-01, -5.0974e-01, -4.9192e-01,  2.0944e+00, -9.9585e-01,\n",
       "         -6.5534e-01, -1.7752e+00, -2.8813e-01,  1.2089e+00, -9.3351e-02,\n",
       "          4.6972e+00,  1.1818e+00,  9.1299e-01,  2.6384e+00,  1.9745e+00,\n",
       "         -1.5490e+00,  2.4318e+00, -5.3895e-01,  8.9282e-01, -1.2548e+00,\n",
       "         -1.2543e+00, -1.5293e+00, -1.5750e+00,  1.1924e+00, -7.0071e-01,\n",
       "         -9.0332e-01,  2.5048e-01, -1.2197e+00,  2.8932e+00, -4.3679e-01,\n",
       "         -1.6542e+00, -3.1202e+00,  6.5742e-01, -7.7794e-01, -5.4016e-01,\n",
       "          1.2312e-01, -1.0702e-02,  4.6764e-01,  4.0133e-01,  1.3335e+00,\n",
       "         -2.3655e+00, -3.9917e+00, -4.9446e-02,  4.8721e-01, -5.5492e-01,\n",
       "         -1.4285e+00, -5.2599e-02, -2.9994e+00, -3.6115e+00, -1.3414e+00,\n",
       "         -2.1964e+00, -2.1170e+00, -2.6896e+00, -3.6692e-01, -1.0170e+00,\n",
       "         -7.5116e-01,  1.1248e-01, -1.8013e+00, -2.7270e+00, -3.8507e+00,\n",
       "          2.5915e-01, -6.5887e-01, -1.7527e+00,  1.6724e+00,  1.3808e+00,\n",
       "         -1.7875e+00, -5.2237e-01,  1.4458e+00, -4.0817e-01, -3.4951e-01,\n",
       "         -1.5230e+00, -2.3686e+00,  3.0946e-01, -3.3409e-01,  1.6092e-01,\n",
       "          1.2283e-01,  2.8469e-02, -1.7153e-01, -3.1001e+00, -3.9549e+00,\n",
       "          3.1136e-01,  5.2228e-01,  3.7792e-01,  1.9123e+00,  2.2481e+00,\n",
       "         -8.3737e-01, -1.5906e+00, -1.7494e+00, -1.8328e+00, -1.3967e+00,\n",
       "          6.4678e-01,  7.6698e-01, -4.0637e-01, -1.8445e+00,  2.0532e+00,\n",
       "         -1.1728e+00, -2.1554e+00,  3.8815e-01, -2.8968e+00, -8.5392e-02,\n",
       "          8.2068e-01, -1.2280e+00, -7.7474e-01, -9.8572e-01, -1.3277e-01,\n",
       "          3.2556e-01, -1.6146e+00,  1.8050e+00, -1.8929e-01, -5.2596e-01,\n",
       "         -5.6798e-02,  5.7347e-01,  2.1511e+00,  1.0767e+00, -2.9967e-01,\n",
       "          4.0441e-01, -2.8609e-01,  1.3579e+00,  2.4750e+00, -1.0867e+00,\n",
       "         -1.3409e-01, -2.5699e+00, -2.1994e+00,  5.5765e-01,  1.8525e+00,\n",
       "          1.2453e+00,  8.1211e-01,  1.7169e+00, -2.6563e+00, -1.2311e+00,\n",
       "         -5.0834e-01,  4.1962e-02, -7.0224e-01,  2.1241e+00,  3.1355e+00,\n",
       "          4.9690e-01, -3.9939e-01, -1.1610e+00,  2.6060e-01, -1.5735e+00,\n",
       "          2.9393e-01, -1.3264e+00,  6.7074e-01,  2.0427e+00,  8.9492e-01,\n",
       "         -1.3691e+00, -9.0066e-01, -1.2307e+00, -8.5907e-01, -8.2648e-01,\n",
       "         -7.2415e-01, -4.3816e-01, -2.3585e-01, -2.2333e+00, -9.6212e-01,\n",
       "         -3.4820e+00,  1.2806e+00,  6.1825e-01, -1.5795e+00, -1.0051e+00,\n",
       "         -9.7119e-01, -1.6972e-02,  4.4361e-01,  9.7902e-01, -2.1455e+00,\n",
       "         -1.0818e+00,  1.8672e+00,  1.0464e-01, -1.1177e+00, -1.0788e+00,\n",
       "          5.7788e-01, -1.1466e+00, -5.1757e-01,  3.0133e-01,  5.9978e-01,\n",
       "         -1.7518e+00,  1.0528e-01,  3.3727e+00,  6.6370e-01, -7.3170e-01,\n",
       "         -1.1157e+00, -1.0760e+00, -1.7644e-01,  1.6160e+00, -5.1676e-01,\n",
       "         -3.7354e-02, -6.5958e-01, -5.1047e-01, -1.5785e+00,  2.0827e-01,\n",
       "          3.6543e-01, -1.5673e+00,  2.5849e+00, -1.3525e+00,  2.3743e+00,\n",
       "         -2.0301e+00, -3.6402e-01, -7.4327e-01, -2.0588e-01,  2.2687e+00,\n",
       "         -1.0287e+00, -1.0464e+00, -5.5536e+00, -1.0947e+00, -2.2432e+00,\n",
       "         -2.2467e+00, -9.7575e-01,  1.7677e+00,  4.8652e-01,  1.4647e-01,\n",
       "         -1.0163e+00, -7.1231e-01,  8.3915e-01, -7.2118e-01,  1.6299e-01,\n",
       "          2.1676e+00,  3.3834e-01, -3.7376e-01, -2.7165e-01, -6.6897e-01,\n",
       "         -1.8339e+00,  1.2424e-01, -1.9148e-01,  2.1828e+00, -5.7099e-02,\n",
       "         -1.3597e+00, -1.7828e+00,  1.2700e+00, -3.7810e-01,  4.6466e+00,\n",
       "          1.5633e+00,  5.4556e-01, -5.3486e-01,  1.6533e+00, -3.6282e-01,\n",
       "          1.2511e+00,  8.7247e-01, -7.2255e-02, -7.4673e-01, -1.2112e+00,\n",
       "         -2.4617e+00,  5.7479e-01,  1.0272e-01, -3.0034e-01, -1.8553e+00,\n",
       "          3.8385e-01,  7.0100e-01, -6.4588e-01, -7.8293e-01,  5.8508e-01,\n",
       "         -7.2640e-01,  2.1400e+00,  2.3116e-01, -1.0434e+00, -2.0175e+00,\n",
       "          4.0911e-01,  6.1559e-01, -2.0256e-01,  1.2906e-01, -1.5174e-01,\n",
       "         -1.5231e+00,  3.7640e-01,  9.1887e-01, -1.1127e-01, -1.7229e+00,\n",
       "         -2.5250e+00, -1.2399e+00, -1.6609e+00, -4.9534e-02,  8.9906e-01,\n",
       "         -1.4543e+00, -6.2980e-02, -9.4104e-01,  1.3903e+00, -1.5849e+00,\n",
       "         -7.5424e-01,  3.4552e+00,  2.0644e+00,  2.5948e-01, -1.1030e+00,\n",
       "         -8.9831e-01, -1.5546e-01,  3.6403e-01, -1.8181e+00,  1.0744e+00,\n",
       "          1.6060e+00,  9.4681e-02, -2.3046e-01,  4.0995e-02, -2.1912e+00,\n",
       "          7.9664e-01,  2.6189e-01,  2.3826e+00, -9.2375e-01, -1.7903e+00,\n",
       "          1.2551e-01, -5.4314e-01, -1.9375e+00,  4.4195e-01,  1.8747e+00,\n",
       "         -6.8337e-01,  2.7900e+00, -2.2576e+00, -1.9741e+00, -8.3866e-01,\n",
       "         -1.0927e+00, -2.3664e+00,  4.1462e-01,  1.4247e-01, -1.7995e+00,\n",
       "         -2.7896e-01,  1.2172e+00,  1.0992e+00, -1.8520e-01,  2.9030e-01,\n",
       "          5.7806e-01, -6.8574e-01,  8.7388e-02, -1.5035e+00,  1.3086e+00,\n",
       "          1.1546e+00,  1.1846e-01,  2.0439e+00, -1.2885e+00, -1.9019e+00,\n",
       "          8.0995e-01,  4.1085e-02,  1.4980e-01,  2.8706e-01,  4.6107e-01,\n",
       "         -4.5139e-01,  4.1344e+00, -5.2125e-01, -1.3821e+00, -1.5420e-02,\n",
       "         -2.0235e-01,  1.9055e+00, -1.0010e+00, -2.0046e+00, -1.2056e+00,\n",
       "          2.9031e-01, -2.1070e+00, -2.1326e+00, -4.9003e-01, -7.9553e-01,\n",
       "         -1.0346e+00, -1.9397e+00, -6.8290e-02,  4.9958e-01, -7.7608e-02,\n",
       "          1.8081e-01, -2.1652e-01, -1.7763e+00,  6.2266e-01, -3.4899e-01,\n",
       "          1.5236e+00, -1.0410e+00, -2.5923e+00,  1.2085e+00, -7.9445e-01,\n",
       "         -2.2573e+00, -1.1834e-01,  2.3174e-01,  9.4567e-01, -1.8829e+00,\n",
       "         -2.3556e+00,  1.3470e+00,  6.4159e-01, -7.2930e-01, -8.8545e-01,\n",
       "         -2.7610e+00,  1.8733e+00, -2.0879e-01,  1.1912e-01,  8.1170e-01,\n",
       "         -7.9686e-01, -9.0680e-01,  1.7048e+00,  1.3574e+00, -1.1055e+00,\n",
       "         -7.4800e-01, -4.5787e+00, -1.0399e+00,  6.1047e-01, -2.2398e+00,\n",
       "         -9.3805e-03,  1.4005e+00, -1.3131e+00,  9.6400e-02, -1.9672e+00,\n",
       "         -2.6771e-01, -1.9140e+00, -1.9674e+00, -3.9607e-01, -4.0739e+00,\n",
       "         -7.0035e-01, -6.7019e-01,  1.0259e+00, -1.8295e+00,  4.9275e-01,\n",
       "         -2.1938e+00,  8.5932e-01, -1.5164e+00,  2.1868e+00,  1.0053e+00,\n",
       "          1.1587e+00,  1.2329e+00,  9.4269e-01, -5.4513e-01,  1.1483e+00,\n",
       "          9.9737e-01,  1.9728e+00, -7.9368e-01, -1.5064e-01, -3.9933e-01,\n",
       "         -4.7156e-01,  2.3797e+00, -1.3125e+00, -4.1223e-01,  5.8889e-01,\n",
       "          1.1337e+00,  2.4878e-01, -1.4671e+00,  3.1672e+00, -3.3441e-01,\n",
       "          3.5220e-01,  1.5297e+00,  6.5348e-01, -7.9813e-01, -1.4292e+00,\n",
       "         -1.7508e+00, -1.3396e+00, -1.8549e+00,  1.3376e+00, -2.8279e+00,\n",
       "         -8.4971e-01, -4.5904e-01,  9.7757e-01,  1.1362e+00,  2.3635e-01,\n",
       "         -4.8809e-01, -1.0056e+00,  1.7697e+00,  4.3909e-02, -2.0320e-01,\n",
       "          4.5245e-01,  2.2109e-01,  2.3199e+00, -8.9795e-01, -2.4936e+00,\n",
       "         -1.8956e+00, -1.9415e+00,  4.3284e-02, -1.4917e+00, -1.8406e-01,\n",
       "         -1.0562e+00, -1.8524e+00,  6.0727e-01, -2.7266e+00, -1.0466e+00,\n",
       "          3.5877e+00, -6.2127e-01,  5.7452e-01, -4.4227e-01, -6.0067e-02,\n",
       "          4.8726e-01,  4.8826e-01, -2.4690e+00,  4.1191e-01,  1.0732e+00,\n",
       "          9.3909e-01, -1.0708e+00,  2.2816e+00,  4.4190e-01, -1.2598e+00,\n",
       "         -4.7484e-01, -1.3660e+00, -4.9099e-01, -2.8423e-01,  2.2655e-01,\n",
       "         -1.0871e-01,  3.9156e+00,  5.6616e-01,  1.0714e-01, -2.7578e+00,\n",
       "         -2.1005e+00, -1.8217e-01, -2.1068e-01, -1.0794e+00,  6.3909e-01,\n",
       "          6.0117e-01,  6.8416e-01,  1.8310e+00,  4.7238e-01, -1.0757e+00,\n",
       "         -4.3509e-01, -5.9590e-01, -1.0432e+00,  1.6481e+00,  4.1502e-01,\n",
       "          1.5491e-01, -1.4294e+00, -3.2228e-01, -3.8464e-01, -3.4619e-01,\n",
       "          1.3294e+00,  4.9685e-01,  4.7617e+00, -1.4655e+00, -9.3554e-01,\n",
       "         -2.5451e-01, -1.8215e+00, -1.2040e+00, -5.4667e-01, -2.0594e+00,\n",
       "         -8.9480e-01, -3.7502e-01,  1.8928e-01, -2.1615e+00,  6.8265e-01,\n",
       "          6.6691e-01, -2.3398e-01,  1.5941e+00, -2.8771e-01,  8.4578e-01,\n",
       "          1.0385e+00, -1.6923e-01, -3.8572e-01, -4.6881e-02, -3.8097e+00,\n",
       "         -6.6123e-01,  2.0870e+00, -2.1004e+00,  5.3965e-01,  1.5145e+00,\n",
       "          1.9913e+00, -1.0415e+00,  8.1989e-01, -1.5048e+00, -1.1611e+00,\n",
       "         -1.2903e+00, -5.0271e-01, -3.4022e+00, -1.3990e+00, -1.1075e+00,\n",
       "         -1.5195e+00, -8.1167e-01, -6.1598e-03, -1.6215e+00, -1.6200e+00,\n",
       "         -3.1629e+00, -2.1124e-04, -4.6137e-01, -2.0681e-01, -7.8552e-01,\n",
       "         -1.9155e-01, -1.6744e+00,  1.4653e+00, -6.4916e-01,  6.6898e-01,\n",
       "         -2.8260e-01, -1.7052e+00, -4.2775e-01, -1.9692e+00,  6.4841e-01,\n",
       "          2.1865e-01, -5.8163e-01,  2.6314e+00, -8.7590e-01, -8.4983e-01,\n",
       "         -3.5468e-01, -9.7866e-02, -1.6981e+00, -7.9244e-01, -4.9086e-01,\n",
       "         -1.9652e+00, -4.5403e-01, -1.9348e+00, -1.4982e-02, -9.1651e-01,\n",
       "         -1.0795e+00, -1.4098e+00, -9.4895e-01, -1.6395e+00,  2.4803e+00,\n",
       "         -8.9321e-01, -1.1523e+00, -6.2900e-01, -2.5171e+00, -4.4568e-01,\n",
       "          7.2549e-01, -7.0211e-01,  2.7455e+00,  2.0548e+00, -1.4174e-01,\n",
       "         -1.4292e+00, -1.0815e+00, -1.6127e+00,  9.2056e-01,  1.1905e-01,\n",
       "         -6.7013e-01, -4.8396e-02, -7.9915e-01,  1.7239e+00,  1.6167e+00,\n",
       "          2.9594e-01, -1.2275e-01, -8.3370e-01, -2.8104e-01,  1.3021e-01,\n",
       "         -1.3819e+00,  1.4736e+00,  2.3947e+00,  1.1290e+00, -1.7545e-02,\n",
       "         -1.8492e+00, -8.5648e-01, -7.4952e-01, -6.0788e-01, -1.6082e+00,\n",
       "         -2.4531e+00,  1.1408e+00, -1.5267e+00,  1.0321e+00,  2.0267e-01,\n",
       "         -1.2124e-02,  2.8846e+00, -7.2888e-01, -3.1148e+00, -1.3770e+00,\n",
       "          2.1375e+00, -1.5760e+00,  5.8303e-01, -7.7441e-01,  4.9826e-01,\n",
       "          1.5099e-01, -2.9593e-02,  8.7164e-01, -1.8424e+00, -1.1896e+00,\n",
       "         -8.2445e-01,  1.2416e+00,  1.3307e+00,  1.3703e-01, -5.1208e-01,\n",
       "          2.1070e+00, -5.7916e-01, -2.6201e-01, -3.3853e+00, -1.1297e+00,\n",
       "         -5.9502e-01, -1.4694e+00, -1.9768e+00,  2.4667e-01, -4.3579e-01]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "import urllib\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "input_image = Image.open(filename)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "    \n",
    "    \n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samoyed 0.8732963800430298\n",
      "Pomeranian 0.030270840972661972\n",
      "white wolf 0.019671104848384857\n",
      "keeshond 0.011073537170886993\n",
      "Eskimo dog 0.00920423399657011\n"
     ]
    }
   ],
   "source": [
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "# print(probabilities)\n",
    "\n",
    "# Read the categories\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "# Show top categories per image\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time required: 0.013755083084106445\n",
      "1.0000001\n",
      "0.03727182\n",
      "-0.0235776\n",
      "-0.018512128\n",
      "0.04793311\n",
      "['the woman with the red dress is eating a pizza', 'person', 'dog', 'ball', 'ice cream']\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# as we could see in this simple example the embeddings\n",
    "# found in the latent space are very similar to each other\n",
    "# when ever the objects are involved in the sentence\n",
    "\n",
    "# I chose the MiniLM model because it is the smallest model\n",
    "# which can perform this task accurately without introducing\n",
    "# overhead\n",
    "\n",
    "sentences = [\"the woman with the red dress is eating a pizza\"]\n",
    "s = [\"person\", \n",
    "             \"dog\",\n",
    "             \"ball\",\n",
    "             \"ice cream\"]\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    return np.linalg.norm(a-b)\n",
    "\n",
    "def coseine_similarity(a, b):\n",
    "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "embeddings = model.encode(sentences)\n",
    "print('time required:',time.time()-start)\n",
    "\n",
    "print(coseine_similarity(embeddings[0], embeddings[0]))\n",
    "print(coseine_similarity(embeddings[0], embeddings[1]))\n",
    "print(coseine_similarity(embeddings[0], embeddings[2]))\n",
    "print(coseine_similarity(embeddings[0], embeddings[3]))\n",
    "print(coseine_similarity(embeddings[0], embeddings[4]))\n",
    "\n",
    "print(sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum number of boxes found:  32\n",
      "mean number of boxes found:  5.56635722066973\n",
      "std number of boxes found:  2.7148657554979914\n",
      "count max number of boxes found:  3\n",
      "count of number of boxes found:\n",
      "[248, 1577, 6883, 8627, 7880, 5624, 3697, 2583, 1698, 1039, 776, 498, 341, 283, 144, 98, 56, 50, 36, 28, 17, 14, 9, 6, 2, 9, 0, 0, 0, 0, 0, 3]\n"
     ]
    }
   ],
   "source": [
    "file_name = 'yolov5l6+clip/1_dictionary_full_train.p'\n",
    "\n",
    "with open(file_name, 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n",
    "\n",
    "print('maximum number of boxes found: ',max([len(dictionary[sample]['image_emb']) for sample in dictionary.keys()]))\n",
    "print('mean number of boxes found: ',np.array([len(dictionary[sample]['image_emb']) for sample in dictionary.keys()]).mean())\n",
    "print('std number of boxes found: ',np.array([len(dictionary[sample]['image_emb']) for sample in dictionary.keys()]).std())\n",
    "print('count max number of boxes found: ',[len(dictionary[sample]['image_emb']) for sample in dictionary.keys()].count(32))\n",
    "print('count of number of boxes found:')\n",
    "print([[len(dictionary[sample]['image_emb']) for sample in dictionary.keys()].count(xx+1) for xx in range(32)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3163cfb8aa3549ad3f5400bc3427ee7a4002d2a0d6d7ead52f641c6a7636395"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
