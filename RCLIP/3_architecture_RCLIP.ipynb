{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-lightning\n",
      "  Downloading pytorch_lightning-2.0.2-py3-none-any.whl (719 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.0/719.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from pytorch-lightning) (4.5.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from pytorch-lightning) (6.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from pytorch-lightning) (1.13.1)\n",
      "Collecting torchmetrics>=0.7.0\n",
      "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from pytorch-lightning) (1.23.5)\n",
      "Collecting lightning-utilities>=0.7.0\n",
      "  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from pytorch-lightning) (2022.11.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from pytorch-lightning) (4.65.0)\n",
      "Requirement already satisfied: packaging>=17.1 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from pytorch-lightning) (23.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.3)\n",
      "Requirement already satisfied: requests in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.28.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (22.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n",
      "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
      "Successfully installed lightning-utilities-0.8.0 pytorch-lightning-2.0.2 torchmetrics-0.11.4\n"
     ]
    }
   ],
   "source": [
    "# !pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, d_model//4)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model//4)\n",
    "        self.linear2 = nn.Linear(d_model//4,  d_model//16)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model//16)\n",
    "        self.linear3 = nn.Linear(d_model//16, d_model//32)\n",
    "        self.layer_norm3 = nn.LayerNorm(d_model//32)\n",
    "        self.linear4 = nn.Linear(d_model//32, 4)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x = x.to(device)\n",
    "        x = x.cuda()\n",
    "\n",
    "        x = self.layer_norm1(self.dropout(self.linear1(x)))\n",
    "        x = self.layer_norm2(self.dropout(self.linear2(x)))\n",
    "        x = self.layer_norm3(self.dropout(self.linear3(x)))\n",
    "        x = self.linear4(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # embedding matching\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # feedforward\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        # activation\n",
    "        self.activation = nn.GELU()\n",
    "        \n",
    "    def forward(self, text_emb, box_emb):\n",
    "\n",
    "        # https://arxiv.org/pdf/2002.04745.pdf\n",
    "        # here we propose the original transformer encoder layer\n",
    "        # however, we designed the architecture in this way\n",
    "        # as the authors of the paper did to improve the convergence\n",
    "\n",
    "        # text_emb = text_emb.to(device)\n",
    "        # box_emb = box_emb.to(device)\n",
    "\n",
    "        text_emb = text_emb.cuda()\n",
    "        box_emb = box_emb.cuda()\n",
    "\n",
    "\n",
    "        # Add & Norm\n",
    "        text_emb = text_emb + self.dropout1(text_emb)\n",
    "        text_emb = self.norm1(text_emb)\n",
    "\n",
    "        box_emb = box_emb + self.dropout1(box_emb)\n",
    "        box_emb = self.norm1(box_emb)\n",
    "\n",
    "        # print(text_emb.shape, box_emb.shape)\n",
    "\n",
    "        # embedding matching\n",
    "        x , _ = self.self_attn(box_emb, text_emb, box_emb)\n",
    "\n",
    "        # print(x.shape, box_emb.shape)\n",
    "        \n",
    "        # Add & Norm\n",
    "        x = box_emb + self.dropout2(x)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        # feedforward\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.encoder_block = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "        # self.transformer_encoder = nn.TransformerEncoder(self.encoder_block, num_layers)\n",
    "\n",
    "        self.transformer_encoder = [TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout).to(device).type(torch.float32) for _ in range(num_layers)]\n",
    "\n",
    "\n",
    "    def forward(self, text_emb, box_emb):\n",
    "\n",
    "        # # matching between the text and the first box\n",
    "        # x0 = self.transformer_encoder(text_emb, box_emb[:,0,:].unsqueeze(1))\n",
    "        # # matching between the text and the second box\n",
    "        # x1 = self.transformer_encoder(text_emb, box_emb[:,1,:].unsqueeze(1))\n",
    "        \n",
    "        # # concatenate the two boxes\n",
    "        # # shape: (batch_size, 2, d_model)\n",
    "        # x = torch.cat([x0, x1], axis=1)\n",
    "\n",
    "\n",
    "        # text_emb = text_emb.to(device)\n",
    "        # box_emb = box_emb.to(device)\n",
    "\n",
    "        text_emb = text_emb.cuda()\n",
    "        box_emb = box_emb.cuda()\n",
    "        \n",
    "        x0 = text_emb.to(device)\n",
    "        x1 = text_emb.to(device)\n",
    "\n",
    "        for layer in self.transformer_encoder:\n",
    "            # matching between the text and the first box\n",
    "            x0 = layer(x0, box_emb[:,0,:].unsqueeze(1))\n",
    "            # matching between the text and the second box\n",
    "            x1 = layer(x1, box_emb[:,1,:].unsqueeze(1))\n",
    "\n",
    "        # concatenate the two boxes\n",
    "        # shape: (batch_size, 2, d_model)\n",
    "        x = torch.cat([x0, x1], axis=1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "class BoxRegressor(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transformer_encoder = TransformerEncoder(d_model, nhead, num_layers, dim_feedforward, dropout).to(device).type(torch.float32)\n",
    "        self.mlp_regressor = MLP(1034, dropout).to(device).type(torch.float16)\n",
    "        self.flatten = nn.Flatten(start_dim=1).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, text_encoding, box_encoding, box_coords):\n",
    "\n",
    "        # text_encoding = text_encoding.to(device)\n",
    "        # box_encoding = box_encoding.to(device)\n",
    "        # box_coords = box_coords.to(device)\n",
    "\n",
    "        text_encoding = text_encoding.cuda()\n",
    "        box_encoding = box_encoding.cuda()\n",
    "        box_coords = box_coords.cuda()\n",
    "\n",
    "        # compute the similarity matrix between the text and the boxes encoding\n",
    "        similarity_matrix = torch.bmm(text_encoding.permute(0, 2, 1), box_encoding)\n",
    "\n",
    "        # get the index of the top two boxes with the highest score\n",
    "        top2_indices = torch.topk(similarity_matrix, k=2, dim=-1).indices.squeeze(1)\n",
    "        top2 = torch.topk(similarity_matrix, k=2, dim=-1).indices.squeeze(1)\n",
    "\n",
    "\n",
    "        # permute the dimensions to get the top two boxes\n",
    "        box_encoding = box_encoding.permute(0, 2, 1)\n",
    "        # get the top two boxes\n",
    "        top2_boxes = box_encoding[torch.arange(box_encoding.shape[0]).unsqueeze(1), top2_indices]\n",
    "        # print(top2_boxes.shape)\n",
    "\n",
    "        # get the top two boxes coordinates\n",
    "        top2_boxes_coords = box_coords[torch.arange(box_encoding.shape[0]).unsqueeze(1), top2_indices]\n",
    "        # print(top2_boxes_coords.shape)\n",
    "\n",
    "        top2_boxes = top2_boxes.to(device)\n",
    "\n",
    "        # compute the matching between the text and the top two boxes\n",
    "        out_matching = self.transformer_encoder(text_encoding.permute(0, 2, 1), top2_boxes)\n",
    "\n",
    "        # concatenate the matching score with the top two boxes coordinates\n",
    "        matching_score = torch.cat([top2.unsqueeze(2), top2_boxes_coords, out_matching], axis=-1)\n",
    "\n",
    "        return self.mlp_regressor(self.flatten(matching_score))\n",
    "    \n",
    "\n",
    "\n",
    "# def SUM_MSE_loss(pred, target):\n",
    "#     return (pred - target).pow(2).sum(axis=-1).mean()\n",
    "\n",
    "\n",
    "class Net(pl.LightningModule):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.box_regressor = BoxRegressor(d_model, nhead, num_layers, dim_feedforward, dropout).to(device)\n",
    "\n",
    "    def forward(self, text_encoding, box_encoding, box_coords):\n",
    "        return self.box_regressor(text_encoding, box_encoding, box_coords)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defined the train loop.\n",
    "        # It is independent of forward\n",
    "        text_encoding, box_encoding, box_coords, labels = batch\n",
    "\n",
    "        # text_encoding = text_encoding.to(device)\n",
    "        # box_encoding = box_encoding.to(device)\n",
    "        # box_coords = box_coords.to(device)\n",
    "        # labels = labels.to(device)\n",
    "\n",
    "        text_encoding = text_encoding.cuda()\n",
    "        box_encoding = box_encoding.cuda()\n",
    "        box_coords = box_coords.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        self = self.to(device)\n",
    "\n",
    "        out = self(text_encoding, box_encoding, box_coords)\n",
    "\n",
    "        print(F.mse_loss(1/(self.forward(text_encoding, box_encoding, box_coords).squeeze(1)**2+1),  1/(labels)**2+1))\n",
    "\n",
    "        out = out.squeeze(1)\n",
    "        loss = F.mse_loss(1/(out**2+1), 1/(labels**2+1))\n",
    "        # loss = SUM_MSE_loss(out, labels)\n",
    "\n",
    "        print(loss)\n",
    "\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        text_encoding, box_encoding, box_coords, labels = batch\n",
    "\n",
    "        # text_encoding = text_encoding.to(device)\n",
    "        # box_encoding = box_encoding.to(device)\n",
    "        # box_coords = box_coords.to(device)\n",
    "        # labels = labels.to(device)\n",
    "\n",
    "        text_encoding = text_encoding.cuda()\n",
    "        box_encoding = box_encoding.cuda()\n",
    "        box_coords = box_coords.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        out = self(text_encoding, box_encoding, box_coords)\n",
    "\n",
    "        print(out.shape)\n",
    "\n",
    "        out = out.squeeze(1)\n",
    "        loss = F.mse_loss(1/(out**2+1), 1/(labels**2+1))\n",
    "        # loss = SUM_MSE_loss(out, labels)\n",
    "\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(device)\n",
    "\n",
    "# text_encoding = torch.randn(10000, 512, 1).to(device)\n",
    "# box_encoding = torch.randn(10000, 512, 10).to(device)\n",
    "# box_coords = torch.randn(10000, 10, 4).to(device)\n",
    "# target_boxes = torch.randn(10000, 1, 4).to(device)\n",
    "\n",
    "\n",
    "# # get dataset\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# batch_size = 256\n",
    "\n",
    "# dataset = TensorDataset(text_encoding, box_encoding, box_coords, target_boxes)\n",
    "# train_loader = DataLoader(dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "\n",
    "# # init model\n",
    "# model = Net(512, 8, 2, 2048, 0.1).cuda()\n",
    "\n",
    "# print(model)\n",
    "# print('number of parameter: ',sum(p.numel() for p in model.parameters() if p.requires_grad)/1000000.0, 'M')\n",
    "\n",
    "# # most basic trainer, uses good defaults\n",
    "# trainer = pl.Trainer(accelerator='auto', max_epochs=10)\n",
    "\n",
    "# # train the model\n",
    "# trainer.fit(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "\n",
    "import pickle\n",
    "\n",
    "# load test dataset\n",
    "file_name = './data/yolo_v8x/yolo_v8x_1_dictionary_full_test.p'\n",
    "with open(file_name, 'rb') as f:\n",
    "    data_test = pickle.load(f)\n",
    "\n",
    "# load val dataset\n",
    "file_name = './data/yolo_v8x/yolo_v8x_1_dictionary_full_val.p'\n",
    "with open(file_name, 'rb') as f:\n",
    "    data_val = pickle.load(f)\n",
    "\n",
    "# load train dataset\n",
    "file_name = './data/yolo_v8x/yolo_v8x_1_dictionary_full_train.p'\n",
    "with open(file_name, 'rb') as f:\n",
    "    data_train = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "# text_encoding = torch.randn(10000, 512, 1).to(device)\n",
    "# box_encoding = torch.randn(10000, 512, 10).to(device)\n",
    "# box_coords = torch.randn(10000, 10, 4).to(device)\n",
    "# target_boxes = torch.randn(10000, 1, 4).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5023/5023 [00:10<00:00, 489.60it/s]\n",
      "100%|██████████| 2573/2573 [00:04<00:00, 556.17it/s]\n",
      "100%|██████████| 42226/42226 [01:20<00:00, 524.97it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_data(full_data):\n",
    "\n",
    "    text_encoding, box_encoding, box_coords, target_boxes = [], [], [], []\n",
    "\n",
    "    for idx in tqdm(list(full_data)):\n",
    "        # for _ in range(data['image_emb'].shape[0]):\n",
    "        for idx_text in range(full_data[idx]['text_emb'].shape[0]):\n",
    "            \n",
    "            # number of available crops\n",
    "            number_of_crop = min(full_data[idx]['image_emb'].shape[0], len(full_data[idx]['df_boxes']))\n",
    "\n",
    "            if number_of_crop == 0:\n",
    "                break\n",
    "\n",
    "            # shape: (number of samples, 512, 1)\n",
    "            text_encoding.append(full_data[idx]['text_emb'][idx_text].unsqueeze(1))\n",
    "\n",
    "            number_of_crop = min(full_data[idx]['image_emb'].shape[0], len(full_data[idx]['df_boxes']))\n",
    "\n",
    "            # shape: (number of samples, 512, number of crop embeddings)\n",
    "            box_encoding.append(full_data[idx]['image_emb'][:number_of_crop,:].permute(1, 0))\n",
    "\n",
    "            # shape: (number of samples, number of boxes, 4)\n",
    "            box_coords.append(torch.stack([torch.tensor(full_data[idx]['df_boxes'].iloc[i][:4]).type(torch.float16) \n",
    "                                                    for i in range(number_of_crop)]))\n",
    "            \n",
    "            # shape: (number of samples, 1, 4)\n",
    "            target_boxes.append(torch.tensor(full_data[idx]['bbox_target']).type(torch.float16).unsqueeze(0))\n",
    "\n",
    "    return torch.stack(text_encoding), torch.stack([torch.nn.functional.pad(b.permute(1, 0), (0, 0, 0, 48 - b.shape[1])).permute(1, 0) for b in box_encoding]), torch.stack([torch.nn.functional.pad(b, (0, 0, 0, 48 - b.shape[0])) for b in box_coords]), torch.stack(target_boxes)\n",
    "\n",
    "\n",
    "\n",
    "text_encoding_test, box_encoding_test, box_coords_test, target_boxes_test = get_data(data_test)\n",
    "text_encoding_val, box_encoding_val, box_coords_val, target_boxes_val = get_data(data_val)\n",
    "text_encoding_train, box_encoding_train, box_coords_train, target_boxes_train = get_data(data_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_norm_rescale(box_target):\n",
    "    \"\"\" Rescale the box_target \n",
    "    Args:\n",
    "        box_target: (number of samples, 1, 4)\n",
    "\n",
    "    Returns:\n",
    "        box_target: (number of samples, 1, 4)\n",
    "\n",
    "    \"\"\"\n",
    "    # convert the box_pred to x1, y1, x2, y2\n",
    "    box_target[:, 0, 2] = box_target[:, 0, 0] + box_target[:, 0, 2]\n",
    "    box_target[:, 0, 3] = box_target[:, 0, 1] + box_target[:, 0, 3]\n",
    "\n",
    "    return box_target\n",
    "\n",
    "# box rescaling\n",
    "target_boxes_test = box_norm_rescale(target_boxes_test)\n",
    "target_boxes_val = box_norm_rescale(target_boxes_val)\n",
    "target_boxes_train = box_norm_rescale(target_boxes_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# dataloaders\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "text_encoding_test = text_encoding_test.cpu().type(torch.float32)\n",
    "box_encoding_test = box_encoding_test.cpu().type(torch.float32)\n",
    "box_coords_test = box_coords_test.cpu().type(torch.float32)\n",
    "\n",
    "text_encoding_val = text_encoding_val.cpu().type(torch.float32)\n",
    "box_encoding_val = box_encoding_val.cpu().type(torch.float32)\n",
    "box_coords_val = box_coords_val.cpu().type(torch.float32)\n",
    "\n",
    "text_encoding_train = text_encoding_train.cpu().type(torch.float32)\n",
    "box_encoding_train = box_encoding_train.cpu().type(torch.float32)\n",
    "box_coords_train = box_coords_train.cpu().type(torch.float32)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "dataset_test = TensorDataset(text_encoding_test, box_encoding_test, box_coords_test, target_boxes_test)\n",
    "dataset_val = TensorDataset(text_encoding_val, box_encoding_val, box_coords_val, target_boxes_val)\n",
    "dataset_train = TensorDataset(text_encoding_train, box_encoding_train, box_coords_train, target_boxes_train)\n",
    "\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# init model\n",
    "model = Net(512, 2, 1, 1024, 0.1).type(torch.float32).cuda()\n",
    "\n",
    "print(model)\n",
    "print('number of parameter: ',sum(p.numel() for p in model.parameters() if p.requires_grad)/1000000.0, 'M')\n",
    "\n",
    "# most basic trainer, uses good defaults\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=3, verbose=True, mode=\"min\")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(accelerator='auto', max_epochs=5, callbacks=[early_stop_callback])\n",
    "\n",
    "# train the model\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# test the model\n",
    "# trainer.test(test_dataloaders=test_loader)\n",
    "\n",
    "# max([b.shape[0] for b in box_coords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 1 6.852848301124163 4.310411508820321\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(max([b.shape[0] for b in box_coords]), min([b.shape[0] for b in box_coords]), np.array([b.shape[0] for b in box_coords]).mean(), np.array([b.shape[0] for b in box_coords]).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_encoding = torch.randn(10000, 512, 1).to(device)\n",
    "# box_encoding = torch.randn(10000, 512, 10).to(device)\n",
    "# box_coords = torch.randn(10000, 10, 4).to(device)\n",
    "# target_boxes = torch.randn(10000, 1, 4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1fcbcc9323be2c72\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1fcbcc9323be2c72\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3163cfb8aa3549ad3f5400bc3427ee7a4002d2a0d6d7ead52f641c6a7636395"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
