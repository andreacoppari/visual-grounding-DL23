{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len training datasets: 42226\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "################################ load the dataset\n",
    "#################################################\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RefCOCOg(Dataset):\n",
    "    def __init__(self, refs, annotations, split=\"train\"):\n",
    "\n",
    "        self.dataset = [{\"file_name\": os.path.join(\"./refcocog/images/\", f'{\"_\".join(elem[\"file_name\"].split(\"_\")[:3])}.jpg'),\n",
    "                            \"caption\": elem[\"sentences\"][0][\"raw\"],\n",
    "                            \"ann_id\": int(elem[\"file_name\"].split(\"_\")[3][:-4]),\n",
    "                            \"bbox\": annotations[int(elem[\"file_name\"].split(\"_\")[3][:-4])]}\n",
    "                        for elem in [d for d in refs if d[\"split\"]==split]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "    \n",
    "    def __call__(self, idx):\n",
    "        print(json.dumps(self.dataset[idx], indent=4))\n",
    "\n",
    "\n",
    "# Load refs and annotations\n",
    "import pickle\n",
    "\n",
    "with open(\"../extractCOCO/refcocog/annotations/refs(umd).p\", \"rb\") as fp:\n",
    "  refs = pickle.load(fp)\n",
    "\n",
    "with open(\"../extractCOCO/refcocog/annotations/instances.json\", \"rb\") as fp:\n",
    "  data = json.load(fp)\n",
    "  annotations = dict(sorted({ann[\"id\"]: ann[\"bbox\"] for ann in data[\"annotations\"]}.items()))\n",
    "\n",
    "\n",
    "# load the train dataset\n",
    "train_dataset = RefCOCOg(refs=refs, annotations=annotations, split=\"train\")\n",
    "\n",
    "print('len training datasets:',len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data_refcoco/dict_preprocessedimages_440_450.p not found\n",
      "File data_refcoco/dict_preprocessedimages_680_690.p not found\n",
      "File data_refcoco/dict_preprocessedimages_1331_1341.p not found\n",
      "File data_refcoco/dict_preprocessedimages_1411_1421.p not found\n",
      "File data_refcoco/dict_preprocessedimages_1931_1941.p not found\n",
      "File data_refcoco/dict_preprocessedimages_2012_2022.p not found\n",
      "File data_refcoco/dict_preprocessedimages_2342_2352.p not found\n",
      "File data_refcoco/dict_preprocessedimages_2872_2882.p not found\n",
      "File data_refcoco/dict_preprocessedimages_3233_3243.p not found\n",
      "File data_refcoco/dict_preprocessedimages_3623_3633.p not found\n",
      "File data_refcoco/dict_preprocessedimages_3673_3683.p not found\n",
      "File data_refcoco/dict_preprocessedimages_4064_4074.p not found\n",
      "File data_refcoco/dict_preprocessedimages_4244_4254.p not found\n",
      "File data_refcoco/dict_preprocessedimages_4334_4344.p not found\n",
      "File data_refcoco/dict_preprocessedimages_4694_4704.p not found\n",
      "File data_refcoco/dict_preprocessedimages_4824_4834.p not found\n",
      "File data_refcoco/dict_preprocessedimages_4864_4874.p not found\n",
      "File data_refcoco/dict_preprocessedimages_5035_5045.p not found\n",
      "File data_refcoco/dict_preprocessedimages_5265_5275.p not found\n",
      "File data_refcoco/dict_preprocessedimages_5775_5785.p not found\n",
      "File data_refcoco/dict_preprocessedimages_6616_6626.p not found\n",
      "File data_refcoco/dict_preprocessedimages_6636_6646.p not found\n",
      "File data_refcoco/dict_preprocessedimages_6846_6856.p not found\n",
      "File data_refcoco/dict_preprocessedimages_6916_6926.p not found\n",
      "File data_refcoco/dict_preprocessedimages_7367_7377.p not found\n",
      "File data_refcoco/dict_preprocessedimages_7667_7677.p not found\n",
      "File data_refcoco/dict_preprocessedimages_9829_9839.p not found\n",
      "File data_refcoco/dict_preprocessedimages_9879_9889.p not found\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "dictionary_full = dict()\n",
    "\n",
    "\n",
    "# initialize steps for the loop\n",
    "min_image = 0\n",
    "max_image = 10000\n",
    "# batch of 10 images\n",
    "N_images_batches = 10\n",
    "steps = int(10000/10)\n",
    "steps = np.linspace(min_image,max_image,steps).astype(int)\n",
    "\n",
    "\n",
    "file_name = [f'data_refcoco/dict_preprocessedimages_{m}_{M}.p' for m,M in zip(steps[:max_image-1], steps[1:])]\n",
    "\n",
    "for file in file_name:\n",
    "    try:\n",
    "        with open(file, 'rb') as handle:\n",
    "            dictionary = pickle.load(handle)\n",
    "\n",
    "        # index of the sample with respect to the whole dataset\n",
    "        idx = int(file.split('_')[3])\n",
    "        for i, key in enumerate(dictionary.keys()):\n",
    "            dictionary_full[idx+i] = dictionary[key]\n",
    "    except:\n",
    "        print(f'File {file} not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of images\n",
    "# dictionary_full.keys()\n",
    "\n",
    "# save the dictionary\n",
    "# with open('data_refcoco/full/dict_preprocessedimages_full.p', 'wb') as handle:\n",
    "#     pickle.dump(dictionary_full, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number of boxes: 1\n",
      "Maximum number of boxes: 80\n",
      "Number of images with less than 3 boxes: 546\n",
      "Number of images with less than 2 boxes: 16\n",
      "Number of images with 1 box: 16\n",
      "Mean number of boxes: 13.155349794238683\n",
      "Std number of boxes: 12.071824752753127\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# find minimum and maximum number of boxes\n",
    "min_boxes = 1000\n",
    "max_boxes = 0\n",
    "\n",
    "list_n_boxes = []\n",
    "count_lessthan3 = 0\n",
    "count_lessthan2 = 0\n",
    "count_1 = 0\n",
    "for key in dictionary_full.keys():\n",
    "    n_boxes = dictionary_full[key]['prob-box-map'].shape[0]\n",
    "    if n_boxes < min_boxes:\n",
    "        min_boxes = n_boxes\n",
    "    if n_boxes > max_boxes:\n",
    "        max_boxes = n_boxes\n",
    "    list_n_boxes.append(n_boxes)\n",
    "    if n_boxes < 3:\n",
    "        count_lessthan3 += 1\n",
    "    if n_boxes < 2:\n",
    "        count_lessthan2 += 1\n",
    "    if n_boxes == 1:\n",
    "        count_1 += 1\n",
    "\n",
    "print(f'Minimum number of boxes: {min_boxes}')\n",
    "print(f'Maximum number of boxes: {max_boxes}')\n",
    "\n",
    "print(f'Number of images with less than 3 boxes: {count_lessthan3}')\n",
    "print(f'Number of images with less than 2 boxes: {count_lessthan2}')\n",
    "print(f'Number of images with 1 box: {count_1}')\n",
    "\n",
    "# compute the mean and std number of boxes\n",
    "mean_n_boxes = np.mean(list_n_boxes)\n",
    "std_n_boxes = np.std(list_n_boxes)\n",
    "print(f'Mean number of boxes: {mean_n_boxes}')\n",
    "print(f'Std number of boxes: {std_n_boxes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 493 has less than 2 boxes\n",
      "Image 1319 has less than 2 boxes\n",
      "Image 1369 has less than 2 boxes\n",
      "Image 2634 has less than 2 boxes\n",
      "Image 2989 has less than 2 boxes\n",
      "Image 3745 has less than 2 boxes\n",
      "Image 4929 has less than 2 boxes\n",
      "Image 5297 has less than 2 boxes\n",
      "Image 6126 has less than 2 boxes\n",
      "Image 6201 has less than 2 boxes\n",
      "Image 6367 has less than 2 boxes\n",
      "Image 6884 has less than 2 boxes\n",
      "Image 7180 has less than 2 boxes\n",
      "Image 8714 has less than 2 boxes\n",
      "Image 9083 has less than 2 boxes\n",
      "Image 9540 has less than 2 boxes\n"
     ]
    }
   ],
   "source": [
    "# produce a dictionary getting the top2 boxes for each image\n",
    "# ordering them in therm of probability found with CLIP\n",
    "import torch\n",
    "\n",
    "\n",
    "dictionary_top2 = dict()\n",
    "\n",
    "for key in dictionary_full.keys():\n",
    "    # get the top2 boxes blurd_out\n",
    "    sorted_tensor, indices = torch.sort(dictionary_full[key]['prob-box-map'][:,0], descending=True)\n",
    "    top2 = dict()\n",
    "    if len(indices) >= 2:\n",
    "\n",
    "        top2['prob-box-map'] = dictionary_full[key]['prob-box-map'][indices]\n",
    "        top2['prob-box-map'] = top2['prob-box-map'][:2]\n",
    "\n",
    "        # get the top2 boxes embeds\n",
    "        top2['embeds-boxes'] = dictionary_full[key]['embeds-boxes'][indices[:2]]\n",
    "\n",
    "        top2['embeds-caption'] = dictionary_full[key]['embeds-caption']\n",
    "\n",
    "        dictionary_top2[key] = top2\n",
    "\n",
    "    else:\n",
    "        print(f'Image {key} has less than 2 boxes')\n",
    "\n",
    "        top2['prob-box-map'] = dictionary_full[key]['prob-box-map'][indices]\n",
    "\n",
    "        # pad with zeros\n",
    "        top2['prob-box-map'] = torch.functional.F.pad(top2['prob-box-map'], (0, 0, 0, 2 - top2['prob-box-map'].shape[0]), 'constant', 0)\n",
    "\n",
    "        top2['prob-box-map'] = top2['prob-box-map'][:2]\n",
    "\n",
    "        # get the top2 boxes embeds\n",
    "        top2['embeds-boxes'] = dictionary_full[key]['embeds-boxes'][indices]\n",
    "\n",
    "        top2['embeds-boxes'] = torch.functional.F.pad(top2['embeds-boxes'], (0, 0, 0, 2 - top2['embeds-boxes'].shape[0]), 'constant', 0)\n",
    "\n",
    "        top2['embeds-caption'] = dictionary_full[key]['embeds-caption']\n",
    "\n",
    "        dictionary_top2[key] = top2\n",
    "\n",
    "\n",
    "\n",
    "# save the dictionary\n",
    "# with open('data_refcoco/full/dict_preprocessedimages_top2.p', 'wb') as handle:\n",
    "#     pickle.dump(dictionary_top2, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prob-box-map': tensor([[  1.,   1.,  15.,   1., 479., 318.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.]], dtype=torch.float16),\n",
       " 'embeds-boxes': tensor([[-0.0290,  0.0247, -0.0278,  ...,  0.0109, -0.0138, -0.1218]],\n",
       "        device='cuda:0', dtype=torch.float16),\n",
       " 'embeds-caption': tensor([[-0.0942,  0.0734, -0.0498,  ...,  0.1699,  0.2249,  0.1455]],\n",
       "        device='cuda:0', dtype=torch.float16)}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of padded image\n",
    "dictionary_top2[9540]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.58, 0.  ],\n",
       "       [0.39, 0.24]], dtype=float16)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as we could see it seems that the first one best match\n",
    "# the caption and is the one with the highest probability\n",
    "# appling blur_out whereas applying blur_in we remove the \n",
    "# object described in the caption and the score is lower\n",
    "\n",
    "dictionary_top2[9541]['prob-box-map'].numpy()[:,:2].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_boxes = torch.stack([dictionary_top2[key]['prob-box-map'] for key in dictionary_top2.keys()])\n",
    "boxes_emb = torch.stack([dictionary_top2[key]['embeds-boxes'] for key in dictionary_top2.keys()])\n",
    "caption_emb = torch.stack([dictionary_top2[key]['embeds-caption'] for key in dictionary_top2.keys()])\n",
    "    \n",
    "# save the torch tensors\n",
    "torch.save(prob_boxes, 'data_refcoco/full/prob_boxes.pt')\n",
    "torch.save(boxes_emb, 'data_refcoco/full/boxes_emb.pt')\n",
    "torch.save(caption_emb, 'data_refcoco/full/caption_emb.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9720, 2, 1024])\n",
      "torch.Size([9720, 1, 1024])\n",
      "torch.Size([9720, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "print(boxes_emb.shape)\n",
    "print(caption_emb.shape)\n",
    "print(prob_boxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data_refcoco/dict_preprocessedimages_440_450.p not found\n",
      "File data_refcoco/dict_preprocessedimages_680_690.p not found\n",
      "File data_refcoco/dict_preprocessedimages_1331_1341.p not found\n",
      "File data_refcoco/dict_preprocessedimages_1411_1421.p not found\n",
      "File data_refcoco/dict_preprocessedimages_1931_1941.p not found\n",
      "File data_refcoco/dict_preprocessedimages_2012_2022.p not found\n",
      "File data_refcoco/dict_preprocessedimages_2342_2352.p not found\n",
      "File data_refcoco/dict_preprocessedimages_2872_2882.p not found\n",
      "File data_refcoco/dict_preprocessedimages_3233_3243.p not found\n",
      "File data_refcoco/dict_preprocessedimages_3623_3633.p not found\n",
      "File data_refcoco/dict_preprocessedimages_3673_3683.p not found\n",
      "File data_refcoco/dict_preprocessedimages_4064_4074.p not found\n",
      "File data_refcoco/dict_preprocessedimages_4244_4254.p not found\n",
      "File data_refcoco/dict_preprocessedimages_4334_4344.p not found\n",
      "File data_refcoco/dict_preprocessedimages_4694_4704.p not found\n",
      "File data_refcoco/dict_preprocessedimages_4824_4834.p not found\n",
      "File data_refcoco/dict_preprocessedimages_4864_4874.p not found\n",
      "File data_refcoco/dict_preprocessedimages_5035_5045.p not found\n",
      "File data_refcoco/dict_preprocessedimages_5265_5275.p not found\n",
      "File data_refcoco/dict_preprocessedimages_5775_5785.p not found\n",
      "File data_refcoco/dict_preprocessedimages_6616_6626.p not found\n",
      "File data_refcoco/dict_preprocessedimages_6636_6646.p not found\n",
      "File data_refcoco/dict_preprocessedimages_6846_6856.p not found\n",
      "File data_refcoco/dict_preprocessedimages_6916_6926.p not found\n",
      "File data_refcoco/dict_preprocessedimages_7367_7377.p not found\n",
      "File data_refcoco/dict_preprocessedimages_7667_7677.p not found\n",
      "File data_refcoco/dict_preprocessedimages_9829_9839.p not found\n",
      "File data_refcoco/dict_preprocessedimages_9879_9889.p not found\n"
     ]
    }
   ],
   "source": [
    "file_name = [f'data_refcoco/dict_preprocessedimages_{m}_{M}.p' for m,M in zip(steps[:max_image-1], steps[1:])]\n",
    "\n",
    "target_boxes = []\n",
    "annotation_id = []\n",
    "\n",
    "for file in file_name:\n",
    "    try:\n",
    "        with open(file, 'rb') as handle:\n",
    "            dictionary = pickle.load(handle)\n",
    "\n",
    "        # index of the sample with respect to the whole dataset\n",
    "        idx = int(file.split('_')[3])\n",
    "        for i, key in enumerate(dictionary.keys()):\n",
    "            target_boxes.append(np.array(train_dataset[idx+i]['bbox']).round())\n",
    "            annotation_id.append(train_dataset[idx+i]['ann_id'])\n",
    "    except:\n",
    "        print(f'File {file} not found')\n",
    "\n",
    "\n",
    "# target_boxes = torch.from_numpy(np.array(target_boxes)).type(torch.float16)\n",
    "# # save the torch tensors\n",
    "# torch.save(target_boxes, 'data_refcoco/full/target_boxes.pt')\n",
    "\n",
    "# # save the annotation id\n",
    "# with open('data_refcoco/full/annotation_id.p', 'wb') as handle:\n",
    "#     pickle.dump(annotation_id, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0.,  46., 239., 409.], dtype=torch.float16)\n",
      "1241542\n",
      "tensor([9.5508e-01, 2.0003e-04, 1.0000e+00, 4.1000e+01, 2.4400e+02, 4.2600e+02],\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(target_boxes[0])\n",
    "print(annotation_id[0])\n",
    "\n",
    "print(dictionary_top2[0]['prob-box-map'][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,  46., 239., 409.])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
