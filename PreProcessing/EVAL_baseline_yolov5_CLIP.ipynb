{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'image_emb': tensor([[ 0.0221,  0.5029, -0.3162,  ...,  1.0371,  0.4727, -0.2151],\n",
       "          [-0.1107,  0.2563,  0.1511,  ...,  1.2871, -0.0994, -0.0670],\n",
       "          [ 0.1321,  0.4282, -0.2360,  ...,  1.2500, -0.0720, -0.0065],\n",
       "          [-0.1364,  0.4077, -0.5093,  ...,  0.9365, -0.0548, -0.2174],\n",
       "          [-0.2417,  0.2808, -0.0729,  ...,  1.1367,  0.1469, -0.1929],\n",
       "          [-0.1302,  0.2028, -0.2114,  ...,  1.1064,  0.1814, -0.1130]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1068, -0.1234, -0.4180,  ...,  0.0963, -0.0219, -0.4702],\n",
       "          [ 0.0802, -0.3291, -0.4617,  ..., -0.0427,  0.0495, -0.2220]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.3977e-01, 4.0894e-03, 7.6408e-03, 7.3242e-01, 3.3069e-04, 1.1584e-01],\n",
       "          [9.4434e-01, 3.4393e-02, 1.0658e-02, 2.7239e-05, 5.2512e-05, 1.0658e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.465897   41.905228  233.664261  466.485474    0.942153      0   \n",
       "  1  374.536560    2.168396  639.489319  476.299683    0.928562      0   \n",
       "  2  194.285980  211.953125  559.413086  449.904907    0.921412     63   \n",
       "  3  135.091156   35.643280  309.594635  224.878021    0.907532      0   \n",
       "  4  261.507874  424.752625  363.576904  479.412903    0.880423     39   \n",
       "  5  220.025009  171.333633  288.526001  249.269989    0.662977     73   \n",
       "  6  460.298340  422.794983  486.305847  454.070251    0.497339     74   \n",
       "  7  342.408447  232.942444  399.530701  251.802063    0.260358     67   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2      laptop  \n",
       "  3      person  \n",
       "  4      bottle  \n",
       "  5        book  \n",
       "  6       clock  \n",
       "  7  cell phone  ,\n",
       "  'caption': ['Two woman one in black eatting and the other has a white shirt at the desk',\n",
       "   'Woman in white shirt looking down at laptop computer.'],\n",
       "  'bbox_target': [0.0, 45.95, 238.92, 408.64]},\n",
       " 1: {'image_emb': tensor([[ 0.5679,  0.0089,  0.2900,  ...,  0.2864,  0.0970, -0.0963],\n",
       "          [ 0.2810,  0.0208, -0.2257,  ...,  0.5898, -0.1539,  0.0566],\n",
       "          [ 0.0724,  0.1888, -0.1571,  ...,  0.5596,  0.3728,  0.4895],\n",
       "          ...,\n",
       "          [ 0.2056, -0.1196, -0.2341,  ...,  0.6147,  0.0620, -0.3711],\n",
       "          [ 0.1996,  0.1320, -0.1458,  ...,  0.2063,  0.1212,  0.2397],\n",
       "          [-0.0186, -0.1460,  0.2712,  ...,  0.4597,  0.0930,  0.0163]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1587, -0.1792, -0.1475,  ..., -0.0270,  0.0130,  0.1665],\n",
       "          [ 0.1628, -0.0522, -0.1302,  ..., -0.1181, -0.1256, -0.1210]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.4571e-06, 2.6822e-06, 5.3644e-07, 8.1934e-01, 4.1723e-07, 2.8610e-06,\n",
       "           1.8005e-01, 2.4796e-05, 1.3709e-06, 3.5858e-04, 2.2054e-06, 6.5565e-07,\n",
       "           9.8348e-06],\n",
       "          [5.9605e-08, 5.7817e-06, 3.5167e-06, 9.7314e-01, 5.9605e-08, 9.8348e-06,\n",
       "           2.5940e-02, 1.8311e-04, 1.2636e-05, 4.9782e-04, 5.9605e-08, 2.4021e-05,\n",
       "           1.1921e-07]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   299.054962  293.122955  517.058899  436.838837    0.911613     53   \n",
       "  1   328.957184   28.654274  471.965515  221.086014    0.897057      0   \n",
       "  2   461.738617   37.447796  598.024414  201.943314    0.868819      0   \n",
       "  3   282.126648  484.883087  399.296295  576.439941    0.853208      0   \n",
       "  4   348.130127  231.284668  512.785828  290.330078    0.838102     53   \n",
       "  5    87.255577  289.778687  119.803589  343.245239    0.834708      0   \n",
       "  6   216.169250  456.961853  406.057922  588.402832    0.827640     62   \n",
       "  7    91.659950  354.936829  178.610764  419.010986    0.814967      0   \n",
       "  8   144.041687  272.443970  191.833694  363.741211    0.804489      0   \n",
       "  9    76.958076   14.710036  223.410721  115.252586    0.774321     62   \n",
       "  10  544.833557  188.305405  596.631775  318.717163    0.758317     39   \n",
       "  11  181.687408  341.650085  284.373566  426.196838    0.739012      0   \n",
       "  12   22.195272  297.216675   96.324852  464.983917    0.669525      0   \n",
       "  13  249.913834  376.256439  294.160248  422.901337    0.637061      0   \n",
       "  14  206.328064  300.602020  246.789886  385.164185    0.636190      0   \n",
       "  15   46.376747   65.572548  159.825195  221.613235    0.627676     40   \n",
       "  16   46.721481   65.970291  159.984222  224.827377    0.600198     41   \n",
       "  17   54.039490  280.363159   88.931290  341.495758    0.571721      0   \n",
       "  18  287.960205   90.831856  327.319061  234.616409    0.482497     39   \n",
       "  19  191.715942  151.969910  212.586166  203.367325    0.350219     39   \n",
       "  20  305.340179  213.741684  595.359375  437.336365    0.333516     60   \n",
       "  21  491.600067  562.806152  522.832275  596.693604    0.327772     41   \n",
       "  22  356.480713  200.261902  374.012665  216.241302    0.296043     39   \n",
       "  23  238.032455   15.957078  340.460175  257.608643    0.277139     39   \n",
       "  \n",
       "              name  \n",
       "  0          pizza  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3         person  \n",
       "  4          pizza  \n",
       "  5         person  \n",
       "  6             tv  \n",
       "  7         person  \n",
       "  8         person  \n",
       "  9             tv  \n",
       "  10        bottle  \n",
       "  11        person  \n",
       "  12        person  \n",
       "  13        person  \n",
       "  14        person  \n",
       "  15    wine glass  \n",
       "  16           cup  \n",
       "  17        person  \n",
       "  18        bottle  \n",
       "  19        bottle  \n",
       "  20  dining table  \n",
       "  21           cup  \n",
       "  22        bottle  \n",
       "  23        bottle  ,\n",
       "  'caption': ['A TV with a woman being interviewed on it.',\n",
       "   'A woman with sunglasses on her head on the television being interviewed.'],\n",
       "  'bbox_target': [213.72, 456.51, 192.01, 133.75]},\n",
       " 2: {'image_emb': tensor([[ 0.6802,  0.2761, -0.1012,  ...,  0.7148,  0.0852, -0.1058],\n",
       "          [ 0.2297,  0.3481, -0.0183,  ...,  0.2603,  0.2191, -0.0602],\n",
       "          [ 0.0015,  0.1122, -0.1383,  ...,  0.2839,  0.4263,  0.0415],\n",
       "          [-0.3755,  0.2026, -0.0496,  ...,  0.9600,  0.0368, -0.6147],\n",
       "          [ 0.2566,  0.1342, -0.2225,  ..., -0.1324,  0.2944, -0.2842]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0750, -0.1373,  0.0826,  ...,  0.2922,  0.3640, -0.1192],\n",
       "          [ 0.0513, -0.0047, -0.2168,  ...,  0.0742,  0.2778, -0.4688]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.9340e-03, 1.9932e-03, 9.1791e-05, 3.4447e-03, 9.8535e-01],\n",
       "          [5.4932e-02, 7.0496e-02, 2.2101e-04, 1.6584e-03, 8.7256e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   94.694550   47.839203  450.389618  278.251373    0.943596      0   \n",
       "  1  202.427826  281.896271  325.136017  412.138306    0.931681      0   \n",
       "  2  399.907227   96.583160  523.130737  200.805664    0.927012      0   \n",
       "  3  349.105042  262.010651  458.362488  313.402985    0.919411     36   \n",
       "  4  499.000305  187.050995  524.483521  216.957123    0.553172     36   \n",
       "  5  313.131378  318.812164  340.445343  404.602020    0.447457     36   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2      person  \n",
       "  3  skateboard  \n",
       "  4  skateboard  \n",
       "  5  skateboard  ,\n",
       "  'caption': ['A young boy doing a skateboard trick on a blue board',\n",
       "   'A man jumping with a skateboard'],\n",
       "  'bbox_target': [93.82, 45.79, 348.45, 229.75]},\n",
       " 3: {'image_emb': tensor([[ 0.1421, -0.1337,  0.2283,  ...,  0.3037,  0.2312, -0.2362],\n",
       "          [-0.0244, -0.0076,  0.0780,  ...,  0.6465,  0.1791, -0.1813],\n",
       "          [ 0.0062, -0.0277,  0.1202,  ...,  0.5957,  0.1871, -0.4912],\n",
       "          [ 0.2292, -0.0679,  0.2457,  ...,  0.1010,  0.0508, -0.5435]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0485,  0.1826, -0.2959,  ...,  0.0706, -0.0273, -0.0407],\n",
       "          [ 0.2347, -0.1685,  0.3901,  ...,  0.1309, -0.1024, -0.2712]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.4233e-01, 8.7708e-02, 7.6953e-01, 1.8024e-04],\n",
       "          [1.9849e-01, 3.1714e-01, 4.3335e-01, 5.0964e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class name\n",
       "  0   36.540695  77.663635  313.739502  295.050354    0.922102     19  cow\n",
       "  1  426.472778  78.406494  639.663086  319.686340    0.913203     19  cow\n",
       "  2  345.488708  68.593399  638.241516  184.704437    0.839560     19  cow,\n",
       "  'caption': ['A long-horn, long-haired brown cow looking at the camera.',\n",
       "   'A brown bull in front of feeding tub.'],\n",
       "  'bbox_target': [39.3, 77.54, 278.76, 216.61]},\n",
       " 4: {'image_emb': tensor([[ 0.1162, -0.0345, -0.3708,  ...,  0.8027,  0.3208, -0.1619],\n",
       "          [-0.0636,  0.6792,  0.2715,  ...,  0.7632,  0.0481, -0.3535],\n",
       "          [-0.0041,  0.2585, -0.1273,  ...,  0.5723,  0.2206,  0.0149],\n",
       "          ...,\n",
       "          [-0.2666,  0.0191,  0.0588,  ...,  0.8638,  0.2043, -0.0750],\n",
       "          [ 0.3132,  0.0352, -0.2462,  ...,  0.9390,  0.1836, -0.1857],\n",
       "          [-0.0399,  0.4707, -0.1567,  ..., -0.0393,  0.0349, -0.1685]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0632, -0.3149, -0.2664,  ...,  0.1632,  0.1573, -0.1261],\n",
       "          [ 0.2179, -0.4492, -0.1987,  ...,  0.3921,  0.0501, -0.1121]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.8999e-01, 9.1171e-03, 8.0490e-03, 3.1641e-01, 1.0272e-01, 7.2136e-03,\n",
       "           5.4993e-02, 1.1345e-02],\n",
       "          [8.6427e-06, 7.3242e-02, 1.2535e-02, 1.9133e-05, 2.3246e-06, 1.6260e-01,\n",
       "           2.8312e-05, 7.5146e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   90.723862   56.561100  350.082123  588.616089    0.944087      0   \n",
       "  1  272.639679  150.029434  599.056946  488.306854    0.931944      0   \n",
       "  2  316.063416  422.086700  500.380035  592.750122    0.838925     55   \n",
       "  3  532.127014  411.768677  563.943787  491.639954    0.834900     40   \n",
       "  4  564.568848  404.269958  594.265564  483.798096    0.809647     40   \n",
       "  5  147.417542  414.619507  603.373169  594.464783    0.756713     60   \n",
       "  6  307.853241  112.457283  340.400055  181.648941    0.756211     40   \n",
       "  7  305.969574  473.686127  334.345520  505.152100    0.589031     43   \n",
       "  8  310.519440  421.807770  504.359863  594.650452    0.336370     60   \n",
       "  9   43.651920  284.082886  145.034332  441.416840    0.262032     62   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2          cake  \n",
       "  3    wine glass  \n",
       "  4    wine glass  \n",
       "  5  dining table  \n",
       "  6    wine glass  \n",
       "  7         knife  \n",
       "  8  dining table  \n",
       "  9            tv  ,\n",
       "  'caption': ['The woman in black dress.',\n",
       "   'A lady in a black dress cuts a wedding cake with her new husband.'],\n",
       "  'bbox_target': [273.07, 144.08, 338.93, 355.4]},\n",
       " 5: {'image_emb': tensor([[-0.4526,  0.0178,  0.1376,  ...,  0.7349,  0.1042,  0.4065],\n",
       "          [-0.4297, -0.2115,  0.2583,  ...,  0.7393,  0.0112,  0.0157],\n",
       "          [-0.4460, -0.2373,  0.4060,  ...,  0.4443,  0.0344,  0.2272]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1018,  0.0701, -0.0108,  ...,  0.1821,  0.1146, -0.1681],\n",
       "          [-0.4404,  0.0656, -0.0255,  ..., -0.0093,  0.0443,  0.1725]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0629, 0.4575, 0.4795],\n",
       "          [0.0275, 0.1293, 0.8433]], dtype=torch.float16),\n",
       "  'df_boxes':         xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0   0.597000    0.000000  209.043503  343.114594    0.949645     20  elephant\n",
       "  1  57.583954  104.150421  377.288300  352.648529    0.905509     20  elephant,\n",
       "  'caption': ['A baby elephant standing in front of a larger elephant.',\n",
       "   'a small eliphant'],\n",
       "  'bbox_target': [59.95, 105.6, 316.98, 245.47]},\n",
       " 6: {'image_emb': tensor([[-0.1785,  0.1066, -0.1042,  ...,  1.1016,  0.2379,  0.1140],\n",
       "          [ 0.0345,  0.1121, -0.0844,  ...,  1.1475,  0.1644, -0.3994],\n",
       "          [ 0.2681,  0.1328,  0.2915,  ...,  0.8267,  0.1997, -0.2281],\n",
       "          [ 0.1460, -0.1746,  0.0604,  ...,  0.6328,  0.2505, -0.0950]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0649,  0.2939,  0.0182,  ...,  0.1030,  0.4707, -0.0059],\n",
       "          [ 0.1888,  0.2688, -0.3157,  ..., -0.3264,  0.0082, -0.3442]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.3443e-02, 1.2693e-03, 5.5371e-01, 4.3140e-01],\n",
       "          [2.0233e-02, 9.7510e-01, 7.2575e-04, 3.8033e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  205.488388  228.837891  383.296387  467.300842    0.908799     14    bird\n",
       "  1  394.653809  320.664581  542.906006  478.471130    0.888573     14    bird\n",
       "  2    0.000000    1.146561  439.435608  395.960327    0.853640      0  person\n",
       "  3  407.466553   99.234283  638.850220  475.715088    0.661488      0  person,\n",
       "  'caption': ['A bird that is close to the baby in a pink shirt.',\n",
       "   'A bird standing on the shoulder of a person with its tail touching her face.'],\n",
       "  'bbox_target': [206.85, 228.28, 172.37, 239.88]},\n",
       " 7: {'image_emb': tensor([[-0.2974,  0.2408, -0.2053,  ...,  0.7300, -0.0834,  0.1066],\n",
       "          [-0.1437,  0.3342, -0.0992,  ...,  1.0312, -0.0774,  0.1404],\n",
       "          [-0.4324,  0.2620, -0.0146,  ...,  0.5894,  0.0989,  0.0088],\n",
       "          [-0.3848,  0.0516, -0.2233,  ...,  0.3960,  0.1722, -0.3840],\n",
       "          [-0.2396, -0.0426, -0.1526,  ...,  0.2715,  0.0518,  0.1373]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1661, -0.2502, -0.3000,  ...,  0.6484,  0.2360, -0.4072],\n",
       "          [-0.3132,  0.0632, -0.1477,  ..., -0.0809, -0.0271, -0.1024]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.5461e-04, 1.4126e-05, 2.2232e-02, 9.7559e-01, 2.0676e-03],\n",
       "          [1.3818e-01, 1.1778e-03, 2.0117e-01, 3.7573e-01, 2.8369e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  186.820892   64.259491  406.284058  638.496704    0.926085      0    person\n",
       "  1  125.180588  121.860855  194.458069  305.335510    0.920168      0    person\n",
       "  2   93.386047  258.802856  226.592529  320.212341    0.867353     30      skis\n",
       "  3    1.426605  480.183197  399.464447  640.000000    0.807205     30      skis\n",
       "  4  124.334442  140.533035  153.370544  190.834976    0.261097     24  backpack,\n",
       "  'caption': ['Salomon cross-country skis placed together on the ground next to a skiier.',\n",
       "   \"ski on the ground beside the man's gloves on the ground\"],\n",
       "  'bbox_target': [0.27, 470.52, 369.61, 108.65]},\n",
       " 8: {'image_emb': tensor([[-0.1866,  0.1831, -0.0771,  ...,  1.2822,  0.2340, -0.1131],\n",
       "          [-0.2433,  0.0077, -0.0142,  ...,  1.2695,  0.2498,  0.5850],\n",
       "          [ 0.0086, -0.0196, -0.2556,  ...,  1.4658, -0.0963, -0.0997],\n",
       "          ...,\n",
       "          [-0.0163,  0.3291, -0.0033,  ...,  1.5635,  0.1431, -0.1219],\n",
       "          [-0.1006,  0.0335, -0.4600,  ...,  0.8252,  0.1676, -0.2164],\n",
       "          [-0.2976,  0.5371,  0.1038,  ...,  1.1514,  0.0648,  0.4172]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0935,  0.0646,  0.0818,  ..., -0.1204,  0.1124, -0.2095],\n",
       "          [ 0.1418, -0.2238,  0.2739,  ...,  0.2405,  0.0228,  0.0814]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.9870e-04, 6.8724e-05, 6.5565e-05, 1.6909e-03, 2.1160e-04, 1.7548e-04,\n",
       "           7.6652e-05, 9.9219e-01, 4.1199e-03, 4.0793e-04],\n",
       "          [5.7602e-03, 7.3242e-04, 9.5510e-04, 2.0300e-01, 9.2602e-04, 6.4278e-03,\n",
       "           1.2074e-03, 7.5439e-01, 1.9180e-02, 7.2823e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   124.336639  213.449158  240.204773  376.032104    0.913856      0   \n",
       "  1   441.023987  347.527496  604.887878  478.267395    0.908972      0   \n",
       "  2     0.314087  306.594604  103.520813  477.153137    0.853748      0   \n",
       "  3   373.120605  182.386841  436.929688  223.333130    0.851444      2   \n",
       "  4   124.643921  361.820709  205.528625  429.614838    0.800508     28   \n",
       "  5   358.258820  258.155640  445.325775  295.226990    0.768258     28   \n",
       "  6   386.513550  241.014343  539.318359  392.350647    0.764028      0   \n",
       "  7   109.653885  120.521011  375.380798  246.647156    0.755617      7   \n",
       "  8     0.316711  128.219742   67.077583  266.782654    0.716665      7   \n",
       "  9   433.348877  161.908920  467.914062  186.493042    0.662157      2   \n",
       "  10  371.157806  293.541748  420.932892  333.010437    0.654538     28   \n",
       "  11  336.307098  233.341522  382.662506  264.351593    0.621240     28   \n",
       "  12  372.705658  162.383240  419.804047  187.923370    0.534274      7   \n",
       "  13  372.502991  162.891724  419.791565  189.695984    0.511256      2   \n",
       "  14   98.115967  362.501770  128.484833  419.670715    0.491855     28   \n",
       "  15  335.369873  342.545105  422.363098  430.494629    0.480478     28   \n",
       "  16  249.753769  245.333038  316.899750  297.375153    0.465990     28   \n",
       "  17   39.061996  263.574097  115.281311  355.875244    0.454555      0   \n",
       "  18  278.055664  224.641388  338.426819  257.817291    0.415872     28   \n",
       "  19  231.856644  286.755249  261.868103  333.651855    0.412782     28   \n",
       "  20  372.516602  143.405334  435.542358  178.179749    0.399734      7   \n",
       "  21  326.213165  268.479523  382.761139  334.341949    0.378740     28   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2     person  \n",
       "  3        car  \n",
       "  4   suitcase  \n",
       "  5   suitcase  \n",
       "  6     person  \n",
       "  7      truck  \n",
       "  8      truck  \n",
       "  9        car  \n",
       "  10  suitcase  \n",
       "  11  suitcase  \n",
       "  12     truck  \n",
       "  13       car  \n",
       "  14  suitcase  \n",
       "  15  suitcase  \n",
       "  16  suitcase  \n",
       "  17    person  \n",
       "  18  suitcase  \n",
       "  19  suitcase  \n",
       "  20     truck  \n",
       "  21  suitcase  ,\n",
       "  'caption': ['The white storage container with the lettering Matson.',\n",
       "   'white tractor trailer'],\n",
       "  'bbox_target': [103.93, 118.39, 266.7, 125.12]},\n",
       " 9: {'image_emb': tensor([[-0.0268,  0.2505, -0.1126,  ...,  1.3867, -0.3223, -0.0804],\n",
       "          [-0.1298,  0.1461, -0.3237,  ...,  0.9277, -0.2703, -0.0036],\n",
       "          [-0.0310,  0.3755, -0.3040,  ...,  1.3428, -0.2153, -0.0391],\n",
       "          ...,\n",
       "          [-0.2686,  0.3271, -0.3137,  ...,  0.9336, -0.0956, -0.0393],\n",
       "          [-0.0568,  0.5327, -0.1953,  ...,  1.3701, -0.3135,  0.3225],\n",
       "          [-0.0760,  0.3020,  0.0486,  ...,  1.0625, -0.3018,  0.0938]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-3.7720e-01, -1.9641e-01,  1.4978e-01, -5.2734e-01, -2.1814e-01,\n",
       "            2.3962e-01, -1.3867e-01, -4.9634e-01,  1.9446e-01, -2.2803e-01,\n",
       "            1.2726e-02,  1.0757e-02,  7.1533e-02, -1.9971e-01, -2.0728e-01,\n",
       "           -3.8544e-02,  1.0132e-01,  4.1943e-01, -5.1367e-01,  9.4666e-02,\n",
       "            4.3457e-01,  1.8835e-01, -2.0813e-01, -2.3450e-01,  3.8477e-01,\n",
       "           -1.8384e-01, -3.6646e-01,  2.4841e-01, -3.0469e-01,  1.0541e-01,\n",
       "            7.4097e-02,  2.6782e-01, -1.8494e-01,  8.4656e-02, -5.0977e-01,\n",
       "           -1.7407e-01, -2.2986e-01,  3.1396e-01,  1.9470e-01,  2.7563e-01,\n",
       "            2.9541e-01, -2.0866e-03,  1.7566e-01, -1.6284e-01,  4.5630e-01,\n",
       "           -7.1259e-03,  1.8640e-01, -8.4290e-02, -3.4058e-01, -2.7173e-01,\n",
       "           -1.8323e-01,  1.8542e-01,  1.2488e-01, -3.9868e-01, -5.0732e-01,\n",
       "           -8.2275e-02,  3.6035e-01, -1.6418e-01, -2.3364e-01,  2.6318e-01,\n",
       "            1.1377e-01,  2.9004e-01, -1.8445e-01,  1.5613e-01, -3.0176e-01,\n",
       "            2.9565e-01, -3.2623e-02,  4.3237e-01, -4.2847e-01, -5.3925e-02,\n",
       "           -2.5806e-01,  1.7624e-02, -2.3853e-01, -1.4148e-01,  3.0957e-01,\n",
       "           -1.2488e-01,  2.6047e-02,  4.0039e-01, -4.4403e-02,  3.2135e-02,\n",
       "           -3.3862e-01,  3.5596e-01,  2.0068e-01,  8.5693e-02, -1.3525e-01,\n",
       "            8.0872e-02, -2.3486e-01, -1.7151e-01, -2.3035e-01,  9.8877e-02,\n",
       "           -2.0850e-01, -7.6807e-01, -6.5967e-01,  2.3206e-01,  1.1377e-01,\n",
       "           -1.1414e-01, -1.9775e-01, -1.4429e-01, -1.2683e-01, -3.0957e-01,\n",
       "            1.6235e-01, -9.2957e-02,  9.4788e-02, -3.4058e-01,  5.7953e-02,\n",
       "            2.1631e-01, -2.6294e-01,  3.8330e-01, -2.0190e-01,  1.0779e-01,\n",
       "           -3.6426e-01, -3.1421e-01, -8.3496e-02,  1.4233e-01, -6.0699e-02,\n",
       "            1.9861e-01, -2.2314e-01, -1.1536e-02, -5.1660e-01,  1.3818e-01,\n",
       "            1.6150e-01, -5.3125e-01, -3.9185e-01,  5.8502e-02,  1.7139e-01,\n",
       "           -1.7960e-02,  1.6724e-01, -1.3635e-01,  2.4536e-01,  2.3511e-01,\n",
       "           -1.4685e-01, -8.1787e-02, -1.8188e-01,  3.9746e+00, -3.0737e-01,\n",
       "            3.2642e-01,  2.5444e-03, -5.5469e-01, -6.9214e-02,  2.7124e-01,\n",
       "           -2.5024e-01, -5.7648e-02, -5.1758e-01, -3.7292e-02,  2.1899e-01,\n",
       "            4.9622e-02,  8.3801e-02, -3.2080e-01,  9.1705e-03,  2.0740e-01,\n",
       "           -4.7266e-01, -1.6565e-01,  5.9375e-01,  2.6880e-01,  1.1572e-01,\n",
       "           -3.3569e-01, -1.0864e-01, -4.5996e-01,  2.8735e-01,  2.3303e-01,\n",
       "            1.3586e-01,  3.0689e-03,  4.0497e-02, -2.1094e-01,  1.8848e-01,\n",
       "            2.9541e-02,  5.9998e-02, -3.1860e-01,  6.2622e-02, -2.8296e-01,\n",
       "            7.4463e-02, -2.0703e-01,  2.4585e-01,  4.3237e-01,  5.8533e-02,\n",
       "           -2.3651e-02,  2.3938e-01, -1.1377e-03, -3.4814e-01, -4.7998e-01,\n",
       "           -1.7957e-01,  4.6777e-01, -1.2537e-01, -2.0227e-01,  1.7090e-02,\n",
       "           -7.4646e-02, -1.4282e-01,  3.2202e-01,  3.2373e-01, -5.4413e-02,\n",
       "            3.0127e-01,  3.5229e-01,  3.4131e-01,  2.3865e-02,  1.4046e-02,\n",
       "            4.1943e-01, -2.8638e-01,  3.4741e-01,  1.0944e-01, -1.9019e-01,\n",
       "           -5.2368e-02,  1.3831e-01,  4.4495e-02,  4.2969e-01, -4.0820e-01,\n",
       "            6.2158e-01,  2.4200e-02,  1.3062e-01, -2.9883e-01, -6.6284e-02,\n",
       "            1.2744e-01, -1.3403e-01, -1.6602e-01,  1.7505e-01,  1.8079e-01,\n",
       "           -1.5747e-01, -6.8176e-02, -4.8584e-02,  3.6377e-01,  2.2131e-01,\n",
       "            4.2651e-01, -1.7432e-01, -2.0239e-01,  1.0461e-01, -5.2637e-01,\n",
       "            1.5088e-01,  5.5603e-02, -3.5889e-01, -4.5581e-01,  1.4160e-01,\n",
       "           -6.9482e-01,  8.3252e-02, -1.5076e-01,  2.8760e-01, -9.2041e-02,\n",
       "           -6.2805e-02, -6.4453e-02, -1.1945e-01, -3.4326e-01,  7.6294e-02,\n",
       "           -2.5659e-01,  1.6052e-01,  2.2864e-01,  4.3018e-01, -2.3669e-01,\n",
       "           -2.1179e-02, -2.7393e-01,  1.0626e-01, -1.0109e-02,  2.5562e-01,\n",
       "           -1.5955e-01,  5.7526e-02,  3.5498e-01, -1.8384e-01, -1.1932e-01,\n",
       "            3.9453e-01, -2.1997e-01,  4.5361e-01,  2.6416e-01, -3.2715e-02,\n",
       "            4.9469e-02,  5.6055e-01, -6.5283e-01, -6.1096e-02, -1.8079e-01,\n",
       "            1.1615e-01,  2.9810e-01,  3.0762e-01, -2.4634e-01, -1.4923e-02,\n",
       "            8.1665e-02, -2.1439e-02,  2.7563e-01,  1.2732e-01, -1.8616e-01,\n",
       "           -2.7710e-01,  3.6125e-03,  4.1846e-01,  1.4697e-01,  1.0059e-01,\n",
       "            8.9600e-02, -6.0425e-02,  1.0223e-01,  9.9915e-02,  1.4990e-01,\n",
       "           -1.8005e-01, -5.2612e-02, -1.4417e-01,  2.3962e-01,  4.2310e-01,\n",
       "           -5.9229e-01, -9.5032e-02,  1.1572e-01, -3.6133e-02, -1.0150e-01,\n",
       "            3.7817e-01, -1.9922e-01, -5.7178e-01,  1.1902e-01, -3.0930e-02,\n",
       "           -2.9443e-01, -2.3755e-01, -4.0991e-01, -6.5575e-03, -8.7585e-02,\n",
       "           -3.9502e-01, -3.7231e-02, -7.8674e-02, -1.3257e-01,  3.8501e-01,\n",
       "           -7.9590e-02,  3.7524e-01,  3.9707e+00, -1.6187e-01, -1.3782e-01,\n",
       "            1.3831e-01,  1.1346e-01, -6.3843e-02, -2.9037e-02, -2.0416e-02,\n",
       "           -5.3174e-01,  9.9304e-02, -9.0027e-02, -4.5685e-02, -2.0959e-01,\n",
       "            2.5586e-01,  1.3477e-01, -4.0552e-01,  1.2378e-01, -7.4170e-01,\n",
       "            4.2188e-01, -6.6757e-03,  4.9951e-01,  3.1143e-02,  2.2900e-01,\n",
       "           -2.8641e-02, -5.0732e-01,  1.2048e-01,  5.7910e-01,  9.5459e-02,\n",
       "           -3.9502e-01, -3.4692e-01,  1.9690e-01, -4.6411e-01,  1.2769e-01,\n",
       "           -1.0284e-01,  4.1187e-01, -1.0107e-01,  7.8812e-03,  7.1838e-02,\n",
       "           -1.1517e-01, -1.7932e-01,  9.6207e-03,  1.5649e-01, -2.2656e-01,\n",
       "            3.3203e-02,  2.1240e-01, -2.4609e-01, -3.1860e-02, -2.1851e-01,\n",
       "           -3.0713e-01, -3.0981e-01, -5.5847e-02,  1.8420e-01,  9.1476e-03,\n",
       "            3.6713e-02,  2.3462e-01,  4.1895e-01, -4.7192e-01,  6.8542e-02,\n",
       "            1.5884e-02, -1.8311e-01, -3.9062e-01, -7.8796e-02, -1.2549e-01,\n",
       "            2.4207e-01,  8.4229e-02, -1.0742e-01,  1.9257e-02, -2.9688e-01,\n",
       "            3.7427e-01,  4.1406e-01,  3.5571e-01, -1.2634e-01, -4.7656e-01,\n",
       "           -1.3416e-01,  4.8096e-01, -5.1270e-02, -1.2152e-01, -9.0408e-03,\n",
       "            1.3904e-01,  1.3818e-01,  9.7778e-02, -1.4392e-01,  2.1777e-01,\n",
       "            7.9346e-03,  1.0229e-01,  2.5970e-02,  2.0850e-01,  3.0908e-01,\n",
       "            3.1226e-01, -3.4521e-01, -4.5197e-02,  2.1698e-02, -2.3560e-01,\n",
       "           -1.6968e-01, -5.7129e-02,  4.4861e-03, -4.3243e-02,  2.9956e-01,\n",
       "            1.5381e-01, -4.0161e-02,  2.3218e-01, -1.2085e-01,  9.4971e-02,\n",
       "           -4.3164e-01, -4.4702e-01, -3.0566e-01,  7.3730e-01,  5.3802e-02,\n",
       "            4.1626e-01, -2.0789e-01, -4.9829e-04, -4.2090e-01,  1.1310e-01,\n",
       "           -3.3154e-01,  2.7368e-01,  3.3112e-02,  4.1650e-01,  7.0190e-03,\n",
       "            1.4526e-02,  3.1494e-01, -2.5757e-01, -5.2393e-01, -1.3733e-01,\n",
       "            7.2327e-02, -4.8730e-01, -2.1130e-01, -3.3374e-01, -7.1045e-02,\n",
       "           -4.4281e-02,  3.2544e-01, -3.7939e-01, -4.5947e-01,  1.4685e-01,\n",
       "           -1.2347e-01, -2.2083e-01, -1.9519e-01, -3.7183e-01,  3.4595e-01,\n",
       "           -9.9365e-01,  4.5288e-01, -1.3928e-01,  4.8364e-01,  9.1248e-02,\n",
       "           -1.4099e-01, -1.4575e-01,  2.1240e-01,  3.4106e-01, -1.9836e-01,\n",
       "           -6.9275e-02,  1.2952e-01,  2.4744e-01, -1.8030e-01,  4.6973e-01,\n",
       "           -7.1167e-02, -2.4841e-01,  1.6284e-01, -1.8506e-01,  2.8540e-01,\n",
       "            3.1812e-01, -3.8232e-01, -1.5869e-01, -8.0505e-02,  4.3481e-01,\n",
       "            1.0345e-01,  8.0505e-02,  2.3041e-02, -6.1615e-02,  5.8075e-02,\n",
       "            3.6896e-02, -4.4092e-01,  5.5811e-01,  3.9368e-02, -2.5604e-02,\n",
       "           -2.7441e-01, -2.1460e-01, -1.7676e-01,  2.6172e-01, -2.1936e-01,\n",
       "            2.7686e-01, -1.2659e-01,  1.8387e-02,  1.2422e+00,  1.0895e-01,\n",
       "           -2.9199e-01, -3.7109e-02, -2.7246e-01,  1.2598e-01, -3.7012e-01,\n",
       "           -5.6055e-01, -3.5864e-01, -2.1927e-02,  1.6541e-02,  7.5732e-01,\n",
       "           -3.6548e-01, -2.5122e-01, -7.7332e-02, -7.9529e-02, -4.1168e-02,\n",
       "           -3.0566e-01, -4.3823e-02]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.8566e-04, 4.7754e-01, 7.0267e-03, 3.1185e-03, 5.0830e-01, 1.6093e-05,\n",
       "           3.5896e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    87.073914   13.819944  479.441345  238.783936    0.903854      0   \n",
       "  1   346.348114    0.025129  438.453003  133.031433    0.832687      0   \n",
       "  2   203.138611   72.871330  293.878387  163.319641    0.831366     24   \n",
       "  3   426.494354   42.173862  456.574188   97.274208    0.822025     28   \n",
       "  4   314.018188   38.883568  393.959534  142.749237    0.775561     13   \n",
       "  5    87.153992  151.317520  192.270203  245.138718    0.759843     24   \n",
       "  6   331.667236  210.155563  456.661407  322.950348    0.685705     28   \n",
       "  7    24.001122   71.420982  313.251556  344.812195    0.571278     13   \n",
       "  8   384.621033    0.000000  433.052917   56.464432    0.483614      0   \n",
       "  9   423.058899   78.364395  447.307190  109.169724    0.413946     26   \n",
       "  10  423.013824  102.983238  467.642731  132.173111    0.361160     26   \n",
       "  11   80.048119   34.806961  475.782776  233.944199    0.344467     13   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2   backpack  \n",
       "  3   suitcase  \n",
       "  4      bench  \n",
       "  5   backpack  \n",
       "  6   suitcase  \n",
       "  7      bench  \n",
       "  8     person  \n",
       "  9    handbag  \n",
       "  10   handbag  \n",
       "  11     bench  ,\n",
       "  'caption': ['Park bench the man is sitting on.'],\n",
       "  'bbox_target': [25.34, 60.39, 314.19, 283.78]},\n",
       " 10: {'image_emb': tensor([[ 0.3000,  0.1666,  0.0147,  ...,  1.0713,  0.2991,  0.1561],\n",
       "          [ 0.0261,  0.1998, -0.0461,  ...,  1.1592,  0.3755,  0.3982],\n",
       "          [-0.1638,  0.2430,  0.1316,  ...,  1.0439,  0.0231,  0.1774],\n",
       "          ...,\n",
       "          [ 0.1764, -0.3201, -0.3640,  ...,  0.8164, -0.1722, -0.0435],\n",
       "          [ 0.1565, -0.3149, -0.3315,  ...,  1.1016,  0.0566, -0.2454],\n",
       "          [ 0.1193,  0.4988, -0.0362,  ...,  1.0420,  0.3271,  0.3311]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2803, -0.0514, -0.1287,  ...,  0.1617,  0.2830, -0.1743],\n",
       "          [ 0.2932,  0.0639, -0.4282,  ..., -0.0042,  0.2341,  0.0390]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9182e-03, 9.2139e-01, 2.3127e-05, 4.9651e-05, 3.2401e-04, 4.9400e-04,\n",
       "           6.7810e-02],\n",
       "          [1.6663e-02, 9.2383e-01, 5.6124e-04, 1.0711e-04, 6.8760e-04, 1.3971e-04,\n",
       "           5.8136e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    0.000000   48.658253  181.313187  250.050385    0.941479      0  person\n",
       "  1  304.999115   49.933861  497.806030  223.290009    0.935710      0  person\n",
       "  2  106.345642   60.390377  325.834045  245.077133    0.884813      0  person\n",
       "  3  320.133575  213.416031  369.324982  235.731583    0.871000     45    bowl\n",
       "  4  477.122833   99.018433  491.583099  140.974716    0.771350     39  bottle\n",
       "  5  164.549667  207.265823  192.842789  220.014984    0.712241     65  remote\n",
       "  6   70.833801  109.826187  496.262848  245.295197    0.506738     57   couch\n",
       "  7  463.683838  150.633453  491.458374  160.289597    0.484723     65  remote\n",
       "  8   45.542995  137.586975   63.271267  185.045197    0.434826     39  bottle\n",
       "  9  411.725464  198.237305  451.948883  212.264221    0.257041     73    book,\n",
       "  'caption': ['A woman in a flowered shirt.', 'woman in red shirt'],\n",
       "  'bbox_target': [307.77, 50.91, 179.06, 172.77]},\n",
       " 11: {'image_emb': tensor([[-0.4744, -0.0127,  0.3130,  ...,  0.7480,  0.1943, -0.1360],\n",
       "          [-0.1630,  0.2900,  0.2460,  ...,  0.4250,  0.1122, -0.1235],\n",
       "          [ 0.2135,  0.0493, -0.3835,  ...,  0.5723, -0.1647,  0.1709],\n",
       "          [ 0.3030,  0.1111, -0.2400,  ...,  0.8125, -0.0189, -0.0116],\n",
       "          [-0.2993, -0.1035,  0.3455,  ...,  0.5264,  0.2983, -0.2192]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1570, -0.1064,  0.4180,  ..., -0.1740,  0.1116, -0.2272],\n",
       "          [-0.1914, -0.1969,  0.6875,  ..., -0.1687,  0.2430, -0.4502]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.3359e-01, 1.8597e-03, 1.7881e-06, 1.4305e-06, 6.4514e-02],\n",
       "          [9.4580e-01, 9.9277e-04, 5.9605e-08, 0.0000e+00, 5.3345e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    50.957172   15.511465  200.971359  247.157959    0.923477     72   \n",
       "  1   251.148392    3.587103  496.667480  246.246429    0.904756     72   \n",
       "  2   444.626678  117.743423  483.795319  235.282303    0.835450     39   \n",
       "  3   428.428284  149.677536  457.137024  231.252640    0.734533     39   \n",
       "  4   274.408234   89.087723  332.356384  118.189194    0.634047     45   \n",
       "  5   349.534271   72.907379  361.816315  108.204773    0.514891     39   \n",
       "  6   334.473663  193.648026  350.360046  238.450745    0.476382     46   \n",
       "  7    64.372986   20.922588   84.954231   46.332909    0.445671      0   \n",
       "  8   411.453339  224.260468  422.901642  247.390076    0.419144     39   \n",
       "  9    37.702740  148.000336   49.981857  161.012894    0.377056     41   \n",
       "  10  105.815460   82.618904  123.026062  105.206825    0.360290      0   \n",
       "  11  442.005402  118.480659  461.127625  142.109085    0.316676     39   \n",
       "  12  377.560242  158.496948  405.410522  176.782745    0.278497     45   \n",
       "  \n",
       "              name  \n",
       "  0   refrigerator  \n",
       "  1   refrigerator  \n",
       "  2         bottle  \n",
       "  3         bottle  \n",
       "  4           bowl  \n",
       "  5         bottle  \n",
       "  6         banana  \n",
       "  7         person  \n",
       "  8         bottle  \n",
       "  9            cup  \n",
       "  10        person  \n",
       "  11        bottle  \n",
       "  12          bowl  ,\n",
       "  'caption': ['A fridge with lots of magnets on it',\n",
       "   'White fridge with lots of magnets and pictures on the front.'],\n",
       "  'bbox_target': [55.5, 8.07, 142.14, 237.84]},\n",
       " 12: {'image_emb': tensor([[-0.2900,  0.4329, -0.0087,  ...,  0.3960, -0.1974,  0.2517],\n",
       "          [-0.4568,  0.1982,  0.0223,  ...,  0.5308, -0.1499,  0.1359],\n",
       "          [-0.4812,  0.4741, -0.0479,  ...,  0.8765, -0.2363,  0.3835],\n",
       "          [-0.4329,  0.1340,  0.0308,  ...,  0.3379, -0.1045,  0.3176]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0736, -0.2834,  0.1421,  ...,  0.0094, -0.1827, -0.0591],\n",
       "          [-0.1613, -0.0384,  0.2893,  ...,  0.7402, -0.6279,  0.0942]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5854, 0.1458, 0.1481, 0.1208],\n",
       "          [0.4731, 0.1824, 0.2871, 0.0574]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   55.690277  304.846741  292.724396  602.104248    0.901891      3   \n",
       "  1   63.053741  224.537582  289.861237  497.173462    0.898805      0   \n",
       "  2  287.924835  264.163574  426.000000  412.402039    0.870648      3   \n",
       "  3  358.827454  234.165588  425.371704  358.088196    0.544580      0   \n",
       "  4   14.537155  596.902771   51.119858  639.677551    0.295146     10   \n",
       "  \n",
       "             name  \n",
       "  0    motorcycle  \n",
       "  1        person  \n",
       "  2    motorcycle  \n",
       "  3        person  \n",
       "  4  fire hydrant  ,\n",
       "  'caption': ['A blue bike which come first', 'the blue dirt bike'],\n",
       "  'bbox_target': [57.53, 309.21, 257.44, 284.77]},\n",
       " 13: {'image_emb': tensor([[-0.1279,  0.6006, -0.2559,  ...,  1.0859, -0.2644,  0.1147],\n",
       "          [-0.3752,  0.3694,  0.0322,  ...,  1.2930, -0.1736, -0.2739],\n",
       "          [ 0.0665,  0.2979, -0.4790,  ...,  1.0029,  0.0909,  0.0664],\n",
       "          [-0.1392,  0.3938,  0.0434,  ...,  0.6270,  0.2267,  0.1276],\n",
       "          [-0.2023,  0.7056, -0.3376,  ...,  0.7461,  0.2212, -0.0354],\n",
       "          [-0.4688,  0.2935,  0.0374,  ...,  0.5547,  0.0930,  0.1350]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2212, -0.2566,  0.2238,  ...,  0.1124, -0.1149,  0.0707],\n",
       "          [-0.1433, -0.1644,  0.1616,  ...,  0.9360, -0.0023, -0.1349]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0203, 0.5752, 0.0045, 0.3081, 0.0829, 0.0089],\n",
       "          [0.0018, 0.0204, 0.0066, 0.0120, 0.8945, 0.0648]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  400.191437  417.129852  479.482483  545.637573    0.901306      0   \n",
       "  1  324.453857  538.104614  479.665039  589.277710    0.887371     31   \n",
       "  2  162.177429  144.889389  297.121948  534.439941    0.868324      0   \n",
       "  3   20.006058  296.570068  143.151505  392.887329    0.865395     31   \n",
       "  4  169.455933  233.265320  365.432861  489.020569    0.856273     31   \n",
       "  5  167.069199  245.405548  230.950806  363.772186    0.633355     24   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1  snowboard  \n",
       "  2     person  \n",
       "  3  snowboard  \n",
       "  4  snowboard  \n",
       "  5   backpack  ,\n",
       "  'caption': ['blue and white striped board',\n",
       "   'The blue and white striped snowboard.'],\n",
       "  'bbox_target': [172.45, 234.43, 193.22, 251.45]},\n",
       " 14: {'image_emb': tensor([[-0.2888, -0.0168, -0.0108,  ...,  0.6792, -0.0331,  0.1198],\n",
       "          [-0.0916,  0.0190, -0.0820,  ...,  0.7412,  0.1180, -0.0974],\n",
       "          [-0.2323,  0.0143, -0.1954,  ...,  0.7925,  0.0538,  0.2355],\n",
       "          [-0.3132, -0.0493, -0.2825,  ...,  0.5405, -0.3918,  0.2732]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0458, -0.3110,  0.0246,  ...,  0.1296,  0.0367,  0.3416],\n",
       "          [-0.2539, -0.3340,  0.1599,  ...,  0.2278,  0.1704,  0.3225]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0407, 0.4812, 0.1770, 0.3010],\n",
       "          [0.2001, 0.6982, 0.0834, 0.0180]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  389.189484  232.389801  483.193756  324.079926    0.923128     22  zebra\n",
       "  1  160.483932  230.231628  364.695557  343.158813    0.906915     22  zebra\n",
       "  2   51.615173  246.239990  187.901352  352.119690    0.787658     22  zebra\n",
       "  3  113.646042  245.323425  225.990768  340.113647    0.465966     22  zebra,\n",
       "  'caption': ['A zebra standing in front of 3 other zebras.',\n",
       "   'A zebra standing facing to the left in front of the others, the closest to the camera.'],\n",
       "  'bbox_target': [154.72, 228.92, 210.03, 117.78]},\n",
       " 15: {'image_emb': tensor([[ 0.7188,  0.2678, -0.1838,  ...,  0.6406, -0.0425, -0.3442],\n",
       "          [ 0.2942,  0.5615, -0.1072,  ...,  1.3438,  0.0109, -0.1085],\n",
       "          [ 0.1528,  0.4172, -0.1281,  ...,  0.5645, -0.2766,  0.2859],\n",
       "          ...,\n",
       "          [-0.2275,  0.3406,  0.0027,  ...,  0.3372, -0.0276,  0.1812],\n",
       "          [ 0.1562, -0.2654, -0.5972,  ...,  1.0869,  0.0494,  0.0742],\n",
       "          [ 0.2002, -0.0130,  0.4006,  ...,  0.6289, -0.2081,  0.0994]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2910,  0.4238, -0.2739,  ...,  0.0980, -0.1183,  0.3782],\n",
       "          [ 0.1343,  0.1908, -0.3894,  ..., -0.3442, -0.0190, -0.0793]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.8477e-01, 5.4283e-03, 4.5538e-05, 3.5950e-02, 1.1921e-07, 2.1458e-06,\n",
       "           7.3792e-02],\n",
       "          [2.7344e-01, 4.9011e-02, 5.1074e-01, 1.2512e-01, 2.8366e-02, 2.0993e-04,\n",
       "           1.2993e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  187.522385   85.935699  381.940430  416.999146    0.948007      0    person\n",
       "  1  479.490173   42.271530  639.141907  419.790955    0.924955      0    person\n",
       "  2    0.089592   79.297699  203.498901  418.686646    0.910210      0    person\n",
       "  3  337.480164   53.662186  496.454285  422.000610    0.904169      0    person\n",
       "  4  101.939285  202.612061  150.322174  335.539490    0.891033     27       tie\n",
       "  5   11.369110  100.511841  201.465790  161.253143    0.804228     25  umbrella\n",
       "  6  548.181824  316.337128  639.819763  369.158173    0.331948      0    person\n",
       "  7  317.617981  165.889191  340.048218  200.904694    0.270595      0    person,\n",
       "  'caption': ['A woman standing between 2 men with green shirt and large belt buckle.',\n",
       "   'A lady standing next to a man wearing a blue suit and tie.'],\n",
       "  'bbox_target': [187.19, 85.06, 193.88, 318.04]},\n",
       " 16: {'image_emb': tensor([[-0.2703,  0.3911, -0.1824,  ...,  1.2129,  0.0487, -0.2235],\n",
       "          [ 0.1152,  0.4863, -0.1511,  ...,  0.9575, -0.2859,  0.1183],\n",
       "          [ 0.1737,  0.4292,  0.0259,  ...,  1.2100, -0.1732,  0.0457],\n",
       "          [-0.0050,  0.3191, -0.2467,  ...,  0.7764, -0.1205,  0.1942],\n",
       "          [-0.4158,  0.2729, -0.1309,  ...,  1.0117, -0.1256,  0.2152],\n",
       "          [ 0.0716,  0.3186, -0.2169,  ...,  0.7529, -0.2722, -0.1764]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0137,  0.1556,  0.0887,  ...,  0.2466, -0.4578,  0.0035],\n",
       "          [-0.1300, -0.1818,  0.0069,  ...,  0.0739, -0.6479,  0.0469]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0495, 0.6616, 0.1753, 0.0815, 0.0174, 0.0146],\n",
       "          [0.1647, 0.3711, 0.1597, 0.1344, 0.0790, 0.0909]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  name\n",
       "  0  434.975159  107.385712  575.655090  425.884155    0.930969     42  fork\n",
       "  1  308.177002  172.697968  522.914307  424.402649    0.895500     55  cake\n",
       "  2   67.734856    0.761139  276.290833  183.551910    0.895322     55  cake\n",
       "  3   52.228012  144.519714  324.084839  406.362488    0.893551     55  cake\n",
       "  4  270.242584    4.642616  534.394165  179.639709    0.888805     55  cake,\n",
       "  'caption': ['A slice of cheese cake at the top of the fork.',\n",
       "   \"A slice of cake near the fork's prongs.\"],\n",
       "  'bbox_target': [272.19, 3.85, 257.76, 176.97]},\n",
       " 17: {'image_emb': tensor([[ 0.0178,  0.5190, -0.0991,  ...,  1.2002, -0.5649,  0.1636],\n",
       "          [-0.2732,  0.2551, -0.3940,  ...,  0.7485, -0.2861,  0.0918],\n",
       "          [-0.1070,  0.3203,  0.1680,  ...,  0.6982, -0.5107, -0.0691],\n",
       "          [ 0.0201,  0.3328,  0.4126,  ...,  0.4729, -0.1595,  0.1410]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0268, -0.2791, -0.0371,  ..., -0.0096, -0.1548, -0.1239],\n",
       "          [-0.0702,  0.0185,  0.2118,  ...,  0.2123,  0.0043, -0.0064]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.8906e-01, 1.5540e-01, 1.7441e-02, 3.8086e-02],\n",
       "          [9.9219e-01, 6.5804e-03, 9.4795e-04, 4.5490e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  150.278198  264.649628  250.815125  410.597717    0.931330      0   \n",
       "  1  226.830887  269.056091  299.591064  379.802673    0.898078      0   \n",
       "  2  215.532623  140.940857  290.707947  294.466614    0.850037      0   \n",
       "  3  270.416321  105.761414  279.485657  188.621887    0.388856     34   \n",
       "  4  512.395691  297.241028  520.451721  305.900024    0.267528     32   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2        person  \n",
       "  3  baseball bat  \n",
       "  4   sports ball  ,\n",
       "  'caption': ['An umpire watching the pitch.',\n",
       "   'An umpire in the ready position, wearing all black as usual.'],\n",
       "  'bbox_target': [150.88, 264.89, 100.08, 142.64]},\n",
       " 18: {'image_emb': tensor([[ 0.0189,  0.3374,  0.6650,  ...,  0.7480, -0.0989,  0.0683],\n",
       "          [-0.3547,  0.2180,  0.5894,  ...,  0.5957, -0.1586, -0.1238],\n",
       "          [-0.2920,  0.2051, -0.0823,  ...,  1.6680, -0.2661, -0.0063],\n",
       "          [-0.3838,  0.1727,  0.4448,  ...,  0.6875, -0.2959, -0.2036],\n",
       "          [-0.0285,  0.1437, -0.1898,  ...,  0.8535,  0.0107, -0.0184],\n",
       "          [-0.3323,  0.4099,  0.5239,  ...,  0.6084, -0.4294, -0.0869]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0279,  0.4475, -0.1420,  ...,  0.6694, -0.3442, -0.2656],\n",
       "          [-0.0948,  0.1500,  0.2469,  ...,  0.4180, -0.3953, -0.1399]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0625, 0.0687, 0.0065, 0.0417, 0.6025, 0.2183],\n",
       "          [0.0068, 0.1521, 0.0230, 0.0373, 0.6714, 0.1096]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  257.624329  278.895691  637.490173  602.367310    0.914766      0   \n",
       "  1    0.000000  109.759689  337.544922  610.453064    0.906288      0   \n",
       "  2  383.154541    0.250107  534.095276   80.112709    0.782225      0   \n",
       "  3   31.858368    0.624115  632.219604  374.800262    0.751454      0   \n",
       "  4  235.890320  473.230103  347.568115  576.785034    0.717477     35   \n",
       "  5  544.622375  152.087341  631.637634  213.727020    0.648429     35   \n",
       "  6  310.735687  114.901527  346.186432  147.874908    0.559109     32   \n",
       "  7   29.900360    0.103642  187.612839   84.555511    0.533995      0   \n",
       "  8  172.184479    0.113270  252.550476   83.436584    0.250859      0   \n",
       "  \n",
       "               name  \n",
       "  0          person  \n",
       "  1          person  \n",
       "  2          person  \n",
       "  3          person  \n",
       "  4  baseball glove  \n",
       "  5  baseball glove  \n",
       "  6     sports ball  \n",
       "  7          person  \n",
       "  8          person  ,\n",
       "  'caption': ['The baseball player making the motion of being safe in baseball.',\n",
       "   'A referee officiating a baseball game'],\n",
       "  'bbox_target': [38.83, 5.59, 582.47, 375.37]},\n",
       " 19: {'image_emb': tensor([[ 0.0887, -0.1223,  0.1163,  ...,  0.1840,  0.2668,  0.2184],\n",
       "          [ 0.3513,  0.3147,  0.0690,  ...,  0.1520,  0.3997,  0.4001],\n",
       "          [ 0.0928,  0.3489,  0.2334,  ...,  0.4519,  0.3340, -0.0597],\n",
       "          ...,\n",
       "          [ 0.5815,  0.0398, -0.3005,  ...,  0.6753,  0.0215,  0.1669],\n",
       "          [ 0.3330,  0.1433,  0.1637,  ...,  0.3782,  0.1566,  0.3135],\n",
       "          [ 0.2900,  0.2637,  0.0345,  ...,  1.0469, -0.0341,  0.2610]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1923, -0.0722,  0.0605,  ..., -0.2512, -0.2450, -0.1625],\n",
       "          [ 0.2316,  0.0900,  0.0507,  ..., -0.0200, -0.5625, -0.1067]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0337, 0.0815, 0.0022, 0.0043, 0.2529, 0.0052, 0.4438, 0.1765],\n",
       "          [0.0104, 0.0583, 0.0131, 0.0200, 0.4995, 0.0430, 0.0754, 0.2803]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     0.187241   87.251114  325.816650  475.571045    0.942268      0   \n",
       "  1   334.414551   65.761810  626.675964  310.173248    0.937338      0   \n",
       "  2   456.994507  256.617065  527.161438  347.595642    0.931045     41   \n",
       "  3   349.233154  292.471252  426.181152  393.794312    0.917122     41   \n",
       "  4   355.467041  209.596436  462.460205  275.976868    0.859128     53   \n",
       "  5   181.213669  273.240356  299.373749  357.932434    0.845426     53   \n",
       "  6   146.787582  186.092896  638.649170  472.685913    0.816979     60   \n",
       "  7   321.435516   23.850410  442.962799  181.494873    0.681119      2   \n",
       "  8     0.032101    1.127853  122.416885  118.218338    0.639839      2   \n",
       "  9   110.506241   19.566826  438.823242  182.401001    0.628446      2   \n",
       "  10  329.765625    0.365860  619.060181  116.812454    0.515884      2   \n",
       "  11  249.470154  262.157104  530.229980  419.125916    0.365926     60   \n",
       "  12  535.680237  124.660248  639.222595  305.827606    0.350215     13   \n",
       "  13  325.040588    0.089714  498.608887   74.251343    0.338149      2   \n",
       "  14  321.401398  162.452881  329.567596  184.952087    0.278950      0   \n",
       "  15  349.556488  289.450043  426.217621  394.802155    0.262241     60   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2            cup  \n",
       "  3            cup  \n",
       "  4          pizza  \n",
       "  5          pizza  \n",
       "  6   dining table  \n",
       "  7            car  \n",
       "  8            car  \n",
       "  9            car  \n",
       "  10           car  \n",
       "  11  dining table  \n",
       "  12         bench  \n",
       "  13           car  \n",
       "  14        person  \n",
       "  15  dining table  ,\n",
       "  'caption': ['A car outside to the right of the red box.',\n",
       "   'Car parked outside the window.'],\n",
       "  'bbox_target': [110.02, 22.56, 336.61, 164.51]},\n",
       " 20: {'image_emb': tensor([[-0.4321,  0.2659, -0.1937,  ...,  1.0811, -0.1215,  0.0127],\n",
       "          [-0.5996,  0.3101, -0.3442,  ...,  1.1807, -0.0328, -0.0251],\n",
       "          [-0.1143, -0.0469,  0.2300,  ...,  0.4565, -0.4126,  0.1088]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3430, -0.1742, -0.0030,  ..., -0.1694, -0.0042,  0.2208],\n",
       "          [-0.2390, -0.1119,  0.0024,  ...,  0.3333, -0.1027,  0.0659]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.7656, 0.1967, 0.0375],\n",
       "          [0.3992, 0.0202, 0.5806]], dtype=torch.float16),\n",
       "  'df_boxes':         xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  98.022888  265.531311  336.583435  412.917358    0.928432     28  suitcase\n",
       "  1  29.705597  345.764709  402.270294  512.681274    0.887638     28  suitcase,\n",
       "  'caption': ['pink case', 'A pink suitcase cake.'],\n",
       "  'bbox_target': [97.8, 261.75, 241.62, 153.89]},\n",
       " 21: {'image_emb': tensor([[ 0.1466,  0.5493, -0.0077,  ...,  0.6226,  0.1306,  0.2881],\n",
       "          [-0.4219,  0.3994,  0.0086,  ...,  0.9595,  0.3555, -0.3069],\n",
       "          [ 0.2598,  0.5303, -0.6313,  ...,  0.6357, -0.1614,  0.4287],\n",
       "          [ 0.6548,  0.1427, -0.4326,  ...,  0.4021,  0.1649,  0.1785],\n",
       "          [ 0.6299,  0.1494, -0.4229,  ...,  0.4805,  0.1857,  0.1738]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0925,  0.2688,  0.0787,  ...,  0.5010, -0.0160, -0.5625],\n",
       "          [ 0.1087,  0.0994, -0.0096,  ...,  0.3516, -0.3784, -0.0949]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9463e-01, 1.1644e-03, 0.0000e+00, 2.5043e-03, 1.6947e-03],\n",
       "          [9.6191e-01, 5.9929e-03, 3.2961e-05, 2.0279e-02, 1.1734e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  246.188232  414.573242  391.063538  595.987732    0.918031     41   \n",
       "  1  286.403412   80.002190  443.370422  239.599426    0.885116     41   \n",
       "  2   12.609634  123.646317  220.427063  339.542114    0.854763     54   \n",
       "  3    3.159378    2.575653  479.875854  631.959595    0.799754     60   \n",
       "  4  196.510193  527.985901  269.956482  557.023743    0.640617     44   \n",
       "  \n",
       "             name  \n",
       "  0           cup  \n",
       "  1           cup  \n",
       "  2         donut  \n",
       "  3  dining table  \n",
       "  4         spoon  ,\n",
       "  'caption': ['A foam-free espresso in a white cup with a spoon atop a white saucer.',\n",
       "   'a cup of coffiee in the table'],\n",
       "  'bbox_target': [247.37, 420.95, 136.63, 178.34]},\n",
       " 22: {'image_emb': tensor([[-0.6860, -0.0690,  0.0120,  ...,  0.6133, -0.5747, -0.0946],\n",
       "          [-0.6411, -0.1989, -0.0315,  ...,  0.4580, -0.4946,  0.1104],\n",
       "          [-0.4746, -0.0903,  0.0543,  ...,  0.8232, -0.3755, -0.0126],\n",
       "          [-0.4683,  0.0406, -0.1321,  ...,  0.7466, -0.5557, -0.0751]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0305, -0.4719, -0.0552,  ...,  0.2482, -0.2400, -0.1851],\n",
       "          [ 0.2085, -0.0037, -0.3806,  ..., -0.2180, -0.4753, -0.6362]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1578, 0.4426, 0.2444, 0.1554],\n",
       "          [0.2318, 0.2145, 0.3425, 0.2111]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class  \\\n",
       "  0  291.627686  40.249466  638.536194  416.352905    0.937465     77   \n",
       "  1    0.493622   1.588806  453.708038  416.885620    0.907881     77   \n",
       "  2  432.082764  20.223022  640.000000  329.340576    0.905850     77   \n",
       "  \n",
       "           name  \n",
       "  0  teddy bear  \n",
       "  1  teddy bear  \n",
       "  2  teddy bear  ,\n",
       "  'caption': ['A sandal colour teddy bear in between the other two teddys.',\n",
       "   'a stuffed bear between two others'],\n",
       "  'bbox_target': [280.83, 38.55, 359.17, 378.84]},\n",
       " 23: {'image_emb': tensor([[ 0.0129,  0.7021, -0.4133,  ...,  0.7520,  0.4805,  0.7065],\n",
       "          [-0.0388,  0.3521,  0.0190,  ...,  0.7734,  0.3582, -0.0209],\n",
       "          [ 0.0147,  0.1780, -0.1344,  ...,  1.0400,  0.2551, -0.0141],\n",
       "          ...,\n",
       "          [-0.3447,  0.2520, -0.0254,  ...,  0.6147,  0.0659, -0.2117],\n",
       "          [ 0.1231,  0.1277, -0.3110,  ...,  0.8374,  0.2009, -0.1359],\n",
       "          [-0.2274,  0.3689, -0.0899,  ...,  0.7485,  0.5205,  0.4907]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0605,  0.1724, -0.1063,  ..., -0.4919,  0.1311, -0.5034],\n",
       "          [-0.0547,  0.3525, -0.2595,  ..., -0.1393,  0.1398,  0.0403]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.7705e-01, 4.1008e-05, 4.7684e-07, 6.5565e-07, 1.1921e-06, 1.0729e-06,\n",
       "           2.2980e-02],\n",
       "          [2.0645e-02, 4.6240e-01, 3.1531e-05, 1.9073e-06, 9.8038e-04, 3.8743e-06,\n",
       "           5.1611e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   160.980270  118.245621  637.883667  423.414612    0.938287      0   \n",
       "  1    81.147362  274.940399  249.140747  424.192810    0.895440     63   \n",
       "  2   158.353348  338.454834  201.989059  364.616577    0.843374     64   \n",
       "  3   107.442154  208.222626  144.890335  273.974762    0.801787     56   \n",
       "  4   481.445618  116.166122  534.757812  174.917175    0.728206     63   \n",
       "  5   141.284775  159.392853  164.679367  192.576874    0.708817     56   \n",
       "  6   129.175354  179.635223  156.129364  224.466156    0.686355     56   \n",
       "  7   486.798950  164.445343  565.011597  264.537201    0.647967     56   \n",
       "  8   244.782547  119.965118  371.890869  336.404877    0.643934      0   \n",
       "  9   622.949097  120.453873  640.000000  159.153198    0.596839     63   \n",
       "  10    7.962212  328.926636   87.423149  425.235657    0.588797     56   \n",
       "  11   82.158455  275.902405  245.699249  426.201233    0.487889     64   \n",
       "  12  326.387756  110.954315  399.385559  155.634674    0.453257     24   \n",
       "  13  197.234299  212.591339  259.343628  268.663605    0.417442     24   \n",
       "  14  134.078827  288.636871  214.879852  326.518158    0.417217     73   \n",
       "  15  181.724747  146.277664  236.988632  165.399323    0.404608     63   \n",
       "  16  179.831802  146.168198  238.017197  165.458923    0.404000     73   \n",
       "  17  143.645477  374.061279  199.216278  425.787170    0.384655     66   \n",
       "  18   11.799721  312.975952   49.624027  393.588501    0.373985     26   \n",
       "  19  325.455139  110.989563  401.694580  155.369080    0.309463     26   \n",
       "  20  621.329651  282.204346  640.000000  333.069275    0.295044     56   \n",
       "  21  197.070740  212.507141  259.077393  269.689697    0.275247     26   \n",
       "  22    0.003860  147.416840  120.775314  242.940948    0.257611     60   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         laptop  \n",
       "  2          mouse  \n",
       "  3          chair  \n",
       "  4         laptop  \n",
       "  5          chair  \n",
       "  6          chair  \n",
       "  7          chair  \n",
       "  8         person  \n",
       "  9         laptop  \n",
       "  10         chair  \n",
       "  11         mouse  \n",
       "  12      backpack  \n",
       "  13      backpack  \n",
       "  14          book  \n",
       "  15        laptop  \n",
       "  16          book  \n",
       "  17      keyboard  \n",
       "  18       handbag  \n",
       "  19       handbag  \n",
       "  20         chair  \n",
       "  21       handbag  \n",
       "  22  dining table  ,\n",
       "  'caption': ['A young man wearing a white shirt, sticking out his tongue.',\n",
       "   'A MAN SHOWING HIS TONGUE AND OPERATING LAPTOP'],\n",
       "  'bbox_target': [165.52, 120.58, 474.48, 306.27]},\n",
       " 24: {'image_emb': tensor([[-0.0500,  0.3635,  0.0313,  ...,  0.9639, -0.0891,  0.2010],\n",
       "          [-0.0647,  0.1952, -0.2236,  ...,  1.0898,  0.1814,  0.2913],\n",
       "          [-0.2274,  0.2500, -0.2629,  ...,  1.2295,  0.2297, -0.0201],\n",
       "          [-0.0094,  0.2285, -0.0133,  ...,  1.0918,  0.1489,  0.0992],\n",
       "          [-0.3147,  0.5078, -0.1652,  ...,  0.7002,  0.0280,  0.4309]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3352,  0.0659, -0.5376,  ..., -0.0893,  0.3267,  0.2576],\n",
       "          [ 0.3770,  0.2471, -0.5225,  ...,  0.0137,  0.2241,  0.1698]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.9422e-03, 3.0273e-01, 6.7188e-01, 1.2497e-02, 4.8943e-03],\n",
       "          [1.2054e-03, 4.5929e-02, 9.5215e-01, 7.5436e-04, 1.3733e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  190.814148   15.378723  413.752441  365.094055    0.928922      0   \n",
       "  1  259.462860   38.939545  542.708374  421.663208    0.896316      0   \n",
       "  2  522.964539   24.966766  639.676819  190.578705    0.846021      0   \n",
       "  3  133.240952  120.800522  592.191284  422.052368    0.788768      3   \n",
       "  4  471.319397  159.017975  640.000000  358.025970    0.485398     24   \n",
       "  5  423.040833  130.799774  545.545166  305.186920    0.430325     26   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2      person  \n",
       "  3  motorcycle  \n",
       "  4    backpack  \n",
       "  5     handbag  ,\n",
       "  'caption': ['The woman in the yellow shirt.', 'A woman in a yellow shirt.'],\n",
       "  'bbox_target': [521.63, 33.0, 118.37, 158.76]},\n",
       " 25: {'image_emb': tensor([[-0.0532,  0.0094, -0.4509,  ...,  0.6426,  0.1924, -0.0184],\n",
       "          [-0.2394,  0.3857, -0.2490,  ...,  0.7988,  0.1896,  0.0685],\n",
       "          [-0.4163,  0.5425, -0.2004,  ...,  1.1904,  0.4307, -0.0597],\n",
       "          ...,\n",
       "          [-0.0890, -0.2012, -0.4275,  ...,  1.1396,  0.2803,  0.0688],\n",
       "          [-0.1885,  0.6118, -0.2045,  ...,  1.2852,  0.3816, -0.3955],\n",
       "          [ 0.0221,  0.0757, -0.5566,  ...,  0.3916,  0.3289,  0.3013]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1730,  0.3562,  0.1375,  ...,  0.0426, -0.0239,  0.1589],\n",
       "          [-0.0533,  0.1428, -0.3147,  ...,  0.1708, -0.2041, -0.2042]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0729e-05, 2.1172e-04, 1.3440e-01, 6.6528e-02, 3.6573e-04, 5.7554e-04,\n",
       "           4.5896e-06, 1.3769e-05, 7.9785e-01, 2.0981e-05],\n",
       "          [7.5722e-03, 9.6680e-02, 9.3689e-02, 4.3579e-02, 4.2236e-02, 2.2964e-02,\n",
       "           2.0264e-02, 3.5977e-04, 6.6064e-01, 1.2100e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    57.934311   28.992157  315.905457  362.432281    0.922310      0   \n",
       "  1   228.945221  291.157104  297.663483  455.383362    0.916228     40   \n",
       "  2   346.932892  403.830597  417.143311  539.632812    0.907975     40   \n",
       "  3    91.633331  381.757355  149.210510  460.251129    0.897466     41   \n",
       "  4   284.464203  474.416656  368.057159  636.107422    0.890324     43   \n",
       "  5    83.869125  265.125488  145.213608  409.351562    0.865586     40   \n",
       "  6     1.205399  294.032837  421.620361  638.578125    0.828676     60   \n",
       "  7   163.658691    0.314850  239.596069  138.963928    0.828096      0   \n",
       "  8     5.832077  269.589233   93.191544  453.001709    0.821785     41   \n",
       "  9   270.992737  123.758888  369.178162  265.433533    0.677109     56   \n",
       "  10    1.421799  499.873047   38.229889  640.000000    0.670201     42   \n",
       "  11  393.857910  114.272911  425.678345  223.340332    0.658415     56   \n",
       "  12  130.473572  316.302612  216.580994  359.565735    0.633591     53   \n",
       "  13  319.292725  368.580444  414.305481  420.978516    0.569450     48   \n",
       "  14  264.288513   97.858231  300.259155  115.188538    0.515835     56   \n",
       "  15  239.461823  101.138397  296.318390  243.932678    0.506590     56   \n",
       "  16  309.389709  415.807678  362.487305  457.274780    0.425168     53   \n",
       "  17  268.459656  117.772552  387.368286  259.512695    0.320571     60   \n",
       "  18  290.520599  116.928223  308.563141  143.706085    0.307454     40   \n",
       "  19  250.163208  247.255005  298.615112  371.894714    0.296268     39   \n",
       "  20  367.482849  116.918510  404.789551  239.836212    0.294695     56   \n",
       "  21    1.626625  474.045044   33.853500  581.402222    0.289043     42   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1     wine glass  \n",
       "  2     wine glass  \n",
       "  3            cup  \n",
       "  4          knife  \n",
       "  5     wine glass  \n",
       "  6   dining table  \n",
       "  7         person  \n",
       "  8            cup  \n",
       "  9          chair  \n",
       "  10          fork  \n",
       "  11         chair  \n",
       "  12         pizza  \n",
       "  13      sandwich  \n",
       "  14         chair  \n",
       "  15         chair  \n",
       "  16         pizza  \n",
       "  17  dining table  \n",
       "  18    wine glass  \n",
       "  19        bottle  \n",
       "  20         chair  \n",
       "  21          fork  ,\n",
       "  'caption': ['A clear water pitcher.', 'the water carafe on the table'],\n",
       "  'bbox_target': [4.73, 266.59, 85.12, 186.01]},\n",
       " 26: {'image_emb': tensor([[ 0.2834,  0.3704, -0.2147,  ...,  0.7915,  0.4409,  0.0031],\n",
       "          [ 0.3372,  0.4856, -0.0607,  ...,  1.1338,  0.2308,  0.2075],\n",
       "          [ 0.2622,  0.0939,  0.0539,  ...,  0.8135,  0.0568,  0.3752],\n",
       "          [ 0.0490,  0.2189, -0.0265,  ...,  0.8638,  0.0820,  0.1208],\n",
       "          [ 0.1263,  0.3335, -0.2686,  ...,  0.4661,  0.0049,  0.2338]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0546, -0.2233, -0.1011,  ..., -0.0600, -0.1713, -0.0266],\n",
       "          [-0.0239, -0.0906, -0.2976,  ..., -0.1027, -0.1655, -0.3655]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.8063e-03, 2.1332e-02, 1.1414e-02, 7.9334e-05, 9.6533e-01],\n",
       "          [7.7942e-02, 2.6108e-02, 8.6475e-01, 8.2169e-03, 2.3041e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   376.087646  110.637726  621.618774  470.559937    0.946477      0  person\n",
       "  1    81.197403  200.007996  215.811935  349.128540    0.932628      0  person\n",
       "  2   188.275421  170.659836  377.796356  474.197815    0.923929      0  person\n",
       "  3     0.112675  202.701233  126.473831  346.205078    0.920591      0  person\n",
       "  4   426.524017  316.498352  452.086273  357.789490    0.629693     41     cup\n",
       "  5   400.530273  355.813904  638.262695  474.236145    0.621289      0  person\n",
       "  6     0.601807  338.899170  214.009521  433.177307    0.508011     57   couch\n",
       "  7    12.320355  164.472824   29.029697  197.926971    0.441135     73    book\n",
       "  8    20.064705  297.806366   42.031998  311.605865    0.429699     65  remote\n",
       "  9   106.890381  181.085754  124.826767  225.455200    0.376314     73    book\n",
       "  10    2.115816  161.662766   17.463171  194.812317    0.355151     73    book\n",
       "  11  135.338242  300.459320  161.391495  315.735626    0.318598     65  remote\n",
       "  12   83.016708  179.929626  103.938797  224.084656    0.305697     73    book\n",
       "  13  107.315437  131.414352  130.990295  175.870300    0.305177     73    book\n",
       "  14    0.338177  215.319519   12.509277  243.697632    0.274937     73    book\n",
       "  15   99.003685  230.488708  113.903526  253.262207    0.270949     73    book,\n",
       "  'caption': ['a man is playing guitar while the others enjoying the same',\n",
       "   'A man holding a guitar'],\n",
       "  'bbox_target': [190.52, 169.44, 177.21, 308.56]},\n",
       " 27: {'image_emb': tensor([[-0.5034,  0.1309, -0.2319,  ...,  0.6084,  0.2952, -0.1158],\n",
       "          [-0.3862,  0.3000, -0.0169,  ...,  0.6997,  0.2859,  0.0797],\n",
       "          [-0.1696,  0.1573,  0.0844,  ...,  1.0254,  0.1219, -0.1824],\n",
       "          ...,\n",
       "          [-0.3704, -0.0423, -0.1422,  ...,  0.7104,  0.2219, -0.2103],\n",
       "          [-0.1937, -0.0146, -0.1530,  ...,  0.7314,  0.2583, -0.2507],\n",
       "          [-0.2041, -0.0788, -0.2761,  ...,  0.5493,  0.2017, -0.0314]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1754,  0.1487,  0.1917,  ...,  0.4441,  0.0191, -0.3486],\n",
       "          [ 0.1372,  0.0412,  0.0492,  ...,  0.5234, -0.3149, -0.0106]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1074, 0.1915, 0.0715, 0.0162, 0.0141, 0.5718, 0.0276],\n",
       "          [0.0103, 0.0031, 0.0157, 0.0056, 0.0187, 0.9429, 0.0039]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class    name\n",
       "  0   31.819458  57.556381  330.461731  410.623962    0.937500     17   horse\n",
       "  1  123.237862  31.899811  275.713348  196.108307    0.889479      0  person\n",
       "  2  413.191986  26.830780  503.112823  121.694122    0.889029      0  person\n",
       "  3  288.271667  16.774635  392.130249  131.272705    0.864000      0  person\n",
       "  4  206.334106  82.717377  452.099792  389.458282    0.841154     17   horse\n",
       "  5  377.310974  60.544037  568.389954  360.438019    0.828659     17   horse,\n",
       "  'caption': ['A horse being ridden by number 6 jockey.',\n",
       "   'Horse which has a blue color rein.'],\n",
       "  'bbox_target': [222.92, 86.87, 233.42, 301.94]},\n",
       " 28: {'image_emb': tensor([[-0.0583,  0.3811,  0.0243,  ...,  1.3135,  0.1949,  0.0646],\n",
       "          [ 0.1094,  0.4944, -0.2450,  ...,  1.1816, -0.0642, -0.0246],\n",
       "          [-0.1505,  0.3828,  0.1819,  ...,  1.3652,  0.3076, -0.2515],\n",
       "          ...,\n",
       "          [-0.0015, -0.0407, -0.2177,  ...,  0.6216,  0.2172, -0.2253],\n",
       "          [ 0.5293,  0.0199, -0.2632,  ...,  0.7573, -0.1122,  0.1991],\n",
       "          [ 0.1442,  0.0345, -0.0168,  ...,  0.9741,  0.3638, -0.0776]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1667, -0.0367, -0.0108,  ..., -0.4009,  0.3203,  0.1388],\n",
       "          [ 0.1619, -0.1589, -0.2203,  ..., -0.1899,  0.1997, -0.1222]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0511, 0.1388, 0.0324, 0.0220, 0.1015, 0.0731, 0.1098, 0.0234, 0.1410,\n",
       "           0.0241, 0.2184, 0.0645],\n",
       "          [0.0006, 0.2336, 0.0072, 0.0021, 0.0770, 0.0326, 0.0022, 0.0013, 0.0079,\n",
       "           0.0035, 0.2527, 0.3792]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   316.166931  189.014740  403.770691  273.867157    0.914722     65   \n",
       "  1    68.454689  359.581360  130.653290  478.621033    0.881485     39   \n",
       "  2   187.914154  409.138550  254.902374  478.357056    0.868133     41   \n",
       "  3   362.340210   43.061554  463.324219  251.277740    0.857276      0   \n",
       "  4   304.822052  273.701324  348.800385  407.822113    0.849349     39   \n",
       "  5     0.461910    0.919701  110.175339  138.452972    0.816229     62   \n",
       "  6   135.029022  215.264832  162.670837  283.790161    0.773043     39   \n",
       "  7   295.064941  231.115692  319.184814  302.431000    0.768182     39   \n",
       "  8   348.201538  275.250183  378.529846  354.378418    0.765197     39   \n",
       "  9   198.351349    0.269836  467.865265  284.483215    0.742256      0   \n",
       "  10  317.661804   14.570908  638.106873  473.403442    0.718595      0   \n",
       "  11  446.878357   49.173996  584.691101  236.883118    0.675676      0   \n",
       "  12    1.817368  263.367554  388.649658  475.787476    0.639283     60   \n",
       "  13  161.214554  247.327332  228.066147  289.954224    0.481354     45   \n",
       "  14    0.168339  363.653564   23.427370  408.929749    0.295075     45   \n",
       "  15  100.610428  316.250092  150.730362  364.558990    0.255440     45   \n",
       "  \n",
       "              name  \n",
       "  0         remote  \n",
       "  1         bottle  \n",
       "  2            cup  \n",
       "  3         person  \n",
       "  4         bottle  \n",
       "  5             tv  \n",
       "  6         bottle  \n",
       "  7         bottle  \n",
       "  8         bottle  \n",
       "  9         person  \n",
       "  10        person  \n",
       "  11        person  \n",
       "  12  dining table  \n",
       "  13          bowl  \n",
       "  14          bowl  \n",
       "  15          bowl  ,\n",
       "  'caption': ['the woman in motion',\n",
       "   'The person standing up in the black dress.'],\n",
       "  'bbox_target': [156.4, 0.0, 305.26, 281.2]},\n",
       " 29: {'image_emb': tensor([[ 0.0633, -0.2057,  0.0087,  ...,  0.8433,  0.1116,  0.2800],\n",
       "          [ 0.0942,  0.0421, -0.0189,  ...,  0.5708,  0.0634, -0.0775],\n",
       "          [-0.2465,  0.3115, -0.2010,  ...,  0.7476,  0.1917, -0.4736],\n",
       "          [ 0.1587, -0.0945,  0.1301,  ...,  0.8228,  0.2915,  0.0171],\n",
       "          [ 0.1633, -0.1217,  0.1537,  ...,  0.6646,  0.1494,  0.1416]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3010,  0.0562, -0.3401,  ..., -0.4819,  0.1552, -0.4812],\n",
       "          [ 0.1802, -0.1135, -0.1646,  ..., -0.3481,  0.1993, -0.2585]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.2616e-04, 2.1362e-01, 1.0307e-02, 3.5767e-01, 4.1821e-01],\n",
       "          [6.5184e-04, 6.0205e-01, 2.5098e-01, 1.1133e-01, 3.5034e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  332.163269  122.750290  600.267517  475.726196    0.913301      0  person\n",
       "  1  183.423523  223.617340  419.904419  415.168121    0.906378     14    bird\n",
       "  2   37.124924  225.967529  172.644073  476.798096    0.896167     14    bird\n",
       "  3   18.080948   33.845337  406.083923  473.535461    0.874114      0  person,\n",
       "  'caption': ['The bird on the right',\n",
       "   \"bird seen in profile on the woman's shoulder\"],\n",
       "  'bbox_target': [183.26, 224.2, 237.48, 191.28]},\n",
       " 30: {'image_emb': tensor([[-0.2900, -0.1566,  0.2006,  ...,  1.0049,  0.1068, -0.0324],\n",
       "          [-0.1549,  0.0156,  0.0849,  ...,  0.8530,  0.1052, -0.1656],\n",
       "          [-0.3064, -0.0287,  0.0897,  ...,  0.7529,  0.0612, -0.4514],\n",
       "          ...,\n",
       "          [-0.0490,  0.5186, -0.3643,  ...,  1.5400,  0.0943, -0.2383],\n",
       "          [ 0.0122,  0.6646, -0.1163,  ...,  1.1689, -0.0640, -0.5156],\n",
       "          [-0.2426,  0.0164,  0.0421,  ...,  0.6235, -0.2998, -0.1738]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0257, -0.2361, -0.0155,  ...,  0.0197, -0.2983, -0.1804],\n",
       "          [-0.1132, -0.1469, -0.1860,  ..., -0.0284, -0.3040, -0.1174]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.1064e-01, 7.0610e-03, 4.3274e-02, 4.1747e-04, 3.1519e-04, 4.5156e-04,\n",
       "           2.2006e-04, 2.2113e-05, 3.7598e-02],\n",
       "          [7.9785e-01, 1.4526e-01, 4.3640e-02, 2.4915e-05, 2.7180e-04, 2.6345e-04,\n",
       "           1.4102e-04, 5.1081e-05, 1.2306e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   270.704987  217.064423  557.687744  639.277710    0.943613     17   horse\n",
       "  1    67.827934  214.411774  321.010010  540.948486    0.928008     17   horse\n",
       "  2   160.894928  153.176025  278.968109  398.502991    0.918051      0  person\n",
       "  3   510.410614  279.412048  574.012695  508.292236    0.914895      0  person\n",
       "  4     0.108242  279.124084   75.237869  396.992554    0.903630      2     car\n",
       "  5     0.233902  237.192352  131.632339  310.249481    0.860225      2     car\n",
       "  6   601.980530  264.720367  640.000000  316.910614    0.763927      2     car\n",
       "  7   112.714630  242.127213  166.537872  281.437683    0.736901      2     car\n",
       "  8   379.081543  260.470978  411.430176  284.359833    0.615071      0  person\n",
       "  9   550.394714  258.261780  614.626892  330.434448    0.578733      2     car\n",
       "  10  426.093658  255.915543  448.308258  274.282715    0.307016      0  person,\n",
       "  'caption': ['A white horse',\n",
       "   'The white horse has the word Autism written on its back end.'],\n",
       "  'bbox_target': [271.63, 212.94, 285.71, 427.06]},\n",
       " 31: {'image_emb': tensor([[-0.3477,  0.3630,  0.1847,  ...,  1.0420,  0.2959,  0.6875],\n",
       "          [-0.4810,  0.5728, -0.4707,  ...,  0.9038,  0.2255,  0.3875],\n",
       "          [-0.4067,  0.6553, -0.2666,  ...,  1.0156,  0.0299,  0.3589],\n",
       "          [-0.1935,  0.5503, -0.3857,  ...,  0.8481, -0.1993,  0.5513]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.4458,  0.5342, -0.0170,  ...,  0.1051,  0.1343,  0.2338],\n",
       "          [-0.0504,  0.4543, -0.0889,  ...,  0.0439,  0.4829,  0.1429]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.7710e-01, 3.6926e-02, 1.6028e-01, 5.2588e-01],\n",
       "          [1.0000e+00, 2.7418e-06, 1.1921e-07, 0.0000e+00]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  163.799454   14.129066  264.378662  164.209747    0.908147     41      cup\n",
       "  1  209.896774  228.738495  376.998291  435.341248    0.843625     52  hot dog\n",
       "  2   59.173466  317.989716  246.242905  440.862946    0.797649     52  hot dog\n",
       "  3  396.943726   44.912674  483.136322  143.269775    0.657873     52  hot dog\n",
       "  4   16.472546   74.924835   47.803410  221.223587    0.440727     41      cup\n",
       "  5   87.701538   15.132880  175.625656  148.417969    0.373434     39   bottle,\n",
       "  'caption': ['Drink cup in the middle of a bunch of fries',\n",
       "   'A dimly lit to-go cup with a picture of a classic coca-cola on it.'],\n",
       "  'bbox_target': [164.27, 14.07, 101.24, 142.81]},\n",
       " 32: {'image_emb': tensor([[-0.2842,  0.1467,  0.0235,  ...,  0.9087, -0.0818, -0.1829],\n",
       "          [-0.2354,  0.3601,  0.0162,  ...,  0.7153,  0.1818, -0.1228],\n",
       "          [ 0.1191,  0.2283,  0.0607,  ..., -0.0205,  0.0363, -0.1821],\n",
       "          [-0.3691,  0.1052,  0.0106,  ...,  0.0247, -0.1113, -0.0952]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2148, -0.2087, -0.0991,  ..., -0.4099, -0.2201, -0.1700],\n",
       "          [-0.1731, -0.3445,  0.1115,  ..., -0.0460, -0.1635, -0.1691]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3862, 0.5444, 0.0284, 0.0407],\n",
       "          [0.0970, 0.8247, 0.0636, 0.0149]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  311.406342  223.564270  585.457397  359.767151    0.906926     13   bench\n",
       "  1   64.562286  211.673218  312.386871  336.582031    0.848336     13   bench\n",
       "  2   68.509842  153.039612  216.443436  317.773193    0.823457      0  person,\n",
       "  'caption': ['The bench to the far left.',\n",
       "   'A wooden bench with a person sitting on it.'],\n",
       "  'bbox_target': [59.89, 203.44, 245.28, 148.31]},\n",
       " 33: {'image_emb': tensor([[ 0.2502,  0.1315,  0.2729,  ..., -0.1760, -0.0765, -0.1650],\n",
       "          [ 0.0825,  0.3120,  0.0065,  ...,  0.6406, -0.0374,  0.0491],\n",
       "          [ 0.2451, -0.0895, -0.2148,  ...,  0.8940, -0.0108, -0.2222],\n",
       "          [ 0.1125,  0.4258,  0.1678,  ...,  0.8511, -0.1697, -0.1539],\n",
       "          [ 0.2634,  0.0158,  0.1566,  ...,  0.2067, -0.0961, -0.2820]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2096,  0.1604, -0.1467,  ...,  0.3997, -0.5122, -0.1721],\n",
       "          [-0.3359,  0.0296, -0.3000,  ...,  0.0981, -0.1356, -0.1838]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.6519e-04, 9.6631e-01, 2.1696e-05, 3.3051e-02, 2.4617e-05],\n",
       "          [4.1366e-05, 9.8584e-01, 5.6565e-05, 1.4061e-02, 2.3246e-06]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class  \\\n",
       "  0  442.169556   9.314880  639.430542  263.283020    0.923266      0   \n",
       "  1   34.772476  84.742523  204.003494  265.299530    0.922679      0   \n",
       "  2  440.601257  58.996643  506.620422  103.072998    0.878179     35   \n",
       "  3   40.210091  16.908844  206.409668  156.674347    0.841379     34   \n",
       "  \n",
       "               name  \n",
       "  0          person  \n",
       "  1          person  \n",
       "  2  baseball glove  \n",
       "  3    baseball bat  ,\n",
       "  'caption': ['A baseball player in green preparing to hit the ball',\n",
       "   'A man in a green shirt readies himself to hit the ball.'],\n",
       "  'bbox_target': [36.13, 83.11, 163.82, 180.67]},\n",
       " 34: {'image_emb': tensor([[-0.4268,  0.2433, -0.1344,  ...,  0.7378,  0.1156, -0.0159],\n",
       "          [-0.2466,  0.4470, -0.3860,  ...,  1.1289,  0.0745,  0.0682],\n",
       "          [-0.5195,  0.1222, -0.0883,  ...,  0.7866,  0.2274, -0.2964],\n",
       "          [ 0.4302,  0.4448, -0.1404,  ...,  0.9619,  0.0949,  0.1069],\n",
       "          [-0.5796, -0.0166,  0.0786,  ...,  0.2988,  0.1842,  0.2411]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0152,  0.1309,  0.1247,  ..., -0.0295,  0.0903, -0.1571],\n",
       "          [ 0.2144, -0.0916,  0.0850,  ...,  0.6323,  0.2123, -0.1034]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.7900e-01, 2.4261e-03, 0.0000e+00, 6.1393e-06, 1.8509e-02],\n",
       "          [8.2373e-01, 4.0985e-02, 3.5477e-04, 2.7905e-03, 1.3232e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  249.277924  112.719666  417.259369  358.869751    0.942006      0   \n",
       "  1  330.738098   30.667953  400.994873  126.514786    0.910441      0   \n",
       "  2  200.057190  307.461212  429.526184  377.044891    0.899871     31   \n",
       "  3  425.667542    0.000000  464.837585   37.780792    0.807389      0   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2  snowboard  \n",
       "  3     person  ,\n",
       "  'caption': ['A person snowboarding in a red, white and black jacket, bending over',\n",
       "   'snow boarder bending'],\n",
       "  'bbox_target': [250.44, 114.19, 166.96, 247.56]},\n",
       " 35: {'image_emb': tensor([[-0.2499,  0.3132, -0.3835,  ...,  1.4717,  0.0658,  0.0829],\n",
       "          [-0.2283,  0.5127,  0.0174,  ...,  1.1777,  0.1061,  0.0456],\n",
       "          [-0.6123,  0.3948, -0.2174,  ...,  1.2734, -0.0023, -0.2295],\n",
       "          ...,\n",
       "          [-0.4934,  0.3835, -0.1733,  ...,  1.2344,  0.3352, -0.1349],\n",
       "          [-0.5757,  0.4236, -0.2805,  ...,  1.3994,  0.1041,  0.0802],\n",
       "          [-0.2883,  0.7275, -0.0929,  ...,  0.7373, -0.5151, -0.0912]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3042, -0.0140, -0.2952,  ..., -0.1486, -0.2368, -0.0476],\n",
       "          [-0.5288,  0.1970, -0.1709,  ..., -0.3416,  0.1027, -0.6899]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.4586e-03, 1.3864e-04, 1.6699e-01, 4.8866e-03, 1.0291e-01, 8.0566e-03,\n",
       "           2.5916e-04, 7.1436e-01],\n",
       "          [1.5621e-03, 1.7262e-04, 1.2610e-01, 1.5383e-03, 8.4814e-01, 1.7433e-03,\n",
       "           1.8448e-02, 2.2373e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    0.804337  304.889801  348.581360  434.320068    0.922882     42    fork\n",
       "  1  168.350250    0.000000  338.514709  201.637329    0.922349     41     cup\n",
       "  2  141.354370  101.548828  321.286102  227.955353    0.890229     44   spoon\n",
       "  3  415.248993  222.107544  639.474976  296.397156    0.863263     42    fork\n",
       "  4  332.934631    5.353714  470.084595  326.470276    0.851149     44   spoon\n",
       "  5    3.250519    0.537682  638.238647  167.940918    0.832007      0  person\n",
       "  6   53.902267  186.377899  252.968887  283.138092    0.755063     44   spoon\n",
       "  7  190.744904  182.789551  615.147705  476.368347    0.659593     45    bowl\n",
       "  8    0.542793    5.520874  133.808716  353.816589    0.281505      0  person,\n",
       "  'caption': ['A man on the right end of the table is putting spoon in the food',\n",
       "   'The biggest spoon showing the back of the spoon.'],\n",
       "  'bbox_target': [332.92, 1.42, 133.81, 322.08]},\n",
       " 36: {'image_emb': tensor([[-3.7378e-01, -1.9006e-01,  4.2664e-02,  ...,  7.4707e-01,\n",
       "            6.0547e-02, -3.3154e-01],\n",
       "          [-2.2302e-01,  3.0566e-01,  3.1543e-01,  ...,  1.1777e+00,\n",
       "            4.0674e-01, -1.0785e-01],\n",
       "          [-1.7651e-01,  5.9113e-02, -3.6792e-01,  ...,  7.9297e-01,\n",
       "           -2.3340e-01, -3.7183e-01],\n",
       "          ...,\n",
       "          [ 7.6904e-02, -2.3926e-02, -4.4824e-01,  ...,  8.7158e-01,\n",
       "           -8.6365e-03, -2.9858e-01],\n",
       "          [ 1.0095e-01,  1.3611e-01, -2.7783e-01,  ...,  8.9697e-01,\n",
       "            2.1130e-01,  2.3352e-01],\n",
       "          [-4.9146e-01,  2.1052e-04, -2.9028e-01,  ...,  3.7036e-01,\n",
       "            3.0933e-01, -2.2217e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3118,  0.0844, -0.2729,  ...,  0.0279,  0.0007, -0.2532],\n",
       "          [ 0.0467, -0.2581, -0.0062,  ..., -0.0043, -0.2566, -0.1508]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.0664e-01, 5.1155e-03, 7.4425e-03, 4.8999e-01, 6.0129e-04, 1.7578e-02,\n",
       "           1.5266e-02, 5.1155e-03, 9.8572e-03, 6.5386e-05, 4.8790e-03, 2.9144e-03,\n",
       "           1.2459e-02, 1.2201e-01],\n",
       "          [2.7026e-01, 1.9205e-04, 8.2612e-05, 6.9043e-01, 2.0444e-04, 1.5106e-03,\n",
       "           1.1225e-03, 3.1161e-04, 1.4191e-03, 2.6584e-05, 2.3384e-03, 1.1578e-03,\n",
       "           9.4509e-04, 2.9861e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   125.987030  150.870026  405.969025  421.346802    0.942049     17   \n",
       "  1   562.769531  199.453857  639.882812  300.682495    0.912805      2   \n",
       "  2   361.234375  117.805405  428.566345  211.897888    0.893895      0   \n",
       "  3    83.107170  158.615814  235.437897  421.558716    0.876870     17   \n",
       "  4    42.092564   59.175354   63.919140  115.248169    0.836596      9   \n",
       "  5   602.237854  173.819366  623.265686  205.544098    0.813995      0   \n",
       "  6    59.290382  174.832489   91.563026  285.076263    0.806791      0   \n",
       "  7   541.103882  103.491226  568.598755  144.831345    0.781468      9   \n",
       "  8   307.595703  166.972595  328.176025  206.433716    0.770784      0   \n",
       "  9     4.924431  166.647430   27.298737  263.764679    0.770469      0   \n",
       "  10  594.481201  105.452789  615.715088  145.294128    0.770462      9   \n",
       "  11  574.770996  101.929993  594.269287  134.315063    0.707567      9   \n",
       "  12  327.538300  172.631287  354.557648  201.457397    0.705175      0   \n",
       "  13   41.947670  171.330200   73.117523  276.235229    0.654002      0   \n",
       "  14  486.222198  175.062561  515.371155  220.238892    0.526116      0   \n",
       "  15  539.146667  177.015045  557.765076  205.505646    0.461416      0   \n",
       "  16  427.772461  174.141663  471.093506  212.435242    0.454502      0   \n",
       "  17  501.812683  178.348358  533.453674  233.109100    0.421541      0   \n",
       "  18  529.383301  183.388672  551.588257  205.647461    0.361653      0   \n",
       "  19  432.275269  176.641235  455.079590  200.002747    0.357938      0   \n",
       "  20   18.273672  172.316833   43.272278  269.022400    0.279302      0   \n",
       "  21  429.371094  178.743713  441.554871  197.516968    0.276085      0   \n",
       "  22  482.801025  173.542511  497.126831  213.900299    0.263524      0   \n",
       "  \n",
       "               name  \n",
       "  0           horse  \n",
       "  1             car  \n",
       "  2          person  \n",
       "  3           horse  \n",
       "  4   traffic light  \n",
       "  5          person  \n",
       "  6          person  \n",
       "  7   traffic light  \n",
       "  8          person  \n",
       "  9          person  \n",
       "  10  traffic light  \n",
       "  11  traffic light  \n",
       "  12         person  \n",
       "  13         person  \n",
       "  14         person  \n",
       "  15         person  \n",
       "  16         person  \n",
       "  17         person  \n",
       "  18         person  \n",
       "  19         person  \n",
       "  20         person  \n",
       "  21         person  \n",
       "  22         person  ,\n",
       "  'caption': ['the white horse in the right hand picture', 'A white horse.'],\n",
       "  'bbox_target': [83.71, 157.95, 148.31, 261.14]},\n",
       " 37: {'image_emb': tensor([[-0.1810,  0.5884, -0.4961,  ...,  0.8726,  0.0564, -0.2124],\n",
       "          [-0.1311,  0.4146, -0.5229,  ...,  1.0195, -0.0586, -0.1318],\n",
       "          [-0.7964,  0.8848, -0.1593,  ...,  1.0869,  0.0094, -0.1829],\n",
       "          ...,\n",
       "          [-0.3345,  0.5239, -0.0532,  ...,  1.2207,  0.1323, -0.2800],\n",
       "          [-0.0775,  0.4534, -0.5947,  ...,  0.8237, -0.0777,  0.0323],\n",
       "          [-0.2915,  0.4529, -0.4995,  ...,  0.8945, -0.3655,  0.1437]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0873, -0.3225,  0.2084,  ..., -0.0255, -0.1404, -0.3269],\n",
       "          [-0.4551,  0.3740,  0.2299,  ..., -0.2822, -0.4333, -0.4182]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.5117e-03, 3.5973e-03, 1.1725e-01, 8.6784e-04, 2.4200e-02, 8.1396e-01,\n",
       "           1.6727e-03, 3.2043e-02],\n",
       "          [3.9577e-05, 1.8206e-03, 5.3192e-02, 7.6354e-05, 5.1355e-04, 9.4287e-01,\n",
       "           9.5940e-04, 2.9731e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.638382  178.100708  223.052383  439.514587    0.895470     54   \n",
       "  1  303.419678   26.797836  450.385254  150.925705    0.886699     54   \n",
       "  2  416.716522  111.860565  639.780640  281.505035    0.870864     45   \n",
       "  3  135.588470   19.045341  305.342133  212.670837    0.853808     54   \n",
       "  4  197.467041   77.040634  446.878052  426.871948    0.850639     54   \n",
       "  5  340.547882  248.003479  638.838745  476.762634    0.845893     45   \n",
       "  6    0.146820   62.395630  193.791321  218.714844    0.810117     54   \n",
       "  7  323.937683    0.234818  417.335571   29.543137    0.522667     40   \n",
       "  \n",
       "           name  \n",
       "  0       donut  \n",
       "  1       donut  \n",
       "  2        bowl  \n",
       "  3       donut  \n",
       "  4       donut  \n",
       "  5        bowl  \n",
       "  6       donut  \n",
       "  7  wine glass  ,\n",
       "  'caption': ['Some kind of white dip for the snacks.',\n",
       "   'a bowl with white cream'],\n",
       "  'bbox_target': [340.54, 252.43, 299.46, 221.62]},\n",
       " 38: {'image_emb': tensor([[ 0.0844,  0.2458, -0.6099,  ...,  0.4099,  0.3015, -0.2057],\n",
       "          [ 0.1566,  0.1005, -0.2168,  ...,  0.9609,  0.1483, -0.4966],\n",
       "          [ 0.2534, -0.2407, -0.3196,  ...,  0.8154, -0.1111, -0.3428],\n",
       "          [-0.2023, -0.2842, -0.1255,  ...,  0.0696, -0.0809, -0.4771]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3003, -0.4727, -0.2720,  ..., -0.0665, -0.0379, -0.5146],\n",
       "          [ 0.1270, -0.0770, -0.1511,  ..., -0.0533, -0.0361, -0.3274]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.8657e-01, 1.8860e-02, 1.3306e-01, 3.6157e-01],\n",
       "          [4.2993e-01, 5.1928e-04, 1.2922e-04, 5.6934e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    0.345978  136.780670  225.052979  475.000366    0.945200      0  person\n",
       "  1  246.464767  193.262573  266.389069  218.965698    0.779262     33    kite\n",
       "  2   88.383179  389.650269  102.629974  413.133301    0.701891      0  person\n",
       "  3  180.260101  392.104828  188.619476  408.032013    0.258633      0  person,\n",
       "  'caption': ['Woman in the white dress.',\n",
       "   'A woman in a dress is playing with a kite'],\n",
       "  'bbox_target': [0.0, 131.6, 235.15, 343.01]},\n",
       " 39: {'image_emb': tensor([[ 0.1172,  0.4111, -0.1783,  ...,  0.7739,  0.4092, -0.3271],\n",
       "          [ 0.3389,  0.2756,  0.0527,  ...,  0.5273,  0.3550, -0.1273],\n",
       "          [ 0.2412,  0.4995, -0.3254,  ...,  0.8540,  0.4546, -0.1672],\n",
       "          ...,\n",
       "          [ 0.2013,  0.2795, -0.0136,  ...,  1.2686,  0.1360, -0.1367],\n",
       "          [-0.1653,  0.1853,  0.1219,  ...,  0.7983,  0.0175, -0.5938],\n",
       "          [-0.0573,  0.4453,  0.0623,  ...,  0.8130,  0.2000, -0.0119]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1909,  0.4551, -0.1254,  ..., -0.1409,  0.0446, -0.1148],\n",
       "          [ 0.0410, -0.1827, -0.3379,  ..., -0.2329, -0.0200,  0.0225]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.5923e-01, 6.6162e-02, 4.1797e-01, 1.2577e-04, 3.4928e-05, 4.1127e-06,\n",
       "           5.9605e-07, 6.8545e-06, 1.0788e-05, 4.2796e-05, 1.6916e-04, 2.3842e-06,\n",
       "           4.1485e-05, 5.1260e-06, 5.5695e-02, 1.8871e-04, 4.0174e-05, 1.1325e-05,\n",
       "           1.2457e-05, 2.0087e-04],\n",
       "          [6.7017e-02, 7.8812e-03, 4.8755e-01, 1.4343e-03, 2.9907e-03, 1.7953e-04,\n",
       "           8.0943e-05, 3.6287e-04, 2.3055e-04, 1.2465e-03, 5.2490e-03, 2.7394e-04,\n",
       "           4.7326e-04, 4.1504e-03, 3.0981e-01, 1.0376e-01, 5.0850e-03, 1.0723e-04,\n",
       "           8.1730e-04, 1.3905e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   433.996094  117.905273  639.773560  358.637329    0.924892      0   \n",
       "  1     0.368774  122.252640  277.679688  475.769409    0.906412      0   \n",
       "  2   368.250946  106.310699  504.371490  301.835052    0.902123      0   \n",
       "  3   177.325012  226.850250  639.126282  475.322754    0.888979     60   \n",
       "  4    56.966114    0.659790  120.515213  126.868835    0.879253     25   \n",
       "  5   175.948181   84.014481  245.374878  132.349167    0.875733      2   \n",
       "  6   250.621124   82.401871  326.311737  125.991776    0.867856      2   \n",
       "  7   465.664490  345.434998  499.074036  430.487244    0.839776     40   \n",
       "  8   271.465088  156.787979  377.015320  248.670837    0.814233     58   \n",
       "  9   103.254898   88.454453  143.214737  133.283493    0.812437      2   \n",
       "  10  433.363892  145.114502  454.299927  200.375671    0.807004     40   \n",
       "  11  404.926910  319.869781  436.528107  403.275299    0.801593     40   \n",
       "  12    0.067511   90.193314   46.638840  145.685989    0.784123      2   \n",
       "  13  436.261017  330.300415  470.320404  411.850647    0.767624     40   \n",
       "  14  111.845352  119.938065  233.410568  397.418457    0.765269      0   \n",
       "  15  399.671875  150.116821  429.972290  198.939026    0.760782     40   \n",
       "  16  384.521362   74.198349  432.027710  105.169052    0.751240      2   \n",
       "  17  319.610352   73.040222  397.358215  121.665222    0.729102      2   \n",
       "  18  169.959900  146.232651  218.926544  197.394287    0.709227     56   \n",
       "  19  300.187744  222.954010  317.411682  267.451263    0.686370     40   \n",
       "  20  134.127853   87.595764  163.731857  110.623444    0.686003      2   \n",
       "  21  440.531219  236.502472  530.039856  313.865814    0.640506     56   \n",
       "  22  359.340729  262.914764  385.275116  296.078156    0.622949     40   \n",
       "  23  564.995728  336.722778  633.044312  387.063721    0.620875     26   \n",
       "  24  443.986481  366.457489  482.653229  458.611267    0.606002     41   \n",
       "  25  282.826508  374.725311  369.617584  402.246552    0.593258     42   \n",
       "  26  150.892960  192.540863  189.488480  249.843475    0.592310     56   \n",
       "  27  160.536682   85.722839  183.692932  110.771301    0.584556      2   \n",
       "  28  213.568420   75.531372  249.528564  105.737701    0.501874      2   \n",
       "  29  610.588074  285.529236  640.000000  345.853638    0.497649     56   \n",
       "  30  423.677094  298.670319  452.667084  365.554291    0.480969     41   \n",
       "  31  319.272247  276.198547  345.542633  336.234558    0.458135     41   \n",
       "  32    0.431061  284.284637  188.901306  476.707520    0.455378     56   \n",
       "  33  422.968933  300.032196  452.502441  369.952545    0.398108     40   \n",
       "  34    0.813263  435.964539  190.881577  477.756775    0.384218     56   \n",
       "  35  238.334305  329.849457  310.545288  351.367950    0.369205     42   \n",
       "  36    0.052335  155.526855   27.229393  217.789856    0.368454     56   \n",
       "  37  284.752869  217.772614  302.058472  266.665436    0.297216     40   \n",
       "  38  303.668396   74.583572  322.784424   97.285629    0.280868      2   \n",
       "  39  267.807281  211.246307  284.945648  242.025299    0.259414     41   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3   dining table  \n",
       "  4       umbrella  \n",
       "  5            car  \n",
       "  6            car  \n",
       "  7     wine glass  \n",
       "  8   potted plant  \n",
       "  9            car  \n",
       "  10    wine glass  \n",
       "  11    wine glass  \n",
       "  12           car  \n",
       "  13    wine glass  \n",
       "  14        person  \n",
       "  15    wine glass  \n",
       "  16           car  \n",
       "  17           car  \n",
       "  18         chair  \n",
       "  19    wine glass  \n",
       "  20           car  \n",
       "  21         chair  \n",
       "  22    wine glass  \n",
       "  23       handbag  \n",
       "  24           cup  \n",
       "  25          fork  \n",
       "  26         chair  \n",
       "  27           car  \n",
       "  28           car  \n",
       "  29         chair  \n",
       "  30           cup  \n",
       "  31           cup  \n",
       "  32         chair  \n",
       "  33    wine glass  \n",
       "  34         chair  \n",
       "  35          fork  \n",
       "  36         chair  \n",
       "  37    wine glass  \n",
       "  38           car  \n",
       "  39           cup  ,\n",
       "  'caption': ['A woman wearing an orange shirt.', 'A woman in red dress'],\n",
       "  'bbox_target': [434.62, 117.82, 205.38, 242.37]},\n",
       " 40: {'image_emb': tensor([[ 0.2183,  0.1011, -0.0389,  ...,  0.7192,  0.2200, -0.0255],\n",
       "          [-0.1747,  0.5664, -0.0736,  ...,  0.7290, -0.1544,  0.1843],\n",
       "          [-0.0039,  0.3640, -0.1292,  ...,  0.9067, -0.1477,  0.1385],\n",
       "          ...,\n",
       "          [ 0.1825,  0.2401, -0.0797,  ...,  0.6426,  0.1750, -0.0740],\n",
       "          [ 0.2462, -0.1089, -0.2024,  ...,  1.0205,  0.0525, -0.0451],\n",
       "          [ 0.0288,  0.1981,  0.1121,  ...,  0.9844,  0.0121, -0.0668]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-1.9153e-01,  1.1017e-02, -1.2903e-01, -3.9258e-01, -9.7534e-02,\n",
       "            1.4905e-01, -2.3242e-01, -4.7217e-01, -1.3672e-01,  9.9487e-02,\n",
       "            1.1920e-01, -8.8989e-02,  1.0608e-01, -1.2375e-02, -2.4451e-01,\n",
       "            8.7036e-02, -3.5547e-01,  9.6558e-02, -5.1514e-01,  1.7053e-01,\n",
       "            2.6807e-01,  1.1299e-02, -6.3660e-02,  4.3411e-03,  1.6504e-01,\n",
       "            1.0809e-01,  8.2474e-03, -1.4053e-02, -6.6309e-01, -1.8103e-01,\n",
       "           -2.8271e-01,  2.7832e-01, -4.8859e-02,  5.7068e-02, -1.3147e-01,\n",
       "           -4.4824e-01, -3.7689e-02, -1.4343e-01,  1.4305e-02,  2.5803e-02,\n",
       "            2.8564e-01,  3.0197e-02, -1.0065e-01, -1.0815e-01,  1.6931e-01,\n",
       "            7.8796e-02, -5.0293e-01,  8.9569e-03,  1.0300e-02,  1.3513e-01,\n",
       "            4.6265e-02, -9.8328e-02,  1.4819e-01, -5.1758e-01, -5.1221e-01,\n",
       "           -3.6530e-02, -6.7261e-02,  2.4353e-02, -2.4365e-01,  2.2595e-01,\n",
       "            2.0325e-02,  8.4229e-02, -2.9639e-01, -1.3879e-01, -8.4534e-02,\n",
       "            3.3545e-01,  2.9224e-01,  7.3193e-01,  1.0999e-01, -4.4653e-01,\n",
       "           -3.7183e-01, -1.4758e-01,  1.6296e-01,  1.4343e-01,  5.6427e-02,\n",
       "            3.7549e-01,  3.9087e-01,  1.9788e-01, -8.5205e-02,  8.6975e-02,\n",
       "           -2.7856e-01,  4.5679e-01, -9.7961e-02,  1.4868e-01, -2.1240e-01,\n",
       "            4.3262e-01, -1.1224e-01, -3.2898e-02, -4.9365e-01,  7.9773e-02,\n",
       "            4.1046e-02, -5.6055e-01, -1.0127e+00, -9.4238e-02,  2.8418e-01,\n",
       "            6.2225e-02,  1.0391e-02,  1.5405e-01, -3.6743e-02, -1.0779e-01,\n",
       "            1.8164e-01,  1.8347e-01, -9.4360e-02, -1.9690e-01,  1.0468e-01,\n",
       "            2.7368e-01, -3.0518e-01,  3.0304e-02,  8.3130e-02, -9.9609e-02,\n",
       "           -5.0537e-01,  4.2920e-01, -1.3748e-02,  2.1875e-01, -2.0728e-01,\n",
       "            1.3806e-01,  9.7229e-02,  1.0999e-01, -3.5205e-01,  4.1895e-01,\n",
       "            4.8340e-02, -3.1714e-01, -3.7329e-01, -3.5620e-01,  6.0234e-03,\n",
       "            1.5417e-01,  5.0342e-01, -3.9966e-01, -1.2244e-01,  2.8369e-01,\n",
       "            1.3220e-01, -5.1611e-01, -2.3376e-01,  2.6289e+00, -1.7627e-01,\n",
       "            2.9810e-01,  4.1229e-02, -4.0088e-01,  1.8054e-01, -1.4478e-01,\n",
       "           -2.8101e-01, -4.3640e-02, -2.8296e-01,  1.1908e-01, -2.7466e-01,\n",
       "           -3.5522e-01,  3.9160e-01, -4.0723e-01,  1.4636e-01, -1.7505e-01,\n",
       "           -1.1908e-01,  1.3965e-01,  4.9194e-01,  2.4805e-01, -1.4844e-01,\n",
       "           -7.9163e-02, -1.5698e-01, -4.3848e-01, -4.1321e-02,  2.1045e-01,\n",
       "            6.9141e-01,  1.2048e-01,  1.3892e-01,  9.7885e-03, -1.1981e-01,\n",
       "            2.3621e-02,  2.6001e-01, -2.1118e-01,  3.4399e-01, -3.4937e-01,\n",
       "           -2.3303e-01, -2.0667e-01,  5.2441e-01,  4.7461e-01, -2.8589e-01,\n",
       "            1.5930e-01, -4.1016e-02, -2.2656e-01, -3.3008e-01, -5.1221e-01,\n",
       "           -2.1619e-01,  5.1318e-01, -1.1658e-01, -8.7219e-02, -3.1689e-01,\n",
       "           -1.6968e-01,  1.1548e-01,  5.6458e-03,  1.0992e-01,  1.4233e-01,\n",
       "            5.0684e-01,  8.7585e-02, -6.1066e-02, -1.7444e-01,  5.8350e-02,\n",
       "            1.8726e-01, -2.8638e-01,  4.2480e-01, -1.0089e-01,  6.2256e-03,\n",
       "            2.3926e-01, -6.7932e-02, -1.0492e-01,  2.9102e-01, -4.3823e-01,\n",
       "            2.6733e-01,  2.4963e-01, -1.7932e-01, -2.8101e-01, -4.6045e-01,\n",
       "           -4.0405e-02,  3.8940e-01, -5.4395e-01,  2.2961e-01,  1.2396e-01,\n",
       "           -2.5171e-01,  6.5002e-02,  9.6512e-03,  6.6357e-01, -1.1917e-02,\n",
       "            6.8115e-01, -2.0605e-01, -4.1016e-01, -3.4619e-01, -2.9199e-01,\n",
       "            1.7017e-01, -3.8330e-02, -5.0049e-01, -5.5225e-01,  6.4453e-02,\n",
       "           -7.2168e-01,  4.1797e-01,  4.3907e-03, -2.4353e-01, -8.7708e-02,\n",
       "            1.7981e-01,  2.6367e-01, -2.9077e-01, -1.4270e-01,  1.1554e-01,\n",
       "            4.2334e-01,  1.6675e-01,  3.2593e-01,  2.7295e-01, -8.2764e-02,\n",
       "            5.0439e-01, -2.6001e-01,  3.3081e-02, -1.8469e-01,  1.0034e-01,\n",
       "           -8.9661e-02, -4.1895e-01,  1.6418e-01,  1.6663e-01, -1.9336e-01,\n",
       "            1.6113e-02,  5.2930e-01,  2.8271e-01,  2.5244e-01, -1.4355e-01,\n",
       "           -8.2275e-02,  1.4417e-01, -2.0676e-02,  1.1462e-01,  2.1343e-03,\n",
       "           -6.3782e-02, -1.0620e-01, -2.0630e-01, -3.1787e-01,  4.1466e-03,\n",
       "            2.9106e-03,  2.7539e-01,  3.1934e-01, -5.1056e-02,  2.3425e-01,\n",
       "           -1.5356e-01, -3.3594e-01,  3.5645e-01,  1.5100e-01, -6.7871e-02,\n",
       "           -6.2561e-02, -7.8613e-02,  2.0349e-01,  5.8990e-02,  2.6953e-01,\n",
       "            1.6064e-01, -6.5735e-02,  1.7944e-01,  4.0356e-01, -1.1273e-01,\n",
       "           -9.2285e-02, -2.6514e-01, -1.6052e-01, -2.3056e-02, -3.6060e-01,\n",
       "            2.2852e-01,  3.2715e-01, -1.4685e-01, -1.2482e-02,  6.9641e-02,\n",
       "           -8.9783e-02, -5.9814e-01,  1.8469e-01,  2.2729e-01,  1.3196e-01,\n",
       "           -6.0889e-01, -7.5073e-02, -8.6594e-03,  3.9014e-01, -4.2877e-02,\n",
       "           -3.0225e-01,  4.6729e-01,  2.6250e+00,  1.2494e-01, -1.4392e-01,\n",
       "            1.5332e-01,  3.5986e-01,  3.7964e-01,  2.1533e-01,  1.4917e-01,\n",
       "           -3.2031e-01, -2.3163e-02,  1.8188e-01,  6.9885e-02, -3.8037e-01,\n",
       "            7.3303e-02,  3.3228e-01, -1.3989e-01, -1.2213e-01, -1.2979e+00,\n",
       "            4.1284e-01, -3.5400e-01,  6.4648e-01, -2.2241e-01,  3.7323e-02,\n",
       "            8.0872e-02, -2.3181e-01,  2.6392e-01,  3.4058e-01, -1.6089e-01,\n",
       "           -5.8691e-01, -3.1494e-01,  5.2246e-01, -1.0358e-01,  5.0439e-01,\n",
       "           -1.4465e-01,  3.7646e-01,  3.5571e-01, -1.5149e-01,  1.2732e-01,\n",
       "            2.1594e-01, -1.9409e-01,  2.4915e-01,  5.3418e-01,  2.1069e-01,\n",
       "           -1.2952e-01,  1.8127e-02, -1.5588e-01, -3.3545e-01, -3.6736e-03,\n",
       "            8.0338e-03,  7.8552e-02,  3.1543e-01, -6.4026e-02,  3.2690e-01,\n",
       "            1.6492e-01,  7.7759e-02,  2.2620e-01, -4.9500e-02, -6.6528e-02,\n",
       "            3.9185e-02, -1.9409e-01, -3.8403e-01,  1.0071e-01, -3.4937e-01,\n",
       "            3.3936e-01,  1.2054e-01, -1.2122e-01,  1.9019e-01, -9.5642e-02,\n",
       "           -1.5808e-01,  2.4261e-02,  2.3315e-01,  1.9592e-02, -4.3652e-01,\n",
       "            1.8848e-01,  1.6431e-01,  7.6294e-02, -4.2065e-01,  1.0199e-01,\n",
       "           -5.2637e-01,  2.1814e-01, -4.2090e-01, -9.4727e-02,  1.3586e-01,\n",
       "            6.0974e-02,  1.3025e-01, -1.4453e-01,  4.4678e-02,  2.3242e-01,\n",
       "           -1.6101e-01, -2.9953e-02,  2.8320e-01,  4.7821e-02, -2.1094e-01,\n",
       "           -3.4082e-01,  6.5613e-03, -6.7749e-02, -5.9229e-01,  1.6199e-01,\n",
       "           -1.6650e-01,  3.5919e-02,  4.6577e-03, -1.7627e-01, -4.0741e-03,\n",
       "           -6.8994e-01, -2.7979e-01, -3.1470e-01,  3.5278e-02, -3.6841e-01,\n",
       "           -4.4159e-02,  7.9773e-02,  4.4922e-01, -4.4214e-01,  1.2292e-01,\n",
       "           -5.7739e-02, -1.3940e-01, -2.0398e-01,  3.3228e-01,  1.3586e-01,\n",
       "           -1.1322e-01, -8.8135e-02,  1.7166e-02, -8.7830e-02, -2.0667e-01,\n",
       "            1.8286e-01, -3.3862e-01,  1.2756e-01, -2.7637e-01, -1.6577e-01,\n",
       "            1.9885e-01,  4.6606e-01, -2.1411e-01,  1.6028e-01,  1.6064e-01,\n",
       "           -2.2498e-01, -5.4053e-01,  3.0563e-02, -1.3489e-01,  5.3369e-01,\n",
       "           -4.5679e-01,  2.2937e-01,  2.6318e-01,  3.6157e-01,  1.2433e-01,\n",
       "           -9.3384e-02,  1.8579e-01,  4.8438e-01,  1.2688e-02, -7.0190e-02,\n",
       "           -1.8445e-01,  4.5801e-01,  2.1802e-01,  2.5049e-01,  2.8711e-01,\n",
       "           -3.6255e-01,  1.5820e-01, -3.3716e-01,  1.5472e-02,  2.4756e-01,\n",
       "            5.4248e-01, -4.3896e-01, -5.6348e-01,  1.7899e-02,  3.1812e-01,\n",
       "           -2.1533e-01,  1.1475e-01, -7.8186e-02, -3.5718e-01,  2.5342e-01,\n",
       "            7.3547e-02, -6.6797e-01,  3.9111e-01,  4.2236e-02, -4.5435e-01,\n",
       "           -6.0791e-02, -8.4900e-02,  1.2231e-01, -1.4514e-01, -3.0835e-01,\n",
       "            2.6337e-02, -2.6489e-01,  1.5540e-01,  7.9102e-01,  2.1204e-01,\n",
       "           -4.5117e-01,  1.1450e-01, -5.9375e-01,  5.2155e-02,  1.6003e-01,\n",
       "           -2.1741e-01, -6.5979e-02,  1.1285e-01, -2.8076e-03,  6.5088e-01,\n",
       "           -4.9951e-01, -4.3823e-01, -1.3123e-01,  2.4182e-01,  1.8042e-01,\n",
       "           -3.1860e-01,  2.2156e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.5945e-02, 2.8271e-01, 4.7607e-02, 5.5322e-01, 5.9235e-02, 2.5368e-04,\n",
       "           2.2388e-04, 4.0710e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    88.979340  189.385315  283.657898  423.725891    0.932066      0  person\n",
       "  1   272.464630  152.603119  387.173187  280.822601    0.888195      0  person\n",
       "  2     1.219040  250.471893  231.809891  421.274414    0.869434     57   couch\n",
       "  3   109.019814  170.353027  265.149139  292.489746    0.846273      0  person\n",
       "  4   432.952026  157.600830  595.283691  258.240845    0.806632      0  person\n",
       "  5   455.869263  249.691589  473.088257  272.537720    0.792200     41     cup\n",
       "  6   323.225677  275.301514  344.468719  289.932800    0.719192     65  remote\n",
       "  7   347.372101  296.178741  377.771393  308.455048    0.699360     65  remote\n",
       "  8   430.510895  261.233398  450.616119  271.516846    0.662677     65  remote\n",
       "  9   263.609711  291.798187  280.284149  306.655548    0.595876     65  remote\n",
       "  10  407.644897  161.473602  592.370483  309.522736    0.592024     57   couch\n",
       "  11   97.474319  171.750671  407.255920  296.922668    0.501794     57   couch\n",
       "  12  394.179260  254.704102  422.454712  264.290771    0.370577     65  remote\n",
       "  13  371.591034  295.977875  413.178497  319.392609    0.273769     73    book,\n",
       "  'caption': ['green oversized chair that the man with his legs crossed is sitting in'],\n",
       "  'bbox_target': [405.27, 166.61, 202.87, 142.43]},\n",
       " 41: {'image_emb': tensor([[-0.0117,  0.4543, -0.0490,  ...,  1.3945,  0.0515,  0.2861],\n",
       "          [ 0.0578,  0.4536, -0.1921,  ...,  1.0430,  0.0692,  0.1876],\n",
       "          [ 0.1591,  0.3083, -0.2529,  ...,  1.0215,  0.4216,  0.0126],\n",
       "          [ 0.0274,  0.2327, -0.2430,  ...,  1.0078, -0.0134,  0.1205]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2412, -0.0375, -0.0043,  ...,  0.0791,  0.1252, -0.0426],\n",
       "          [-0.2280,  0.2040, -0.0274,  ..., -0.5688, -0.0692, -0.0620]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.8730e-01, 5.7793e-03, 1.5640e-04, 6.9695e-03],\n",
       "          [9.9902e-01, 1.1511e-03, 1.7881e-06, 2.5809e-05]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   473.129333  212.642639  637.889343  474.075806    0.923342      0  person\n",
       "  1     8.083015  299.914856   78.306419  448.159302    0.895545      0  person\n",
       "  2   163.345047   87.444504  207.088211  125.277115    0.705720     33    kite\n",
       "  3    63.617802    0.000000  118.250824   61.318130    0.692009     33    kite\n",
       "  4   415.771606  148.868774  528.924866  240.631653    0.672023     33    kite\n",
       "  5   102.971794   97.388580  138.759857  120.208282    0.554183     33    kite\n",
       "  6   431.021576  191.363983  517.568420  240.436127    0.540187     33    kite\n",
       "  7   257.563843  260.590820  279.829224  293.183044    0.530018     33    kite\n",
       "  8   439.055206  259.930420  464.026276  291.207764    0.472241     33    kite\n",
       "  9   577.903564    2.314240  622.041626   61.623871    0.459773     33    kite\n",
       "  10   51.742722  150.275101  208.419998  243.035217    0.454797     33    kite\n",
       "  11  480.672699  143.480118  599.184814  228.088562    0.426957     33    kite\n",
       "  12  168.396179  183.014099  217.482544  235.252747    0.419551     33    kite\n",
       "  13  395.445190  259.875549  440.109192  298.093994    0.411904     33    kite\n",
       "  14  405.234375  105.266861  490.256653  139.261276    0.386215     33    kite\n",
       "  15  603.001221    8.613342  622.641724   60.907181    0.326149     33    kite\n",
       "  16  245.929642  316.229980  292.615692  371.103088    0.322487     33    kite\n",
       "  17  505.595245  113.249207  513.330200  135.199158    0.319643     33    kite\n",
       "  18   18.322725  261.068939   59.519379  310.305450    0.310276     33    kite\n",
       "  19  626.238464  191.117554  639.783386  207.612061    0.295940     33    kite,\n",
       "  'caption': ['A man wearing a blue shirt and bluejeans looks at kites.',\n",
       "   'A man in a blue jacket looks to the right.'],\n",
       "  'bbox_target': [473.62, 210.98, 164.69, 261.56]},\n",
       " 42: {'image_emb': tensor([[-0.1810, -0.1919, -0.0196,  ...,  0.5889, -0.0379,  0.2959],\n",
       "          [-0.2380, -0.3279, -0.0562,  ...,  0.9570,  0.3176,  0.1666],\n",
       "          [-0.1830, -0.2063,  0.0676,  ...,  0.5952,  0.0763,  0.1538]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0365,  0.0352,  0.0010,  ...,  0.2581, -0.1056, -0.0371],\n",
       "          [ 0.0627,  0.0193, -0.1154,  ...,  0.1794, -0.0455,  0.0402]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5361, 0.1768, 0.2871],\n",
       "          [0.1251, 0.3401, 0.5347]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class      name\n",
       "  0    0.677856  31.700745  364.797607  615.939514    0.939617     20  elephant\n",
       "  1  162.978180  27.903069  466.695312  239.875885    0.760490     20  elephant,\n",
       "  'caption': [\"An elephant with it's trunk in it's mouth.\",\n",
       "   'An elephant raising its trunk behind another elephant.'],\n",
       "  'bbox_target': [162.79, 32.3, 302.73, 207.5]},\n",
       " 43: {'image_emb': tensor([[-0.3425,  0.3362, -0.0622,  ...,  0.8276, -0.0112, -0.1349],\n",
       "          [-0.3159,  0.5620,  0.0627,  ...,  0.8022, -0.0247, -0.0877],\n",
       "          [ 0.0230,  0.6846, -0.1954,  ...,  0.9707,  0.0219,  0.0159],\n",
       "          ...,\n",
       "          [-0.4958,  0.5347,  0.0530,  ...,  0.7383, -0.1487, -0.2034],\n",
       "          [ 0.1084, -0.2008, -0.0934,  ...,  1.0156, -0.1438, -0.1794],\n",
       "          [ 0.1242, -0.0515, -0.2399,  ...,  0.7178, -0.0560,  0.1505]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0223, -0.0840, -0.0056,  ...,  0.3770, -0.1018, -0.0711],\n",
       "          [ 0.1736,  0.0327, -0.2388,  ...,  0.1078, -0.1458,  0.0323]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.6851e-01, 4.6387e-02, 4.6921e-04, 7.0810e-05, 8.9526e-05, 4.7583e-01,\n",
       "           2.4498e-05, 8.7128e-03],\n",
       "          [3.2446e-01, 2.5269e-01, 1.4343e-03, 1.9550e-05, 2.3544e-05, 4.0405e-01,\n",
       "           2.3842e-07, 1.7197e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  231.532288  181.799805  422.031799  323.827942    0.941861     17   horse\n",
       "  1  380.350067  165.915955  576.861023  319.869019    0.932785     17   horse\n",
       "  2  285.631165  137.211868  340.082825  256.411804    0.923298      0  person\n",
       "  3  435.065857  131.453995  489.867493  259.141998    0.906034      0  person\n",
       "  4  209.278976  128.871216  252.845963  232.475891    0.859593      0  person\n",
       "  5  135.307831  152.981750  312.352264  304.159912    0.853126     17   horse\n",
       "  6  581.245728   80.994080  601.618896  106.171204    0.717979      8    boat,\n",
       "  'caption': ['darkest colored horse',\n",
       "   'a dark brown horse being rode by a girl in light blue'],\n",
       "  'bbox_target': [229.61, 183.69, 193.8, 141.44]},\n",
       " 44: {'image_emb': tensor([[ 0.1049,  0.4417,  0.1691,  ...,  0.7188, -0.1488,  0.2017],\n",
       "          [-0.3481,  0.5845,  0.0703,  ...,  1.0205, -0.1174, -0.1096],\n",
       "          [ 0.2488, -0.2949, -0.0479,  ...,  0.5654, -0.0791,  0.0156],\n",
       "          ...,\n",
       "          [ 0.1257,  0.6475,  0.2568,  ...,  0.8467, -0.2573,  0.2883],\n",
       "          [-0.1698,  0.5967, -0.1000,  ...,  0.9746, -0.1945,  0.1158],\n",
       "          [ 0.3381,  0.1139,  0.0291,  ...,  0.5825, -0.3843,  0.2479]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3459, -0.2389, -0.0993,  ..., -0.0609, -0.3904, -0.0801],\n",
       "          [ 0.3003, -0.0678, -0.2386,  ...,  0.0093, -0.3298, -0.0785]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.9385e-01, 7.1716e-03, 6.6956e-02, 1.8811e-04, 2.5320e-04, 5.3883e-05,\n",
       "           2.4261e-02, 5.0664e-05, 3.7939e-01, 1.4717e-02, 5.2155e-02, 2.6074e-01],\n",
       "          [1.2030e-01, 1.5778e-02, 6.5039e-01, 6.6137e-04, 2.1687e-03, 5.3465e-05,\n",
       "           1.7059e-02, 5.6028e-05, 4.0924e-02, 1.4824e-02, 8.5327e-02, 5.2551e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   375.138306  107.351624  538.590698  233.250458    0.944650     41   \n",
       "  1   457.012756    0.000000  576.830383   82.006821    0.937066     41   \n",
       "  2   262.474609  175.858521  421.721802  361.242126    0.897000     48   \n",
       "  3   467.992798   60.011337  592.439575  104.263535    0.882672     44   \n",
       "  4   588.800232   81.322601  640.000000  192.191528    0.879776     41   \n",
       "  5   609.677002  218.180267  639.957275  304.015533    0.867639     42   \n",
       "  6    66.967148  145.029022  277.971802  311.666779    0.818888     48   \n",
       "  7    34.685425    0.000000  126.383072   79.090607    0.774040     42   \n",
       "  8     0.000000    1.694855  633.410645  469.449219    0.747489     60   \n",
       "  9    51.618889  250.217346  202.910065  388.221191    0.735086     45   \n",
       "  10  249.190414    7.177521  351.535553   99.524536    0.731356     45   \n",
       "  11  249.193848    6.180038  352.103394   97.968796    0.533982     41   \n",
       "  12  199.785248    0.539719  428.884735   68.900894    0.530151     45   \n",
       "  13  208.488297    0.000000  427.939270   69.702682    0.494096     48   \n",
       "  14   71.022911    1.316574  125.763771   90.934998    0.304935     43   \n",
       "  15   51.945381  249.417480  201.599274  388.017578    0.285639     41   \n",
       "  \n",
       "              name  \n",
       "  0            cup  \n",
       "  1            cup  \n",
       "  2       sandwich  \n",
       "  3          spoon  \n",
       "  4            cup  \n",
       "  5           fork  \n",
       "  6       sandwich  \n",
       "  7           fork  \n",
       "  8   dining table  \n",
       "  9           bowl  \n",
       "  10          bowl  \n",
       "  11           cup  \n",
       "  12          bowl  \n",
       "  13      sandwich  \n",
       "  14         knife  \n",
       "  15           cup  ,\n",
       "  'caption': ['A toasted slice of  sandwich on the right  on a white plate beside a cup of coffee',\n",
       "   'The right half of the sandwich in front, next to te coffee cup'],\n",
       "  'bbox_target': [265.35, 176.58, 156.4, 187.68]},\n",
       " 45: {'image_emb': tensor([[ 0.0568,  0.3811, -0.5024,  ...,  0.6860, -0.1066, -0.0341],\n",
       "          [-0.0017,  0.4131, -0.3962,  ...,  0.4221, -0.1448,  0.1761],\n",
       "          [ 0.0403,  0.5615, -0.5332,  ...,  0.6392,  0.0655,  0.1512],\n",
       "          [ 0.0814,  0.0953, -0.1414,  ...,  0.9995,  0.0823, -0.3508],\n",
       "          [ 0.1844,  0.1476, -0.2915,  ...,  1.1152, -0.1442, -0.2170],\n",
       "          [ 0.1460,  0.1970, -0.3420,  ...,  0.0818, -0.1752,  0.0674]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2927, -0.0412,  0.0129,  ..., -0.3354, -0.2487, -0.0309],\n",
       "          [ 0.1311,  0.4775, -0.1658,  ...,  0.3113, -0.1099, -0.1874]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.9219e-01, 2.1497e-01, 1.5735e-01, 9.2468e-02, 2.4506e-02, 1.8494e-02],\n",
       "          [3.6377e-01, 3.6377e-01, 1.9470e-01, 5.2166e-04, 9.9540e-05, 7.7454e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  118.453262   14.977188  239.598434  342.675659    0.945999      0   \n",
       "  1  438.190186    9.916046  620.322510  360.170013    0.937449      0   \n",
       "  2  337.701172   38.394135  451.133911  347.381500    0.936458      0   \n",
       "  3  119.456306  133.614883  148.103760  178.044983    0.823455     35   \n",
       "  4  418.807281  169.689392  472.231049  208.939758    0.741124     35   \n",
       "  5  405.653717  168.366821  440.794769  200.789978    0.297057     35   \n",
       "  \n",
       "               name  \n",
       "  0          person  \n",
       "  1          person  \n",
       "  2          person  \n",
       "  3  baseball glove  \n",
       "  4  baseball glove  \n",
       "  5  baseball glove  ,\n",
       "  'caption': ['Player 22.',\n",
       "   'A boy in a baseball uniform reaching out to the guy in front of him.'],\n",
       "  'bbox_target': [440.15, 13.7, 181.01, 346.12]},\n",
       " 46: {'image_emb': tensor([[-0.1221,  0.1826, -0.0388,  ...,  0.8027, -0.0581, -0.2347],\n",
       "          [-0.2119,  0.2218, -0.1571,  ...,  0.5503, -0.1714, -0.1197]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0052,  0.1318, -0.1187,  ..., -0.3340, -0.1068, -0.4360],\n",
       "          [ 0.1727, -0.0263, -0.4346,  ...,  0.4756, -0.1997, -0.3696]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5156, 0.4844],\n",
       "          [0.7056, 0.2942]], dtype=torch.float16),\n",
       "  'df_boxes':         xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  28.923431   46.848164  294.169464  440.091949    0.923301      0   \n",
       "  1  84.757370    0.398266  261.826843  204.686722    0.684746      0   \n",
       "  2  70.660454  152.040833  103.562492  214.492508    0.442113     67   \n",
       "  3   0.310036  184.100143   40.514870  440.541718    0.284266      0   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2  cell phone  \n",
       "  3      person  ,\n",
       "  'caption': ['a model is talkig',\n",
       "   'lady in white shirt talking on cell phone'],\n",
       "  'bbox_target': [26.82, 39.73, 269.18, 402.27]},\n",
       " 47: {'image_emb': tensor([[-0.2961, -0.0247, -0.2102,  ...,  0.5122, -0.0338,  0.0828],\n",
       "          [-0.4446,  0.1152, -0.1991,  ...,  0.9292, -0.1259,  0.1572],\n",
       "          [-0.4126,  0.1277, -0.1552,  ...,  0.3557, -0.2433,  0.1488]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2588,  0.0493, -0.1364,  ...,  0.0302, -0.0488,  0.5981],\n",
       "          [-0.2854, -0.5635,  0.0917,  ...,  0.4666, -0.0870,  0.5596]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3093, 0.2408, 0.4500],\n",
       "          [0.5713, 0.0359, 0.3926]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class   name\n",
       "  0  127.525070  35.366455  425.938171  640.000000    0.921751     22  zebra\n",
       "  1   82.944321   0.643204  421.110962  237.666702    0.715220     22  zebra,\n",
       "  'caption': ['the in focus zebra that is sniffing a tree root',\n",
       "   'A zebra bends down to eat some food.'],\n",
       "  'bbox_target': [121.7, 40.23, 305.3, 589.89]},\n",
       " 48: {'image_emb': tensor([[-0.0911,  0.8047, -0.0032,  ...,  1.3057, -0.0631, -0.3640],\n",
       "          [-0.0950,  0.5430, -0.0849,  ...,  0.9434,  0.6348, -0.0096],\n",
       "          [-0.3757,  0.6626,  0.1959,  ...,  0.6592, -0.0579,  0.0575]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0998,  0.4058, -0.0864,  ..., -0.4617, -0.2465,  0.0658],\n",
       "          [-0.2893,  0.2764,  0.2073,  ...,  0.2491,  0.0528, -0.1746]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.9121, 0.0307, 0.0574],\n",
       "          [0.9048, 0.0676, 0.0277]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  211.372375  394.080414  410.761658  514.517090    0.845634     75   \n",
       "  1  198.836853  288.199799  419.332703  421.393646    0.831891     75   \n",
       "  2  199.941040  212.200363  416.519104  463.013977    0.507844     58   \n",
       "  \n",
       "             name  \n",
       "  0          vase  \n",
       "  1          vase  \n",
       "  2  potted plant  ,\n",
       "  'caption': ['A reflection of the brown bowl.', 'orange base of flower pot'],\n",
       "  'bbox_target': [214.29, 399.82, 208.54, 119.37]},\n",
       " 49: {'image_emb': tensor([[ 0.1193,  0.3291, -0.1666,  ...,  1.5498, -0.2747,  0.0498],\n",
       "          [ 0.3230,  0.2896, -0.1941,  ...,  0.9556,  0.0067, -0.1282],\n",
       "          [ 0.1298,  0.0538, -0.2417,  ...,  0.6035,  0.1493,  0.0592],\n",
       "          [ 0.3594,  0.0966,  0.2089,  ...,  0.0722,  0.2035,  0.2954]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0064,  0.2235, -0.1775,  ..., -0.6616, -0.2522, -0.0836],\n",
       "          [ 0.4380,  0.3003, -0.3120,  ...,  0.1016,  0.1160,  0.2185]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.1723e-04, 8.8871e-05, 5.3644e-07, 9.9951e-01],\n",
       "          [5.7487e-03, 1.1986e-02, 1.6153e-05, 9.8242e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  157.873932  308.281616  272.409149  580.045715    0.871538      0   \n",
       "  1  379.630554  273.781891  406.455139  342.755463    0.769696      0   \n",
       "  2  370.196411  112.593643  413.909058  163.314957    0.702520     11   \n",
       "  3  197.864441  365.452332  260.400085  484.576965    0.632515     26   \n",
       "  4  396.561768  273.462036  425.871460  336.423767    0.531804      2   \n",
       "  5  368.803284  276.322205  383.912964  321.230530    0.408016      0   \n",
       "  6  378.928009  272.786377  410.682800  341.312805    0.377347      2   \n",
       "  7  178.476715   84.391365  242.644684  150.502075    0.352269     11   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2  stop sign  \n",
       "  3    handbag  \n",
       "  4        car  \n",
       "  5     person  \n",
       "  6        car  \n",
       "  7  stop sign  ,\n",
       "  'caption': ['A woman with her hands on her hips standing in front of a wall.',\n",
       "   'the woman with the black shirt and jeans.'],\n",
       "  'bbox_target': [157.12, 301.98, 113.87, 281.08]},\n",
       " 50: {'image_emb': tensor([[-0.0190,  0.5220,  0.1022,  ...,  0.9688,  0.1083, -0.1573],\n",
       "          [-0.1295,  0.5698,  0.2546,  ...,  0.9395,  0.0776, -0.2203],\n",
       "          [-0.2900,  0.3826, -0.1450,  ...,  1.4121,  0.1302, -0.0206],\n",
       "          [-0.1998,  0.1202, -0.2510,  ...,  1.0654, -0.0200,  0.0043],\n",
       "          [-0.0504,  0.3525,  0.0431,  ...,  0.7246,  0.0567,  0.1289]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0681,  0.0350, -0.3237,  ...,  0.6489,  0.0343, -0.0333],\n",
       "          [ 0.0416,  0.2578,  0.0879,  ...,  0.7212, -0.1428,  0.2238]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.2683e-01, 7.7686e-01, 6.9351e-03, 5.2032e-02, 3.7476e-02],\n",
       "          [4.1924e-03, 9.9463e-01, 1.5759e-04, 4.9305e-04, 6.7377e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    9.988117   66.778763  298.749023  327.274048    0.942323     63   \n",
       "  1  260.892395  197.146194  392.307373  328.953583    0.898414     66   \n",
       "  2    0.000000    0.000000  157.639816  123.360039    0.879689     58   \n",
       "  3  228.019577  149.921921  294.454773  181.367615    0.853990     64   \n",
       "  4  460.774658  164.619751  499.945007  310.599304    0.635195     56   \n",
       "  \n",
       "             name  \n",
       "  0        laptop  \n",
       "  1      keyboard  \n",
       "  2  potted plant  \n",
       "  3         mouse  \n",
       "  4         chair  ,\n",
       "  'caption': ['Black keyboards of a laptop lying on the table.',\n",
       "   'The black keys with white letters that make up the keyboard of an Apple laptop.'],\n",
       "  'bbox_target': [101.76, 225.48, 97.44, 106.52]},\n",
       " 51: {'image_emb': tensor([[ 0.0864,  0.0259, -0.2373,  ...,  0.5591,  0.2258, -0.6504],\n",
       "          [ 0.1603,  0.4180, -0.3711,  ...,  0.5840,  0.4980, -0.4160],\n",
       "          [-0.0703,  0.3147, -0.1738,  ...,  0.8618,  0.3167, -0.3950],\n",
       "          ...,\n",
       "          [ 0.3157, -0.1135, -0.6816,  ...,  0.5459, -0.0667,  0.0444],\n",
       "          [ 0.2474,  0.0820, -0.2747,  ...,  0.8843,  0.1531, -0.2024],\n",
       "          [-0.1519,  0.1556, -0.0694,  ...,  0.2245,  0.5601,  0.0512]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2063,  0.3560, -0.1534,  ...,  0.1161,  0.1945, -0.3003],\n",
       "          [ 0.3328,  0.2322, -0.3748,  ..., -0.1492,  0.0890, -0.5205]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.1539e-03, 8.8330e-01, 9.8705e-04, 1.1407e-01, 4.2677e-05, 2.5511e-05,\n",
       "           4.2439e-04],\n",
       "          [2.7275e-04, 9.6533e-01, 1.0624e-03, 3.3051e-02, 3.8683e-05, 1.7464e-05,\n",
       "           2.7835e-05]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  291.695343  134.751114  339.210999  258.472443    0.892056      0    person\n",
       "  1    9.609879    0.203422  210.531799   92.504799    0.781128     25  umbrella\n",
       "  2    0.071025   46.100353  101.301552  249.631378    0.729026      5       bus\n",
       "  3  196.973083   22.435968  352.539337   97.368881    0.722503     25  umbrella\n",
       "  4  359.682465  145.837158  373.072998  172.756500    0.705031      0    person\n",
       "  5   87.089455  121.539597  143.679001  203.134583    0.702646      0    person\n",
       "  6  307.183868  202.119995  330.296051  236.941650    0.670802     26   handbag\n",
       "  7   69.109711  218.252609  111.124268  238.683319    0.355071     53     pizza\n",
       "  8   68.230614  218.181259  111.839951  239.225464    0.287905     46    banana\n",
       "  9  265.214203   93.204117  273.387573  114.715645    0.251455     39    bottle,\n",
       "  'caption': ['A blue and yellow umbrella that has soda bottles under it.',\n",
       "   'a blue and yellow umbrella on the right'],\n",
       "  'bbox_target': [206.02, 23.79, 150.76, 66.42]},\n",
       " 52: {'image_emb': tensor([[-0.1370, -0.4590,  0.4153,  ...,  0.6323,  0.1621, -0.1022],\n",
       "          [ 0.0547, -0.3379,  0.2015,  ...,  1.0215,  0.1819,  0.2793],\n",
       "          [-0.4470,  0.0684,  0.0061,  ...,  1.4131, -0.1624, -0.1261],\n",
       "          ...,\n",
       "          [ 0.3430,  0.3665, -0.2104,  ...,  1.2246,  0.1328, -0.1047],\n",
       "          [ 0.2004,  0.0224, -0.4697,  ...,  1.0713, -0.1313, -0.0541],\n",
       "          [-0.0394, -0.2959,  0.3098,  ...,  0.4255,  0.1445, -0.0971]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0657, -0.0667,  0.0646,  ..., -0.1294,  0.0552,  0.1152],\n",
       "          [-0.0062, -0.1803, -0.2620,  ..., -0.0980,  0.2456,  0.3455]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.1725e-01, 8.5754e-02, 5.6572e-03, 1.7700e-02, 4.1389e-03, 3.9886e-02,\n",
       "           7.2949e-01],\n",
       "          [3.4149e-02, 1.7715e-02, 2.9087e-04, 2.4994e-02, 4.6492e-04, 1.3588e-02,\n",
       "           9.0869e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   102.591309  104.830399  516.226257  420.132629    0.954752     19     cow\n",
       "  1    83.082985    0.967773  237.869598  179.002655    0.899595      0  person\n",
       "  2   475.816467   69.280319  640.000000  159.742004    0.878860     19     cow\n",
       "  3   463.771027    3.055847  506.295990  107.610168    0.833941      0  person\n",
       "  4   502.246490    3.627670  556.739746   81.420609    0.832774      0  person\n",
       "  5   361.311127    0.000000  407.411407   98.885620    0.701298      0  person\n",
       "  6   271.742981    0.014725  316.106079  125.993942    0.682330      0  person\n",
       "  7   243.876434    0.000000  294.458466  130.221313    0.655200      0  person\n",
       "  8   180.023315    0.000000  253.355011  131.441101    0.629867      0  person\n",
       "  9   325.449799    0.000000  365.398956  107.614853    0.408790      0  person\n",
       "  10  596.727234   72.618500  624.462952  100.455475    0.356786      0  person\n",
       "  11  561.591675   63.738495  588.203613   93.659241    0.250720      0  person,\n",
       "  'caption': ['cow in back',\n",
       "   'The cow next to the corner of the pen in front of the two people in blue and black'],\n",
       "  'bbox_target': [476.56, 70.9, 163.44, 90.15]},\n",
       " 53: {'image_emb': tensor([[-0.1610,  0.8369, -0.2306,  ...,  1.0635, -0.1726, -0.3071],\n",
       "          [ 0.1558,  0.6846,  0.2659,  ...,  1.0576, -0.3650, -0.0015],\n",
       "          [ 0.0575,  0.7402, -0.1613,  ...,  0.6978, -0.2112, -0.2964]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 1.0175e-01,  2.1289e-01,  1.3843e-01,  3.5083e-01, -2.0374e-01,\n",
       "            1.6504e-01, -1.7578e-01,  7.4463e-02, -2.5439e-01, -5.5298e-02,\n",
       "            2.0703e-01,  2.4731e-01,  1.1658e-01, -9.1553e-02, -1.4404e-01,\n",
       "           -1.7990e-02,  2.4084e-01, -1.4722e-01,  7.0923e-02,  1.1023e-01,\n",
       "           -2.9980e-01, -3.1226e-01, -1.9812e-01,  1.3257e-01,  7.0068e-01,\n",
       "            2.0157e-02, -6.2927e-02,  5.7520e-01,  3.8818e-02, -7.5806e-02,\n",
       "           -4.1046e-02,  2.8125e-01,  6.7078e-02, -3.0289e-02, -3.2257e-02,\n",
       "            2.9883e-01,  3.7622e-01, -1.1456e-01, -4.7722e-03,  2.1875e-01,\n",
       "           -2.9922e-02,  1.7493e-01,  3.0908e-01,  3.2080e-01, -3.2166e-02,\n",
       "            1.3269e-01,  5.0000e-01,  4.6021e-01,  3.1274e-01, -2.2986e-01,\n",
       "           -2.5830e-01,  6.3293e-02, -1.2805e-01,  1.3672e-01,  1.3806e-01,\n",
       "           -2.2314e-01, -1.4246e-01, -2.2888e-01,  1.7346e-01,  2.5537e-01,\n",
       "            2.9126e-01, -1.9202e-01, -1.7810e-01, -6.3843e-02, -1.3257e-01,\n",
       "            1.4233e-01,  4.6436e-01,  6.5186e-01, -2.9883e-01, -1.7139e-01,\n",
       "            1.2016e-03,  2.9251e-02,  7.3914e-02,  5.8594e-01,  3.9233e-01,\n",
       "            3.4521e-01, -1.5894e-01,  8.0688e-02,  1.6296e-01,  9.9304e-02,\n",
       "           -6.5857e-02,  3.4644e-01, -1.5723e-01, -8.0383e-02, -3.0322e-01,\n",
       "            2.5366e-01,  4.1431e-01, -4.4775e-01, -8.0566e-02,  2.9346e-01,\n",
       "            1.6895e-01,  1.5213e-02, -6.9043e-01,  8.7354e-01, -2.0544e-01,\n",
       "           -1.1542e-01, -1.3550e-01, -7.6233e-02,  1.6345e-01,  2.4011e-01,\n",
       "            1.5540e-01,  2.4561e-01, -1.6113e-01,  1.6708e-02, -4.1968e-01,\n",
       "           -2.2873e-02,  4.5204e-03,  4.6295e-02,  3.6353e-01,  2.0361e-01,\n",
       "           -2.2629e-02, -2.6147e-01,  3.9624e-01, -3.6523e-01, -8.0261e-02,\n",
       "           -1.3037e-01,  3.5376e-01,  1.5613e-01,  4.4897e-01, -1.2012e-01,\n",
       "           -7.0862e-02, -8.5645e-01,  2.7466e-01, -2.0355e-02,  2.8027e-01,\n",
       "           -1.5161e-01, -1.1639e-01, -3.8727e-02, -2.7563e-01,  3.0200e-01,\n",
       "            3.2330e-03, -4.2511e-02, -4.9756e-01,  3.7305e+00,  3.5620e-01,\n",
       "           -1.4091e-02, -1.2500e-01, -6.7920e-01, -4.0186e-01, -2.0264e-01,\n",
       "            3.2129e-01,  1.3452e-01,  1.0864e-01,  3.9795e-01, -5.2148e-01,\n",
       "           -1.9055e-01, -9.5886e-02, -2.0386e-01, -4.6045e-01, -1.1182e-01,\n",
       "           -2.9321e-01, -2.9739e-02,  4.0698e-01,  2.5269e-01, -1.8823e-01,\n",
       "           -1.9641e-01,  5.9143e-02, -3.7256e-01, -1.2561e-01,  2.0279e-02,\n",
       "           -7.1594e-02,  5.0049e-01,  3.7549e-01,  1.4600e-01, -1.2032e-02,\n",
       "           -6.3525e-01,  2.3083e-01, -1.8860e-01,  1.7075e-02,  1.1078e-01,\n",
       "            1.0266e-01, -6.3721e-02,  1.6248e-01, -1.5289e-02, -2.0959e-01,\n",
       "            2.5391e-01, -8.1848e-02,  2.2900e-01,  6.6345e-02,  1.0468e-01,\n",
       "           -1.3049e-01, -4.0436e-02, -3.5352e-01, -2.8027e-01, -1.2830e-01,\n",
       "            3.4668e-01, -1.7334e-01, -2.5122e-01,  8.3557e-02, -9.8267e-02,\n",
       "            4.8877e-01, -1.6541e-01,  5.2344e-01,  6.3896e-03,  4.1333e-01,\n",
       "            1.3477e-01, -1.0858e-01, -1.8115e-01,  3.2711e-03, -8.9050e-02,\n",
       "            1.0437e-01, -1.9821e-02, -1.3477e-01, -2.5220e-01,  3.0200e-01,\n",
       "           -7.3547e-02,  2.7441e-01, -1.9824e-01,  2.9199e-01, -6.3354e-02,\n",
       "            4.7180e-02,  5.5225e-01,  6.7688e-02, -6.1816e-01,  3.0762e-01,\n",
       "            2.0898e-01, -2.4512e-01, -5.9692e-02,  3.0908e-01, -3.9111e-01,\n",
       "            3.2568e-01, -8.9294e-02,  1.1444e-02,  3.7256e-01,  2.2498e-01,\n",
       "           -4.8340e-01, -4.2090e-01, -6.3354e-02, -1.5161e-01, -1.2292e-01,\n",
       "            1.0840e-01,  5.7922e-02,  4.9219e-01, -1.3135e-01, -6.5002e-02,\n",
       "            1.1487e-01,  2.1338e-01, -4.2938e-02, -2.3889e-01, -3.0493e-01,\n",
       "           -1.2421e-02,  4.1168e-02,  3.5889e-02,  1.6882e-01, -6.9580e-02,\n",
       "           -2.7808e-01, -4.3915e-02,  1.6919e-01, -2.8125e-01,  1.1884e-01,\n",
       "            8.1543e-02, -5.4626e-02,  2.3767e-01, -1.8646e-02, -7.8223e-01,\n",
       "           -3.1104e-01, -8.2178e-01, -1.1432e-01, -2.1472e-01,  8.9966e-02,\n",
       "           -6.4746e-01, -1.7139e-01,  3.4131e-01,  1.7139e-01, -1.5857e-01,\n",
       "           -2.1277e-01, -1.1938e-01, -2.2437e-01, -2.4084e-01,  1.3745e-01,\n",
       "           -7.8796e-02, -1.3733e-01, -1.9885e-01, -8.4717e-02,  2.5220e-01,\n",
       "           -1.8005e-01,  2.3535e-01,  3.2837e-01, -8.0139e-02,  1.5015e-01,\n",
       "            5.1453e-02, -3.3643e-01,  1.4905e-01,  3.4985e-01, -3.2837e-01,\n",
       "           -9.7046e-03, -1.4526e-01,  1.5076e-01,  4.0894e-01, -1.0870e-01,\n",
       "            4.4580e-01,  1.8384e-01, -7.1526e-03,  2.3022e-01, -3.7018e-02,\n",
       "            6.5613e-02,  1.2341e-01,  4.8047e-01, -2.4307e-02, -1.9751e-01,\n",
       "           -5.5817e-02, -5.7959e-01, -4.9902e-01, -1.0107e-01, -9.2896e-02,\n",
       "           -3.8428e-01, -9.0393e-02, -1.1591e-01,  2.6465e-01, -5.3864e-02,\n",
       "           -1.9653e-01,  1.7383e-01,  3.7363e+00, -8.0109e-03,  3.8062e-01,\n",
       "            1.4252e-02,  4.2786e-02,  1.8677e-01,  5.7190e-02,  3.5362e-03,\n",
       "           -6.3782e-02,  2.6001e-01,  2.9834e-01,  1.6150e-01, -2.2961e-01,\n",
       "            4.3237e-01,  2.9370e-01,  1.9727e-01,  1.6101e-01, -3.5181e-01,\n",
       "           -5.0830e-01, -5.6787e-01, -1.7725e-01, -3.9136e-01, -1.9727e-01,\n",
       "           -2.5562e-01, -2.1826e-01, -7.4280e-02,  6.4819e-02, -1.8933e-01,\n",
       "           -3.7500e-01, -4.0967e-01,  4.3311e-01,  1.0065e-01, -7.0007e-02,\n",
       "            1.6284e-01,  3.3521e-01, -6.2866e-02, -8.8989e-02,  1.8811e-01,\n",
       "            1.8579e-01, -1.1267e-01, -1.7993e-01,  1.3130e-02, -4.2261e-01,\n",
       "           -3.1348e-01,  1.6769e-02, -1.9397e-01,  1.8713e-01,  1.7676e-01,\n",
       "           -2.1692e-01,  1.1566e-01, -1.7993e-01,  1.1652e-01,  9.7046e-02,\n",
       "            4.3121e-02,  3.2153e-01,  2.6810e-02,  2.7930e-01, -6.4941e-02,\n",
       "            2.5586e-01,  5.5054e-02, -3.0884e-01,  6.7871e-01,  3.3252e-01,\n",
       "           -2.8540e-01, -1.7920e-01,  7.3853e-02, -1.8640e-01, -1.4612e-01,\n",
       "           -1.9080e-01, -1.7102e-01,  4.0552e-01, -1.6345e-01,  8.7463e-02,\n",
       "            9.7229e-02, -3.0151e-01, -4.7119e-02, -6.3330e-01, -1.7053e-01,\n",
       "           -6.0791e-01,  2.1729e-01, -1.6068e-02, -1.3992e-02,  2.3132e-01,\n",
       "           -1.3281e-01, -1.8945e-01,  2.4390e-01, -2.7808e-01,  1.8933e-01,\n",
       "           -1.3354e-01, -6.1340e-02,  2.3352e-01,  7.1094e-01, -3.7012e-01,\n",
       "           -6.0577e-02, -7.8186e-02, -2.2644e-01, -1.9275e-01, -2.8662e-01,\n",
       "           -3.3887e-01, -1.6980e-01,  7.2510e-02,  3.3887e-01, -2.2980e-02,\n",
       "           -3.3691e-01, -6.4514e-02,  2.4063e-02,  2.2986e-01, -6.2164e-02,\n",
       "            2.5970e-02, -1.7444e-01,  6.3293e-02, -5.2344e-01,  2.3669e-01,\n",
       "           -7.7148e-02, -1.8506e-01, -8.6792e-02, -8.6308e-04,  2.0935e-01,\n",
       "           -4.6411e-01, -3.5706e-02,  1.3342e-01,  6.5967e-01, -5.4346e-01,\n",
       "           -1.8762e-01, -1.7426e-02,  2.9565e-01,  6.2256e-02,  3.2812e-01,\n",
       "           -2.3804e-01, -2.3462e-01,  3.3936e-02, -3.4570e-01,  9.7885e-03,\n",
       "           -5.7983e-02,  1.7407e-01, -4.3799e-01,  3.0151e-01,  7.1777e-02,\n",
       "            2.1960e-01,  1.3989e-01,  4.7095e-01, -2.6831e-01,  2.8394e-01,\n",
       "            1.9104e-01, -9.2285e-02,  4.3286e-01, -3.2690e-01, -5.1514e-01,\n",
       "           -2.9858e-01, -1.6516e-01, -4.8560e-01,  4.6021e-02, -1.3159e-01,\n",
       "           -1.3892e-01,  1.0028e-01, -5.2643e-02, -1.1359e-01, -1.9995e-01,\n",
       "           -2.9199e-01, -4.9683e-02, -6.6772e-02,  2.3157e-01,  1.2764e-02,\n",
       "           -1.0498e-02,  3.9453e-01, -4.9292e-01, -1.1694e-01,  2.0862e-01,\n",
       "            2.7390e-03, -3.2135e-02, -1.7065e-01, -1.9055e-01,  1.6333e-01,\n",
       "            1.7297e-01,  3.1616e-01, -3.1281e-02, -2.0801e-01, -1.5735e-01,\n",
       "           -3.0591e-01, -7.2266e-02, -1.0223e-01,  1.3516e+00,  3.2080e-01,\n",
       "            9.2407e-02,  3.7939e-01,  4.3945e-01, -1.7273e-01,  2.4890e-01,\n",
       "            5.5176e-01, -6.1707e-02,  8.4167e-02,  4.7998e-01,  2.7490e-01,\n",
       "           -1.1548e-01,  1.1206e-01,  3.8062e-01, -1.3269e-01, -2.3987e-01,\n",
       "           -7.1289e-02, -2.0239e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.7255e-04, 9.9902e-01, 5.7888e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0    0.000000   91.376610  131.611053  247.623932    0.884809     45      bowl\n",
       "  1  146.983154  445.523254  322.639587  579.658875    0.765075     55      cake\n",
       "  2   29.323456  462.465149  184.780792  588.460388    0.668318     55      cake\n",
       "  3    0.000000  411.993256  138.149902  542.291016    0.655676     55      cake\n",
       "  4  225.567902    3.307007  426.235718  576.813171    0.594172      0    person\n",
       "  5    0.000000  353.302277  110.232605  421.367828    0.352213     50  broccoli\n",
       "  6  155.358368  135.577850  322.149261  273.434357    0.321640     55      cake,\n",
       "  'caption': ['a swirled piece of sandwich food underneath an identical piece'],\n",
       "  'bbox_target': [26.6, 460.47, 167.89, 132.98]},\n",
       " 54: {'image_emb': tensor([[-0.0456,  0.3447, -0.0160,  ...,  0.6387, -0.1400, -0.0641],\n",
       "          [ 0.0273,  0.7534, -0.0720,  ...,  1.3799, -0.0171, -0.0539],\n",
       "          [ 0.0504,  0.1398,  0.1560,  ...,  0.9678, -0.1017, -0.3005],\n",
       "          [ 0.1627,  0.2167, -0.0379,  ...,  0.6157, -0.2288, -0.2494]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0164,  0.2084, -0.3000,  ...,  0.6675, -0.1384, -0.0817],\n",
       "          [ 0.2700, -0.0235, -0.0745,  ...,  0.4561,  0.1799, -0.1054]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.5032e-02, 8.0811e-01, 3.3212e-04, 9.6497e-02],\n",
       "          [2.7142e-03, 9.9658e-01, 2.4080e-04, 3.0923e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  112.824448    0.000000  379.269836  421.667908    0.955208      0   \n",
       "  1    0.081215  154.096344  151.809479  261.906158    0.932159      2   \n",
       "  2  427.183899   65.544678  612.686584  419.991333    0.867632      0   \n",
       "  3   64.020126  144.129547  188.025772  189.821320    0.669022      2   \n",
       "  4  435.720245  138.012939  583.045776  241.858521    0.647815     34   \n",
       "  5  435.751587  138.399261  583.044495  242.002411    0.645299     27   \n",
       "  6  131.173279  274.307739  164.754150  286.092896    0.440575     67   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1           car  \n",
       "  2        person  \n",
       "  3           car  \n",
       "  4  baseball bat  \n",
       "  5           tie  \n",
       "  6    cell phone  ,\n",
       "  'caption': ['A black car driving down the street next to a man on his cell phone.',\n",
       "   'A black car riding down the street.'],\n",
       "  'bbox_target': [0.0, 154.32, 151.54, 108.03]},\n",
       " 55: {'image_emb': tensor([[ 0.0415,  0.5332, -0.0422,  ...,  0.3857, -0.2898, -0.2106],\n",
       "          [ 0.0668,  0.6602,  0.4255,  ...,  0.4324, -0.4514,  0.3601],\n",
       "          [ 0.1304,  0.1716, -0.2238,  ...,  0.9985,  0.2554, -0.2295],\n",
       "          [ 0.2986,  0.5229,  0.5161,  ..., -0.1168, -0.3784,  0.3420]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2607, -0.1294,  0.0206,  ...,  0.4348, -0.1331, -0.4492],\n",
       "          [-0.0488, -0.4202, -0.2135,  ..., -0.0014,  0.2683, -0.3716]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.8467e-01, 5.5969e-02, 1.1921e-07, 1.5942e-01],\n",
       "          [2.2595e-01, 2.0569e-01, 4.1604e-04, 5.6787e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class  \\\n",
       "  0  320.892822  92.058273  437.945618  347.715851    0.950079      0   \n",
       "  1   53.298260  78.237785  349.695038  338.242432    0.918393      0   \n",
       "  2   52.334583  76.241707   91.741058  113.416336    0.878835     35   \n",
       "  \n",
       "               name  \n",
       "  0          person  \n",
       "  1          person  \n",
       "  2  baseball glove  ,\n",
       "  'caption': ['A baseball player with white jersey and Neives and 23 written in red running over base.',\n",
       "   'the man running to first base on the white team'],\n",
       "  'bbox_target': [318.84, 92.74, 116.71, 255.28]},\n",
       " 56: {'image_emb': tensor([[ 0.1437, -0.3333, -0.1589,  ...,  0.3569, -0.0098, -0.1276],\n",
       "          [-0.1797, -0.4082, -0.3259,  ...,  0.3257,  0.1172, -0.0226],\n",
       "          [ 0.1487, -0.3628, -0.0260,  ...,  0.1003,  0.0895, -0.1676]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0454, -0.1761, -0.2544,  ...,  0.2384, -0.2581, -0.3750],\n",
       "          [ 0.2441, -0.2805, -0.2283,  ...,  0.2722, -0.1915, -0.2732]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3037, 0.4631, 0.2329],\n",
       "          [0.3066, 0.5215, 0.1720]], dtype=torch.float16),\n",
       "  'df_boxes':         xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  69.961945   53.512833  382.169342  557.647156    0.952712     23  giraffe\n",
       "  1  41.141174  283.048798  237.313171  491.447296    0.932231     23  giraffe,\n",
       "  'caption': ['Smaller giraffe that is eating.',\n",
       "   'A giraffe with grass hanging out of its mouth.'],\n",
       "  'bbox_target': [41.71, 276.13, 162.51, 211.42]},\n",
       " 57: {'image_emb': tensor([[ 0.2566,  0.2208, -0.2291,  ...,  0.9702,  0.5815, -0.0475],\n",
       "          [ 0.3682,  0.2198, -0.1796,  ...,  0.9038,  0.0359, -0.4585],\n",
       "          [ 0.3518,  0.4131,  0.2292,  ...,  0.6465,  0.3411, -0.2759],\n",
       "          [ 0.4905,  0.3135,  0.1812,  ...,  0.7964,  0.3589, -0.3408],\n",
       "          [ 0.3950,  0.4404,  0.1075,  ...,  0.8555,  0.2766, -0.2644],\n",
       "          [ 0.3264,  0.2456, -0.2264,  ...,  0.9390,  0.1630, -0.3440]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3862,  0.2037, -0.2808,  ...,  0.7827,  0.1758, -0.6143],\n",
       "          [ 0.3723,  0.0484, -0.4082,  ...,  0.5962,  0.5322, -0.6450]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.2926e-03, 5.8398e-01, 1.6571e-02, 5.4665e-03, 1.3599e-03, 3.8306e-01],\n",
       "          [1.6068e-02, 3.4888e-01, 2.1744e-03, 7.0620e-04, 2.1195e-04, 6.3184e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  name\n",
       "  0   76.480728    0.732895  348.729187  211.467957    0.936544     33  kite\n",
       "  1  319.205322   75.036911  639.064575  371.996948    0.928787     33  kite\n",
       "  2  508.423279  302.009674  601.747009  411.204895    0.909267     14  bird\n",
       "  3  102.706467  270.179565  235.731705  381.632446    0.897890     14  bird\n",
       "  4  377.154358    0.000000  513.458374   89.094238    0.886084     14  bird\n",
       "  5  377.004883    0.000000  513.281799   88.471375    0.675337     33  kite,\n",
       "  'caption': ['Yellow octopus with green eyes and long tentacle kites up in the sky with several other kites.',\n",
       "   'yellow jellyfish kite flying in sky'],\n",
       "  'bbox_target': [313.77, 73.89, 321.45, 301.29]},\n",
       " 58: {'image_emb': tensor([[-1.1218e-01,  6.2207e-01, -2.9370e-01,  ...,  6.7920e-01,\n",
       "           -3.1104e-01,  1.4153e-03],\n",
       "          [ 1.1163e-01,  7.4951e-02, -5.2344e-01,  ...,  2.9761e-01,\n",
       "            3.2788e-01,  3.4790e-02],\n",
       "          [-1.7297e-01,  2.7759e-01, -3.7524e-01,  ...,  1.5107e+00,\n",
       "            6.0547e-02, -1.7981e-01],\n",
       "          [-9.2163e-02, -1.3318e-01, -4.0845e-01,  ...,  6.8213e-01,\n",
       "           -5.8691e-01, -3.4106e-01],\n",
       "          [-3.7988e-01,  8.7988e-01,  7.6843e-02,  ...,  4.5435e-01,\n",
       "           -2.2681e-01,  2.0093e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0505,  0.1362,  0.0325,  ...,  0.2722,  0.1663,  0.0107],\n",
       "          [ 0.2161,  0.1516, -0.0054,  ..., -0.0040,  0.1671, -0.2267]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0047, 0.0060, 0.0086, 0.0209, 0.9600],\n",
       "          [0.0892, 0.0449, 0.0500, 0.0083, 0.8076]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   298.616852  140.936417  497.598969  420.210083    0.932563      0   \n",
       "  1   369.704041  231.578217  476.140686  424.177979    0.880066     26   \n",
       "  2   244.612366   84.057816  632.440063  195.157532    0.858580      4   \n",
       "  3   466.560303  189.902344  482.048889  230.620667    0.721695      0   \n",
       "  4   531.023193  196.513000  545.178589  233.427612    0.570010      0   \n",
       "  5   118.734673  177.355865  130.435638  201.149628    0.470322      0   \n",
       "  6    87.929314  172.389008   95.887520  194.487274    0.393664      0   \n",
       "  7   233.224365  179.952606  243.224762  207.019379    0.347836      0   \n",
       "  8    11.556305  150.758972  251.469391  190.957642    0.333623      4   \n",
       "  9   136.073898  177.890533  145.553665  202.238861    0.300408      0   \n",
       "  10  125.467178  177.922974  137.847488  201.729797    0.261402      0   \n",
       "  11  247.477020  184.240417  256.350311  206.500732    0.259669      0   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1    handbag  \n",
       "  2   airplane  \n",
       "  3     person  \n",
       "  4     person  \n",
       "  5     person  \n",
       "  6     person  \n",
       "  7     person  \n",
       "  8   airplane  \n",
       "  9     person  \n",
       "  10    person  \n",
       "  11    person  ,\n",
       "  'caption': ['the fucilage of the airplane',\n",
       "   'LANDING THE AEROPLAIN NEAR BY THE MAHN'],\n",
       "  'bbox_target': [238.37, 134.98, 298.68, 59.35]},\n",
       " 59: {'image_emb': tensor([[ 0.1952, -0.0906,  0.1333,  ...,  0.1954, -0.0735,  0.0638],\n",
       "          [ 0.1222,  0.2473, -0.0870,  ...,  0.9419,  0.2803,  0.0290],\n",
       "          [ 0.2896, -0.2209,  0.1140,  ...,  0.2756, -0.1501, -0.0316]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1125, -0.1160,  0.2871,  ..., -0.0831, -0.1725, -0.5386],\n",
       "          [ 0.0733,  0.0316, -0.2151,  ..., -0.4197, -0.0494, -0.2469]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1503, 0.6626, 0.1870],\n",
       "          [0.2998, 0.5098, 0.1906]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class      name\n",
       "  0    3.522339  57.495941  640.000000  470.597778    0.931754      4  airplane\n",
       "  1  434.916687   7.789749  637.889221   96.731003    0.787855      4  airplane\n",
       "  2    0.532581  29.548080   61.639164   81.934464    0.383193      4  airplane,\n",
       "  'caption': ['A white aircraft.',\n",
       "   'AN AEROPLANE IS STANDING BESIDE  ANOTHER ONE'],\n",
       "  'bbox_target': [425.74, 8.03, 213.67, 93.51]},\n",
       " 60: {'image_emb': tensor([[ 0.6914,  0.2559,  0.0656,  ...,  0.3586,  0.1124, -0.2700],\n",
       "          [-0.0667,  0.2480,  0.3330,  ...,  0.9121,  0.3865,  0.0279],\n",
       "          [ 0.1484,  0.2942,  0.2507,  ...,  0.3176,  0.1532, -0.1759],\n",
       "          [ 0.0602, -0.0310,  0.1530,  ...,  0.3311,  0.2485,  0.0045]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1470, -0.1947, -0.3296,  ..., -0.9014,  0.5293, -0.3516],\n",
       "          [-0.1757,  0.1294, -0.2932,  ...,  0.0709, -0.0279, -0.2542]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.5711e-05, 9.9463e-01, 1.9817e-03, 3.1166e-03],\n",
       "          [7.3433e-05, 9.2188e-01, 4.4495e-02, 3.3569e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   19.346283    7.716187  270.554413  346.149902    0.922745     53   \n",
       "  1  246.027023  247.679901  520.546814  474.858521    0.917396      0   \n",
       "  2    0.234787    4.027252  410.168579  472.170776    0.901841      0   \n",
       "  3    0.179878  347.741333   88.351189  429.048767    0.473034     56   \n",
       "  4    0.946068  346.516632  124.738701  475.175964    0.272407     56   \n",
       "  5  367.895813  362.932587  478.624023  406.919403    0.267704     79   \n",
       "  \n",
       "           name  \n",
       "  0       pizza  \n",
       "  1      person  \n",
       "  2      person  \n",
       "  3       chair  \n",
       "  4       chair  \n",
       "  5  toothbrush  ,\n",
       "  'caption': ['a baby looking angry', 'A baby sitting in the woman lap.'],\n",
       "  'bbox_target': [250.25, 239.53, 267.5, 235.15]},\n",
       " 61: {'image_emb': tensor([[-0.2109,  0.0414,  0.2627,  ..., -0.0409, -0.0017, -0.2328],\n",
       "          [-0.3323,  0.3882, -0.1370,  ...,  0.5283,  0.0753, -0.2671],\n",
       "          [-0.2166,  0.0852,  0.2996,  ...,  0.2186,  0.0068, -0.2893],\n",
       "          ...,\n",
       "          [ 0.0425, -0.0318, -0.4412,  ...,  0.4614,  0.0952, -0.2212],\n",
       "          [ 0.1989,  0.0007, -0.3660,  ...,  0.4670,  0.0388, -0.1996],\n",
       "          [-0.1694, -0.2988, -0.1160,  ..., -0.1891,  0.2966, -0.3279]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1605, -0.0465, -0.3171,  ...,  0.1572, -0.0455, -0.3533],\n",
       "          [-0.1605, -0.0465, -0.3171,  ...,  0.1572, -0.0455, -0.3533]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0552, 0.1647, 0.2216, 0.1571, 0.0526, 0.0295, 0.0295, 0.0096, 0.2800],\n",
       "          [0.0552, 0.1647, 0.2216, 0.1571, 0.0526, 0.0295, 0.0295, 0.0096, 0.2800]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  418.324432   23.715652  592.665894  412.639038    0.941049      0   \n",
       "  1  379.412842   36.746216  425.989685  155.787292    0.928549      0   \n",
       "  2  107.433113   22.160660  282.731689  412.378052    0.922760      0   \n",
       "  3   78.202087   36.172272  125.929001  155.061523    0.904824      0   \n",
       "  4  457.211548  128.749771  593.317261  196.622864    0.881735     38   \n",
       "  5  144.764496  129.410233  282.142212  196.226898    0.878955     38   \n",
       "  6  530.049622  139.614120  551.623352  157.564270    0.844489     32   \n",
       "  7  218.825073  139.506699  240.104462  158.580902    0.834327     32   \n",
       "  8   88.203934   87.171616   97.445160   97.372208    0.293727     32   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3         person  \n",
       "  4  tennis racket  \n",
       "  5  tennis racket  \n",
       "  6    sports ball  \n",
       "  7    sports ball  \n",
       "  8    sports ball  ,\n",
       "  'caption': ['The woman hitting a tennis ball to the right of an identical woman.',\n",
       "   'The woman hitting a tennis ball to the right of an identical woman.'],\n",
       "  'bbox_target': [421.32, 24.94, 171.83, 390.26]},\n",
       " 62: {'image_emb': tensor([[ 0.1016, -0.1221, -0.2051,  ...,  0.2023,  0.2576,  0.0074],\n",
       "          [-0.2583, -0.1677, -0.1860,  ...,  0.1687,  0.3306,  0.1267],\n",
       "          [-0.2238, -0.0854, -0.1483,  ...,  0.3284,  0.6050,  0.0024],\n",
       "          [ 0.2463,  0.0507, -0.3093,  ...,  1.0430, -0.0464,  0.1127],\n",
       "          [-0.0411,  0.0249, -0.0340,  ...,  0.3662,  0.2202,  0.1306]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2163, -0.2686, -0.0388,  ...,  0.1498,  0.0922,  0.3235],\n",
       "          [-0.2307, -0.2649,  0.1677,  ..., -0.0362,  0.0623,  0.2253]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.6069e-01, 1.1469e-01, 9.8083e-02, 2.0266e-06, 3.2666e-01],\n",
       "          [1.4197e-01, 7.2070e-01, 2.3178e-02, 1.7881e-07, 1.1407e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0   27.990356  160.968246  457.031128  638.592773    0.958706     22    zebra\n",
       "  1    0.802704  400.592865  398.719757  639.718750    0.890762     22    zebra\n",
       "  2  261.235596   13.372498  456.882324  321.208466    0.865080     22    zebra\n",
       "  3  238.501495   60.058506  269.815399  107.340179    0.814341     23  giraffe,\n",
       "  'caption': ['Zebra leaning under other zebra.',\n",
       "   'A young zebra feeding from its mother.'],\n",
       "  'bbox_target': [2.88, 412.32, 378.24, 218.61]},\n",
       " 63: {'image_emb': tensor([[-2.4500e-01,  6.3525e-01, -3.3301e-01,  ...,  1.2393e+00,\n",
       "           -1.3733e-01, -2.9321e-01],\n",
       "          [-3.0347e-01,  2.1655e-01,  1.6642e-03,  ...,  1.4443e+00,\n",
       "           -8.8013e-02,  8.4595e-02],\n",
       "          [-1.1017e-01,  3.1934e-01, -1.3855e-01,  ...,  8.8281e-01,\n",
       "           -3.0078e-01, -9.9277e-04],\n",
       "          ...,\n",
       "          [-4.8560e-01,  4.7974e-01, -6.5308e-02,  ...,  1.3486e+00,\n",
       "           -9.7717e-02, -6.5857e-02],\n",
       "          [-3.1006e-01,  9.5703e-02, -2.7466e-01,  ...,  1.3584e+00,\n",
       "            1.0052e-01, -4.5662e-03],\n",
       "          [-9.5398e-02,  2.6709e-01, -1.9775e-01,  ...,  3.2690e-01,\n",
       "           -2.1008e-01, -8.8562e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3132, -0.3271, -0.3584,  ...,  0.0958, -0.0496, -0.0754],\n",
       "          [-0.4817, -0.0640, -0.3567,  ...,  0.6025, -0.2578, -0.1244]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.2210e-04, 1.2276e-02, 1.9228e-04, 8.6212e-04, 6.8605e-05, 9.7510e-01,\n",
       "           1.0017e-02, 9.7656e-04],\n",
       "          [1.2665e-03, 3.5534e-03, 2.6131e-04, 1.0967e-05, 7.1754e-03, 9.8486e-01,\n",
       "           2.6398e-03, 7.8678e-06]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.436611  122.182129   75.855713  281.162659    0.916425     67   \n",
       "  1  513.673523  273.302124  639.561340  315.745605    0.803446     43   \n",
       "  2  277.160797   20.588440  412.258698  174.682648    0.799531     48   \n",
       "  3    0.000000    0.679276  637.676208  353.724182    0.790784     60   \n",
       "  4  563.899109   80.058929  639.550354  144.864655    0.775788     67   \n",
       "  5   41.438400   82.552292  129.906158  329.600281    0.725305     42   \n",
       "  6  572.428467  215.114868  640.000000  257.595032    0.711332     42   \n",
       "  7   50.129421   80.217529  168.935043  337.637756    0.697684     43   \n",
       "  \n",
       "             name  \n",
       "  0    cell phone  \n",
       "  1         knife  \n",
       "  2      sandwich  \n",
       "  3  dining table  \n",
       "  4    cell phone  \n",
       "  5          fork  \n",
       "  6          fork  \n",
       "  7         knife  ,\n",
       "  'caption': ['Silverware sitting on table to the left of breakfast.',\n",
       "   'Metal fork placed beside metal butter knife and cell phone.'],\n",
       "  'bbox_target': [39.4, 84.47, 88.72, 242.95]},\n",
       " 64: {'image_emb': tensor([[-0.4121,  0.1605, -0.4060,  ...,  0.2269, -0.0802, -0.7842],\n",
       "          [-0.9448,  0.2408, -0.0093,  ...,  0.9971,  0.0148,  0.1694],\n",
       "          [-0.1818,  0.1333, -0.3181,  ...,  1.2061, -0.1167, -0.0144],\n",
       "          ...,\n",
       "          [-0.3826,  0.3406, -0.0743,  ...,  1.2920, -0.1486,  0.2593],\n",
       "          [-0.1481,  0.0591, -0.1537,  ...,  0.7866,  0.1037, -0.0180],\n",
       "          [-0.3843,  0.0536, -0.1941,  ..., -0.2048,  0.2423, -0.2729]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1915, -0.1263, -0.0886,  ...,  0.6211, -0.1649, -0.0771],\n",
       "          [-0.0651,  0.0144, -0.3677,  ..., -0.1882, -0.2086, -0.2961]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0748e-01, 7.0496e-02, 1.0276e-04, 6.3171e-02, 4.7803e-05, 4.7517e-04,\n",
       "           2.9445e-05, 1.4484e-04, 7.5781e-01],\n",
       "          [5.2441e-01, 1.5503e-01, 1.4963e-03, 1.1163e-01, 1.3199e-03, 5.8250e-03,\n",
       "           4.5609e-04, 9.8133e-04, 1.9897e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   106.667694  151.697464  308.199554  612.563782    0.933959      0   \n",
       "  1    26.631607   98.060783  140.211411  169.024841    0.912909     38   \n",
       "  2   295.687103  173.372711  414.965057  473.151520    0.904059      0   \n",
       "  3     1.065033  259.944244  292.695984  382.775360    0.875200      2   \n",
       "  4   378.222778  308.108612  446.915833  425.100922    0.874379     56   \n",
       "  5   114.137390   18.472017  132.106537   36.930428    0.866106     32   \n",
       "  6     0.782333  232.761353  122.159775  291.669312    0.790391      2   \n",
       "  7   112.716049  224.631897  185.197845  259.270050    0.745963      2   \n",
       "  8   373.344757  230.172333  447.945129  332.755035    0.628879      2   \n",
       "  9   219.253967  247.081146  309.452393  350.959747    0.538016      2   \n",
       "  10  109.182922  224.476730  267.500854  260.252869    0.291508      2   \n",
       "  \n",
       "               name  \n",
       "  0          person  \n",
       "  1   tennis racket  \n",
       "  2          person  \n",
       "  3             car  \n",
       "  4           chair  \n",
       "  5     sports ball  \n",
       "  6             car  \n",
       "  7             car  \n",
       "  8             car  \n",
       "  9             car  \n",
       "  10            car  ,\n",
       "  'caption': ['A fat girl, trying to hit the serve in the tennis court',\n",
       "   'the woman hitting the ball'],\n",
       "  'bbox_target': [109.55, 149.19, 210.45, 461.26]},\n",
       " 65: {'image_emb': tensor([[ 0.1451,  0.0627, -0.3191,  ...,  1.6064,  0.5303,  0.1667],\n",
       "          [ 0.0564, -0.0668, -0.2316,  ...,  1.5703,  0.6079,  0.2289],\n",
       "          [ 0.1282, -0.0526, -0.2832,  ...,  1.0869,  0.5405,  0.0099]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0151, -0.2428, -0.3223,  ...,  0.5005,  0.6255, -0.0324],\n",
       "          [ 0.0847, -0.0012, -0.4087,  ...,  0.1906,  0.5239,  0.0140]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2754, 0.6714, 0.0534],\n",
       "          [0.0392, 0.0253, 0.9355]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class name\n",
       "  0  193.312042  75.814888  487.363525  289.476379    0.942433     16  dog\n",
       "  1   33.737904  61.557915  207.669998  366.302643    0.928777     16  dog,\n",
       "  'caption': ['Pug with heart shaped tag dangling from his neck.',\n",
       "   'pug on the left side'],\n",
       "  'bbox_target': [37.32, 62.64, 169.08, 308.46]},\n",
       " 66: {'image_emb': tensor([[ 0.3235,  0.5371, -0.0183,  ...,  0.5356, -0.2083,  0.0042],\n",
       "          [-0.3311,  0.2622,  0.2416,  ...,  1.2480, -0.0538, -0.4355],\n",
       "          [-0.0300,  0.5088, -0.2590,  ...,  1.0107,  0.1418, -0.3364],\n",
       "          [ 0.0926,  0.5664,  0.0425,  ...,  0.4470, -0.0707, -0.0065]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2056,  0.1203,  0.3093,  ...,  0.4700, -0.6016, -0.4856],\n",
       "          [-0.3677,  0.3230,  0.2715,  ...,  0.3953, -0.5869, -0.6123]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.0374e-02, 6.3801e-04, 3.9258e-01, 5.3662e-01],\n",
       "          [2.2232e-02, 1.7147e-03, 7.8369e-01, 1.9214e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  111.360779   57.080368  406.965332  319.431580    0.915166     55   \n",
       "  1  535.180725   48.327545  635.616394  227.072571    0.913867     42   \n",
       "  2  385.875031  136.571823  517.415405  302.900330    0.800650     55   \n",
       "  3  198.218170    0.233231  289.536011   27.279099    0.634983     41   \n",
       "  4    0.048981    0.000000  638.705078  471.908997    0.621698     60   \n",
       "  5  115.717270   23.799271  247.719101   63.292343    0.546633     42   \n",
       "  6  293.423401   23.407425  443.293579   81.021622    0.491381     42   \n",
       "  7  310.654724    3.687469  406.753296   39.641815    0.337860     42   \n",
       "  \n",
       "             name  \n",
       "  0          cake  \n",
       "  1          fork  \n",
       "  2          cake  \n",
       "  3           cup  \n",
       "  4  dining table  \n",
       "  5          fork  \n",
       "  6          fork  \n",
       "  7          fork  ,\n",
       "  'caption': ['A whip cream creation for a dessert plate',\n",
       "   'Decorative pile of whip cream on a white plate.'],\n",
       "  'bbox_target': [389.39, 136.99, 129.44, 166.11]},\n",
       " 67: {'image_emb': tensor([[ 0.1329,  0.2808, -0.1304,  ...,  0.9922,  0.2749,  0.0813],\n",
       "          [-0.1592,  0.0905, -0.1241,  ...,  0.9131,  0.4600,  0.0983],\n",
       "          [ 0.0277,  0.1207, -0.4241,  ...,  1.0352,  0.2969,  0.0634],\n",
       "          ...,\n",
       "          [ 0.1946, -0.0886, -0.2102,  ...,  0.8472,  0.0562, -0.0777],\n",
       "          [ 0.0602, -0.0135, -0.2473,  ...,  0.9644,  0.1272, -0.1818],\n",
       "          [ 0.0099,  0.2505, -0.1538,  ...,  0.8667,  0.3967, -0.0803]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2803,  0.3491, -0.3608,  ..., -0.0221,  0.1506, -0.2249],\n",
       "          [ 0.1956,  0.7041, -0.2235,  ..., -0.2260,  0.2869, -0.0461]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.8682e-01, 4.1618e-03, 8.9979e-04, 7.1585e-05, 2.3246e-05, 6.3181e-05,\n",
       "           7.8964e-03],\n",
       "          [9.9902e-01, 4.4394e-04, 9.8348e-06, 2.1756e-05, 1.5497e-06, 5.6028e-06,\n",
       "           5.0306e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   33.375488   23.926746  217.983002  371.502686    0.947948      0   \n",
       "  1  277.247925   68.214241  493.235596  361.468872    0.945945      0   \n",
       "  2  264.267639  172.447723  294.906342  280.065094    0.880465     39   \n",
       "  3  230.079437  237.910751  255.107040  335.529480    0.868977     39   \n",
       "  4  186.058090  220.207031  215.404938  272.722565    0.741251     40   \n",
       "  5   18.334352  229.335358   39.263580  373.462921    0.718155     56   \n",
       "  6  192.467621  295.836792  462.528137  373.737000    0.666307     60   \n",
       "  7  459.615662  280.886322  499.488892  373.807129    0.402216     56   \n",
       "  8  165.583115  197.315903  212.574478  219.852493    0.400187     60   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2        bottle  \n",
       "  3        bottle  \n",
       "  4    wine glass  \n",
       "  5         chair  \n",
       "  6  dining table  \n",
       "  7         chair  \n",
       "  8  dining table  ,\n",
       "  'caption': ['A man holding a wineglass.',\n",
       "   'A man in a black shirt with glasses holding a glass.'],\n",
       "  'bbox_target': [32.95, 22.81, 185.05, 348.97]},\n",
       " 68: {'image_emb': tensor([[-0.2323,  0.1949, -0.1234,  ...,  0.8447,  0.0688,  0.2319],\n",
       "          [-0.4153,  0.1231,  0.1595,  ...,  1.3857,  0.0305,  0.0809],\n",
       "          [-0.2231,  0.1434,  0.0442,  ...,  0.6885, -0.0097, -0.1844],\n",
       "          ...,\n",
       "          [ 0.2185, -0.0318, -0.3262,  ...,  0.9492, -0.0767,  0.0152],\n",
       "          [ 0.0021,  0.5820, -0.3992,  ...,  1.3691, -0.3135,  0.1135],\n",
       "          [-0.1132, -0.3633, -0.1681,  ...,  0.3020, -0.0523,  0.3374]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0529, -0.1149, -0.3284,  ...,  0.0552,  0.1741,  0.3301],\n",
       "          [-0.1725,  0.1663, -0.3167,  ..., -0.0128,  0.1510,  0.2057]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.0664e-01, 6.9397e-02, 5.0068e-05, 9.4461e-04, 2.3878e-04, 8.4639e-04,\n",
       "           1.3742e-03, 1.1987e-01, 6.2895e-04],\n",
       "          [1.1115e-01, 1.0773e-01, 3.3438e-05, 3.4084e-03, 1.1307e-04, 1.6356e-03,\n",
       "           4.1771e-03, 7.7148e-01, 2.7132e-04]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0   232.853348  169.584946  402.502350  371.923553    0.940129      0   person\n",
       "  1     0.034475  150.893448   81.166061  371.278473    0.887699      0   person\n",
       "  2   173.922424  157.456451  227.029831  202.833694    0.887359     23  giraffe\n",
       "  3   384.650513  161.578232  405.270386  208.656494    0.843760     23  giraffe\n",
       "  4     2.850580    9.311223  499.171387  370.228668    0.823119      5      bus\n",
       "  5    52.429806  176.825760   85.234138  217.358444    0.807366     23  giraffe\n",
       "  6   176.821838  170.388504  193.811478  202.230453    0.799290     23  giraffe\n",
       "  7   389.133728  235.552902  499.602936  371.494293    0.774415      0   person\n",
       "  8   330.250488  270.863831  369.431976  318.507904    0.510748      0   person\n",
       "  9   374.212158  198.786545  391.887360  220.419891    0.492239     23  giraffe\n",
       "  10  367.870819  169.471069  376.813354  197.038361    0.456710     23  giraffe\n",
       "  11  360.537415  175.910019  370.829895  195.774292    0.316352     23  giraffe\n",
       "  12  441.809021  216.263870  499.731110  274.225342    0.303261      0   person,\n",
       "  'caption': ['The back of the person wearing a striped shirt',\n",
       "   'a man in a striped shirt'],\n",
       "  'bbox_target': [388.4, 216.07, 111.6, 158.7]},\n",
       " 69: {'image_emb': tensor([[-0.4172,  0.3215,  0.2488,  ...,  1.0088, -0.1727, -0.0792],\n",
       "          [ 0.1385, -0.1520, -0.2150,  ...,  0.9023, -0.0643, -0.0244],\n",
       "          [-0.0925,  0.4995, -0.1259,  ...,  1.1572,  0.2200,  0.0185],\n",
       "          ...,\n",
       "          [ 0.0398, -0.2537,  0.0971,  ...,  0.8335,  0.1744, -0.5542],\n",
       "          [-0.4143,  0.2090, -0.3711,  ...,  1.1797, -0.0187,  0.1096],\n",
       "          [-0.3596, -0.0811,  0.1082,  ...,  1.0869,  0.2644, -0.2949]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0506,  0.5996, -0.2551,  ..., -0.1300, -0.0668, -0.1910],\n",
       "          [-0.3044,  0.2756, -0.1418,  ..., -0.1989, -0.0765, -0.7822]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.1890e-01, 5.7617e-01, 2.1204e-01, 7.0984e-02, 4.4525e-05, 8.4839e-03,\n",
       "           2.8858e-03, 2.6846e-04, 1.2094e-04, 1.0071e-02],\n",
       "          [2.8870e-02, 4.9896e-02, 8.8428e-01, 1.4977e-02, 4.5228e-04, 7.1869e-03,\n",
       "           1.0195e-03, 5.8670e-03, 7.8619e-05, 7.3013e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     0.000000  100.141602  294.615204  424.274475    0.942124      0   \n",
       "  1   198.981873  120.813995  346.281189  413.024048    0.897310      0   \n",
       "  2   165.766708  193.861176  209.618912  263.527130    0.881608     41   \n",
       "  3   258.318634  116.127472  412.579071  381.744293    0.880697      0   \n",
       "  4   373.540009  346.762024  422.771454  426.978394    0.821472     39   \n",
       "  5   484.525604  372.660400  532.093445  427.122131    0.787281     41   \n",
       "  6   275.810150  253.061981  311.220490  308.165131    0.756798     41   \n",
       "  7     2.240417    2.089584  639.987061  222.373169    0.748778     25   \n",
       "  8   603.701477  278.940277  639.968689  422.747070    0.736070      0   \n",
       "  9   310.498840  389.865509  375.649719  427.433655    0.661690     45   \n",
       "  10  421.044861  351.978516  476.038452  426.452515    0.652994     39   \n",
       "  11  479.155426  314.508911  517.032349  378.436340    0.597408     39   \n",
       "  12  434.289124  280.596741  465.760681  338.424377    0.551052     39   \n",
       "  13  400.324585  309.573486  420.045044  350.185181    0.299507     39   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2        cup  \n",
       "  3     person  \n",
       "  4     bottle  \n",
       "  5        cup  \n",
       "  6        cup  \n",
       "  7   umbrella  \n",
       "  8     person  \n",
       "  9       bowl  \n",
       "  10    bottle  \n",
       "  11    bottle  \n",
       "  12    bottle  \n",
       "  13    bottle  ,\n",
       "  'caption': ['A man wearing brown jacket and black pants drinking coffee from a white cup.',\n",
       "   'A man wearing a hat sipping on a white cup.'],\n",
       "  'bbox_target': [1.92, 102.62, 280.85, 319.32]},\n",
       " 70: {'image_emb': tensor([[ 0.2467,  0.2443,  0.4119,  ...,  0.4253, -0.1208, -0.0054],\n",
       "          [-0.3083,  0.0071, -0.0393,  ...,  0.4651, -0.3120,  0.1017],\n",
       "          [-0.0134,  0.3174,  0.0690,  ...,  0.7192,  0.0897,  0.1595],\n",
       "          [ 0.0125,  0.1161,  0.0635,  ...,  0.3271, -0.1199, -0.1029],\n",
       "          [ 0.0208,  0.2507,  0.3091,  ...,  0.6978, -0.3872, -0.0228],\n",
       "          [ 0.0471,  0.0406,  0.4131,  ...,  0.5474, -0.3389, -0.1953]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0557,  0.1482, -0.3855,  ...,  0.0980,  0.0186,  0.0407],\n",
       "          [ 0.2013,  0.0926,  0.0302,  ..., -0.2239,  0.0358, -0.1801]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.9360e-01, 2.0416e-02, 6.2500e-01, 1.2311e-01, 8.9188e-03, 2.8778e-02],\n",
       "          [1.6546e-03, 4.3774e-01, 5.2832e-01, 3.6001e-05, 2.2141e-02, 1.0300e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    3.437483   21.478188  260.829224  366.396149    0.946529      0  person\n",
       "  1  278.393555  271.406067  462.341736  347.948730    0.913084     63  laptop\n",
       "  2    9.792101  175.298370  196.202545  303.538086    0.899160     63  laptop\n",
       "  3  256.319092   27.178240  447.989319  276.388214    0.886308      0  person\n",
       "  4    1.569390   23.066557  500.000000  368.649200    0.737661     57   couch\n",
       "  5   90.605270  232.172638  110.619606  251.399994    0.502072     47   apple\n",
       "  6  160.378708    4.544437  486.448822   83.908501    0.465689     57   couch,\n",
       "  'caption': ['The laptop front of the man with glasses hanging around his neck',\n",
       "   'A laptop almost covered in stickers.'],\n",
       "  'bbox_target': [278.43, 271.51, 184.77, 74.99]},\n",
       " 71: {'image_emb': tensor([[-0.2515,  0.2871, -0.2278,  ...,  0.9819, -0.0350, -0.3035],\n",
       "          [-0.2493,  0.4565, -0.1669,  ...,  1.0303,  0.1450, -0.1571],\n",
       "          [-0.1814, -0.0917, -0.2659,  ...,  1.0547,  0.0147, -0.2749],\n",
       "          [-0.2051,  0.2993, -0.1696,  ...,  1.0078,  0.2045, -0.3767],\n",
       "          [-0.3691,  0.2532, -0.1879,  ...,  0.8804,  0.0331,  0.1964]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0362, -0.2654, -0.1377,  ...,  0.2634, -0.2947, -0.4207],\n",
       "          [-0.0806, -0.0051, -0.2028,  ..., -0.1591, -0.1324, -0.1375]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.8140e-01, 2.0874e-01, 2.5606e-04, 6.0400e-01, 5.4779e-03],\n",
       "          [1.1298e-01, 1.1298e-01, 1.0145e-04, 7.6025e-01, 1.3710e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   66.718170  239.841187  346.107574  638.796875    0.940566     17   horse\n",
       "  1  159.756714   43.107529  320.525391  467.764771    0.909043      0  person\n",
       "  2  313.499054   88.808372  350.603851  149.255844    0.891777      0  person\n",
       "  3  266.774048  146.462341  426.987183  336.228882    0.885509     17   horse\n",
       "  4  369.679138  131.161682  426.336487  293.832336    0.504149     17   horse,\n",
       "  'caption': ['A horse with a white snout.',\n",
       "   'A brown horse standing in the background, waiting for a rider'],\n",
       "  'bbox_target': [274.68, 148.33, 152.32, 192.27]},\n",
       " 72: {'image_emb': tensor([[-0.0687, -0.3494,  0.0701,  ...,  0.9346,  0.2458,  0.3269],\n",
       "          [ 0.0948,  0.0195,  0.0938,  ...,  1.1846, -0.2549,  0.1360],\n",
       "          [ 0.2070,  0.1366, -0.2123,  ...,  0.9453,  0.1543, -0.1921],\n",
       "          ...,\n",
       "          [-0.3062, -0.1697, -0.0906,  ...,  0.9575,  0.2162,  0.2439],\n",
       "          [-0.1127,  0.1619, -0.2568,  ...,  1.4268,  0.2705,  0.0024],\n",
       "          [-0.0806, -0.3708,  0.0681,  ...,  0.7197,  0.2228,  0.4031]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0442,  0.0766, -0.0346,  ...,  0.2465,  0.0078,  0.0602],\n",
       "          [-0.2527,  0.0017, -0.4353,  ..., -0.2201,  0.1908,  0.0961]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.8909e-02, 3.6168e-04, 1.8477e-06, 6.8545e-06, 4.6444e-04, 9.2236e-01,\n",
       "           2.8896e-03, 5.1498e-03],\n",
       "          [1.8692e-02, 4.7541e-04, 2.0766e-04, 2.7514e-04, 4.0030e-04, 8.7305e-01,\n",
       "           9.3445e-02, 1.3466e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     0.000000  287.975616  424.899292  638.694458    0.944368     20   \n",
       "  1    21.344879  198.938492  231.021027  493.897339    0.940962      0   \n",
       "  2   112.978104   27.547745  269.197937  341.295837    0.928058      0   \n",
       "  3   222.485046   20.961731  388.307922  381.986542    0.899154      0   \n",
       "  4   115.488556    0.000000  178.243591   88.300232    0.840351      0   \n",
       "  5     0.000000  100.400513  106.601181  333.677673    0.758817     20   \n",
       "  6    65.057251   75.404602  170.641327  168.536484    0.741778     20   \n",
       "  7   145.824295  168.768051  199.952881  301.280212    0.639917     26   \n",
       "  8   348.821960    0.186766  376.075317   38.820946    0.502136     20   \n",
       "  9   199.194824  173.969360  261.868408  318.013428    0.491563     24   \n",
       "  10  404.670441  132.977341  427.000000  322.011841    0.440749     20   \n",
       "  11  197.662811  170.476685  266.713348  316.354370    0.343058     13   \n",
       "  12   96.305923  168.316071  413.215515  320.486053    0.274025     13   \n",
       "  13  199.155182  171.554047  261.100555  316.923553    0.266425     26   \n",
       "  \n",
       "          name  \n",
       "  0   elephant  \n",
       "  1     person  \n",
       "  2     person  \n",
       "  3     person  \n",
       "  4     person  \n",
       "  5   elephant  \n",
       "  6   elephant  \n",
       "  7    handbag  \n",
       "  8   elephant  \n",
       "  9   backpack  \n",
       "  10  elephant  \n",
       "  11     bench  \n",
       "  12     bench  \n",
       "  13   handbag  ,\n",
       "  'caption': [\"An elephant whose leg is hidden behind a boy's cap.\",\n",
       "   'The elephant in the background on the left.'],\n",
       "  'bbox_target': [0.0, 103.63, 106.2, 225.87]},\n",
       " 73: {'image_emb': tensor([[-0.1624,  0.3472, -0.0679,  ...,  0.8662, -0.0615, -0.2125],\n",
       "          [-0.4253,  0.2573, -0.1460,  ...,  1.2666,  0.1189, -0.5669],\n",
       "          [-0.1980,  0.5791, -0.1698,  ...,  1.0820,  0.2445, -0.6411],\n",
       "          [-0.1166,  0.4414, -0.4075,  ...,  1.3506, -0.2468, -0.1395],\n",
       "          [ 0.2223,  0.2859,  0.1968,  ...,  0.7363,  0.0422, -0.2417]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-7.3364e-02,  3.4302e-01,  1.9385e-01,  5.7471e-01, -5.3253e-02,\n",
       "            2.4670e-01, -2.6978e-01, -6.2207e-01, -1.6162e-01,  5.0293e-01,\n",
       "           -3.1958e-01,  1.5991e-01,  8.8440e-02,  2.0044e-01,  3.7329e-01,\n",
       "           -1.9031e-01,  3.2617e-01, -1.8567e-01,  1.7273e-01,  2.8857e-01,\n",
       "            1.2238e-01,  2.1045e-01, -1.9690e-01,  4.0698e-01,  8.1970e-02,\n",
       "            2.3181e-01, -1.5295e-01,  2.0007e-01, -5.9229e-01,  2.9834e-01,\n",
       "           -5.9082e-02, -1.3330e-01, -4.1504e-02,  3.2031e-01, -1.7078e-01,\n",
       "           -3.0298e-01, -6.0455e-02, -9.9792e-02, -3.3765e-01,  9.3811e-02,\n",
       "            2.3718e-01,  8.0872e-02,  3.5620e-01,  3.4888e-01, -1.4966e-01,\n",
       "           -1.2039e-02,  3.6890e-01,  2.3560e-01, -1.7197e-02, -1.2115e-01,\n",
       "           -5.7800e-02, -5.5615e-01, -3.6719e-01, -3.3008e-01, -1.9141e-01,\n",
       "            6.1084e-01,  6.1188e-02,  4.5746e-02,  2.3514e-02,  4.2969e-02,\n",
       "            5.6946e-02, -4.2920e-01,  1.2189e-01, -6.6504e-01,  4.1199e-02,\n",
       "           -3.1616e-01,  1.7700e-01,  6.1084e-01, -1.0052e-01, -1.0992e-01,\n",
       "            1.0706e-01, -4.3457e-02,  4.6973e-01,  2.2949e-01, -1.7929e-02,\n",
       "            1.2781e-01, -3.3521e-01, -2.7148e-01, -5.6152e-01, -6.0272e-02,\n",
       "           -2.1582e-01,  1.7957e-01,  3.0933e-01, -1.1188e-01,  1.3196e-01,\n",
       "           -1.4783e-01, -6.2866e-02,  5.1270e-01, -2.5366e-01, -1.8823e-01,\n",
       "           -3.5596e-01, -1.2036e-01, -1.1475e+00,  1.6797e-01, -4.5020e-01,\n",
       "            1.2500e-01,  4.9927e-01, -1.2323e-01,  8.3862e-02,  2.6260e-02,\n",
       "            4.8755e-01,  1.4183e-02, -3.2288e-02, -3.0029e-01, -4.5947e-01,\n",
       "            4.2236e-01, -1.7065e-01, -1.9165e-01,  3.1665e-01,  1.3611e-01,\n",
       "           -1.1505e-01,  1.1084e-01,  2.6343e-01, -2.0703e-01, -1.2042e-01,\n",
       "            3.2898e-02,  1.5344e-01, -1.2018e-01, -1.8018e-01, -4.6295e-02,\n",
       "           -5.2673e-02, -6.6797e-01,  2.2461e-01,  5.0385e-02,  3.4717e-01,\n",
       "           -3.1934e-01,  1.4880e-01, -1.0907e-01,  3.4424e-01,  2.2030e-03,\n",
       "           -1.2378e-01, -1.0828e-01,  6.4648e-01,  3.8301e+00,  1.4490e-01,\n",
       "           -6.9092e-02, -5.0049e-01, -3.5303e-01, -2.2620e-01, -5.7312e-02,\n",
       "            1.5918e-01, -1.2659e-01, -3.5107e-01,  3.6304e-01, -1.5527e-01,\n",
       "           -2.0862e-01, -2.0630e-01, -4.9268e-01, -1.9214e-01,  3.5132e-01,\n",
       "            3.9291e-03, -3.2666e-01,  1.4885e-02, -1.5287e-03,  9.0332e-02,\n",
       "           -1.1395e-01, -2.5635e-02, -1.2494e-01, -2.1713e-02,  1.3196e-01,\n",
       "           -1.2988e-01, -2.1350e-01,  3.7041e-03, -2.6562e-01, -2.8979e-01,\n",
       "           -7.4341e-02,  1.4136e-01, -1.3257e-01,  2.4536e-01,  4.8065e-02,\n",
       "            4.1260e-01, -1.5063e-01,  1.1694e-01,  4.1895e-01,  6.3721e-02,\n",
       "            3.7256e-01,  3.7939e-01, -3.9575e-01,  4.2432e-01, -1.5820e-01,\n",
       "            8.9661e-02, -3.1641e-01, -1.8555e-01,  2.0178e-01, -5.0635e-01,\n",
       "            2.7661e-01, -4.1406e-01, -3.7866e-01,  3.7329e-01,  2.1436e-01,\n",
       "            5.2295e-01,  1.2195e-01,  1.3098e-01, -2.8662e-01, -4.7028e-02,\n",
       "           -2.4561e-01,  2.7686e-01,  2.6062e-02, -2.5659e-01, -6.8542e-02,\n",
       "           -2.6001e-02,  1.0284e-01, -1.4990e-01, -1.3989e-01, -7.2861e-03,\n",
       "           -6.5796e-02,  4.3994e-01, -2.5284e-02, -2.0752e-01, -7.0923e-02,\n",
       "            1.3477e-01,  1.7737e-01,  2.3767e-01, -1.6333e-01,  2.0483e-01,\n",
       "            7.7332e-02,  3.3472e-01, -2.8320e-01, -1.2189e-01,  2.5024e-03,\n",
       "            3.4546e-01, -2.2034e-01,  4.7021e-01,  1.9714e-01,  5.7983e-02,\n",
       "           -6.1615e-02,  3.9673e-02,  2.0782e-02, -9.9182e-02,  1.3016e-02,\n",
       "           -4.5013e-02,  3.7842e-02,  2.0081e-01,  2.0129e-01,  1.5918e-01,\n",
       "            7.1350e-02,  3.0103e-01,  2.0862e-01, -9.4849e-02, -8.4778e-02,\n",
       "            5.4359e-03,  6.0986e-01, -6.4087e-02,  1.3647e-01, -2.5854e-01,\n",
       "           -5.1727e-03,  1.9177e-01,  3.7354e-01,  4.2432e-01,  5.4443e-02,\n",
       "           -4.3604e-01, -1.4087e-01, -8.6487e-02, -1.1475e-01, -1.6687e-01,\n",
       "            5.3192e-02,  5.1849e-02, -2.0477e-02, -9.4604e-02,  3.6084e-01,\n",
       "           -2.9712e-01,  2.6855e-01,  1.8237e-01,  4.0674e-01, -1.0205e-01,\n",
       "           -2.7271e-01, -1.4270e-01,  5.6976e-02,  8.6975e-02, -2.6465e-01,\n",
       "           -3.0469e-01,  5.7526e-02, -2.6709e-01,  2.9126e-01,  2.3462e-01,\n",
       "            4.2969e-01,  2.6636e-01, -9.4238e-02, -5.0537e-02, -4.6802e-01,\n",
       "           -4.3793e-02, -1.6638e-01, -2.4490e-02,  2.8369e-01, -2.2620e-01,\n",
       "           -1.4526e-01, -8.3374e-02,  3.3911e-01,  3.0273e-01,  1.2067e-01,\n",
       "            2.1619e-01, -2.4048e-01, -8.0505e-02,  8.3069e-02, -6.9580e-02,\n",
       "            2.8662e-01, -7.7576e-02,  2.0923e-01,  2.5806e-01, -4.2651e-01,\n",
       "           -1.5039e-01,  1.8417e-02, -3.3875e-02, -7.7271e-02,  2.9272e-01,\n",
       "           -1.1841e-01,  5.8533e-02,  2.4438e-01, -3.6194e-02, -1.8726e-01,\n",
       "           -2.4704e-02,  2.5317e-01,  3.8301e+00, -2.4988e-01, -2.1655e-01,\n",
       "            1.1609e-01, -1.4612e-01, -1.4062e-01,  3.5858e-02,  9.5520e-02,\n",
       "            2.7368e-01,  1.6370e-01,  1.6138e-01,  4.9658e-01, -3.8483e-02,\n",
       "            5.6152e-01, -1.9922e-01, -3.8062e-01, -1.0016e-01, -1.5107e+00,\n",
       "           -6.2103e-02,  1.4929e-01,  3.2617e-01,  3.2666e-01,  2.7759e-01,\n",
       "           -7.2693e-02, -2.2437e-01,  3.6133e-02,  2.9102e-01,  6.0645e-01,\n",
       "           -2.5317e-01, -1.7676e-01,  1.8726e-01,  7.0312e-02, -1.4465e-01,\n",
       "           -1.0828e-01,  1.4319e-01, -2.6276e-02,  1.3147e-01, -5.3076e-01,\n",
       "            2.9443e-01, -2.5806e-01,  4.4434e-01, -2.8320e-01, -2.7026e-01,\n",
       "           -1.5442e-01, -3.3521e-01,  3.4082e-01,  1.3770e-01, -8.5510e-02,\n",
       "           -1.9373e-01, -1.1945e-01,  1.9446e-01, -2.6807e-01,  1.6040e-01,\n",
       "            1.9397e-01, -1.2128e-01, -1.5228e-02, -1.4661e-01, -2.9236e-02,\n",
       "           -1.4923e-02, -3.0640e-01,  3.8525e-01,  3.3813e-01,  5.2490e-01,\n",
       "           -3.6963e-01,  1.0938e-01,  8.5815e-02, -3.7915e-01, -1.7419e-01,\n",
       "            2.7893e-02, -3.5742e-01,  2.4158e-01, -3.1201e-01,  7.0007e-02,\n",
       "           -8.2336e-02, -2.0789e-01, -1.3257e-01, -1.5845e-01, -1.8274e-01,\n",
       "           -6.2500e-01,  1.5320e-01, -7.8003e-02,  9.0820e-02,  5.8154e-01,\n",
       "            1.7319e-02, -2.3071e-01, -1.6260e-01, -2.8662e-01,  5.6152e-01,\n",
       "           -7.3486e-02, -4.8877e-01,  5.3864e-02,  8.8501e-02, -2.2522e-01,\n",
       "           -1.8494e-01,  9.5459e-02,  2.0105e-01,  3.2520e-01,  1.5088e-01,\n",
       "            1.3321e-02, -4.6753e-01, -1.5637e-01,  1.2378e-01,  5.7892e-02,\n",
       "            1.6077e-01, -6.8750e-01, -1.2671e-01, -4.0283e-02,  2.0288e-01,\n",
       "            3.0884e-01,  1.5552e-01,  2.3120e-01, -2.1411e-01, -6.1096e-02,\n",
       "            1.6309e-01, -3.0347e-01, -6.0883e-02,  4.7211e-02,  3.6353e-01,\n",
       "           -6.8237e-02, -1.0461e-01,  1.3892e-01,  5.1416e-01,  3.8361e-02,\n",
       "            4.9744e-02, -5.8398e-01,  1.6052e-01,  1.5393e-01,  4.6240e-01,\n",
       "           -5.8556e-03, -2.0959e-01, -1.4661e-01,  1.8237e-01, -6.5674e-01,\n",
       "            3.9764e-02, -5.0293e-02, -7.3096e-01, -7.8796e-02, -1.3684e-01,\n",
       "           -2.4402e-01,  1.9238e-01, -2.0837e-01,  1.2805e-01,  1.2103e-01,\n",
       "           -3.7781e-02,  1.7090e-01, -3.3862e-01,  9.6069e-02, -6.0645e-01,\n",
       "           -1.6577e-01, -2.4185e-03, -3.4204e-01,  4.0308e-01,  5.8594e-01,\n",
       "            2.7295e-01, -4.1602e-01,  2.3572e-01,  8.5938e-02, -2.2278e-01,\n",
       "            3.8354e-01, -3.2446e-01,  6.5674e-02, -5.5371e-01,  4.7290e-01,\n",
       "            2.7026e-01,  2.3633e-01, -2.8931e-01,  3.4497e-01,  5.6299e-01,\n",
       "            1.5686e-01, -3.1592e-01, -7.7148e-02, -3.1836e-01, -1.8103e-01,\n",
       "            2.9395e-01,  2.6855e-01, -1.3672e-01, -9.1858e-02, -2.6489e-01,\n",
       "            4.2969e-01, -5.5298e-02,  2.7002e-01,  6.5820e-01,  2.1240e-01,\n",
       "           -1.9409e-01,  2.7026e-01, -2.2522e-02,  1.3306e-02, -1.7993e-01,\n",
       "            1.2671e-01, -7.2900e-01,  2.1179e-01,  5.8203e-01,  9.8572e-02,\n",
       "            2.3242e-01, -3.5791e-01,  7.6294e-02,  2.4268e-01,  8.4351e-02,\n",
       "            2.0776e-01, -1.8896e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.1543e-01, 3.6957e-02, 9.7961e-03, 6.8760e-04, 1.3733e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  204.613953   77.966736  577.298035  349.303345    0.931230     46    banana\n",
       "  1    0.622200    1.291061  220.608612  314.486206    0.893250     39    bottle\n",
       "  2  167.730255    1.632912  317.730225  161.138794    0.829068     39    bottle\n",
       "  3  267.255615  224.793396  552.257202  474.633545    0.811270     48  sandwich,\n",
       "  'caption': ['A white container near a banana and mars bar.'],\n",
       "  'bbox_target': [147.98, 3.73, 166.63, 159.17]},\n",
       " 74: {'image_emb': tensor([[ 0.1501,  0.4854, -0.0853,  ...,  0.5737,  0.4917,  0.1948],\n",
       "          [-0.7026,  0.3711,  0.0922,  ...,  1.0225,  0.0789,  0.1775],\n",
       "          [-0.0133,  0.4031, -0.3008,  ...,  0.5142,  0.6055,  0.1548],\n",
       "          [-0.1997,  0.2192, -0.3040,  ...,  0.8110,  0.1370, -0.0162],\n",
       "          [ 0.1013,  0.4094, -0.1613,  ...,  0.5830,  0.3621,  0.1246]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0600,  0.1781, -0.5068,  ...,  0.2349, -0.2150, -0.5366],\n",
       "          [-0.0328,  0.0150, -0.1431,  ..., -0.2273, -0.3542,  0.1289]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3074, 0.0019, 0.1985, 0.0008, 0.4912],\n",
       "          [0.0665, 0.1322, 0.0486, 0.7148, 0.0379]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.846741   66.602173  282.209869  637.594482    0.923481      0   \n",
       "  1  162.455109  354.606506  245.484955  439.841309    0.883174     67   \n",
       "  2  131.061584   68.553986  480.000000  638.183960    0.876074      0   \n",
       "  3  184.282471  292.485840  232.725525  364.948730    0.850097     27   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1  cell phone  \n",
       "  2      person  \n",
       "  3         tie  ,\n",
       "  'caption': ['The smiling bride wearing white dress and holding a mobile phone.',\n",
       "   'girl'],\n",
       "  'bbox_target': [159.64, 72.01, 320.36, 468.85]},\n",
       " 75: {'image_emb': tensor([[-0.4058, -0.4014, -0.1672,  ...,  0.7241,  0.1035,  0.2708],\n",
       "          [-0.6064, -0.1215, -0.2627,  ...,  0.6377,  0.4138,  0.1560],\n",
       "          [-0.3064, -0.6094, -0.1880,  ...,  0.2429,  0.0382,  0.2029]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1619, -0.2690, -0.2371,  ...,  0.0265,  0.0755,  0.2600],\n",
       "          [-0.2539, -0.2756,  0.1383,  ...,  0.4324, -0.0010,  0.5190]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2712, 0.5479, 0.1807],\n",
       "          [0.1959, 0.7998, 0.0045]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class   name\n",
       "  0  120.163513  23.944981  257.475891  326.893799    0.903049     22  zebra\n",
       "  1  216.753433  30.587048  321.408844  208.170944    0.894344     22  zebra,\n",
       "  'caption': ['The zebra on the right.', 'Zebra with eye visible.'],\n",
       "  'bbox_target': [217.76, 28.96, 101.77, 176.6]},\n",
       " 76: {'image_emb': tensor([[-0.1846,  0.4622, -0.5435,  ...,  1.2666, -0.1329,  0.0129],\n",
       "          [-0.2317,  0.3894, -0.1779,  ...,  0.7959, -0.0895,  0.1736],\n",
       "          [ 0.1302,  0.0173, -0.0875,  ...,  1.0234,  0.1713, -0.3335],\n",
       "          [-0.1185, -0.0867, -0.0511,  ...,  1.0615, -0.0933, -0.2017],\n",
       "          [-0.0243,  0.3513, -0.2048,  ...,  1.2881,  0.1855, -0.1996],\n",
       "          [-0.0412,  0.1573, -0.1595,  ...,  0.8330, -0.2522, -0.4475]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.4998,  0.4133, -0.3635,  ..., -0.1281,  0.4814, -0.4026],\n",
       "          [ 0.5142,  0.1207, -0.3408,  ...,  0.1141,  0.1674, -0.3867]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.4060e-03, 1.2749e-02, 6.5470e-04, 1.5140e-05, 2.2949e-01, 7.5244e-01],\n",
       "          [2.3193e-03, 4.0703e-03, 1.4656e-02, 9.7215e-05, 9.2090e-01, 5.7953e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   213.751740  306.295654  355.626801  614.419189    0.930932      0   \n",
       "  1     5.492020  397.001648  159.830643  635.116760    0.905998      0   \n",
       "  2   360.307190  503.809296  418.088440  525.957031    0.808985     25   \n",
       "  3   413.246002  460.286835  477.695618  582.224731    0.758355      0   \n",
       "  4   251.236542  249.633636  463.775879  348.962128    0.707862     25   \n",
       "  5   373.279907  520.801941  426.401306  586.656189    0.519247      0   \n",
       "  6   435.842285  426.477875  479.362671  449.352264    0.463219     25   \n",
       "  7   152.377563  521.676880  230.713074  620.940430    0.423487      0   \n",
       "  8   149.131531  521.016235  230.802917  620.653687    0.393419     26   \n",
       "  9     0.154022  353.148499   22.585205  547.327454    0.378857      0   \n",
       "  10  335.267395  548.251709  356.437500  579.986816    0.274993      0   \n",
       "  11  154.309738  258.237457  160.875626  280.882172    0.272901      0   \n",
       "  12    0.000000  472.622223   42.488037  637.026001    0.272355     26   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2   umbrella  \n",
       "  3     person  \n",
       "  4   umbrella  \n",
       "  5     person  \n",
       "  6   umbrella  \n",
       "  7     person  \n",
       "  8    handbag  \n",
       "  9     person  \n",
       "  10    person  \n",
       "  11    person  \n",
       "  12   handbag  ,\n",
       "  'caption': ['A black upside umbrella that is being held up by a woman.',\n",
       "   'Upside down black umbrella'],\n",
       "  'bbox_target': [248.86, 246.58, 216.62, 104.24]},\n",
       " 77: {'image_emb': tensor([[ 0.0426,  0.5718, -0.3069,  ...,  1.1367, -0.0255, -0.1569],\n",
       "          [-0.2194,  0.1530, -0.2192,  ...,  0.0806,  0.0724,  0.0527],\n",
       "          [-0.2275,  0.5767, -0.2438,  ...,  1.1279, -0.0758, -0.2325],\n",
       "          [-0.2009,  0.1986, -0.3503,  ...,  0.0228,  0.1787, -0.0798]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0332,  0.1522, -0.2554,  ...,  0.3049,  0.3645, -0.1040],\n",
       "          [-0.0370,  0.2827, -0.1273,  ..., -0.0769, -0.0219, -0.0950]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.0207e-03, 9.6875e-01, 2.5024e-02, 2.3632e-03],\n",
       "          [2.9694e-02, 9.0918e-01, 6.0913e-02, 1.9693e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  168.151093  288.810669  258.559448  517.030762    0.928006      0  person\n",
       "  1   15.804810  194.673523  638.725891  493.250610    0.923616      7   truck\n",
       "  2  102.285042  278.326050  173.526764  518.364197    0.919555      0  person\n",
       "  3  211.080948  320.909241  218.073715  335.172852    0.426789     27     tie\n",
       "  4  143.718842  317.552429  156.884491  353.778442    0.403891     27     tie,\n",
       "  'caption': [\"An african american man wearing a suit shaking a white man's hand in front of a Shepherd truck.\",\n",
       "   'A dark skinned man in a black suit standing next to a large truck.'],\n",
       "  'bbox_target': [168.55, 289.18, 90.31, 229.27]},\n",
       " 78: {'image_emb': tensor([[ 0.3335, -0.3564,  0.0139,  ..., -0.0955,  0.3025, -0.2729],\n",
       "          [ 0.2086, -0.4758, -0.3062,  ...,  0.2335,  0.1387, -0.3142],\n",
       "          [ 0.1639, -0.5112, -0.1730,  ...,  0.0181,  0.0492, -0.2874]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2603, -0.2603, -0.1838,  ...,  0.3140, -0.4314, -0.3401],\n",
       "          [ 0.2952, -0.1036, -0.2566,  ...,  0.3687, -0.3750, -0.4038]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.8125, 0.0843, 0.1033],\n",
       "          [0.5942, 0.3281, 0.0779]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  237.618896  153.280685  639.238770  637.921570    0.915606     23  giraffe\n",
       "  1  155.855560  153.008331  367.075531  634.637939    0.905119     23  giraffe,\n",
       "  'caption': ['A giraffe with dark brown spots',\n",
       "   'The giraffe whose body is on the right'],\n",
       "  'bbox_target': [232.62, 156.52, 402.07, 473.86]},\n",
       " 79: {'image_emb': tensor([[ 0.0370,  0.1486, -0.1542,  ...,  1.2686, -0.0900,  0.0050],\n",
       "          [ 0.0172,  0.2135, -0.1525,  ...,  1.3311,  0.1486,  0.2153],\n",
       "          [ 0.1869,  0.5454, -0.2161,  ...,  1.1680, -0.0844, -0.0625]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0299,  0.0857, -0.4968,  ...,  0.4358, -0.1857,  0.1194],\n",
       "          [-0.0534,  0.0831, -0.1029,  ...,  0.5435, -0.3442, -0.2043]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0594, 0.3018, 0.6387],\n",
       "          [0.1251, 0.4648, 0.4102]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class name\n",
       "  0  289.094971  101.206284  639.678955  475.343872    0.946462     16  dog\n",
       "  1  185.316193  129.655075  408.956146  476.098694    0.700894     16  dog\n",
       "  2    4.659027    0.000000  639.317627  476.103882    0.281600      2  car,\n",
       "  'caption': ['Dog in passenger seat.',\n",
       "   'An alert, tan dog, sitting in the passenger seat of a car.'],\n",
       "  'bbox_target': [108.39, 127.35, 312.58, 348.39]},\n",
       " 80: {'image_emb': tensor([[-0.2605,  0.8750, -0.1345,  ...,  0.9648,  0.3684,  0.2128],\n",
       "          [-0.1328,  0.8149,  0.1511,  ...,  1.2139,  0.2269, -0.2515],\n",
       "          [ 0.4973,  0.1018,  0.0233,  ...,  0.4199, -0.1014,  0.4641],\n",
       "          ...,\n",
       "          [ 0.2372,  0.2423,  0.1401,  ...,  0.9219,  0.0831,  0.2087],\n",
       "          [ 0.1443, -0.0319, -0.1661,  ...,  1.0820,  0.0603, -0.1458],\n",
       "          [ 0.8013,  0.1956, -0.1561,  ...,  0.8604, -0.0379,  0.0760]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0779, -0.1062,  0.0342,  ...,  0.3335, -0.2559, -0.0940],\n",
       "          [ 0.2314, -0.5938,  0.0399,  ...,  0.1324, -0.6616,  0.1837]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.9012e-04, 1.1206e-04, 3.3398e-01, 2.1228e-01, 2.6416e-01, 9.1309e-02,\n",
       "           1.0252e-05, 9.7229e-02],\n",
       "          [6.4969e-06, 1.7881e-07, 2.6398e-02, 3.4253e-01, 5.8594e-02, 5.6445e-01,\n",
       "           8.3447e-07, 8.0490e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   70.604782   27.892303  201.269699  196.299316    0.949093     41   \n",
       "  1  358.975006    3.640602  471.592438  156.805939    0.927923     41   \n",
       "  2   89.902672  255.483887  288.206665  403.088257    0.909339     48   \n",
       "  3  105.008499  150.866272  279.074463  296.872253    0.902334     48   \n",
       "  4  276.832672  170.645599  430.685333  323.436737    0.884771     48   \n",
       "  5  478.191650  130.384674  630.122314  233.037506    0.864191     48   \n",
       "  6    0.109117   45.765778   30.237000  224.188538    0.791166     41   \n",
       "  7    0.000000    1.683365  639.463135  423.781616    0.592032     60   \n",
       "  8  444.063965    0.000000  483.220154   40.368759    0.409577     44   \n",
       "  9  163.029068    0.000000  218.104782   68.561615    0.367848     44   \n",
       "  \n",
       "             name  \n",
       "  0           cup  \n",
       "  1           cup  \n",
       "  2      sandwich  \n",
       "  3      sandwich  \n",
       "  4      sandwich  \n",
       "  5      sandwich  \n",
       "  6           cup  \n",
       "  7  dining table  \n",
       "  8         spoon  \n",
       "  9         spoon  ,\n",
       "  'caption': ['A half eaten sandwich that contains cheese, roll, meat, and peppers.',\n",
       "   'A sandwich with a poppy seed bun half eaten on a plate.'],\n",
       "  'bbox_target': [88.07, 254.64, 202.95, 149.34]},\n",
       " 81: {'image_emb': tensor([[-0.3794, -0.1792,  0.0631,  ...,  0.1555,  0.2512,  0.1088],\n",
       "          [ 0.1787,  0.1885, -0.2688,  ...,  0.9487,  0.1072, -0.0789],\n",
       "          [-0.2230,  0.2213, -0.1608,  ...,  0.6353,  0.1920,  0.3364],\n",
       "          [-0.4231, -0.0702, -0.0975,  ..., -0.4736,  0.4834,  0.1967]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0560,  0.0707, -0.3564,  ..., -0.2905, -0.0623, -0.2681],\n",
       "          [-0.3550, -0.1709, -0.1664,  ..., -0.5259,  0.0232, -0.3137]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.4897, 0.0023, 0.2581, 0.2500],\n",
       "          [0.3977, 0.0052, 0.4436, 0.1533]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  290.618347   91.338776  593.720337  401.028442    0.912214      0  person\n",
       "  1  239.798859  232.559509  307.128448  282.884094    0.756797     27     tie\n",
       "  2  162.120483  114.605042  367.737915  402.374451    0.741207      0  person\n",
       "  3   16.071611  340.973206   78.125000  402.663635    0.557539      0  person\n",
       "  4  239.446045  231.233673  306.491943  284.046906    0.339439      0  person,\n",
       "  'caption': ['The man in a suit on the right is proppeing his elbow and hand on his mouth',\n",
       "   'man sitting on the right side of the image'],\n",
       "  'bbox_target': [287.43, 91.11, 304.35, 311.86]},\n",
       " 82: {'image_emb': tensor([[-0.2798,  0.1851, -0.3042,  ...,  0.8203,  0.2482,  0.0341],\n",
       "          [-0.0724,  0.4070, -0.1063,  ...,  1.1289,  0.1748, -0.2739],\n",
       "          [-0.3362,  0.3333, -0.3079,  ...,  1.0459,  0.3005, -0.0205],\n",
       "          [ 0.1499,  0.0482, -0.4060,  ...,  0.7363,  0.1295,  0.2566],\n",
       "          [-0.7051,  0.0827,  0.0601,  ..., -0.0916,  0.1915,  0.4963]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0313,  0.1902, -0.2394,  ..., -0.4038,  0.0200, -0.3643],\n",
       "          [ 0.0539,  0.0248,  0.2771,  ..., -0.2932,  0.1931, -0.4224]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0348, 0.6875, 0.2651, 0.0095, 0.0031],\n",
       "          [0.1890, 0.4751, 0.2881, 0.0390, 0.0090]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  197.464584  130.226883  258.476318  299.506134    0.929390      0    person\n",
       "  1  359.710754  143.285995  426.369415  300.094604    0.908468      0    person\n",
       "  2   53.006935  121.036102  112.534523  289.650146    0.904106      0    person\n",
       "  3  214.905594  155.862930  244.621567  196.594543    0.711801     24  backpack\n",
       "  4   45.637432  253.750946  134.494507  313.985962    0.594770     30      skis\n",
       "  5  184.253998  256.552612  286.171417  307.119446    0.451133     30      skis\n",
       "  6  316.924438  242.724792  364.026062  282.988556    0.299316     30      skis\n",
       "  7  319.441925  250.065536  430.672272  304.315674    0.294794     30      skis,\n",
       "  'caption': ['A man in all black standing next to a man with a backpack.',\n",
       "   'A man who is on the right side'],\n",
       "  'bbox_target': [360.63, 146.76, 67.35, 153.31]},\n",
       " 83: {'image_emb': tensor([[-0.1637,  0.2556, -0.2300,  ...,  0.9219, -0.0768, -0.1493],\n",
       "          [-0.3850,  0.7573, -0.1923,  ...,  0.9009, -0.1180, -0.2893],\n",
       "          [-0.3384,  0.7827, -0.3452,  ...,  0.7515,  0.3955, -0.1622],\n",
       "          [-0.0330,  0.3867, -0.0845,  ...,  0.8594, -0.2429, -0.1177]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.4714,  0.3169, -0.0318,  ...,  0.6406, -0.2656, -0.1780],\n",
       "          [ 0.3765, -0.0365,  0.1724,  ...,  0.3018, -0.1678, -0.1969]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.0172e-02, 9.7168e-01, 1.1921e-07, 8.0261e-03],\n",
       "          [6.2469e-02, 9.3213e-01, 4.7684e-07, 5.2071e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   122.555626    1.788361  268.904816  119.890045    0.880600     50   \n",
       "  1   283.463440   98.524048  484.295166  322.957397    0.855716     50   \n",
       "  2   104.074768  118.975937  176.837982  198.372833    0.799463     51   \n",
       "  3     4.844971    0.000000  635.170715  475.081970    0.620078     45   \n",
       "  4   162.467178  172.379608  250.745834  224.604340    0.571586     51   \n",
       "  5    38.376457   99.228180  155.865723  216.774689    0.495665     50   \n",
       "  6   278.168640   95.531403  486.635071  323.162323    0.437575     45   \n",
       "  7    86.402603  379.348694  195.308609  477.819336    0.369400     51   \n",
       "  8   107.805283  248.431183  209.374374  370.029694    0.322222     51   \n",
       "  9   168.132416  350.150970  292.789673  477.923523    0.300082     50   \n",
       "  10   89.411819  109.551086  174.795151  198.904877    0.288844     50   \n",
       "  \n",
       "          name  \n",
       "  0   broccoli  \n",
       "  1   broccoli  \n",
       "  2     carrot  \n",
       "  3       bowl  \n",
       "  4     carrot  \n",
       "  5   broccoli  \n",
       "  6       bowl  \n",
       "  7     carrot  \n",
       "  8     carrot  \n",
       "  9   broccoli  \n",
       "  10  broccoli  ,\n",
       "  'caption': ['a full piece of green broccoli in a mix of vegetables, meats, and white rice.',\n",
       "   'Broccoli amongst rice.'],\n",
       "  'bbox_target': [282.35, 100.52, 205.56, 223.62]},\n",
       " 84: {'image_emb': tensor([[-0.0366,  0.2163, -0.1506,  ...,  1.0312,  0.0352, -0.2323],\n",
       "          [ 0.1533, -0.1379,  0.2744,  ...,  0.3726, -0.2632, -0.2549],\n",
       "          [-0.0025,  0.1055,  0.2130,  ...,  0.7041, -0.2695, -0.4060],\n",
       "          [ 0.1261,  0.4299,  0.2223,  ...,  0.5239,  0.1157, -0.2815],\n",
       "          [ 0.0030,  0.1619, -0.0457,  ...,  0.7378, -0.0049, -0.2008],\n",
       "          [ 0.3569, -0.1279,  0.3804,  ...,  0.1216, -0.1628, -0.1722]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.5483,  0.0439,  0.2433,  ..., -0.3667, -0.1028, -0.1443],\n",
       "          [-0.0853, -0.2827,  0.0061,  ...,  0.4734, -0.5771, -0.6504]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.7598, 0.0046, 0.0685, 0.1569, 0.0063, 0.0039],\n",
       "          [0.4561, 0.2480, 0.1576, 0.0617, 0.0055, 0.0710]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   13.514282   54.450699  136.071442  275.942383    0.938387      0   \n",
       "  1   44.903870    3.430176  441.059998  638.300781    0.927059      0   \n",
       "  2    0.617195  263.944885  191.891998  639.091003    0.910234      0   \n",
       "  3  406.428864  530.079407  501.968994  639.346985    0.876708     34   \n",
       "  4   79.985992  501.536804  192.305237  588.192810    0.869897     35   \n",
       "  5   82.833160  122.103127  117.223450  156.467667    0.632593     35   \n",
       "  \n",
       "               name  \n",
       "  0          person  \n",
       "  1          person  \n",
       "  2          person  \n",
       "  3    baseball bat  \n",
       "  4  baseball glove  \n",
       "  5  baseball glove  ,\n",
       "  'caption': ['The out of focus player in white',\n",
       "   'A baseball fielder in white.'],\n",
       "  'bbox_target': [11.73, 56.88, 119.9, 216.28]},\n",
       " 85: {'image_emb': tensor([[ 0.0027,  0.4695,  0.0294,  ...,  0.5044,  0.4900, -0.0054],\n",
       "          [ 0.0927,  0.5366, -0.0509,  ...,  0.8755,  0.0534,  0.2930],\n",
       "          [-0.0850,  0.6372,  0.3354,  ...,  0.4866,  0.2186,  0.2476],\n",
       "          [ 0.2629,  0.3301,  0.4092,  ...,  0.1962,  0.5850,  0.2625],\n",
       "          [-0.1411,  0.6870,  0.1231,  ...,  0.7202,  0.2017,  0.3674]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2830,  0.3794,  0.0475,  ..., -0.1560, -0.4028, -0.5366],\n",
       "          [-0.3101,  0.2010, -0.0487,  ...,  0.4229, -0.2272, -0.5557]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.8911e-03, 4.5807e-02, 7.2803e-01, 9.3994e-02, 1.3049e-01],\n",
       "          [8.5831e-06, 2.9755e-04, 9.4385e-01, 2.2903e-02, 3.2806e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  450.858704    0.756332  639.986877  250.861023    0.923649     41     cup\n",
       "  1  120.156555    4.386642  406.148438  170.870224    0.854059     44   spoon\n",
       "  2   64.203568  270.361816  558.562317  639.840515    0.753128     45    bowl\n",
       "  3   12.683395  217.250763  466.043030  469.407684    0.750297     44   spoon\n",
       "  4    3.287323   56.064655  127.657959  125.876511    0.480142     49  orange\n",
       "  5  275.927185   94.343201  408.573059  171.650665    0.365210     44   spoon\n",
       "  6  275.977081   96.144196  408.162567  172.268982    0.343113     43   knife\n",
       "  7  336.619080  218.492889  471.237305  289.214111    0.329384     44   spoon,\n",
       "  'caption': ['The big spoon by the soup.',\n",
       "   'A spoon sitting underneath a bowl of duck sauce.'],\n",
       "  'bbox_target': [28.83, 222.7, 439.64, 246.49]},\n",
       " 86: {'image_emb': tensor([[-0.1646,  0.7095,  0.0555,  ...,  0.4016,  0.2881,  0.1803],\n",
       "          [ 0.4365, -0.0499,  0.5522,  ...,  0.7144, -0.3757,  0.4353],\n",
       "          [-0.2871,  0.5747, -0.0323,  ...,  0.6758,  0.1510, -0.2092],\n",
       "          [-0.2012,  0.9922,  0.2815,  ...,  0.8203, -0.0703,  0.0658],\n",
       "          [ 0.3374,  0.1128,  0.2039,  ...,  0.4763, -0.3945,  0.3787],\n",
       "          [ 0.5220,  0.3274,  0.2771,  ...,  0.4524, -0.1621,  0.3174]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1616,  0.3003,  0.2168,  ...,  0.2080, -0.2585, -0.3291],\n",
       "          [ 0.1440, -0.1882,  0.2354,  ..., -0.2125, -0.6577, -0.3403]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.1992e-03, 2.6932e-03, 5.7364e-04, 8.7830e-02, 5.7907e-03, 9.0088e-01],\n",
       "          [1.2922e-04, 1.5497e-03, 8.3447e-05, 2.8351e-02, 1.3466e-03, 9.6875e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.000000  319.476288  189.054337  557.662964    0.923548     44   \n",
       "  1  327.098938  262.450409  611.488403  525.826294    0.846493     48   \n",
       "  2  199.412048    0.253511  351.145355   95.189285    0.833747     41   \n",
       "  3    0.000000   54.327038  315.433624  387.612915    0.829882     45   \n",
       "  4  366.870605  124.730255  610.506714  417.833893    0.814956     48   \n",
       "  5  594.865906    0.098104  612.000000   62.239243    0.427978     41   \n",
       "  6    0.315798    0.266012   89.737686   85.504234    0.414904     41   \n",
       "  7    0.446608    1.327071  612.000000  604.445251    0.256511     60   \n",
       "  \n",
       "             name  \n",
       "  0         spoon  \n",
       "  1      sandwich  \n",
       "  2           cup  \n",
       "  3          bowl  \n",
       "  4      sandwich  \n",
       "  5           cup  \n",
       "  6           cup  \n",
       "  7  dining table  ,\n",
       "  'caption': ['A white bowl of red soup beside a sandwich in 2 wedges, and a silver spoon.',\n",
       "   'A bowl of soup and a sandwich.'],\n",
       "  'bbox_target': [0.0, 2.75, 612.0, 602.37]},\n",
       " 87: {'image_emb': tensor([[-0.1833, -0.4697, -0.2129,  ...,  0.4827,  0.0186, -0.1821],\n",
       "          [ 0.1591, -0.2091, -0.1587,  ...,  0.6255,  0.1072, -0.0658],\n",
       "          [ 0.0784, -0.3269, -0.1604,  ...,  0.3589,  0.0758, -0.0346],\n",
       "          [ 0.0089, -0.4282, -0.1445,  ...,  0.4326, -0.0347,  0.0587]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-3.2568e-01, -2.7881e-01, -1.6846e-01, -4.9438e-02, -3.0078e-01,\n",
       "            1.9946e-01, -4.2773e-01, -1.2480e+00,  4.2212e-01, -3.4271e-02,\n",
       "            6.2622e-02, -3.3472e-01, -1.7700e-02, -2.3132e-01,  2.2717e-01,\n",
       "           -1.6541e-01, -3.8599e-01,  8.2922e-04, -2.2229e-01,  8.6182e-01,\n",
       "            2.2742e-01,  3.9233e-01, -1.1841e-01, -2.5708e-01,  2.0584e-02,\n",
       "           -9.7900e-02, -1.7383e-01,  2.5195e-01, -2.3987e-01, -1.3867e-01,\n",
       "            2.2064e-02,  6.0974e-02, -1.6711e-01,  2.2449e-01, -3.9380e-01,\n",
       "           -2.4353e-01,  9.9609e-02,  2.4573e-01, -1.1523e-01, -6.1684e-03,\n",
       "           -3.0542e-01, -2.4500e-01,  1.8652e-01,  3.2056e-01, -4.0991e-01,\n",
       "           -2.4792e-01, -4.6417e-02,  2.4268e-01,  1.0846e-01, -1.5182e-03,\n",
       "           -2.2815e-01, -2.8247e-01, -2.8400e-03, -1.0620e-01, -5.8301e-01,\n",
       "            2.0593e-01, -3.9917e-02, -2.9053e-01,  3.5791e-01,  1.6235e-01,\n",
       "           -1.2195e-01,  2.1027e-02,  6.0059e-02,  2.1069e-01, -4.0550e-03,\n",
       "           -1.5930e-02, -2.7008e-02, -1.9238e-01,  4.8706e-01,  4.1260e-02,\n",
       "            3.4576e-02, -1.8909e-01,  2.2437e-01, -3.0701e-02,  5.7471e-01,\n",
       "           -7.1729e-01, -7.3914e-02,  7.5684e-02, -6.5735e-02, -2.1362e-01,\n",
       "            1.8823e-01,  3.7964e-01, -1.6809e-01,  2.5659e-01, -2.6489e-01,\n",
       "            1.5137e-01, -5.3528e-02,  2.7435e-02, -5.8380e-02, -1.7236e-01,\n",
       "            3.1219e-02, -2.7328e-02, -1.3379e+00,  3.7500e-01, -2.5586e-01,\n",
       "            7.8796e-02, -3.8177e-02, -3.7500e-01,  3.7811e-02, -2.0093e-01,\n",
       "           -1.5686e-02, -3.5254e-01, -2.1533e-01, -1.0052e-01, -2.5635e-01,\n",
       "           -2.0020e-01, -1.5869e-01,  5.2197e-01,  8.2764e-02,  1.2360e-01,\n",
       "            6.4880e-02, -1.8420e-01, -3.6259e-03, -2.2510e-01,  4.8889e-02,\n",
       "            1.0876e-01, -1.4709e-01, -3.3618e-01,  4.6326e-02,  1.7871e-01,\n",
       "           -2.6093e-02, -5.5957e-01,  1.2939e-01, -1.5710e-01, -6.0181e-02,\n",
       "            8.5510e-02, -2.0645e-02,  2.3877e-01,  2.2766e-02,  2.3303e-01,\n",
       "           -2.2742e-01, -2.9614e-01, -8.4534e-02,  4.9844e+00, -3.3356e-02,\n",
       "           -2.4463e-01, -2.7295e-01, -3.3398e-01, -4.4043e-01, -1.0309e-01,\n",
       "           -1.0803e-01,  1.9360e-01, -3.5718e-01,  1.0925e-01, -1.0730e-01,\n",
       "            1.7773e-01,  4.0192e-02, -7.4902e-01, -3.3252e-01,  2.3474e-01,\n",
       "            1.8896e-01, -2.8394e-01,  4.4287e-01, -1.4783e-01,  5.5615e-01,\n",
       "           -3.3911e-01,  4.2578e-01, -5.4993e-02,  3.4027e-02,  2.9907e-01,\n",
       "            1.5808e-01, -1.9287e-01, -2.8198e-01, -3.1982e-02,  9.0149e-02,\n",
       "            3.7646e-01,  2.9370e-01, -4.1602e-01,  2.2644e-01, -9.3689e-02,\n",
       "           -3.0151e-02, -1.2720e-01,  2.4475e-01,  1.6626e-01, -1.4563e-01,\n",
       "           -1.3464e-01, -4.5361e-01, -1.0309e-01, -2.7881e-01,  2.5073e-01,\n",
       "           -1.9043e-01, -1.6785e-01,  1.6907e-01,  3.1647e-02, -1.4856e-01,\n",
       "           -7.8979e-02, -5.4736e-01, -1.0443e-01,  1.9312e-01,  1.2244e-01,\n",
       "           -9.2773e-02,  3.0664e-01,  2.9526e-02, -7.0068e-02,  1.8875e-02,\n",
       "            1.8005e-01, -8.8989e-02,  8.4839e-02,  2.3819e-02, -3.0835e-01,\n",
       "            8.0750e-02, -5.7861e-02,  2.4060e-01, -3.8666e-02,  3.7378e-01,\n",
       "            2.3181e-01, -2.3865e-01, -2.8125e-01,  2.1899e-01, -4.3579e-01,\n",
       "           -7.5195e-02,  4.7510e-01,  3.4546e-01,  1.6321e-01,  1.1743e-01,\n",
       "            1.9519e-01,  3.8757e-02, -1.8213e-01,  2.3450e-01, -5.4665e-03,\n",
       "           -1.0474e-01, -6.7505e-02,  1.7444e-01,  2.9272e-01, -1.3147e-01,\n",
       "            9.8572e-02, -3.4521e-01, -3.1616e-01, -1.3416e-01, -2.7405e-02,\n",
       "           -6.6833e-03,  2.5220e-01,  1.0474e-01,  2.7246e-01,  1.1035e-01,\n",
       "           -8.0139e-02,  6.1218e-02, -4.8798e-02, -4.1089e-01,  2.7191e-02,\n",
       "            2.1130e-01,  2.0203e-01,  7.1655e-02, -5.2887e-02, -3.8159e-01,\n",
       "            2.8076e-01, -7.2083e-02, -2.3157e-01,  1.1804e-01,  7.8369e-02,\n",
       "           -2.8027e-01,  6.2646e-01,  1.4001e-01,  1.1432e-01, -6.6699e-01,\n",
       "           -3.7231e-01,  8.0322e-02,  1.2347e-01,  4.4849e-01,  4.1870e-01,\n",
       "            1.9751e-01,  1.9165e-01,  2.3596e-01, -1.7624e-02,  1.8054e-01,\n",
       "            4.7516e-02,  2.6416e-01, -5.6396e-02, -1.0400e-01,  8.8562e-02,\n",
       "           -2.5562e-01, -1.9501e-02, -2.6657e-02, -6.9275e-03, -1.2352e-02,\n",
       "            4.3701e-01, -4.5380e-02,  1.0321e-01,  1.3159e-01, -1.1060e-01,\n",
       "            1.1572e-01,  4.3481e-01,  2.0898e-01, -1.4893e-01,  6.3293e-02,\n",
       "           -8.7097e-02,  3.1323e-01,  9.1675e-02,  3.1030e-01, -7.2510e-02,\n",
       "           -2.4756e-01, -1.4404e-01,  1.1218e-01, -3.3539e-02, -9.6069e-02,\n",
       "            3.3130e-01, -3.3862e-01,  1.3599e-01, -1.0681e-01, -6.7383e-02,\n",
       "           -3.9478e-01,  1.5076e-01,  2.1924e-01,  3.6963e-01, -5.9586e-03,\n",
       "            2.0767e-02, -2.8549e-02, -4.2749e-01,  3.5229e-01, -1.9324e-01,\n",
       "            3.7402e-01,  4.9512e-01,  4.9727e+00,  1.0107e-01,  3.7280e-01,\n",
       "            1.7407e-01,  1.5027e-01,  2.8442e-01, -1.2634e-01,  7.5488e-01,\n",
       "           -9.2285e-02, -7.5317e-02,  1.8835e-01,  2.7075e-01,  2.3218e-01,\n",
       "            1.4563e-01, -1.1133e-01, -3.1055e-01,  2.6245e-01, -2.2852e+00,\n",
       "           -1.8665e-01,  2.4872e-02,  2.8003e-01, -1.1273e-01, -1.2457e-01,\n",
       "           -4.5312e-01, -1.6968e-01,  1.9974e-02,  7.6538e-02,  3.2715e-01,\n",
       "           -2.5732e-01,  1.5771e-01, -1.7908e-01, -2.1252e-01,  2.7759e-01,\n",
       "           -1.0382e-01,  3.0957e-01, -1.8848e-01, -1.6479e-01, -2.6810e-02,\n",
       "            4.6582e-01,  5.6877e-03,  2.3474e-01,  1.0248e-01, -2.4170e-01,\n",
       "           -3.5767e-02,  1.0413e-01,  1.1023e-01,  1.2335e-01,  1.2299e-01,\n",
       "            1.7029e-01,  1.1200e-02,  8.7646e-02, -2.6123e-01, -1.2952e-01,\n",
       "           -2.8442e-01, -1.8420e-01, -2.5986e-02,  1.7749e-01,  2.9114e-02,\n",
       "            1.1499e-01, -1.4465e-01,  2.4194e-01,  8.2336e-02,  1.4966e-01,\n",
       "            4.4220e-02, -2.3926e-01,  2.3718e-01,  5.0842e-02,  2.1570e-01,\n",
       "           -2.4255e-01, -1.8158e-02,  2.5830e-01,  1.1505e-01, -6.6772e-02,\n",
       "            2.9810e-01,  1.4636e-01,  3.2861e-01, -5.3857e-01, -2.3901e-01,\n",
       "           -8.3301e-01, -1.4819e-01, -6.0254e-01, -1.3879e-01,  1.2671e-01,\n",
       "           -1.5271e-01,  3.2227e-01,  3.6987e-01, -2.7893e-02,  8.9697e-01,\n",
       "            3.5815e-01, -4.6326e-02,  1.0468e-01, -9.3811e-02, -3.4485e-03,\n",
       "            2.1802e-01, -1.0260e-01, -9.5276e-02,  3.8672e-01, -9.7595e-02,\n",
       "            5.9277e-01,  5.1117e-02, -2.0581e-01,  1.8457e-01,  7.4768e-02,\n",
       "           -1.3110e-01,  6.1523e-02, -4.1382e-02,  4.2139e-01, -2.2791e-01,\n",
       "           -3.1921e-02, -1.5527e-01, -3.3691e-01, -2.3254e-01,  3.4839e-01,\n",
       "            1.1047e-01, -1.7908e-01, -5.0391e-01,  3.5083e-01,  3.9429e-02,\n",
       "           -2.8857e-01,  2.0850e-01, -3.3112e-02,  4.5837e-02,  4.7803e-01,\n",
       "           -2.1057e-01, -3.0591e-01, -1.4197e-01, -1.3086e-01, -1.7603e-01,\n",
       "           -3.2227e-02, -4.8126e-02, -1.9189e-01, -1.0278e-01,  2.7808e-01,\n",
       "            1.5845e-01,  5.2246e-02, -1.1414e-01,  4.1504e-01, -1.8567e-01,\n",
       "           -4.8950e-01,  3.9282e-01,  3.6316e-02, -2.4377e-01,  5.9357e-02,\n",
       "           -1.1914e-01,  4.1431e-01, -2.6099e-01,  7.8064e-02,  6.8848e-02,\n",
       "           -2.1973e-01,  3.2166e-02, -1.6467e-01,  1.7334e-01, -1.5588e-01,\n",
       "           -5.6183e-02, -3.0176e-01,  1.1169e-01,  1.1108e-01, -2.9541e-01,\n",
       "            3.0884e-01,  3.4668e-02,  4.7437e-01, -3.2520e-01,  2.2302e-01,\n",
       "           -6.0852e-02,  6.0760e-02, -1.1621e-01, -1.7090e-02,  4.7095e-01,\n",
       "           -2.6794e-02, -6.2402e-01, -3.8849e-02,  1.5454e-01, -1.5002e-01,\n",
       "           -3.9307e-01, -5.8022e-03, -4.9023e-01, -4.8828e-02,  3.0420e-01,\n",
       "           -1.0419e-01, -1.7151e-01,  8.2886e-02,  5.7031e-01,  5.0635e-01,\n",
       "           -1.3318e-01,  6.2927e-02,  2.0825e-01, -1.4844e-01, -3.4399e-01,\n",
       "            2.5732e-01,  3.0685e-02,  1.4709e-01,  1.1090e-01, -2.9492e-01,\n",
       "           -1.8884e-01,  2.5854e-01,  2.3911e-02, -1.2183e-01, -1.9547e-02,\n",
       "            1.0056e-02,  1.5430e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3884, 0.0534, 0.3884, 0.1697]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0   57.891983  145.177200  336.704773  357.208313    0.945429     22  zebra\n",
       "  1  370.983337  147.182129  541.237000  329.467163    0.893373     22  zebra\n",
       "  2  246.808807  150.294693  454.378632  347.238098    0.808492     22  zebra\n",
       "  3  625.088562  208.672791  639.765198  221.218079    0.257437     22  zebra,\n",
       "  'caption': ['The zebra to the far left of the group.'],\n",
       "  'bbox_target': [58.38, 144.32, 282.16, 214.06]},\n",
       " 88: {'image_emb': tensor([[ 0.3501, -0.1416, -0.2603,  ...,  1.1631, -0.0096,  0.0480],\n",
       "          [ 0.4961, -0.4622, -0.1415,  ...,  0.4724, -0.3628, -0.2651]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2009, -0.2664,  0.1890,  ..., -0.1542,  0.1306, -0.4380],\n",
       "          [-0.0089, -0.0883,  0.4458,  ..., -0.2102, -0.2542, -0.4888]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0876, 0.9126],\n",
       "          [0.0628, 0.9370]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  name\n",
       "  0    0.412417  473.974091   31.952126  496.790863    0.708803      8  boat\n",
       "  1  115.384476    0.465372  605.706726  607.359619    0.504474      8  boat\n",
       "  2  419.377167  263.863861  610.910828  609.212341    0.425987      8  boat,\n",
       "  'caption': ['the white boat on the right in the right hand picture',\n",
       "   'A white boat.'],\n",
       "  'bbox_target': [446.97, 253.05, 163.65, 347.95]},\n",
       " 89: {'image_emb': tensor([[-2.6831e-01,  4.5312e-01, -4.9957e-02,  ...,  8.2080e-01,\n",
       "            2.9590e-01,  2.9007e-02],\n",
       "          [-1.0559e-01,  3.7012e-01, -1.9629e-01,  ...,  7.5488e-01,\n",
       "           -1.6748e-01,  9.0698e-02],\n",
       "          [-4.2023e-02, -1.1444e-04, -2.4036e-01,  ...,  9.4092e-01,\n",
       "            1.2549e-01, -1.1249e-01],\n",
       "          [-4.5068e-01,  5.0262e-02, -1.0729e-03,  ...,  3.8989e-01,\n",
       "            2.4597e-01, -1.7328e-03]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2078,  0.2622, -0.1440,  ..., -0.0682,  0.1667, -0.0045],\n",
       "          [-0.0134,  0.2661,  0.0369,  ..., -0.3486,  0.0620, -0.0848]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.8103e-01, 7.2754e-01, 3.9601e-04, 9.1064e-02],\n",
       "          [2.4414e-02, 9.4531e-01, 2.7390e-03, 2.7664e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  186.116837   69.841705  452.635559  316.948700    0.949289      0   person\n",
       "  1   45.766479   58.228989  180.849411  317.353455    0.939825      0   person\n",
       "  2   83.360802  178.818298  145.358063  215.955505    0.816819     29  frisbee\n",
       "  3   73.173454  108.599945  155.330872  280.814362    0.337261      0   person,\n",
       "  'caption': ['A man in sunglasses and a red shirt.',\n",
       "   'a man wearing sunglasses'],\n",
       "  'bbox_target': [46.74, 61.12, 135.91, 255.28]},\n",
       " 90: {'image_emb': tensor([[ 0.3701,  0.3867, -0.3545,  ...,  0.8579, -0.4810,  0.2817],\n",
       "          [ 0.4114,  0.2869, -0.2061,  ...,  0.8521, -0.5210,  0.4314]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3428,  0.1246, -0.4214,  ...,  0.0067, -0.3311, -0.1736],\n",
       "          [ 0.0390,  0.3057,  0.1093,  ...,  0.1146, -0.1833,  0.2037]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.4072, 0.5928],\n",
       "          [0.1688, 0.8311]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class  name\n",
       "  0    1.463441  29.005119  588.325745  608.382568    0.870053     55  cake\n",
       "  1  377.968475   0.476936  537.199890   71.807312    0.520457     41   cup,\n",
       "  'caption': ['the part of the plate on the left of the cake',\n",
       "   'A part of a plate behind a slice of pie.'],\n",
       "  'bbox_target': [0.0, 89.39, 211.79, 231.05]},\n",
       " 91: {'image_emb': tensor([[-0.1600,  0.4761,  0.0545,  ...,  0.7407, -0.0726, -0.7451],\n",
       "          [ 0.0847,  0.5542, -0.0474,  ...,  0.0932,  0.1580,  0.1200],\n",
       "          [ 0.1532,  0.6875, -0.0466,  ...,  0.6924,  0.1704, -0.3833],\n",
       "          ...,\n",
       "          [ 0.2028,  0.2169, -0.2766,  ...,  0.8716,  0.1516, -0.3093],\n",
       "          [-0.1718,  0.0678, -0.2294,  ...,  0.7441,  0.2837, -0.2456],\n",
       "          [-0.2651,  0.0991, -0.2314,  ...,  0.5576,  0.3921, -0.1444]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2993,  0.0151, -0.5049,  ..., -0.6460,  0.1436, -0.1230],\n",
       "          [-0.1005, -0.2688,  0.0754,  ...,  0.0419,  0.0045, -0.0092]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.2051e-01, 2.2278e-03, 1.4221e-01, 3.2056e-01, 5.5790e-05, 2.7733e-03,\n",
       "           1.1856e-02],\n",
       "          [4.9756e-01, 1.1816e-01, 1.3391e-01, 1.5906e-01, 1.9360e-04, 8.9172e-02,\n",
       "           2.0657e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   10.028595   14.284653  175.027100  426.889282    0.927809      0   \n",
       "  1  406.138245   97.599487  613.826965  410.056030    0.918022     62   \n",
       "  2  118.572281   73.944641  268.408997  427.008484    0.913857      0   \n",
       "  3  274.852112  107.219803  403.767334  375.681763    0.910420      0   \n",
       "  4  177.563629  182.527130  215.741913  244.810699    0.830804     67   \n",
       "  5   75.153442  120.623108  128.856781  218.182922    0.828549     67   \n",
       "  6  358.317627   94.915680  376.515808  126.028778    0.685502     67   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1          tv  \n",
       "  2      person  \n",
       "  3      person  \n",
       "  4  cell phone  \n",
       "  5  cell phone  \n",
       "  6  cell phone  ,\n",
       "  'caption': ['The little kid next to the man', 'Child in navy.'],\n",
       "  'bbox_target': [118.66, 73.55, 148.9, 351.52]},\n",
       " 92: {'image_emb': tensor([[-0.0144,  0.2135, -0.2539,  ...,  0.8809,  0.2231,  0.2708],\n",
       "          [-0.1978,  0.4026,  0.0450,  ...,  1.2275,  0.1705,  0.1388],\n",
       "          [-0.0412,  0.5542, -0.2424,  ...,  1.1084, -0.0039,  0.2485],\n",
       "          [-0.1117,  0.1630,  0.0523,  ...,  0.9248,  0.1650,  0.0928]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-1.2549e-01,  5.7404e-02, -3.6011e-01, -1.8265e-02, -1.2646e-01,\n",
       "           -1.5442e-01, -3.2861e-01, -9.4421e-02,  1.6003e-01, -1.3904e-01,\n",
       "            9.8511e-02, -6.7139e-02,  3.8135e-01, -2.6465e-01,  1.6797e-01,\n",
       "            2.2266e-01,  5.9326e-02, -1.4441e-01,  7.7942e-02, -1.0468e-01,\n",
       "            4.7778e-01,  4.3994e-01,  3.1470e-01, -2.9633e-02, -3.9844e-01,\n",
       "           -1.8298e-01, -3.4351e-01, -1.0902e-02,  1.4172e-01,  1.8726e-01,\n",
       "           -4.0405e-01,  2.3267e-01, -4.3848e-01,  2.4731e-01, -8.8196e-02,\n",
       "           -4.1895e-01, -6.2378e-02,  8.0811e-02,  4.0039e-02, -4.2871e-01,\n",
       "            7.0984e-02,  1.6089e-01,  2.3242e-01, -5.6836e-01,  9.7168e-02,\n",
       "            2.6953e-01,  6.1719e-01, -2.0996e-01,  2.3279e-01,  6.6162e-02,\n",
       "           -1.8994e-01,  1.5710e-01,  2.6733e-01,  2.3401e-01, -8.6594e-03,\n",
       "           -1.5106e-02,  2.5894e-02, -5.9845e-02, -1.4038e-01,  3.2324e-01,\n",
       "            1.6006e-02, -1.2903e-01, -2.1477e-03,  1.4758e-01,  1.1652e-01,\n",
       "           -1.2964e-01, -1.3159e-01, -1.1218e-01,  2.4329e-01, -2.8152e-02,\n",
       "            1.3489e-01, -1.8872e-01,  7.0190e-02, -9.5825e-02,  2.5513e-02,\n",
       "            5.7159e-02,  2.6367e-01,  1.2810e-02,  2.5781e-01,  1.7542e-01,\n",
       "           -2.5659e-01, -2.6392e-01,  2.2107e-01,  1.4600e-01, -1.9666e-01,\n",
       "           -5.5885e-03,  4.3970e-01, -3.1299e-01, -2.1606e-01,  4.1046e-03,\n",
       "            4.5581e-01, -1.2184e-02, -7.5928e-01,  1.8164e-01, -5.4639e-01,\n",
       "           -1.4539e-01,  4.1473e-02, -5.1074e-01,  6.0669e-02, -3.3264e-02,\n",
       "           -6.4697e-02,  6.8750e-01, -1.6919e-01,  3.0212e-02, -1.8140e-01,\n",
       "            4.2822e-01, -2.9443e-01, -4.3359e-01,  3.7720e-02, -5.3809e-01,\n",
       "           -1.2878e-01, -3.1055e-01,  6.1005e-02,  1.0254e-01, -5.3345e-02,\n",
       "            1.2781e-01,  3.0029e-01,  1.3171e-01, -5.2295e-01,  1.1523e-01,\n",
       "           -1.7120e-02, -5.3271e-01, -3.2806e-02, -3.0933e-01,  2.7759e-01,\n",
       "           -3.6035e-01,  3.4082e-01, -2.2595e-01,  3.2422e-01,  2.6318e-01,\n",
       "           -1.3757e-01, -3.4033e-01, -2.6831e-01,  4.2578e+00, -1.7493e-01,\n",
       "           -1.7712e-01,  1.3672e-01, -2.4060e-01, -1.1493e-01,  5.5725e-02,\n",
       "           -1.3757e-01,  4.4238e-01,  1.8787e-01,  4.2212e-01, -1.5894e-01,\n",
       "           -2.4146e-01,  2.1436e-01, -2.8223e-01,  1.0339e-01, -2.4622e-01,\n",
       "           -3.7109e-01, -2.9810e-01,  1.1700e-01, -6.4331e-02, -6.0272e-02,\n",
       "            4.7424e-02,  1.1536e-01, -3.7476e-01, -2.8784e-01,  1.8677e-01,\n",
       "           -7.1228e-02, -2.0081e-01, -4.9268e-01,  2.9712e-01,  3.2593e-01,\n",
       "           -6.7322e-02,  6.2109e-01, -1.3684e-01,  1.1200e-01,  5.8350e-02,\n",
       "           -4.5135e-02, -3.7793e-01, -3.2959e-01,  2.1591e-02, -9.9945e-03,\n",
       "           -1.3940e-01,  4.6826e-01,  1.3269e-01, -1.1371e-01, -1.0858e-01,\n",
       "           -5.6445e-01,  9.6924e-02, -2.5696e-02, -4.7583e-01, -3.3252e-01,\n",
       "           -6.8994e-01,  2.0886e-01, -4.2358e-01,  3.1055e-01, -1.9629e-01,\n",
       "            1.0114e-01,  6.0547e-01, -7.2754e-02, -3.0200e-01,  2.3828e-01,\n",
       "           -2.8809e-01, -3.8354e-01, -9.2041e-02, -1.7432e-01, -1.8640e-01,\n",
       "            1.0852e-01,  7.3914e-02,  3.0594e-02,  2.8229e-02, -1.5271e-01,\n",
       "            3.6450e-01, -1.9299e-01,  7.9346e-03, -6.5125e-02,  8.5754e-03,\n",
       "            2.5439e-01,  2.7905e-01,  1.1139e-01,  3.4882e-02, -3.1079e-01,\n",
       "            2.0813e-01,  2.1692e-01, -2.7686e-01,  2.2778e-01, -3.0078e-01,\n",
       "            4.7388e-01, -5.8643e-01,  1.5723e-01,  2.3529e-02, -3.5791e-01,\n",
       "            1.3928e-01,  3.2202e-01, -2.9932e-01,  4.0918e-01, -3.7622e-01,\n",
       "           -2.5415e-01,  9.4788e-02,  1.4740e-02, -2.6337e-02,  3.8794e-01,\n",
       "           -2.1045e-01, -3.7842e-01, -1.4282e-01, -4.5044e-02,  1.2219e-01,\n",
       "           -3.7671e-01, -2.2925e-01, -5.9723e-02,  2.0801e-01, -3.3960e-01,\n",
       "            4.5581e-01, -3.3618e-01,  4.0649e-01,  2.3499e-02,  2.4670e-01,\n",
       "           -7.2021e-01, -4.3335e-01, -3.1860e-01, -1.4893e-01, -3.3789e-01,\n",
       "            1.1053e-01, -2.1106e-01,  1.3049e-01, -6.4575e-02, -1.6589e-01,\n",
       "           -2.4792e-01,  1.8616e-01, -1.0571e-01,  3.0322e-01,  2.9102e-01,\n",
       "           -6.3538e-02,  3.8892e-01,  1.3208e-01,  2.6779e-02,  8.0688e-02,\n",
       "           -2.2815e-01,  4.5052e-03,  1.1108e-01, -2.1094e-01, -2.6505e-02,\n",
       "            4.0942e-01, -3.3875e-02,  5.9021e-02,  1.7041e-01,  2.5537e-01,\n",
       "            4.4019e-01, -3.4149e-02,  2.2498e-01,  2.2510e-01, -7.4036e-02,\n",
       "           -1.7566e-01,  2.2430e-02,  4.8950e-02,  3.8330e-01, -2.1133e-03,\n",
       "           -1.2646e-01,  2.0947e-01, -1.6431e-01,  1.3062e-01, -1.0345e-01,\n",
       "           -1.8823e-01,  2.0654e-01, -6.8750e-01,  4.4556e-01, -2.4768e-01,\n",
       "           -1.8188e-01,  1.1908e-01,  2.7930e-01,  5.7251e-02,  3.1836e-01,\n",
       "           -6.4355e-01, -3.1543e-01,  1.1188e-01, -2.8979e-01, -8.2947e-02,\n",
       "           -2.0172e-02,  5.1270e-01,  4.2656e+00, -3.4576e-02,  2.2266e-01,\n",
       "            1.4441e-01,  1.6479e-01, -4.1187e-01,  2.6660e-01,  1.4978e-01,\n",
       "            8.7585e-02,  1.8591e-01, -4.9438e-02, -3.5950e-02, -4.6460e-01,\n",
       "           -1.8909e-01,  1.0168e-01,  3.4375e-01,  1.7627e-01, -8.2764e-01,\n",
       "            1.1945e-01,  1.3354e-01,  1.7212e-01, -2.4866e-01,  6.4453e-02,\n",
       "           -9.4543e-02,  1.4807e-01, -2.0203e-02, -3.5083e-01,  2.0435e-01,\n",
       "            7.8003e-02, -9.6985e-02,  1.7432e-01, -1.2012e-01,  6.3721e-01,\n",
       "           -1.0236e-01,  1.0327e-01, -2.8809e-01,  1.5430e-01, -2.1301e-01,\n",
       "            2.5732e-01, -3.4302e-01, -1.8164e-01,  3.0853e-02, -4.2139e-01,\n",
       "           -6.9873e-01, -7.0679e-02,  1.0504e-01,  1.0992e-01,  1.7090e-02,\n",
       "           -4.3915e-02, -1.5393e-01, -1.7471e-02,  2.9321e-01, -4.7729e-02,\n",
       "           -1.1462e-01,  5.1025e-01,  1.4990e-01, -1.5479e-01,  3.2031e-01,\n",
       "            3.7549e-01,  4.3335e-02, -5.0392e-03,  1.4722e-01,  3.5828e-02,\n",
       "           -8.4473e-02, -3.8037e-01, -1.6370e-01, -4.0747e-01, -1.4172e-01,\n",
       "            7.5012e-02, -5.1025e-02, -7.7026e-02, -2.7905e-01, -5.9961e-01,\n",
       "            3.7445e-02,  2.0239e-01, -6.3896e-03,  7.1960e-02, -5.9021e-02,\n",
       "           -3.9453e-01,  1.2201e-01, -5.9753e-02,  1.7224e-01, -3.7402e-01,\n",
       "            3.2861e-01, -2.2217e-02,  2.9712e-01, -3.0005e-01,  2.0728e-01,\n",
       "            2.6392e-01, -3.0441e-02,  1.2109e-01,  4.1479e-01,  7.3792e-02,\n",
       "           -1.7065e-01, -4.6631e-01, -1.3989e-01,  2.3425e-01,  2.4951e-01,\n",
       "           -1.2079e-01, -1.0577e-01, -3.2471e-01, -3.9337e-02,  2.2202e-02,\n",
       "           -3.7842e-01, -1.5918e-01, -4.4067e-01,  2.5684e-01, -5.9113e-02,\n",
       "            3.2196e-02,  6.9763e-02,  1.0596e-01, -4.7314e-01, -3.6743e-01,\n",
       "            6.8054e-02, -5.9265e-02,  4.5923e-01, -5.4016e-02,  2.8052e-01,\n",
       "           -1.7395e-02,  2.0874e-01, -7.0752e-01, -2.2083e-01,  3.2349e-01,\n",
       "           -2.2620e-01, -4.6582e-01, -5.0720e-02, -9.3201e-02, -3.8916e-01,\n",
       "           -1.5649e-01, -3.1519e-01, -1.2866e-01, -5.1025e-01,  1.5161e-01,\n",
       "           -1.8799e-02, -1.6525e-02,  8.3557e-02, -2.2217e-02,  8.7891e-02,\n",
       "           -5.3271e-01,  5.0195e-01, -2.8271e-01, -1.2854e-01, -6.3477e-01,\n",
       "            4.4159e-02, -7.0801e-02, -5.0354e-02, -1.4185e-01,  1.7761e-01,\n",
       "            5.8990e-02,  2.2351e-01,  2.2546e-01,  2.0276e-01,  3.0078e-01,\n",
       "            3.9478e-01, -8.0872e-03, -4.7882e-02, -3.9459e-02, -1.6830e-02,\n",
       "           -1.5723e-01,  6.5063e-02,  2.2705e-01,  6.9922e-01,  1.0651e-01,\n",
       "            3.0945e-02, -6.3171e-02,  1.6284e-01,  1.3660e-01,  4.3726e-01,\n",
       "           -1.5015e-01, -4.7461e-01, -7.6538e-02,  2.4500e-01,  2.9663e-02,\n",
       "           -6.2500e-02,  3.7415e-02, -3.3008e-01, -8.5327e-02, -9.6924e-02,\n",
       "            8.1711e-03, -4.1797e-01, -2.8687e-01,  1.2471e+00,  6.3770e-01,\n",
       "           -2.0496e-01,  4.0259e-01, -7.3280e-03, -4.4287e-01, -1.3232e-01,\n",
       "            4.4482e-01, -1.5320e-01,  9.6924e-02, -9.8328e-02,  9.1846e-01,\n",
       "           -5.0598e-02, -3.5229e-01,  2.5806e-01,  2.3486e-01,  5.3027e-01,\n",
       "            2.1851e-01,  4.9713e-02]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.4136, 0.1429, 0.0099, 0.4336]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    0.296539   50.170944  437.463928  339.485718    0.931168     63  laptop\n",
       "  1  278.976685    0.792313  638.662598  278.358093    0.921212      0  person\n",
       "  2  190.090881  113.017288  360.902954  259.555054    0.722103     62      tv\n",
       "  3  533.902893  189.598816  586.931091  238.869751    0.633137     45    bowl\n",
       "  4  187.643707  114.220520  359.759735  260.356323    0.345086      0  person\n",
       "  5  238.259399  163.257812  292.134888  235.705200    0.289555      0  person,\n",
       "  'caption': ['Black keyboard on an unused open laptop.'],\n",
       "  'bbox_target': [15.6, 321.56, 334.38, 101.42]},\n",
       " 93: {'image_emb': tensor([[ 0.4038,  0.5039,  0.1379,  ...,  0.4634,  0.0811,  0.0852],\n",
       "          [ 0.2517,  0.3379,  0.0634,  ...,  0.6587,  0.2454, -0.1897],\n",
       "          [-0.1947,  0.3157, -0.2388,  ...,  1.1084,  0.1323, -0.1940],\n",
       "          [ 0.1011,  0.2974, -0.0568,  ...,  0.2023,  0.1510, -0.0761]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1635,  0.1500,  0.0844,  ..., -0.7485, -0.0950,  0.0976],\n",
       "          [ 0.3140, -0.1964, -0.2252,  ...,  0.5049,  0.4014, -0.2505]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2012, 0.1077, 0.6597, 0.0313],\n",
       "          [0.0008, 0.3335, 0.5947, 0.0710]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  147.287247    0.000000  377.346252  404.981354    0.954093      0   \n",
       "  1  515.696228  126.140793  639.737000  215.413452    0.774864      0   \n",
       "  2  123.517670  361.442078  468.323761  413.798767    0.745470     37   \n",
       "  3  513.573364  127.183212  640.000000  214.890869    0.482426     37   \n",
       "  4  545.900269  150.497772  596.813965  177.789276    0.295486     37   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2  surfboard  \n",
       "  3  surfboard  \n",
       "  4  surfboard  ,\n",
       "  'caption': ['{}', 'A yellow surfboard with a black nose.'],\n",
       "  'bbox_target': [137.31, 361.76, 329.59, 51.17]},\n",
       " 94: {'image_emb': tensor([[ 0.0488,  0.4976,  0.3044,  ...,  0.3059, -0.3748,  0.1833],\n",
       "          [-0.2551,  0.2903, -0.4443,  ...,  0.4546,  0.1382, -0.3096],\n",
       "          [ 0.1321,  0.1498, -0.0538,  ...,  0.6333,  0.1509, -0.0414],\n",
       "          [ 0.0361,  0.1104, -0.1451,  ...,  0.5107,  0.1885, -0.1185],\n",
       "          [ 0.2991,  0.4565, -0.0436,  ...,  0.6870,  0.1782, -0.2264]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0748,  0.1691, -0.1129,  ...,  0.1575,  0.1981, -0.2139],\n",
       "          [-0.4487,  0.1527, -0.2148,  ...,  0.1755, -0.2458, -0.2278]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.3264e-02, 1.2164e-01, 2.5928e-05, 4.6706e-04, 8.4473e-01],\n",
       "          [1.7881e-07, 9.9951e-01, 4.7565e-05, 6.7055e-05, 3.8004e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  150.022385  118.355530  412.529480  395.975891    0.938225     63  laptop\n",
       "  1  499.916016  241.875305  601.953491  411.246277    0.926916     39  bottle\n",
       "  2  223.255264   62.731918  278.043152  120.139633    0.902466     41     cup\n",
       "  3  231.275116    0.500977  251.874268   31.553482    0.708540     41     cup\n",
       "  4  282.990753   58.900665  336.134247  119.589294    0.696389     41     cup\n",
       "  5  294.798859    0.000000  330.663483   71.082733    0.517890     39  bottle\n",
       "  6    0.879883    1.119827  225.951691   85.413666    0.469841     57   couch\n",
       "  7  362.743622   42.343704  516.149170  110.953690    0.457535     56   chair\n",
       "  8    0.085024   89.706360   35.611298  161.379547    0.310042     73    book,\n",
       "  'caption': ['Half of a jar of red sauce  next to a laptop.',\n",
       "   'A jar of pasta sauce is on a counter.'],\n",
       "  'bbox_target': [496.15, 237.71, 107.47, 173.11]},\n",
       " 95: {'image_emb': tensor([[-0.4102,  0.1058, -0.0718,  ...,  0.9648,  0.0220, -0.1876],\n",
       "          [-0.2390,  0.1594, -0.1300,  ...,  0.8511,  0.1245, -0.2245],\n",
       "          [-0.4014,  0.3293,  0.3650,  ...,  0.7603, -0.1375, -0.1301],\n",
       "          [-0.7749,  0.2786,  0.2250,  ...,  0.5669, -0.0236, -0.0432]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.5986,  0.0398,  0.2069,  ...,  0.1938, -0.1460, -0.1702],\n",
       "          [-0.4736, -0.1028,  0.1002,  ...,  0.1774,  0.0426, -0.1688]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1427, 0.0826, 0.4678, 0.3069],\n",
       "          [0.0031, 0.0034, 0.0377, 0.9561]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  182.539093    0.116150  346.590607   74.369446    0.905356     51  carrot\n",
       "  1  418.443390  380.806641  458.710114  437.246826    0.821843     51  carrot\n",
       "  2  482.756958  182.166138  640.000000  453.675049    0.815952     51  carrot\n",
       "  3  379.984772    0.000000  540.942688   68.135468    0.297661     39  bottle,\n",
       "  'caption': ['A carrot cut into pieces and kept ready to cook',\n",
       "   'The choopped arrots next to the celery.'],\n",
       "  'bbox_target': [481.04, 184.97, 158.96, 265.56]},\n",
       " 96: {'image_emb': tensor([[-0.8643,  0.1847,  0.0980,  ...,  0.6118,  0.0534, -0.2115],\n",
       "          [-0.1648,  0.5005, -0.1561,  ...,  1.3408,  0.0379, -0.0616],\n",
       "          [ 0.1914, -0.2112, -0.3550,  ...,  0.8477, -0.1451,  0.0187],\n",
       "          ...,\n",
       "          [-1.0420,  0.1046, -0.1350,  ...,  0.9712,  0.0812, -0.2939],\n",
       "          [ 0.0328,  0.2200, -0.1968,  ...,  1.2500, -0.0220, -0.0250],\n",
       "          [-0.8320,  0.1212,  0.0572,  ...,  0.7471, -0.0196, -0.2220]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2024, -0.0297, -0.3567,  ...,  0.6865, -0.0191, -0.3369],\n",
       "          [-0.2157, -0.2063, -0.1418,  ...,  0.6587,  0.0682, -0.1963]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.6955e-05, 3.0398e-06, 3.5167e-06, 6.8604e-01, 3.1396e-01, 1.3113e-06,\n",
       "           4.8280e-06],\n",
       "          [7.6413e-05, 2.7239e-05, 4.0293e-05, 5.4688e-01, 4.5312e-01, 3.3915e-05,\n",
       "           1.5080e-05]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    1.345078    0.000000  409.484924  404.662048    0.918587      0   \n",
       "  1  397.879730  189.349640  528.164429  422.301758    0.896285      0   \n",
       "  2  353.005890  139.253311  375.479218  163.958801    0.831742     32   \n",
       "  3  611.014648  357.572845  640.000000  423.900269    0.797113     38   \n",
       "  4  216.021576  298.276489  380.776520  383.958374    0.784792     38   \n",
       "  5  583.708069  283.014282  639.564636  345.712097    0.751867      0   \n",
       "  6  129.789124   31.576859  581.063904  412.884277    0.411727      0   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2    sports ball  \n",
       "  3  tennis racket  \n",
       "  4  tennis racket  \n",
       "  5         person  \n",
       "  6         person  ,\n",
       "  'caption': ['A tennis racket hanging over a tennis net.',\n",
       "   'The black tennis racket.'],\n",
       "  'bbox_target': [216.31, 297.31, 162.95, 89.57]},\n",
       " 97: {'image_emb': tensor([[-0.5986,  0.4893,  0.0532,  ...,  0.4910,  0.0176,  0.0960],\n",
       "          [-0.1652,  0.1992, -0.1136,  ...,  0.9795,  0.1163,  0.0575],\n",
       "          [ 0.1246, -0.1407, -0.4854,  ...,  1.3867, -0.0978, -0.2896],\n",
       "          [ 0.1448,  0.0385, -0.2515,  ...,  0.5908,  0.0231, -0.3516],\n",
       "          [-0.0634, -0.1212, -0.3062,  ...,  1.1084,  0.0837, -0.1390],\n",
       "          [-0.9106,  0.0466,  0.0072,  ...,  0.7568, -0.0327, -0.0983]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.5386, -0.0936, -0.1869,  ...,  0.5381, -0.3438, -0.4988],\n",
       "          [-0.3804, -0.2032, -0.1997,  ...,  0.3313, -0.2356, -0.6475]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.7646e-01, 7.2718e-06, 5.1856e-05, 1.3380e-03, 2.3842e-06, 1.2238e-01],\n",
       "          [9.3262e-01, 1.5318e-05, 6.2585e-05, 9.9373e-04, 8.9407e-07, 6.6467e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   26.764172  188.908691  149.203522  330.072815    0.931795      0   \n",
       "  1  142.037949  256.240906  202.156174  288.791260    0.879946     38   \n",
       "  2  212.576294  254.247406  221.808533  262.784760    0.857403     32   \n",
       "  3  606.488281  175.726349  623.681519  217.404449    0.824353      0   \n",
       "  4  234.852783  194.555450  316.076416  212.228241    0.729768     13   \n",
       "  5   78.387100  185.247223  156.444397  204.858551    0.649071     13   \n",
       "  6    0.573518  189.653198   15.804178  199.151855    0.498365     13   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1  tennis racket  \n",
       "  2    sports ball  \n",
       "  3         person  \n",
       "  4          bench  \n",
       "  5          bench  \n",
       "  6          bench  ,\n",
       "  'caption': ['A man with white T-shirt on the tennis court',\n",
       "   'A man in a white shirt getting ready to hit a tennis ball'],\n",
       "  'bbox_target': [26.76, 192.48, 122.49, 138.95]},\n",
       " 98: {'image_emb': tensor([[-0.1976,  0.1984, -0.1675,  ...,  0.1032,  0.0478, -0.1559],\n",
       "          [ 0.3525,  0.2649, -0.2808,  ...,  0.8486,  0.1528, -0.0202],\n",
       "          [ 0.1465, -0.1230, -0.2371,  ...,  1.2139,  0.1699, -0.1362],\n",
       "          [ 0.0605, -0.0068, -0.1831,  ...,  1.1338,  0.1765, -0.3657],\n",
       "          [ 0.2305,  0.0228, -0.2007,  ...,  0.5381,  0.0378, -0.1250],\n",
       "          [-0.2400,  0.1199, -0.0292,  ...,  0.1039,  0.0399, -0.0742]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 4.1943e-01, -3.7476e-01, -3.7134e-01,  ...,  2.2925e-01,\n",
       "           -1.4197e-01, -3.6621e-04],\n",
       "          [ 3.5400e-01, -3.2397e-01, -1.1975e-01,  ...,  3.1567e-01,\n",
       "           -6.2500e-02, -9.3506e-02]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.4399e-01, 2.6123e-02, 3.2377e-04, 7.9870e-06, 6.4011e-03, 6.2305e-01],\n",
       "          [7.2363e-01, 1.6495e-02, 4.9067e-04, 2.2948e-05, 3.8361e-02, 2.2083e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    20.483124  104.295654  423.275024  368.502502    0.938491      5   \n",
       "  1   418.992493  265.046173  621.234924  357.206573    0.916270      2   \n",
       "  2   356.198639  153.419006  385.986481  182.113037    0.826484      0   \n",
       "  3   135.701279  160.703827  157.842636  187.811005    0.787703      0   \n",
       "  4     0.000000  254.647583   25.905102  316.210938    0.777495      2   \n",
       "  5   497.461700  275.821869  520.652100  297.489349    0.682003      0   \n",
       "  6   101.278488  163.050446  120.317795  190.462494    0.663822      0   \n",
       "  7   268.783752  144.794495  292.851318  177.176270    0.628310      0   \n",
       "  8   614.960022  308.782501  638.875793  334.855377    0.585998      2   \n",
       "  9   445.849884  225.268097  457.902802  247.248383    0.552881      9   \n",
       "  10  426.575195  214.845734  443.110046  248.820221    0.547645      9   \n",
       "  11  584.708557  231.966675  600.736267  267.152832    0.445953      9   \n",
       "  12   69.347610  179.482880   83.523743  193.618927    0.353223      0   \n",
       "  13  133.238235  258.162628  150.392136  281.732697    0.335099      0   \n",
       "  14  117.376678  260.790588  139.223602  281.287537    0.333949      0   \n",
       "  15   74.259445  253.986145   89.512070  279.522217    0.302911      0   \n",
       "  16  535.365845  279.343445  554.186890  295.820618    0.296511      0   \n",
       "  17  365.154297  271.623657  376.686401  298.864929    0.283400      0   \n",
       "  18   83.231544  178.615265   96.946098  192.332123    0.281698      0   \n",
       "  19  468.158630  275.615509  485.888550  294.203827    0.251894      0   \n",
       "  \n",
       "               name  \n",
       "  0             bus  \n",
       "  1             car  \n",
       "  2          person  \n",
       "  3          person  \n",
       "  4             car  \n",
       "  5          person  \n",
       "  6          person  \n",
       "  7          person  \n",
       "  8             car  \n",
       "  9   traffic light  \n",
       "  10  traffic light  \n",
       "  11  traffic light  \n",
       "  12         person  \n",
       "  13         person  \n",
       "  14         person  \n",
       "  15         person  \n",
       "  16         person  \n",
       "  17         person  \n",
       "  18         person  \n",
       "  19         person  ,\n",
       "  'caption': ['A black car next to a double decker bus.',\n",
       "   \"A black car with a logo on it's door is next to a big red double level bus\"],\n",
       "  'bbox_target': [423.62, 263.36, 196.34, 92.6]},\n",
       " 99: {'image_emb': tensor([[-0.0412, -0.2900,  0.0532,  ...,  0.5298,  0.0105,  0.0845],\n",
       "          [-0.0272, -0.2725, -0.0417,  ...,  0.5376,  0.2485,  0.1804],\n",
       "          [ 0.0161, -0.2333,  0.1161,  ...,  0.5571,  0.0398,  0.0215]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0687,  0.0255, -0.1536,  ...,  0.0338,  0.2435,  0.0792],\n",
       "          [-0.3035, -0.0539,  0.1124,  ...,  0.1351,  0.1752,  0.1282]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2283, 0.4832, 0.2886],\n",
       "          [0.2490, 0.4106, 0.3403]], dtype=torch.float16),\n",
       "  'df_boxes':         xmin      ymin        xmax        ymax  confidence  class      name\n",
       "  0  15.962524  1.520203  636.479248  472.694214    0.910051     20  elephant\n",
       "  1   0.776527  1.387878  210.608276  472.926575    0.834345     20  elephant,\n",
       "  'caption': ['The elephant behind the other one',\n",
       "   'left barely visible elephant in the right hand picture'],\n",
       "  'bbox_target': [0.0, 0.75, 206.02, 469.21]},\n",
       " 100: {'image_emb': tensor([[ 0.0539,  0.4377, -0.4634,  ...,  1.3477, -0.0038, -0.2791],\n",
       "          [-0.0656,  0.0635, -0.0479,  ...,  1.2959, -0.0594, -0.4685],\n",
       "          [ 0.1876,  0.2164, -0.0423,  ...,  1.4443, -0.0880, -0.3699],\n",
       "          [ 0.2125,  0.2979, -0.0858,  ...,  1.2070, -0.2261,  0.1159],\n",
       "          [-0.1938, -0.0769, -0.1443,  ...,  1.1006,  0.2155, -0.3174],\n",
       "          [ 0.4587, -0.0457,  0.3640,  ...,  1.1162, -0.3584, -0.1351]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0138, -0.0674,  0.4402,  ..., -0.2004, -0.2837, -0.5205],\n",
       "          [ 0.1759,  0.0240, -0.0331,  ...,  0.0403,  0.1870, -0.3113]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.3602e-03, 6.4746e-01, 4.0970e-03, 6.4850e-04, 7.3792e-02, 2.6978e-01],\n",
       "          [1.4901e-06, 6.5332e-01, 2.2769e-05, 2.3246e-06, 2.3657e-01, 1.1005e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  244.084290   26.043655  316.623108  108.029099    0.909044      0  person\n",
       "  1    1.195862  106.005005  639.023438  401.747253    0.897558      8    boat\n",
       "  2  278.806946   21.249634  371.404724  117.922607    0.892842      0  person\n",
       "  3  429.819794  334.161560  547.089905  408.905518    0.875122     16     dog\n",
       "  4  176.544189   45.057152  530.033020  184.254700    0.826751     17   horse\n",
       "  5  550.807251  138.496613  639.880005  209.438477    0.675756      8    boat\n",
       "  6  548.762329   86.967651  639.808472  150.210632    0.547843      8    boat\n",
       "  7    1.526672    5.030563  274.614685  132.746536    0.494170      8    boat\n",
       "  8  487.999817   38.527359  552.526184  158.255905    0.472100      0  person\n",
       "  9  574.993225   23.832047  639.643250   96.991257    0.425515      0  person,\n",
       "  'caption': ['a white boat', 'boat with horse on it'],\n",
       "  'bbox_target': [1.08, 111.1, 638.92, 299.87]},\n",
       " 101: {'image_emb': tensor([[ 0.1029,  0.0537, -0.2642,  ...,  1.2705,  0.1819,  0.2057],\n",
       "          [ 0.1954, -0.3350, -0.5166,  ...,  0.9917,  0.0617, -0.0813],\n",
       "          [ 0.0161,  0.1855, -0.0744,  ...,  1.3691,  0.1693,  0.0848],\n",
       "          ...,\n",
       "          [-0.1943,  0.4287, -0.3528,  ...,  1.2686,  0.1307, -0.2335],\n",
       "          [-0.0919,  0.3291, -0.2266,  ...,  1.3037,  0.3000, -0.2036],\n",
       "          [ 0.7378, -0.0112,  0.0857,  ...,  0.5200,  0.3333,  0.0624]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0107,  0.2010,  0.1035,  ...,  0.0725,  0.2019, -0.2252],\n",
       "          [ 0.2380,  0.0380,  0.1978,  ...,  0.3315,  0.2316, -0.3870]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.2688e-02, 6.8970e-03, 1.9531e-01, 1.7996e-03, 6.3086e-01, 2.1576e-02,\n",
       "           8.4972e-04, 1.3013e-01],\n",
       "          [7.9880e-03, 1.0252e-05, 5.0476e-02, 1.6832e-04, 8.7662e-03, 3.3998e-04,\n",
       "           9.3384e-03, 9.2285e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  370.420654  159.156555  422.812256  213.984009    0.894036      0   person\n",
       "  1  147.219635  141.757095  175.888397  202.234619    0.850280      0   person\n",
       "  2  322.268860  146.204697  377.591797  211.791199    0.841743      0   person\n",
       "  3  158.594223  139.859634  222.519547  207.259674    0.814395      0   person\n",
       "  4   56.246292  192.610840  447.666077  239.973450    0.806769      8     boat\n",
       "  5    0.141787   80.287018   42.915154  203.372437    0.774079      8     boat\n",
       "  6  223.521011  155.024841  308.113525  210.822327    0.757527      1  bicycle\n",
       "  7  284.036377  159.884918  340.741089  211.281158    0.627487      1  bicycle\n",
       "  8   56.309662  187.266937  445.124786  240.748810    0.531966      1  bicycle,\n",
       "  'caption': ['the boat all the people are in',\n",
       "   'A boat with bicycles and people on it.'],\n",
       "  'bbox_target': [57.71, 187.49, 389.58, 55.65]},\n",
       " 102: {'image_emb': tensor([[-0.6270,  0.0432,  0.3184,  ...,  0.8164,  0.2112, -0.1390],\n",
       "          [-0.0316, -0.1050, -0.2690,  ...,  1.2695,  0.0397, -0.1146],\n",
       "          [-0.0368, -0.0074, -0.3262,  ...,  0.5391,  0.0953,  0.1083],\n",
       "          [-0.1863,  0.2133,  0.1137,  ...,  0.9492,  0.1227, -0.0687]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0203,  0.2891, -0.2793,  ..., -0.0977, -0.0670,  0.2173],\n",
       "          [ 0.1306, -0.0213, -0.4111,  ...,  0.3213, -0.1508,  0.0814]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.8877e-02, 7.3486e-02, 6.5923e-05, 8.2764e-01],\n",
       "          [7.1472e-02, 8.4839e-02, 1.0133e-05, 8.4375e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  312.515320    3.014160  638.631958  475.656372    0.956957      0   \n",
       "  1    0.369236  135.435181  116.511414  473.188599    0.895831      0   \n",
       "  2  329.808594  324.618225  491.730347  358.993225    0.794283     79   \n",
       "  3  432.780731  333.499847  492.758087  359.042816    0.368225     79   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2  toothbrush  \n",
       "  3  toothbrush  ,\n",
       "  'caption': ['the reflection of a man brushing his teeth in a mirror',\n",
       "   'Reflection of a bearded man brushing his teeth in front of the mirror.'],\n",
       "  'bbox_target': [2.16, 136.99, 125.12, 336.54]},\n",
       " 103: {'image_emb': tensor([[-1.8408e-01,  3.6230e-01, -2.0056e-01,  ...,  1.1670e+00,\n",
       "           -1.4610e-02, -2.5757e-01],\n",
       "          [ 3.0200e-01,  3.0444e-01, -1.6211e-01,  ...,  3.9062e-01,\n",
       "            5.6445e-01,  1.0840e-01],\n",
       "          [ 4.8413e-01, -7.9590e-02, -7.3051e-04,  ...,  5.8789e-01,\n",
       "           -1.3452e-01, -5.2783e-01],\n",
       "          ...,\n",
       "          [-2.9724e-02,  3.4253e-01, -2.3071e-01,  ...,  7.7002e-01,\n",
       "            2.4963e-01,  2.2754e-01],\n",
       "          [-1.2772e-02,  4.6582e-01,  3.6713e-02,  ...,  7.8467e-01,\n",
       "            4.7546e-02, -8.4229e-02],\n",
       "          [ 1.0211e-01,  6.9092e-02,  3.0975e-02,  ...,  3.8184e-01,\n",
       "            4.8804e-01, -1.0931e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1748,  0.1176, -0.3872,  ..., -0.0208,  0.0132,  0.1561],\n",
       "          [-0.1663, -0.2135, -0.2583,  ..., -0.4177, -0.0502, -0.1797]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.1352e-04, 5.4932e-01, 1.2279e-04, 3.4937e-01, 2.9469e-04, 2.3842e-07,\n",
       "           9.6985e-02, 1.7881e-07, 3.4237e-03],\n",
       "          [8.2493e-04, 2.6901e-02, 1.7881e-07, 6.5552e-02, 7.4387e-05, 6.5918e-03,\n",
       "           2.1286e-02, 1.7738e-03, 8.7695e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   24.769188  268.338928  229.645020  396.636108    0.936427     56   \n",
       "  1  422.061707  162.701675  584.570740  460.693542    0.933651      0   \n",
       "  2  166.836243   58.788834  381.553894  399.591064    0.931116      0   \n",
       "  3  375.526581  111.510391  498.225616  421.778809    0.922650      0   \n",
       "  4    0.206543  249.780579   47.711803  365.794739    0.884260     56   \n",
       "  5  250.203156  364.533112  369.530670  439.765442    0.881101     55   \n",
       "  6  507.988556  146.767883  578.660767  301.129883    0.848649      0   \n",
       "  7    2.299316  350.004639  602.319824  479.366638    0.792550     60   \n",
       "  8  415.652130  351.050690  454.201569  423.802032    0.559224     56   \n",
       "  \n",
       "             name  \n",
       "  0         chair  \n",
       "  1        person  \n",
       "  2        person  \n",
       "  3        person  \n",
       "  4         chair  \n",
       "  5          cake  \n",
       "  6        person  \n",
       "  7  dining table  \n",
       "  8         chair  ,\n",
       "  'caption': ['The boy with the blue shirt',\n",
       "   'A younng kid about to blow his candles'],\n",
       "  'bbox_target': [427.03, 163.78, 155.67, 299.46]},\n",
       " 104: {'image_emb': tensor([[ 0.0238,  0.0015,  0.0916,  ...,  0.8398, -0.2344,  0.1370],\n",
       "          [-0.4336,  0.2729,  0.0189,  ...,  1.0693, -0.3572,  0.1199],\n",
       "          [-0.1886,  0.2903, -0.1967,  ...,  1.1211,  0.0385, -0.1730],\n",
       "          [ 0.0963,  0.5078, -0.3052,  ...,  1.2686,  0.0334,  0.0591],\n",
       "          [ 0.0104,  0.4158, -0.2651,  ...,  1.1582, -0.0053,  0.0175],\n",
       "          [-0.1411,  0.0122,  0.1617,  ...,  0.8774, -0.5591,  0.2162]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1077, -0.2043,  0.0352,  ...,  0.2910, -0.3281, -0.2969],\n",
       "          [-0.0640,  0.0850, -0.0870,  ...,  0.0279,  0.1064, -0.1409]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.2188e-01, 4.5197e-02, 2.6566e-02, 1.7083e-04, 4.3154e-05, 6.2103e-03],\n",
       "          [1.2122e-01, 1.6309e-01, 6.9466e-03, 4.7192e-01, 2.0935e-01, 2.7466e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   85.510147   43.020706  354.484436  393.048828    0.912049     77   \n",
       "  1  326.990479   72.095551  555.649902  390.773285    0.886789     77   \n",
       "  2  108.834831  150.736328  236.206787  251.741150    0.877833     27   \n",
       "  3  320.944305  149.605530  385.522491  253.291748    0.791313     27   \n",
       "  4  321.402222  149.033356  384.478699  253.146088    0.718584     77   \n",
       "  \n",
       "           name  \n",
       "  0  teddy bear  \n",
       "  1  teddy bear  \n",
       "  2         tie  \n",
       "  3         tie  \n",
       "  4  teddy bear  ,\n",
       "  'caption': ['A brown bear with a plaid ribbon.',\n",
       "   'The toy in light yellow color is looking nice'],\n",
       "  'bbox_target': [84.24, 39.09, 276.66, 352.29]},\n",
       " 105: {'image_emb': tensor([[ 0.1758, -0.0066, -0.5459,  ...,  0.1287,  0.0817,  0.7349],\n",
       "          [-0.3757,  0.2096, -0.1534,  ...,  1.3125,  0.2759,  0.1127],\n",
       "          [ 0.1572,  0.1416, -0.4194,  ...,  0.6865,  0.1554,  0.2004],\n",
       "          [ 0.1603, -0.0936, -0.4812,  ...,  0.1622,  0.0640,  0.7632]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 1.1542e-01, -5.4993e-02,  2.5192e-02,  ..., -6.1035e-05,\n",
       "            7.5867e-02, -3.1201e-01],\n",
       "          [-8.1665e-02,  1.8176e-01, -1.6199e-01,  ..., -2.1375e-01,\n",
       "           -1.1090e-01, -5.6592e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.4053e-01, 4.4632e-03, 6.9916e-05, 4.5508e-01],\n",
       "          [3.6106e-03, 4.0430e-01, 5.8838e-01, 3.5534e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    4.350400   41.545284  417.053040  371.837677    0.948134      0  person\n",
       "  1  290.817200    1.865900  499.373230  369.359253    0.855113      0  person\n",
       "  2  134.087967  312.122192  174.708557  374.622742    0.827612     27     tie\n",
       "  3  435.098877  274.512604  499.534943  372.709839    0.279216      0  person,\n",
       "  'caption': ['A captain speaking to a lady captain', 'The man in the hat,'],\n",
       "  'bbox_target': [284.19, 0.94, 215.81, 369.95]},\n",
       " 106: {'image_emb': tensor([[-0.1565,  0.5151,  0.0042,  ...,  0.7886, -0.0345, -0.0023],\n",
       "          [-0.3003,  0.1243, -0.1661,  ...,  0.3003,  0.3779, -0.4070],\n",
       "          [-0.1719,  0.4458, -0.2042,  ...,  0.6235,  0.3923, -0.1422]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0264, -0.0443, -0.0163,  ...,  0.0867,  0.1672, -0.3357],\n",
       "          [-0.0264, -0.0443, -0.0163,  ...,  0.0867,  0.1672, -0.3357]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.6757e-04, 8.9697e-01, 1.0223e-01],\n",
       "          [6.6757e-04, 8.9697e-01, 1.0223e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  583.564941  249.276031  639.769043  306.476959    0.909882      2      car\n",
       "  1    4.211853  189.350342  572.074097  473.093872    0.836861      1  bicycle\n",
       "  2    0.296852  282.485840   69.414688  378.169128    0.306214      1  bicycle,\n",
       "  'caption': ['the bicycle nearest to the camera',\n",
       "   'the bicycle nearest to the camera'],\n",
       "  'bbox_target': [0.0, 200.17, 562.76, 275.17]},\n",
       " 107: {'image_emb': tensor([[ 0.2139,  0.3645, -0.5181,  ...,  0.8154, -0.2668,  0.3477],\n",
       "          [ 0.1212, -0.0965, -0.7612,  ...,  0.2406,  0.1445,  0.1271],\n",
       "          [ 0.0777,  0.1725, -0.4358,  ...,  1.0195,  0.3618, -0.0263],\n",
       "          ...,\n",
       "          [ 0.2495, -0.3147, -0.1991,  ...,  0.8823, -0.1235,  0.1869],\n",
       "          [ 0.1890, -0.2971, -0.4973,  ...,  0.9561, -0.2483, -0.0485],\n",
       "          [-0.2849,  0.1014,  0.0190,  ...,  0.1941,  0.1704,  0.0817]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0178, -0.3938, -0.1281,  ...,  0.1807, -0.1279,  0.1960],\n",
       "          [ 0.3135,  0.0604, -0.1416,  ...,  0.0993, -0.2783,  0.1591]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.5781e-01, 2.8946e-02, 2.2766e-01, 1.9775e-01, 5.4901e-02, 1.3171e-01,\n",
       "           9.6375e-02, 4.7226e-03],\n",
       "          [7.7930e-01, 7.9298e-04, 1.9026e-03, 1.0388e-01, 1.0065e-01, 1.0284e-02,\n",
       "           3.0403e-03, 7.5102e-06]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    64.962090   73.943100  111.163933  241.203720    0.897871      0   \n",
       "  1   257.972168   78.324776  312.672272  254.213837    0.895481      0   \n",
       "  2   240.032898   74.619278  279.273315  132.474640    0.883366     56   \n",
       "  3   178.863678  167.203751  200.536560  228.214752    0.848985     38   \n",
       "  4   149.499420  101.586739  201.280548  258.462280    0.799852      0   \n",
       "  5   254.783051  139.175720  274.108765  205.664917    0.770813     38   \n",
       "  6   108.435234  122.239166  117.268013  131.720123    0.755448     32   \n",
       "  7   151.064270   66.259613  251.363327  244.288498    0.663172      0   \n",
       "  8     0.000000    0.354047   18.684574   27.846085    0.478051     56   \n",
       "  9    34.448296   81.573296   82.207184  153.316833    0.435596     56   \n",
       "  10   31.596920    0.000000   82.897072   41.451538    0.375136     56   \n",
       "  11  175.726715   76.652191  220.245132  126.619621    0.267168     38   \n",
       "  \n",
       "               name  \n",
       "  0          person  \n",
       "  1          person  \n",
       "  2           chair  \n",
       "  3   tennis racket  \n",
       "  4          person  \n",
       "  5   tennis racket  \n",
       "  6     sports ball  \n",
       "  7          person  \n",
       "  8           chair  \n",
       "  9           chair  \n",
       "  10          chair  \n",
       "  11  tennis racket  ,\n",
       "  'caption': ['Woman in jeans and sneakers brushes her hair back',\n",
       "   'Woman wearing blue jeans'],\n",
       "  'bbox_target': [67.12, 77.03, 51.81, 168.39]},\n",
       " 108: {'image_emb': tensor([[-0.1824, -0.2330, -0.0420,  ...,  0.4614,  0.1520,  0.0343],\n",
       "          [-0.1404,  0.2332, -0.1199,  ...,  0.9629,  0.2244, -0.3149],\n",
       "          [-0.0350,  0.5293,  0.3650,  ...,  0.7432,  0.4595,  0.1142],\n",
       "          [-0.0070,  0.3398, -0.5625,  ...,  1.3857,  0.0252, -0.0347],\n",
       "          [-0.0288,  0.3777, -0.2812,  ...,  1.4277,  0.1266, -0.2988],\n",
       "          [-0.0698,  0.0401,  0.2759,  ...,  0.4749,  0.1600,  0.2529]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2908,  0.1965, -0.2269,  ..., -0.0040,  0.0262, -0.1926],\n",
       "          [-0.2761, -0.1439, -0.1925,  ..., -0.0768,  0.1155,  0.1223]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0073e-05, 9.9658e-01, 1.1325e-06, 8.4519e-05, 3.2730e-03, 4.3511e-06],\n",
       "          [3.8481e-04, 2.4048e-01, 2.7142e-03, 2.0569e-01, 5.5029e-01, 1.4615e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   29.017426    1.710159  637.427002  422.060608    0.930049      7   truck\n",
       "  1  139.205276  210.946655  257.020477  424.387451    0.920675      0  person\n",
       "  2    0.898308  214.869934  225.249176  423.538879    0.909043      0  person\n",
       "  3    0.000000  231.229889   44.405998  411.349670    0.825922      0  person\n",
       "  4   81.797134  221.470215  146.044846  425.218994    0.737196      0  person\n",
       "  5  186.012039  261.755890  198.955551  313.992096    0.527503     27     tie\n",
       "  6  149.557617  249.276794  245.028717  379.306396    0.259295     27     tie,\n",
       "  'caption': ['Black man in a grey and white suite with brown tie visible',\n",
       "   'The man in the grey suit'],\n",
       "  'bbox_target': [140.4, 211.86, 114.71, 214.95]},\n",
       " 109: {'image_emb': tensor([[ 0.0186,  0.1520, -0.1437,  ...,  0.7163,  0.0820, -0.2769],\n",
       "          [ 0.0191, -0.0087, -0.0145,  ...,  0.9678,  0.0415, -0.4666],\n",
       "          [ 0.0247, -0.0366, -0.1941,  ...,  0.5234,  0.2045, -0.0383],\n",
       "          [ 0.3911, -0.1097,  0.1523,  ...,  0.3630,  0.1117,  0.1466]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1500,  0.5596, -0.4067,  ...,  0.0770, -0.2245, -0.2969],\n",
       "          [ 0.0798,  0.1628, -0.1376,  ..., -0.1317, -0.0643, -0.1520]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0018, 0.9136, 0.0036, 0.0811],\n",
       "          [0.3455, 0.5186, 0.0275, 0.1087]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   361.571625  251.428741  479.841187  638.231689    0.908755      0   \n",
       "  1   161.487213   73.798309  394.161041  600.844360    0.848714      0   \n",
       "  2     1.969498  369.759583  355.365662  639.393616    0.783597      1   \n",
       "  3   411.086609  186.273468  451.651917  270.359375    0.675724      0   \n",
       "  4   105.205292  134.111145  233.504211  289.189392    0.670434     25   \n",
       "  5     0.031982  170.452011  113.118561  227.866226    0.654787      2   \n",
       "  6   161.723297  133.705170  245.678680  356.073639    0.560241      0   \n",
       "  7   295.818390  205.085999  413.060944  425.390259    0.554918     24   \n",
       "  8   314.585449  522.911682  391.204041  640.000000    0.527274     26   \n",
       "  9    16.887642  144.682953   46.683739  168.944794    0.464509      2   \n",
       "  10   42.565697  238.915436  132.627716  461.312347    0.458770      1   \n",
       "  11  142.792831  131.761246  244.191345  573.200928    0.448770      0   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2    bicycle  \n",
       "  3     person  \n",
       "  4   umbrella  \n",
       "  5        car  \n",
       "  6     person  \n",
       "  7   backpack  \n",
       "  8    handbag  \n",
       "  9        car  \n",
       "  10   bicycle  \n",
       "  11    person  ,\n",
       "  'caption': ['A guy smiling while holding a closed umbrella, he is wearing a black beanie cap',\n",
       "   'guy with mouth open'],\n",
       "  'bbox_target': [163.99, 71.59, 239.46, 510.84]},\n",
       " 110: {'image_emb': tensor([[-0.0783, -0.0649,  0.2406,  ...,  0.9839,  0.1705, -0.2837],\n",
       "          [ 0.0137,  0.2986,  0.2415,  ...,  0.8423,  0.2771, -0.0121],\n",
       "          [ 0.1504,  0.0975,  0.2783,  ...,  0.8306,  0.2974, -0.0429],\n",
       "          [ 0.1470,  0.0763,  0.1807,  ...,  0.7300,  0.3308,  0.1202]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1956, -0.0563, -0.3574,  ..., -0.2468,  0.3027, -0.1542],\n",
       "          [-0.0128,  0.1289, -0.3652,  ...,  0.0345, -0.0691, -0.3159]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1293, 0.7324, 0.0483, 0.0902],\n",
       "          [0.1954, 0.2249, 0.3650, 0.2146]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class    name\n",
       "  0  236.162567  72.137817  579.467041  342.291138    0.926631      0  person\n",
       "  1    0.000000  64.775024  320.692505  354.409119    0.916027      0  person\n",
       "  2   32.796509   1.970627  637.936157  353.056213    0.731297     59     bed,\n",
       "  'caption': ['The boy with a bunny on his arm',\n",
       "   'Baby laying in the bed beside a little boy.'],\n",
       "  'bbox_target': [235.42, 73.62, 340.58, 271.82]},\n",
       " 111: {'image_emb': tensor([[ 0.4495, -0.1700, -0.0613,  ..., -0.0541,  0.1853, -0.0886],\n",
       "          [ 0.0548, -0.2991, -0.0510,  ..., -0.0339,  0.2358, -0.0116],\n",
       "          [-0.5205, -0.1501, -0.0611,  ...,  0.3530,  0.2445,  0.1616],\n",
       "          [ 0.2339, -0.0149, -0.3628,  ...,  0.8125,  0.0473, -0.2769],\n",
       "          [ 0.2944, -0.3125,  0.1069,  ...,  0.0802,  0.2056,  0.0370]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1230, -0.4487, -0.2812,  ..., -0.0210, -0.2656, -0.3169],\n",
       "          [ 0.2438, -0.1043, -0.3784,  ...,  0.1534,  0.0288, -0.4946]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.5405e-01, 3.6377e-01, 1.8120e-05, 7.0632e-05, 4.8193e-01],\n",
       "          [1.1169e-01, 9.3155e-03, 9.8765e-05, 8.0585e-05, 8.7891e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0    0.600970   17.017651  214.358429  496.490417    0.937860     23  giraffe\n",
       "  1   67.041183  322.981567  119.274902  412.181000    0.906440     23  giraffe\n",
       "  2  290.736969  394.233643  370.167633  499.533356    0.833542      0   person\n",
       "  3  304.210510  366.802521  322.335449  412.083527    0.709071     51   carrot,\n",
       "  'caption': ['A giraff with its neck streched out looking at a snack',\n",
       "   'A giraffe right next to a lady holding a carrot.'],\n",
       "  'bbox_target': [0.0, 22.47, 211.24, 471.91]},\n",
       " 112: {'image_emb': tensor([[ 0.5830,  0.1061, -0.2649,  ...,  0.1346,  0.1517, -0.4619],\n",
       "          [ 0.2805,  0.3186, -0.5327,  ...,  0.8809,  0.1271, -0.4038],\n",
       "          [ 0.5542,  0.4365, -0.1638,  ..., -0.1562, -0.0173, -0.1753]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3091,  0.0649, -0.4956,  ..., -0.2406, -0.2111, -0.4631],\n",
       "          [-0.1267,  0.0979, -0.1663,  ...,  0.0379, -0.1030, -0.3101]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1173, 0.1530, 0.7300],\n",
       "          [0.0697, 0.6411, 0.2891]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class name\n",
       "  0  165.199860   93.547562  409.177094  477.757019    0.927986     16  dog\n",
       "  1  299.996521  101.064835  472.439209  477.031189    0.918044     16  dog,\n",
       "  'caption': ['yorkshire dog standing to hold onto other dog',\n",
       "   'dog in white and red jacket'],\n",
       "  'bbox_target': [167.76, 93.76, 242.42, 380.13]},\n",
       " 113: {'image_emb': tensor([[-0.0087,  0.1594,  0.2206,  ...,  0.9629,  0.3193,  0.3132],\n",
       "          [ 0.0753,  0.0349, -0.0109,  ...,  0.7578,  0.2443,  0.2881],\n",
       "          [ 0.0594,  0.1621,  0.3259,  ...,  0.7319,  0.4727, -0.0164],\n",
       "          [ 0.1118,  0.2729,  0.4751,  ...,  0.7778,  0.1431,  0.1643]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1639, -0.1991,  0.2605,  ...,  0.1841, -0.2180, -0.2322],\n",
       "          [-0.1128,  0.3157, -0.1863,  ..., -0.1885, -0.4119, -0.2032]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.6784e-04, 1.0309e-03, 7.1955e-04, 9.9756e-01],\n",
       "          [5.8777e-02, 4.7493e-03, 2.4643e-03, 9.3408e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  164.369827  166.607956  374.584900  364.255432    0.926985      0  person\n",
       "  1  110.678802  213.597595  210.184784  354.588867    0.897658      0  person\n",
       "  2  291.599182  202.193481  401.795593  319.860107    0.896394      0  person\n",
       "  3   73.635452  213.398468  529.282043  475.005615    0.619794     59     bed\n",
       "  4  224.874512  279.098145  295.995850  304.794678    0.604745     73    book\n",
       "  5   58.123840    1.502396  536.187256  172.429611    0.308379     59     bed,\n",
       "  'caption': ['A quilt on the bottom part of the bunk bed',\n",
       "   'The bed upon which the people are sitting on.'],\n",
       "  'bbox_target': [83.95, 220.57, 453.09, 254.0]},\n",
       " 114: {'image_emb': tensor([[-1.0590e-01, -1.4563e-01, -3.0228e-02,  ...,  8.0811e-01,\n",
       "           -1.7578e-01,  5.9131e-01],\n",
       "          [ 2.0239e-01,  1.3757e-01,  7.2754e-02,  ...,  8.0029e-01,\n",
       "            1.4185e-01, -1.3257e-01],\n",
       "          [ 2.4658e-01,  2.6294e-01, -3.1763e-01,  ...,  4.3286e-01,\n",
       "           -5.9387e-02,  2.7197e-01],\n",
       "          ...,\n",
       "          [-2.4927e-01,  9.6619e-02, -2.4887e-02,  ...,  1.1514e+00,\n",
       "            2.9993e-04,  5.7831e-02],\n",
       "          [-4.3896e-01,  3.1860e-01, -1.6467e-01,  ...,  1.3936e+00,\n",
       "            1.1285e-01, -2.2217e-01],\n",
       "          [-2.9343e-02,  2.4377e-01, -1.4380e-01,  ...,  7.6660e-01,\n",
       "           -2.3975e-03,  2.0093e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2003,  0.5137, -0.1799,  ..., -0.0728, -0.1520,  0.1978],\n",
       "          [-0.0396,  0.3430, -0.0482,  ..., -0.3899, -0.3289, -0.2467]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.8730e-01, 4.2953e-03, 1.8299e-04, 7.9041e-03, 0.0000e+00, 3.5763e-06,\n",
       "           9.5367e-07, 0.0000e+00, 1.3053e-05, 2.2650e-06, 1.6212e-05, 9.4175e-06,\n",
       "           8.2850e-06, 1.3387e-04],\n",
       "          [8.0664e-01, 3.4363e-02, 3.1471e-03, 1.4929e-01, 2.9802e-07, 1.8418e-05,\n",
       "           1.4126e-05, 5.3644e-07, 1.9491e-04, 1.5664e-04, 2.1076e-04, 3.0208e-04,\n",
       "           4.9515e-03, 5.2166e-04]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     4.619797   62.000290  300.271790  422.110840    0.947339      0   \n",
       "  1   479.863983    0.000000  639.809021  305.095398    0.933094      0   \n",
       "  2   322.330353    6.846092  532.532288  230.731476    0.922638      0   \n",
       "  3    19.452217   20.100876  178.769531  264.256897    0.910830      0   \n",
       "  4   290.092255  290.776703  394.079987  364.304779    0.909346     55   \n",
       "  5   551.817261  360.770599  600.784302  425.403992    0.900774     41   \n",
       "  6   299.360504  257.261627  334.656769  308.394196    0.866832     41   \n",
       "  7   178.511414  202.734192  639.137695  421.989990    0.848638     60   \n",
       "  8   362.785553  184.935455  393.898712  284.596588    0.846363     39   \n",
       "  9   264.966278  233.016876  305.390106  255.901276    0.807885     45   \n",
       "  10    0.293999  311.298889   53.498577  424.488220    0.804426     56   \n",
       "  11  419.153931  187.587524  484.744507  343.737366    0.796195     39   \n",
       "  12  190.789902  279.841339  237.992233  310.555817    0.731260     45   \n",
       "  13  481.327026  264.446228  496.494446  319.005493    0.604353     41   \n",
       "  14  266.199280  254.188843  303.729675  279.491943    0.577665     45   \n",
       "  15  193.458328  240.732910  220.323715  278.948181    0.575175     41   \n",
       "  16  402.945465  260.708557  422.526398  292.620911    0.326205     41   \n",
       "  17  184.685272  262.178162  205.507233  289.144897    0.324490     41   \n",
       "  18  331.417847  267.898895  358.568665  293.347015    0.300921     45   \n",
       "  19  402.571320  261.263763  422.487457  292.496918    0.265491     45   \n",
       "  20  397.111298  357.005920  493.930389  383.801208    0.264056     43   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3         person  \n",
       "  4           cake  \n",
       "  5            cup  \n",
       "  6            cup  \n",
       "  7   dining table  \n",
       "  8         bottle  \n",
       "  9           bowl  \n",
       "  10         chair  \n",
       "  11        bottle  \n",
       "  12          bowl  \n",
       "  13           cup  \n",
       "  14          bowl  \n",
       "  15           cup  \n",
       "  16           cup  \n",
       "  17           cup  \n",
       "  18          bowl  \n",
       "  19          bowl  \n",
       "  20         knife  ,\n",
       "  'caption': ['A person in a green shirt and black thick rimmed glasses with long blond hair.',\n",
       "   'A young person with long blonde hair and his chin resting on a fist.'],\n",
       "  'bbox_target': [5.93, 62.56, 299.48, 364.2]},\n",
       " 115: {'image_emb': tensor([[ 0.0052,  0.3030, -0.2468,  ...,  1.0889,  0.2610, -0.1364],\n",
       "          [-0.1672,  0.0605,  0.1046,  ...,  0.9312,  0.4141,  0.0282],\n",
       "          [ 0.1895, -0.0877, -0.2712,  ...,  0.5630, -0.0164, -0.1821],\n",
       "          ...,\n",
       "          [-0.1709, -0.2313, -0.3101,  ...,  0.7021,  0.1930, -0.3926],\n",
       "          [ 0.1593, -0.2571, -0.5850,  ...,  0.6406, -0.2227, -0.0178],\n",
       "          [-0.2629,  0.0306,  0.1436,  ...,  0.6440,  0.4626,  0.1333]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2321,  0.0568, -0.0361,  ...,  0.1111,  0.2859, -0.1825],\n",
       "          [ 0.0251, -0.0587, -0.0806,  ...,  0.1938,  0.2206, -0.0676]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.4626e-02, 2.8351e-02, 1.8120e-03, 2.9869e-03, 2.5702e-04, 1.3056e-03,\n",
       "           8.8310e-04, 9.0967e-01],\n",
       "          [1.5967e-01, 1.2433e-01, 5.2109e-03, 8.5907e-03, 1.3459e-04, 6.9046e-03,\n",
       "           1.6928e-03, 6.9336e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   316.558136  130.768250  525.814697  476.994934    0.934730      0   \n",
       "  1   283.053772    1.555496  533.997314  475.155090    0.907270     37   \n",
       "  2     5.330794  173.184692   26.211170  246.918091    0.843120      0   \n",
       "  3   117.266899  204.765442  132.819901  229.614746    0.841412      0   \n",
       "  4   548.278687  212.708466  567.980103  240.794830    0.833409      0   \n",
       "  5     0.150623  142.794037  125.098434  194.097198    0.769107      8   \n",
       "  6   156.457443  196.699646  166.713577  214.971741    0.722231      0   \n",
       "  7   234.746597  187.669861  248.991562  203.329041    0.661236      0   \n",
       "  8   560.202515  191.590240  566.914062  200.791718    0.532511      0   \n",
       "  9   545.274231  203.725891  555.688416  213.517639    0.511086      0   \n",
       "  10  391.588989  401.621002  396.798584  413.253632    0.268331      0   \n",
       "  11   83.598007  148.599380   91.236465  159.637527    0.258656      0   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1   surfboard  \n",
       "  2      person  \n",
       "  3      person  \n",
       "  4      person  \n",
       "  5        boat  \n",
       "  6      person  \n",
       "  7      person  \n",
       "  8      person  \n",
       "  9      person  \n",
       "  10     person  \n",
       "  11     person  ,\n",
       "  'caption': ['the surfboard behind the woman',\n",
       "   'White surf board with blue stripes held by woman.'],\n",
       "  'bbox_target': [288.0, 0.0, 247.01, 474.61]},\n",
       " 116: {'image_emb': tensor([[-0.1007,  0.6270,  0.0268,  ...,  0.8237, -0.2394,  0.4326],\n",
       "          [-0.2734,  0.8657, -0.1805,  ...,  1.0391, -0.1188,  0.0083],\n",
       "          [-0.1652,  0.6289,  0.0886,  ...,  0.8281, -0.3186,  0.5581]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1750, -0.1764,  0.2177,  ..., -0.2600, -0.1962, -0.1600],\n",
       "          [-0.2600, -0.0829,  0.1549,  ...,  0.4839, -0.3057,  0.3049]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0920, 0.8457, 0.0622],\n",
       "          [0.6255, 0.1053, 0.2690]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    24.653458   13.404584  467.595093  358.987427    0.735106     45   \n",
       "  1   182.989944   76.877380  278.605011  164.525131    0.710293     50   \n",
       "  2   157.647812  132.579987  189.879883  204.159973    0.688362     50   \n",
       "  3    68.563988  185.018875  132.436829  237.033798    0.670680     50   \n",
       "  4   247.943405  139.259918  286.297516  179.255859    0.668766     50   \n",
       "  5   472.049988  176.377960  499.947937  307.622345    0.616843     41   \n",
       "  6   370.427307    0.000000  447.017059   60.843300    0.562919     44   \n",
       "  7     5.717111    9.758341  494.010437  360.192627    0.467595     60   \n",
       "  8   367.517670    0.270498  446.492188   59.979630    0.394096     42   \n",
       "  9     0.270665  201.671646   30.736319  223.963593    0.316097     43   \n",
       "  10  260.573853   86.163475  342.271667  154.246445    0.280796     50   \n",
       "  \n",
       "              name  \n",
       "  0           bowl  \n",
       "  1       broccoli  \n",
       "  2       broccoli  \n",
       "  3       broccoli  \n",
       "  4       broccoli  \n",
       "  5            cup  \n",
       "  6          spoon  \n",
       "  7   dining table  \n",
       "  8           fork  \n",
       "  9          knife  \n",
       "  10      broccoli  ,\n",
       "  'caption': ['broccoli', 'Cook broccoli served with penne noodles.'],\n",
       "  'bbox_target': [185.29, 79.41, 163.24, 88.24]},\n",
       " 117: {'image_emb': tensor([[ 0.1647,  0.2542,  0.1136,  ...,  1.0547,  0.2281, -0.0793],\n",
       "          [ 0.3806, -0.3010, -0.4573,  ...,  0.9409, -0.1100,  0.0056],\n",
       "          [ 0.1141,  0.3523,  0.1881,  ...,  0.7168,  0.2228, -0.1331]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0187, -0.0706, -0.0919,  ...,  0.1205,  0.2289, -0.2430],\n",
       "          [ 0.0164,  0.4543, -0.0131,  ...,  0.2407,  0.3601, -0.2517]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.1738e-01, 1.3113e-05, 6.8262e-01],\n",
       "          [2.3096e-01, 5.9605e-08, 7.6904e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  116.321289  155.501465  376.706146  375.268555    0.949835      0   \n",
       "  1   13.744513  116.248001   32.945946  130.368393    0.742361      0   \n",
       "  2  142.486694  358.209137  350.981049  411.883209    0.430611     37   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2  surfboard  ,\n",
       "  'caption': ['a man surfing in the sea',\n",
       "   'A brown haired man standing on a surfboard catching a wave.'],\n",
       "  'bbox_target': [116.15, 156.86, 261.64, 216.22]},\n",
       " 118: {'image_emb': tensor([[ 0.2394,  0.1404,  0.2349,  ...,  0.9434, -0.0234,  0.5879],\n",
       "          [-0.1288,  0.2260, -0.1099,  ...,  1.1807,  0.1841,  0.0765],\n",
       "          [ 0.0406,  0.1840, -0.1367,  ...,  1.0459,  0.4373,  0.0367],\n",
       "          [ 0.1338,  0.1714,  0.1112,  ...,  0.5938,  0.1279,  0.0674]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0836,  0.1552, -0.5488,  ...,  0.2217, -0.2786, -0.0692],\n",
       "          [ 0.1964,  0.4084, -0.5361,  ...,  0.0430, -0.0942, -0.2561]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.0178e-01, 2.9397e-04, 0.0000e+00, 7.9785e-01],\n",
       "          [7.4609e-01, 1.9312e-05, 2.8014e-06, 2.5391e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  281.483948   73.185501  618.379395  478.025574    0.879310      0  person\n",
       "  1    0.489143  158.788971  133.513214  460.152100    0.834054     57   couch\n",
       "  2  218.775467  356.336060  247.850937  402.911499    0.767453     41     cup\n",
       "  3  246.021973  378.555908  270.247986  394.392883    0.618672     65  remote\n",
       "  4  108.019043  148.282990  631.615723  477.171387    0.617995     57   couch\n",
       "  5    0.278326  309.225769   37.158005  323.805481    0.615814     65  remote,\n",
       "  'caption': ['a shirtless teen on a couch with his father',\n",
       "   'A shirtless teen guy holding another man.'],\n",
       "  'bbox_target': [290.13, 73.69, 239.7, 406.31]},\n",
       " 119: {'image_emb': tensor([[ 1.2636e-04,  3.1006e-01, -2.7271e-01,  ...,  1.0703e+00,\n",
       "            2.6538e-01, -3.6194e-02],\n",
       "          [ 8.7402e-02,  3.7891e-01, -1.9226e-01,  ...,  8.4033e-01,\n",
       "           -4.1553e-01, -3.6523e-01],\n",
       "          [-4.7388e-01,  3.7646e-01, -5.2246e-01,  ...,  1.2832e+00,\n",
       "            6.5460e-03, -1.8967e-02],\n",
       "          ...,\n",
       "          [-5.5469e-01,  6.1035e-01, -3.9062e-01,  ...,  1.0859e+00,\n",
       "            3.8605e-02, -1.1188e-01],\n",
       "          [-5.7080e-01,  4.5825e-01, -5.5615e-01,  ...,  1.0303e+00,\n",
       "           -1.0208e-02, -2.7246e-01],\n",
       "          [-4.4019e-01,  1.8896e-01, -6.5674e-01,  ...,  4.4189e-01,\n",
       "            5.4474e-02,  2.5513e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0080,  0.2649, -0.2637,  ...,  0.0098, -0.3308, -0.3132],\n",
       "          [-0.0860,  0.3145, -0.6548,  ...,  0.0663, -0.3511, -0.1870]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.2997e-04, 9.9951e-01, 9.9123e-05, 2.3544e-05, 4.9829e-05, 1.4484e-05,\n",
       "           1.3125e-04],\n",
       "          [6.4421e-04, 5.9473e-01, 1.1301e-03, 1.1663e-03, 3.3226e-03, 2.7542e-03,\n",
       "           3.9624e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     8.633270    0.000000  114.704681  189.946121    0.925294     39   \n",
       "  1   186.293762    0.740883  314.004211  158.059937    0.918702     39   \n",
       "  2   170.766052  333.565796  377.404480  441.120361    0.902569     52   \n",
       "  3   224.656494  386.715210  424.554321  497.757812    0.900926     52   \n",
       "  4   136.262970  271.137482  344.162445  366.419403    0.879592     52   \n",
       "  5    78.929840  215.339325  261.418915  302.968353    0.832433     52   \n",
       "  6    86.225800  245.510239  274.882843  335.215149    0.656717     52   \n",
       "  7   122.148346  345.120392  161.510406  386.204132    0.651822     52   \n",
       "  8     7.851730   43.059967  426.000000  634.372803    0.555829     60   \n",
       "  9     0.466492  518.326904  113.135132  639.096313    0.310115     52   \n",
       "  10  136.569382  325.430634  307.609192  415.297699    0.308333     52   \n",
       "  \n",
       "              name  \n",
       "  0         bottle  \n",
       "  1         bottle  \n",
       "  2        hot dog  \n",
       "  3        hot dog  \n",
       "  4        hot dog  \n",
       "  5        hot dog  \n",
       "  6        hot dog  \n",
       "  7        hot dog  \n",
       "  8   dining table  \n",
       "  9        hot dog  \n",
       "  10       hot dog  ,\n",
       "  'caption': ['The red ketchup bottle',\n",
       "   'A bottle of ketchup on a table near some hot dogs.'],\n",
       "  'bbox_target': [187.06, 0.0, 126.41, 159.35]},\n",
       " 120: {'image_emb': tensor([[-0.0510, -0.2203, -0.2170,  ...,  0.4580,  0.1572, -0.1633],\n",
       "          [-0.3254,  0.1786, -0.0894,  ...,  1.1221,  0.3735,  0.1766],\n",
       "          [ 0.0431, -0.2856, -0.2279,  ...,  0.3972,  0.1519, -0.1400]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3755, -0.2603, -0.2021,  ...,  0.0806,  0.0091,  0.4587],\n",
       "          [-0.0237, -0.0801, -0.4365,  ..., -0.7148,  0.0016, -0.0653]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3767, 0.1620, 0.4614],\n",
       "          [0.3813, 0.0892, 0.5293]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin      ymin        xmax        ymax  confidence  class   name\n",
       "  0   77.127693  6.739205  498.079590  329.500122    0.951276     22  zebra\n",
       "  1  288.536499  3.835911  499.011078  205.964920    0.887878     22  zebra,\n",
       "  'caption': ['The zebra facing the camera', 'the animal in the foreground'],\n",
       "  'bbox_target': [78.47, 7.17, 421.01, 324.43]},\n",
       " 121: {'image_emb': tensor([[ 0.1920,  0.1687, -0.3105,  ...,  1.3242, -0.2942,  0.0950],\n",
       "          [-0.0743,  0.3743,  0.1334,  ...,  1.1787, -0.1315, -0.1342],\n",
       "          [-0.1788, -0.0421,  0.1045,  ...,  0.9751, -0.2280,  0.0287]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3267, -0.0180, -0.4729,  ..., -0.0424, -0.2522,  0.0119],\n",
       "          [-0.1740, -0.0771, -0.2045,  ...,  0.0245, -0.1600, -0.0549]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.7256, 0.2112, 0.0634],\n",
       "          [0.0230, 0.7847, 0.1923]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  113.154152  224.081924  194.428940  401.329620    0.917807      0    person\n",
       "  1  295.164276  215.951752  374.763733  400.136810    0.913961      0    person\n",
       "  2  140.424423   99.138237  320.410767  283.824310    0.584223     25  umbrella\n",
       "  3  123.275841  145.530563  246.936035  227.844238    0.476588     25  umbrella\n",
       "  4  142.237823   98.949081  320.728607  152.302414    0.429932     25  umbrella\n",
       "  5  123.294258  146.877853  247.232300  176.437271    0.350749     25  umbrella,\n",
       "  'caption': ['the woman sweeping wearing a long skirt',\n",
       "   'a woman cleaning the streets in bright sunlight'],\n",
       "  'bbox_target': [93.12, 224.66, 99.1, 174.55]},\n",
       " 122: {'image_emb': tensor([[ 0.1018,  0.1351, -0.4871,  ...,  1.4697,  0.0865, -0.1373],\n",
       "          [-0.0150,  0.1301, -0.3728,  ...,  0.8838,  0.1992, -0.0158],\n",
       "          [-0.0745,  0.6426, -0.2732,  ...,  1.1104, -0.1208, -0.0020],\n",
       "          [ 0.3040,  0.0181, -0.0558,  ...,  0.4219, -0.1416,  0.0134],\n",
       "          [ 0.0887,  0.2615, -0.3518,  ...,  0.9087, -0.1179,  0.0072],\n",
       "          [ 0.2974,  0.0347, -0.3003,  ...,  0.5972,  0.0640, -0.3975]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0987, -0.1132,  0.2314,  ...,  0.2375, -0.3506, -0.6680],\n",
       "          [-0.1644, -0.2413,  0.0244,  ...,  0.1412, -0.2886, -0.4143]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0034, 0.2303, 0.3679, 0.0076, 0.3857, 0.0049],\n",
       "          [0.0006, 0.0704, 0.5625, 0.0075, 0.3574, 0.0017]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  369.415558   87.487305  518.986816  422.920044    0.928472      0  person\n",
       "  1   61.473816   82.304382  166.451019  356.518311    0.871026      0  person\n",
       "  2    0.000000  154.813965   85.687622  241.883240    0.850199      2     car\n",
       "  3    0.783676   37.569946  385.334412  355.900391    0.807720     33    kite\n",
       "  4  150.561310  157.092621  319.064423  281.084503    0.797979      2     car\n",
       "  5  616.484619  134.614670  639.943237  238.868103    0.430611      7   truck\n",
       "  6  616.352661  135.360794  639.919312  238.089111    0.396598      2     car,\n",
       "  'caption': ['a white car on the street', 'the white car next to the curb'],\n",
       "  'bbox_target': [139.84, 155.66, 180.51, 126.13]},\n",
       " 123: {'image_emb': tensor([[-2.6807e-01,  3.2104e-01, -6.9275e-02,  ...,  9.8975e-01,\n",
       "           -3.1860e-01,  6.3110e-02],\n",
       "          [-6.5430e-02,  2.9272e-01, -1.4880e-01,  ...,  1.1680e+00,\n",
       "           -2.2998e-01,  1.7334e-01],\n",
       "          [ 1.1091e-03,  3.4106e-01, -2.4182e-01,  ...,  1.1875e+00,\n",
       "            3.0981e-01, -2.0654e-01],\n",
       "          ...,\n",
       "          [-1.0034e-01,  6.3428e-01, -3.4485e-02,  ...,  9.3457e-01,\n",
       "           -2.0801e-01, -1.6370e-01],\n",
       "          [-1.0358e-01,  2.6733e-01, -3.5913e-01,  ...,  9.9902e-01,\n",
       "           -1.4294e-01,  1.2842e-01],\n",
       "          [ 6.1340e-02,  5.1074e-01,  1.7944e-02,  ...,  9.7461e-01,\n",
       "           -2.4329e-01,  2.0325e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2664, -0.0654, -0.0354,  ...,  0.0800, -0.0779, -0.0445],\n",
       "          [-0.2228, -0.1056, -0.0192,  ..., -0.1691, -0.1555,  0.0231]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3684, 0.2098, 0.0007, 0.1740, 0.1768, 0.0094, 0.0611],\n",
       "          [0.3542, 0.0803, 0.0005, 0.1432, 0.3542, 0.0140, 0.0535]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  247.608124  350.582214  398.735809  621.539612    0.927691     56   \n",
       "  1  393.597565  302.630157  486.000000  397.415253    0.852212     56   \n",
       "  2   82.188782  332.622406  127.212311  396.365448    0.841558     75   \n",
       "  3    0.466019  458.298218  112.573929  615.828857    0.803596     56   \n",
       "  4   40.879974  352.991699  316.309845  620.701599    0.793347     60   \n",
       "  5  198.265564  298.628235  398.284241  382.408630    0.772209     57   \n",
       "  6  406.257690  371.116516  458.785156  398.312073    0.635557     73   \n",
       "  7   66.885773  250.243408  183.055420  381.612183    0.286198     58   \n",
       "  \n",
       "             name  \n",
       "  0         chair  \n",
       "  1         chair  \n",
       "  2          vase  \n",
       "  3         chair  \n",
       "  4  dining table  \n",
       "  5         couch  \n",
       "  6          book  \n",
       "  7  potted plant  ,\n",
       "  'caption': ['Beige chair on the right side of the table',\n",
       "   'Fully visible chair at table.'],\n",
       "  'bbox_target': [247.43, 340.39, 150.67, 281.99]},\n",
       " 124: {'image_emb': tensor([[ 8.6914e-02,  2.9248e-01,  2.9694e-02,  ...,  7.6709e-01,\n",
       "            1.0962e-01,  2.5781e-01],\n",
       "          [ 2.7633e-04,  3.9258e-01, -1.5674e-01,  ...,  1.2393e+00,\n",
       "            6.3354e-02, -2.5928e-01],\n",
       "          [ 3.7079e-02,  3.9624e-01, -3.6144e-03,  ...,  1.3057e+00,\n",
       "            3.3179e-01, -6.0486e-02],\n",
       "          [-2.1286e-02,  2.8091e-02, -2.3608e-01,  ...,  1.3291e+00,\n",
       "           -5.9937e-02, -1.7004e-01],\n",
       "          [ 2.3895e-02, -7.6904e-02, -3.8257e-01,  ...,  1.1279e+00,\n",
       "           -1.3806e-01, -4.8065e-02],\n",
       "          [ 3.1323e-01,  3.4961e-01,  1.5405e-01,  ...,  5.1611e-01,\n",
       "            2.0850e-01,  1.5930e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0649,  0.1572, -0.1345,  ...,  0.2450,  0.3572,  0.0320],\n",
       "          [ 0.3237, -0.0360, -0.2368,  ..., -0.1666,  0.2471,  0.4429]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0490e-02, 7.9274e-06, 0.0000e+00, 3.5763e-07, 4.7684e-07, 9.8975e-01],\n",
       "          [6.2695e-01, 2.6855e-03, 1.2493e-03, 3.1891e-03, 2.8591e-03, 3.6304e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class  \\\n",
       "  0  336.053162  41.722672  544.728333  418.310547    0.948359      0   \n",
       "  1   15.313538  70.103790  108.427689  271.015778    0.908889     10   \n",
       "  2  188.068726   0.000000  327.236755   59.107803    0.899875      2   \n",
       "  3  399.645813   0.000000  429.646973   54.227615    0.830569      0   \n",
       "  4  422.792725   0.414627  447.359619   49.893387    0.770448      0   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1  fire hydrant  \n",
       "  2           car  \n",
       "  3        person  \n",
       "  4        person  ,\n",
       "  'caption': ['A little girl standing by what appears to be a puddle on a sidewalk. She is wearing a pink shirt, with blue jeans and sandals.',\n",
       "   'The girl in the purple top.'],\n",
       "  'bbox_target': [334.34, 41.17, 210.16, 380.19]},\n",
       " 125: {'image_emb': tensor([[ 0.0057,  0.3240, -0.0991,  ...,  0.8223, -0.1364,  0.0129],\n",
       "          [ 0.1265,  0.0616, -0.4058,  ...,  1.4834,  0.0272, -0.1677],\n",
       "          [ 0.2489,  0.2903, -0.1472,  ...,  0.8130, -0.0931, -0.2482],\n",
       "          ...,\n",
       "          [ 0.1002,  0.2422, -0.0891,  ...,  1.1816, -0.1321, -0.0193],\n",
       "          [ 0.1749,  0.5020, -0.2939,  ...,  1.0947, -0.2500, -0.3782],\n",
       "          [-0.0231,  0.3171, -0.1479,  ...,  0.6953,  0.0387, -0.1860]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0266, -0.4182, -0.0472,  ...,  0.1328, -0.3237,  0.0555],\n",
       "          [ 0.0703, -0.0144, -0.3267,  ...,  0.1643, -0.0217, -0.4229]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.3368e-03, 1.0902e-04, 1.2524e-01, 3.1006e-01, 2.6417e-03, 3.1494e-01,\n",
       "           2.4146e-01],\n",
       "          [4.5190e-01, 5.3662e-01, 3.3319e-05, 9.0480e-05, 7.1192e-04, 3.2406e-03,\n",
       "           7.5340e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  197.197937  141.326187  415.828491  485.221558    0.938338      0   \n",
       "  1   15.377296   81.700623  218.631378  545.317871    0.921360      0   \n",
       "  2  231.514984  511.307251  343.460419  583.612915    0.838539     55   \n",
       "  3  166.614319  481.079559  264.099670  544.935120    0.824226     55   \n",
       "  4    0.000000  548.911743  115.731628  639.881470    0.796138     56   \n",
       "  5   86.881180  465.166962  477.979492  638.283508    0.747296     60   \n",
       "  6  441.375854  446.968079  479.688721  521.501831    0.666803     56   \n",
       "  7  297.058716  434.523254  319.701843  469.851135    0.306216     43   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2          cake  \n",
       "  3          cake  \n",
       "  4         chair  \n",
       "  5  dining table  \n",
       "  6         chair  \n",
       "  7         knife  ,\n",
       "  'caption': ['boy celebrates his birthday by cutting cake',\n",
       "   'Guy with a white shirt'],\n",
       "  'bbox_target': [198.26, 139.89, 217.39, 346.27]},\n",
       " 126: {'image_emb': tensor([[ 0.0142,  0.6509, -0.0152,  ...,  0.9849,  0.0876,  0.4575],\n",
       "          [-0.0486,  0.8477, -0.0176,  ...,  0.7573,  0.0085,  0.2440],\n",
       "          [-0.0565,  0.2883, -0.4211,  ...,  1.3809,  0.0222, -0.0331],\n",
       "          [ 0.3005,  0.3154, -0.2224,  ...,  0.9971,  0.0031,  0.3328],\n",
       "          [ 0.0600,  0.4934, -0.1321,  ...,  0.8765, -0.2057,  0.3621]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2417,  0.0676,  0.1830,  ...,  0.4971, -0.0663, -0.0542],\n",
       "          [-0.0881,  0.0836,  0.2539,  ...,  0.1161, -0.3730, -0.0085]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9869e-03, 1.2703e-03, 9.1457e-04, 9.8779e-01, 1.9181e-04],\n",
       "          [9.1309e-02, 1.6272e-01, 3.9886e-02, 6.8506e-01, 2.1011e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  254.182587   37.710648  427.708374  332.187500    0.937436      0   \n",
       "  1   97.253014  126.415604  542.399231  362.328400    0.928589      3   \n",
       "  2    0.152793   67.625977   36.722279  277.279480    0.860358      0   \n",
       "  3    3.937654  125.136497  153.561584  258.331421    0.855224      3   \n",
       "  4  432.771027  174.448410  530.468201  232.720337    0.368042     28   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1  motorcycle  \n",
       "  2      person  \n",
       "  3  motorcycle  \n",
       "  4    suitcase  ,\n",
       "  'caption': ['A black motorcycle labeled 77',\n",
       "   'The motorcycle that is only half visible.'],\n",
       "  'bbox_target': [6.17, 140.33, 146.17, 117.06]},\n",
       " 127: {'image_emb': tensor([[ 0.1820,  0.1731,  0.1038,  ...,  1.0557,  0.0713,  0.2191],\n",
       "          [-0.3091,  0.4722,  0.1476,  ...,  0.9204,  0.0337,  0.0032],\n",
       "          [-0.2188,  0.6890, -0.0161,  ...,  1.1084,  0.0701,  0.1126],\n",
       "          ...,\n",
       "          [ 0.0657,  0.2844, -0.2644,  ...,  0.5815, -0.1693, -0.0330],\n",
       "          [-0.0594,  0.3118, -0.0852,  ...,  1.4609,  0.0260, -0.1917],\n",
       "          [-0.1082,  0.5205, -0.3059,  ...,  1.0732, -0.0192,  0.4392]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.4312, -0.0967, -0.5220,  ..., -0.6372,  0.1572,  0.1884],\n",
       "          [-0.0829,  0.4429, -0.0686,  ..., -0.0433, -0.1075,  0.2568]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.4424e-01, 2.0397e-04, 6.0892e-04, 6.8237e-02, 7.4951e-02, 8.2779e-03,\n",
       "           1.6041e-03, 1.9054e-03],\n",
       "          [1.2672e-04, 3.5675e-02, 2.3633e-01, 4.6997e-01, 1.8408e-01, 6.9160e-03,\n",
       "           2.5988e-04, 6.6650e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     0.000000    0.322052   93.172363  160.769073    0.940306      0   \n",
       "  1   243.500610   27.725616  478.856995  266.771881    0.932968     16   \n",
       "  2     0.000000   99.491608  600.167114  421.465332    0.918950      3   \n",
       "  3   379.752625    0.000000  494.155579  157.017914    0.899773      0   \n",
       "  4   460.311096    0.562271  630.910950  200.601624    0.811320      0   \n",
       "  5   194.730835    0.000000  298.363556  157.042267    0.793329      0   \n",
       "  6   117.342758   41.507523  212.993668  155.415710    0.710265     24   \n",
       "  7   469.298126  174.289703  530.502991  239.537384    0.424401     26   \n",
       "  8   233.139008   40.028076  250.609985   60.325836    0.347611     67   \n",
       "  9    21.836309   33.793274   42.439133   46.800201    0.320142     67   \n",
       "  10  607.845947   12.619705  639.799072  422.661743    0.319160      0   \n",
       "  \n",
       "            name  \n",
       "  0       person  \n",
       "  1          dog  \n",
       "  2   motorcycle  \n",
       "  3       person  \n",
       "  4       person  \n",
       "  5       person  \n",
       "  6     backpack  \n",
       "  7      handbag  \n",
       "  8   cell phone  \n",
       "  9   cell phone  \n",
       "  10      person  ,\n",
       "  'caption': ['Man standing in background of photo.',\n",
       "   'A person wearing gray pants, standing behind a motorcycle.'],\n",
       "  'bbox_target': [378.87, 2.17, 112.79, 153.28]},\n",
       " 128: {'image_emb': tensor([[ 0.3965,  0.0982,  0.0200,  ...,  0.6396, -0.0660, -0.1793],\n",
       "          [ 0.4121,  0.1641, -0.5146,  ...,  1.1357, -0.0331, -0.1310],\n",
       "          [ 0.3394,  0.3157, -0.2798,  ...,  1.1162,  0.0028,  0.1281],\n",
       "          ...,\n",
       "          [ 0.1124,  0.0404, -0.5728,  ...,  1.2900,  0.0754, -0.3826],\n",
       "          [ 0.1844,  0.4290, -0.3010,  ...,  1.1172, -0.1024,  0.0490],\n",
       "          [-0.1605, -0.1202, -0.1451,  ...,  0.4902, -0.0088, -0.4880]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0718, -0.1473, -0.4727,  ..., -0.0454, -0.1636, -0.0492],\n",
       "          [ 0.4009,  0.1221, -0.5415,  ...,  0.3230, -0.1149,  0.0228]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.8442e-04, 6.8130e-03, 4.5395e-03, 3.8433e-04, 4.8876e-05, 4.0913e-04,\n",
       "           5.4980e-01, 1.3838e-03, 1.7223e-03, 4.3481e-01, 4.1127e-05],\n",
       "          [5.3585e-05, 6.0737e-05, 4.5300e-06, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           4.8438e-01, 5.9605e-08, 0.0000e+00, 5.1562e-01, 0.0000e+00]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0   319.875000    0.000000  640.000000  422.997803    0.953326      0   person\n",
       "  1    93.325424   97.975235  211.358261  367.722412    0.898681      0   person\n",
       "  2   193.769638   78.251816  315.578247  402.092773    0.885686      0   person\n",
       "  3   223.838943  341.468170  259.952454  425.401306    0.880482     39   bottle\n",
       "  4   326.729736  352.069946  612.159180  424.910217    0.872347     53    pizza\n",
       "  5   181.970139  341.283600  216.484787  425.623230    0.848180     39   bottle\n",
       "  6     0.000000   63.385361  118.071068  383.581604    0.829774      0   person\n",
       "  7   202.627441  112.610870  255.940704  186.297089    0.804715      0   person\n",
       "  8    52.222321   65.568817   98.108841  214.142212    0.802612      0   person\n",
       "  9     0.000000  174.083740  123.009216  376.467712    0.754084     26  handbag\n",
       "  10   87.008789  148.300674  123.116974  196.835419    0.561728      0   person\n",
       "  11  146.564316  192.577820  215.504410  361.923584    0.475292     26  handbag,\n",
       "  'caption': [\"the woman in blue's purse in the right hand picture\",\n",
       "   'A brown and black woven purse, being held by an older woman in a blue shirt.'],\n",
       "  'bbox_target': [0.0, 177.22, 121.39, 211.2]},\n",
       " 129: {'image_emb': tensor([[-0.3911,  0.3247, -0.0547,  ...,  1.0576,  0.2167,  0.3870],\n",
       "          [ 0.0798,  0.2225, -0.1231,  ...,  1.2070, -0.0104,  0.0186],\n",
       "          [ 0.1843, -0.0355,  0.1064,  ...,  0.5996,  0.0735, -0.5405],\n",
       "          [ 0.2043, -0.1478, -0.2010,  ...,  0.6729, -0.0534, -0.2025],\n",
       "          [-0.1217,  0.4929,  0.1359,  ...,  0.2029, -0.0133,  0.0988]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0903,  0.1747, -0.0032,  ..., -0.0185,  0.0650,  0.0480],\n",
       "          [ 0.2937,  0.4197, -0.1035,  ...,  0.1196, -0.2051, -0.1664]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.5346e-03, 9.3994e-01, 2.7954e-02, 2.3529e-02, 1.8721e-03],\n",
       "          [6.7520e-04, 9.9658e-01, 2.4319e-03, 4.0317e-04, 6.3777e-05]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     1.836884   66.279541  226.299683  634.013550    0.922903      0   \n",
       "  1     0.194275  101.862495   96.121521  334.668060    0.846476      0   \n",
       "  2   193.497559  406.876465  210.016113  461.945435    0.842743     39   \n",
       "  3   119.781631  504.936188  135.531296  560.638672    0.827586     39   \n",
       "  4    99.999207  339.085876  207.099426  460.242371    0.650947     57   \n",
       "  5   304.529633  424.486633  391.633698  501.316467    0.632283     24   \n",
       "  6   215.685059  339.112061  390.600586  451.565796    0.622268     56   \n",
       "  7   140.035889  441.049164  308.187988  620.393982    0.612116     60   \n",
       "  8   308.126953  488.467468  328.804321  550.552795    0.585914     39   \n",
       "  9   216.075195  338.928223  390.355347  451.159607    0.527452     57   \n",
       "  10  138.898560  480.284698  190.817261  492.002716    0.525170     65   \n",
       "  11  165.264252  419.436676  193.726044  479.835907    0.477369     41   \n",
       "  12  169.023697   82.549820  184.719635  100.014343    0.430101     65   \n",
       "  13  266.640320  394.964233  302.305481  409.142761    0.387328     73   \n",
       "  14  255.066833  375.725372  294.090454  397.583710    0.347799     73   \n",
       "  15  289.508026  382.868073  321.201996  394.226898    0.346674     73   \n",
       "  16  416.027313  616.547119  478.888245  639.650269    0.329866     73   \n",
       "  17  228.379547  440.034821  247.762238  477.567902    0.263233     41   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         bottle  \n",
       "  3         bottle  \n",
       "  4          couch  \n",
       "  5       backpack  \n",
       "  6          chair  \n",
       "  7   dining table  \n",
       "  8         bottle  \n",
       "  9          couch  \n",
       "  10        remote  \n",
       "  11           cup  \n",
       "  12        remote  \n",
       "  13          book  \n",
       "  14          book  \n",
       "  15          book  \n",
       "  16          book  \n",
       "  17           cup  ,\n",
       "  'caption': ['man wearing glasses',\n",
       "   'A person with brown shirt and specs on his eyes'],\n",
       "  'bbox_target': [1.44, 97.8, 89.17, 232.99]},\n",
       " 130: {'image_emb': tensor([[-0.3257, -0.2886, -0.1241,  ...,  0.6724,  0.1588,  0.3921],\n",
       "          [-0.2144, -0.1477, -0.1285,  ...,  0.8008, -0.1022,  0.1870],\n",
       "          [-0.1416, -0.3865, -0.0521,  ...,  0.7827,  0.0072,  0.0312],\n",
       "          [-0.2395, -0.0647, -0.0484,  ...,  0.6426, -0.0414,  0.2051]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0455, -0.1037, -0.1232,  ...,  0.1484, -0.2095, -0.0620],\n",
       "          [ 0.1068,  0.2284, -0.2876,  ...,  0.0521, -0.0675,  0.0169]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2024, 0.3665, 0.1504, 0.2810],\n",
       "          [0.4939, 0.1578, 0.2192, 0.1289]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0    0.247360   96.246735  186.110092  476.111084    0.944513     20  elephant\n",
       "  1   31.974121   31.407532  575.341553  476.132263    0.934559     20  elephant\n",
       "  2  253.993515  279.992584  545.087280  473.255493    0.934229     20  elephant,\n",
       "  'caption': [\"Three elephatns, the middle one is the biggest and is raising its' trunk\",\n",
       "   'The larger of the three elephants.'],\n",
       "  'bbox_target': [38.83, 31.28, 535.01, 443.33]},\n",
       " 131: {'image_emb': tensor([[-0.2261,  0.1729,  0.0233,  ...,  1.0469, -0.1309,  0.1024],\n",
       "          [-0.0507, -0.0704, -0.1315,  ...,  0.4885, -0.0575,  0.4148],\n",
       "          [-0.2498,  0.3608,  0.0571,  ...,  0.9448, -0.0545,  0.1923],\n",
       "          ...,\n",
       "          [-0.0142,  0.2742, -0.3882,  ...,  1.2490,  0.3904,  0.0460],\n",
       "          [ 0.2133,  0.4463, -0.0162,  ...,  0.8472, -0.4121,  0.2483],\n",
       "          [ 0.2888,  0.1288, -0.3447,  ...,  0.4971, -0.1451, -0.3367]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0134, -0.0263, -0.1727,  ...,  0.1169, -0.5239, -0.6250],\n",
       "          [ 0.1241, -0.2391, -0.3303,  ..., -0.0820, -0.1032, -0.3157]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.9731e-01, 1.3252e-02, 3.1311e-02, 4.0030e-04, 4.7264e-03, 5.6744e-05,\n",
       "           5.2512e-05, 2.0862e-05, 4.5288e-01],\n",
       "          [3.1152e-01, 2.1741e-01, 3.5840e-01, 9.4557e-04, 1.9703e-03, 1.9515e-04,\n",
       "           4.1962e-04, 3.3016e-03, 1.0596e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     2.062805  236.316589  185.109863  410.216919    0.916917     77   \n",
       "  1   177.204681  213.678955  357.031342  387.628296    0.880806     77   \n",
       "  2   379.643097  271.481323  479.871460  495.886902    0.861670     77   \n",
       "  3   163.122208  249.136780  212.164215  324.500427    0.827436     75   \n",
       "  4   323.258972  373.222137  399.591003  510.514435    0.825400     39   \n",
       "  5    78.819138  358.354370  114.461380  427.235718    0.806563     39   \n",
       "  6   355.861664  257.244934  416.045441  352.709900    0.798825     75   \n",
       "  7   160.741684  405.283447  279.270172  484.292236    0.731217     48   \n",
       "  8     2.836029  273.988861  478.312012  633.617310    0.594600     60   \n",
       "  9    43.484406  403.554443  106.066498  465.129761    0.449352     45   \n",
       "  10    0.769958  402.563782   52.020996  444.770508    0.312237     44   \n",
       "  \n",
       "              name  \n",
       "  0     teddy bear  \n",
       "  1     teddy bear  \n",
       "  2     teddy bear  \n",
       "  3           vase  \n",
       "  4         bottle  \n",
       "  5         bottle  \n",
       "  6           vase  \n",
       "  7       sandwich  \n",
       "  8   dining table  \n",
       "  9           bowl  \n",
       "  10         spoon  ,\n",
       "  'caption': ['a brown teddy bear with a white jar sitting in front of it',\n",
       "   'Teddy bear on the left'],\n",
       "  'bbox_target': [0.0, 231.55, 188.4, 172.58]},\n",
       " 132: {'image_emb': tensor([[ 0.2117, -0.0604, -0.2959,  ...,  0.1259,  0.0889, -0.1620],\n",
       "          [-0.0681,  0.2954, -0.3362,  ...,  0.0287,  0.3372,  0.0217],\n",
       "          [ 0.1373, -0.0113, -0.4236,  ...,  0.1415,  0.1896, -0.2874]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0061, -0.2242, -0.2203,  ...,  0.3875,  0.0968,  0.2983],\n",
       "          [ 0.1775,  0.0315,  0.0040,  ...,  0.2335, -0.0290,  0.2356]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0221, 0.9233, 0.0546],\n",
       "          [0.1888, 0.3213, 0.4900]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class   name\n",
       "  0  248.113861  68.772369  639.113525  386.721344    0.947104      6  train\n",
       "  1    0.893341  89.823212  281.440765  336.697479    0.916950      6  train,\n",
       "  'caption': ['There is a train on the tracks beside a blue colored train',\n",
       "   'Torbay Express train.'],\n",
       "  'bbox_target': [252.36, 69.33, 387.64, 325.29]},\n",
       " 133: {'image_emb': tensor([[-0.2129,  0.1103, -0.1578,  ...,  1.0811,  0.2502, -0.1031],\n",
       "          [-0.1197,  0.4641, -0.2708,  ...,  1.2344, -0.3840, -0.1704],\n",
       "          [ 0.1104,  0.6104, -0.1316,  ...,  1.0791, -0.4285,  0.1451],\n",
       "          [ 0.0327,  0.3108,  0.2279,  ...,  0.7798, -0.2881,  0.1731],\n",
       "          [-0.0098,  0.1746,  0.2327,  ...,  0.8711,  0.0345,  0.0856],\n",
       "          [ 0.0434,  0.0627,  0.2725,  ...,  0.9668, -0.2737,  0.2917]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1549,  0.1603,  0.2097,  ...,  0.4722, -0.4038, -0.2334],\n",
       "          [ 0.0853, -0.1912, -0.2637,  ...,  0.4360, -0.3596, -0.4395]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.9276e-02, 3.1261e-03, 4.1723e-07, 5.1239e-02, 8.8037e-01, 2.6169e-02],\n",
       "          [2.5537e-01, 8.5571e-02, 4.1723e-07, 3.3508e-02, 6.0303e-01, 2.2324e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0    3.583008  183.895538  349.593079  349.421722    0.944403      0   person\n",
       "  1  281.825562  211.862427  500.050903  314.712219    0.883687      0   person\n",
       "  2  414.004395  306.136566  509.200623  421.878265    0.829226     26  handbag\n",
       "  3  244.214859  167.516602  640.000000  470.748901    0.826487     59      bed\n",
       "  4    1.210022  173.716995  535.800049  477.708435    0.820447     59      bed\n",
       "  5  168.125839  358.566437  309.707764  423.231293    0.542780     26  handbag\n",
       "  6  453.193390  216.713226  598.123169  292.343292    0.527466      0   person,\n",
       "  'caption': ['Hotel bed with a large blonde woman sitting on it.',\n",
       "   'bed being laid on by woman in white shirt'],\n",
       "  'bbox_target': [1.08, 188.26, 536.35, 286.05]},\n",
       " 134: {'image_emb': tensor([[ 0.0521,  0.8008, -0.2883,  ...,  0.8403, -0.2783,  0.0487],\n",
       "          [ 0.3721, -0.4353, -0.5488,  ...,  0.6968, -0.1416,  0.1481],\n",
       "          [ 0.2445,  0.1853, -0.1541,  ...,  0.6108, -0.2206,  0.0152]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1799,  0.2483, -0.4036,  ..., -0.0261,  0.1749, -0.3110],\n",
       "          [ 0.3489,  0.3765,  0.0926,  ..., -0.0467,  0.0526, -0.4607]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1692, 0.7705, 0.0603],\n",
       "          [0.5151, 0.1365, 0.3484]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   95.646568   64.363525  246.352722  488.904663    0.937848      0   \n",
       "  1  149.698303  150.353577  163.273804  170.692780    0.733259     67   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1  cell phone  ,\n",
       "  'caption': ['blue and white umbrella behind woman',\n",
       "   'a white and purple umbrella'],\n",
       "  'bbox_target': [138.12, 392.76, 185.38, 63.92]},\n",
       " 135: {'image_emb': tensor([[ 0.0703,  0.2480, -0.3853,  ...,  0.6870,  0.0410, -0.0345],\n",
       "          [-0.1230,  0.2554, -0.4160,  ...,  0.6504,  0.2642,  0.0488],\n",
       "          [ 0.3306,  0.1879, -0.2678,  ...,  0.5938,  0.1324,  0.0147],\n",
       "          [ 0.1298,  0.0881, -0.3306,  ...,  1.0811,  0.1234, -0.1177],\n",
       "          [-0.4883,  0.1947, -0.1931,  ...,  0.1432,  0.8369, -0.3149]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0268,  0.0912, -0.3181,  ...,  0.0420,  0.1478, -0.2964],\n",
       "          [-0.2737,  0.1738, -0.1721,  ...,  0.0557, -0.0936,  0.1447]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.6025e-01, 1.3709e-06, 6.4492e-05, 2.3961e-04, 2.3926e-01],\n",
       "          [7.2168e-01, 1.0729e-06, 1.9550e-05, 1.2636e-05, 2.7832e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  295.598999  280.132080  373.959290  524.501282    0.913032      0  person\n",
       "  1  167.277496   43.382313  205.682953   85.674225    0.887243     74   clock\n",
       "  2  252.415344  283.253052  316.765076  518.433777    0.886408      0  person\n",
       "  3  223.083954   47.876331  242.276642   91.740173    0.848305     74   clock,\n",
       "  'caption': ['A very happy looking young man standing in his blue graduation cap and gown smiling and holding onto his diploma',\n",
       "   'A man in a blue graduation robe.'],\n",
       "  'bbox_target': [299.41, 280.14, 74.51, 245.97]},\n",
       " 136: {'image_emb': tensor([[-0.2386,  0.1035, -0.2177,  ...,  1.5781,  0.0448,  0.0094],\n",
       "          [-0.0762,  0.3264, -0.1121,  ...,  0.9194,  0.0622,  0.0125],\n",
       "          [-0.0538,  0.6079,  0.0273,  ...,  0.1953, -0.0995,  0.7627],\n",
       "          ...,\n",
       "          [ 0.1517,  0.3879,  0.0551,  ...,  0.7339,  0.0507,  0.3213],\n",
       "          [-0.2197,  0.2322,  0.1990,  ...,  0.8589, -0.0134, -0.1141],\n",
       "          [-0.1575,  0.1201, -0.1732,  ...,  0.8442, -0.1598, -0.0615]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1783,  0.0466, -0.3577,  ...,  0.3022, -0.3684, -0.1910],\n",
       "          [ 0.0056, -0.1602, -0.3977,  ...,  0.5874, -0.2300, -0.0311]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.5477e-04, 8.1658e-05, 1.6499e-04, 3.5065e-02, 4.2796e-04, 8.3780e-04,\n",
       "           9.6289e-01],\n",
       "          [5.1355e-04, 1.8787e-03, 3.5254e-01, 1.7822e-02, 1.2522e-03, 7.2021e-03,\n",
       "           6.1865e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  396.046295   41.348503  499.550964  369.816742    0.924480      0  person\n",
       "  1  223.889664  128.870010  301.770935  256.560028    0.923803      0  person\n",
       "  2  148.472137  120.189873  226.209930  209.815262    0.898571      0  person\n",
       "  3   68.370415  227.021698  412.208832  371.813965    0.824136     57   couch\n",
       "  4    1.333231  123.732689   74.219688  241.829041    0.767789      0  person\n",
       "  5  341.368958  155.777069  401.876587  219.723511    0.737881     56   chair\n",
       "  6    1.032460  141.493317  236.070541  250.292542    0.572581     57   couch\n",
       "  7  299.327423  106.924889  307.076416  126.431534    0.374572     39  bottle\n",
       "  8  196.885437  136.006531  221.742767  167.988281    0.353351      0  person,\n",
       "  'caption': ['A couch with two musicians sitting on it.',\n",
       "   'Black couch with man playing trombone sitting on it.'],\n",
       "  'bbox_target': [7.0, 141.35, 208.16, 86.36]},\n",
       " 137: {'image_emb': tensor([[-0.0213,  0.3757, -0.2069,  ...,  1.3496, -0.2500, -0.2157],\n",
       "          [-0.1591, -0.0787, -0.0504,  ...,  0.8862,  0.1148, -0.2783],\n",
       "          [-0.0349,  0.3413, -0.1780,  ...,  1.2930, -0.4133, -0.0908],\n",
       "          ...,\n",
       "          [ 0.3792,  0.2690, -0.3416,  ...,  1.0195, -0.3928, -0.2610],\n",
       "          [-0.3545,  0.5503, -0.4788,  ...,  1.1533, -0.0747,  0.0025],\n",
       "          [ 0.1898,  0.3213, -0.4175,  ...,  1.3301, -0.4988, -0.0316]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1636,  0.1647, -0.4141,  ...,  0.1415, -0.1747, -0.2299],\n",
       "          [-0.1810, -0.2051, -0.2384,  ...,  0.2505, -0.1958, -0.0197]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.2720e-04, 0.0000e+00, 2.2054e-06, 1.7285e-06, 9.9902e-01, 1.8477e-06,\n",
       "           8.5592e-04],\n",
       "          [5.7716e-03, 5.9605e-08, 7.6115e-05, 3.3593e-04, 9.8584e-01, 1.4782e-05,\n",
       "           8.0109e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  231.153992  173.671738  425.122253  464.930481    0.934824     28  suitcase\n",
       "  1  511.278473    0.580246  606.357239  154.574539    0.931602      0    person\n",
       "  2   31.235100  248.275909  235.535339  477.406616    0.887968     24  backpack\n",
       "  3  389.676147  230.902161  637.501221  477.353027    0.854924     24  backpack\n",
       "  4   76.020576    0.834702  243.987549  359.934814    0.808569     28  suitcase\n",
       "  5  239.918869    4.798515  435.312805  240.729950    0.794912     24  backpack\n",
       "  6  389.018188  171.906769  487.581970  377.440765    0.473478     28  suitcase,\n",
       "  'caption': ['A carry on suit case with an american red cross bag on top.',\n",
       "   'The roller suitcase that has the red cross bag on top of it.'],\n",
       "  'bbox_target': [89.73, 4.86, 150.27, 344.87]},\n",
       " 138: {'image_emb': tensor([[ 0.1979,  0.3406, -0.2639,  ...,  0.7466,  0.1718, -0.0194],\n",
       "          [-0.1270, -0.1119, -0.3083,  ...,  0.7646,  0.5933, -0.3242],\n",
       "          [-0.0459,  0.3438, -0.2386,  ...,  0.8140,  0.4485, -0.0487],\n",
       "          ...,\n",
       "          [-0.1465,  0.0148,  0.0820,  ...,  0.8145,  1.0098, -0.2467],\n",
       "          [ 0.0641,  0.4387, -0.2898,  ...,  1.1621, -0.4824, -0.2164],\n",
       "          [ 0.1884,  0.0970, -0.1915,  ...,  0.5356, -0.0266, -0.0737]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0140,  0.4055, -0.3752,  ...,  0.1537,  0.3767,  0.0106],\n",
       "          [-0.5005,  0.2791,  0.1827,  ..., -0.1057,  0.1433, -0.0215]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.5728e-01, 1.2311e-01, 1.2500e-01, 1.1265e-05, 5.9395e-03, 3.6377e-02,\n",
       "           2.2290e-01, 2.1398e-05, 2.9236e-02],\n",
       "          [2.0435e-01, 2.0554e-02, 1.5038e-02, 1.0272e-01, 1.8311e-01, 2.2568e-02,\n",
       "           2.1411e-01, 2.2437e-01, 1.3062e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   121.456085   70.414825  311.819611  437.000000    0.954412     40   \n",
       "  1   250.209198    0.000000  363.791046  393.284180    0.921596     39   \n",
       "  2   326.475067   61.140717  509.625519  426.470947    0.920782     40   \n",
       "  3    21.493103  273.980133  210.256653  400.718414    0.879680     45   \n",
       "  4   574.512573  251.384155  639.948975  434.411560    0.862099     40   \n",
       "  5   397.710571   43.154327  639.407715  272.619965    0.835320     56   \n",
       "  6   328.809235    0.000000  427.233978  348.682251    0.801416     39   \n",
       "  7     0.004837  353.727692  106.273811  434.686768    0.796607     45   \n",
       "  8   318.992126  349.571228  532.478943  434.560242    0.592160     40   \n",
       "  9   319.449799  349.060638  533.448425  435.385498    0.487706     41   \n",
       "  10  431.662170  249.310455  632.778625  384.549591    0.416436     45   \n",
       "  11   99.568398  372.729095  325.564575  435.061096    0.275153     60   \n",
       "  12    0.015762   40.149826  153.652191  326.132874    0.251759     58   \n",
       "  \n",
       "              name  \n",
       "  0     wine glass  \n",
       "  1         bottle  \n",
       "  2     wine glass  \n",
       "  3           bowl  \n",
       "  4     wine glass  \n",
       "  5          chair  \n",
       "  6         bottle  \n",
       "  7           bowl  \n",
       "  8     wine glass  \n",
       "  9            cup  \n",
       "  10          bowl  \n",
       "  11  dining table  \n",
       "  12  potted plant  ,\n",
       "  'caption': ['The top of a wine glass distorting the stem of another wine glass.',\n",
       "   'The cup that is mostly unvisible but that you can see the entire rim.'],\n",
       "  'bbox_target': [313.28, 346.98, 225.88, 90.02]},\n",
       " 139: {'image_emb': tensor([[ 0.1279,  0.4072, -0.1508,  ...,  0.7358, -0.1222,  0.0290],\n",
       "          [ 0.3108,  0.4758, -0.5020,  ...,  1.1211,  0.0161, -0.0092],\n",
       "          [ 0.2399,  0.1399, -0.3313,  ...,  1.2734, -0.0719, -0.2362],\n",
       "          [-0.3511,  0.7114, -0.1995,  ...,  0.6602,  0.2759,  0.1879],\n",
       "          [ 0.0504,  0.2893, -0.2578,  ...,  0.7417,  0.2343,  0.0844],\n",
       "          [ 0.0634,  0.5142, -0.2751,  ...,  0.4219,  0.0550,  0.1986]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0349, -0.1296, -0.1132,  ..., -0.0972,  0.0840, -0.1292],\n",
       "          [-0.1183, -0.0184, -0.0291,  ...,  0.1732,  0.3662, -0.3755]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.3270e-04, 2.2209e-04, 9.8526e-05, 9.1895e-01, 7.9041e-02, 1.4706e-03],\n",
       "          [5.0545e-03, 8.9169e-04, 7.6711e-05, 8.4961e-01, 4.4342e-02, 9.9915e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0    0.706482   38.132919  404.297882  373.704895    0.934925      5       bus\n",
       "  1  463.700562  176.690369  547.120667  410.629700    0.910742      0    person\n",
       "  2   79.691765  146.397995  155.040558  403.654175    0.909606      0    person\n",
       "  3  519.297058  301.878448  639.919739  477.015686    0.908693      1   bicycle\n",
       "  4  333.139130  216.888184  414.760406  338.982483    0.786919      1   bicycle\n",
       "  5  405.345764  250.815735  440.237244  298.323059    0.674126     24  backpack\n",
       "  6  511.721008  216.594116  550.483826  298.334351    0.656854     24  backpack\n",
       "  7  441.622650  194.503265  483.818756  263.296417    0.512640      1   bicycle,\n",
       "  'caption': ['The front of a bicycle',\n",
       "   'one wheel of the bicycle to the right of the person wearing yellow helmet.'],\n",
       "  'bbox_target': [516.99, 303.34, 123.01, 176.66]},\n",
       " 140: {'image_emb': tensor([[ 0.0161,  0.3928, -0.2781,  ...,  1.0742,  0.0379,  0.2335],\n",
       "          [-0.0760,  0.1865, -0.3955,  ...,  1.0566, -0.0844,  0.1697],\n",
       "          [ 0.0822,  0.7114, -0.4202,  ...,  0.8730,  0.0370, -0.0318],\n",
       "          [ 0.1615,  0.6978, -0.4397,  ...,  0.8037,  0.0363, -0.0909]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 2.9028e-01,  1.0272e-01, -4.3774e-01,  5.0977e-01,  5.8301e-01,\n",
       "           -4.3823e-01, -8.7097e-02,  3.5248e-02,  5.0684e-01,  3.5278e-01,\n",
       "           -5.9131e-01, -1.0333e-01,  1.5152e-02,  1.5373e-02,  1.7471e-02,\n",
       "            2.4377e-01,  1.2268e-01,  1.8295e-02, -3.8770e-01,  2.6123e-01,\n",
       "           -1.4050e-01, -1.8433e-01,  2.1509e-01,  3.8208e-01,  1.3306e-01,\n",
       "           -2.3499e-01, -1.4844e-01,  4.0356e-01, -6.0425e-02,  1.4526e-01,\n",
       "           -4.2175e-02,  1.9409e-01, -1.3916e-01, -4.7070e-01, -2.8183e-02,\n",
       "           -1.4319e-01,  3.6890e-01, -2.3941e-02, -1.7590e-01,  1.5613e-01,\n",
       "            1.5735e-01,  3.6230e-01, -1.1383e-01, -1.5515e-01,  3.4375e-01,\n",
       "            1.5747e-01,  8.8745e-02,  4.1870e-02, -2.6587e-01, -1.2067e-01,\n",
       "            1.8738e-01, -3.1738e-02, -2.4170e-01, -7.8125e-02, -1.2939e-01,\n",
       "           -3.6572e-01,  4.3884e-02,  2.5732e-01, -1.3318e-01, -3.1079e-01,\n",
       "            5.4718e-02, -3.1348e-01, -1.4111e-01,  2.9395e-01,  1.2657e-02,\n",
       "           -3.6304e-01,  2.7148e-01,  2.7271e-01, -5.5389e-02, -2.7246e-01,\n",
       "            2.3059e-01, -1.1957e-01,  2.1033e-01,  7.6221e-01,  1.1682e-01,\n",
       "           -2.4634e-01,  6.2485e-03, -2.7466e-01, -4.0588e-02, -1.2189e-01,\n",
       "           -6.3086e-01, -1.3232e-01, -2.3218e-01, -3.4009e-01,  5.2051e-01,\n",
       "            3.1274e-01, -4.3060e-02, -3.2202e-01,  2.6392e-01,  1.1255e-01,\n",
       "           -1.4392e-01, -4.3652e-01, -6.9775e-01,  2.5879e-01,  5.1384e-03,\n",
       "           -9.1187e-02,  1.6724e-01, -6.6528e-02,  3.7354e-01, -9.5154e-02,\n",
       "            6.3538e-02, -1.9440e-02,  4.7638e-02,  5.0598e-02,  3.4253e-01,\n",
       "           -2.0056e-01, -9.5642e-02,  4.3433e-01,  1.4368e-01, -4.8877e-01,\n",
       "            1.4221e-01, -6.9922e-01, -1.0126e-01,  8.0017e-02, -3.8037e-01,\n",
       "            1.3416e-01, -4.6875e-01,  2.3254e-01, -1.8481e-01,  1.9104e-01,\n",
       "            3.2959e-02, -7.7588e-01, -3.4595e-01,  1.8940e-03,  5.1318e-01,\n",
       "           -1.0242e-01,  4.5679e-01, -2.2400e-01, -3.3252e-01, -1.1292e-01,\n",
       "            4.4336e-01, -3.1641e-01, -1.1841e-01,  1.5244e+00, -4.7803e-01,\n",
       "           -8.3313e-02, -2.5757e-01, -1.6138e-01, -9.3765e-03, -9.1782e-03,\n",
       "           -4.4653e-01,  3.0273e-01,  1.5381e-01, -1.1841e-01,  9.0576e-02,\n",
       "           -4.1089e-01,  8.1665e-02, -5.6348e-01, -4.3732e-02,  6.1676e-02,\n",
       "            9.3567e-02,  3.1299e-01, -2.0422e-01,  3.4741e-01, -2.5024e-01,\n",
       "            4.6997e-03,  7.7209e-02, -6.6345e-02,  2.5171e-01, -1.2378e-01,\n",
       "            6.8420e-02, -2.4048e-01,  3.3496e-01, -1.2305e-01,  1.6284e-01,\n",
       "           -1.4417e-01,  3.5034e-01, -2.3291e-01,  5.7953e-02,  1.9360e-01,\n",
       "            6.8237e-02, -6.1218e-02,  1.6699e-01,  9.7412e-02, -8.8440e-02,\n",
       "           -4.9023e-01,  3.2959e-01, -3.9642e-02, -1.0382e-01, -1.4915e-02,\n",
       "           -3.7476e-01,  2.2163e-03,  2.1477e-03, -5.2295e-01, -9.7839e-02,\n",
       "           -1.0913e-01,  1.1511e-01, -2.7539e-01, -2.9736e-01,  4.2212e-01,\n",
       "            8.1787e-02, -2.0862e-01,  1.2366e-01,  3.2446e-01, -9.5215e-02,\n",
       "           -6.9385e-01, -5.3613e-01,  1.2488e-01, -6.9141e-01, -4.3237e-01,\n",
       "           -1.5088e-01,  8.6243e-02, -3.7085e-01,  1.7969e-01, -1.2842e-01,\n",
       "            3.6346e-02,  1.6553e-01, -3.3618e-01,  3.3936e-01, -1.8225e-01,\n",
       "            9.1736e-02,  5.6348e-01, -3.8635e-02, -2.0337e-01, -2.7197e-01,\n",
       "           -7.4463e-02,  1.1029e-01,  6.6772e-02,  6.1377e-01, -4.7900e-01,\n",
       "            6.8542e-02,  4.0955e-02, -5.8838e-01,  5.4047e-02, -4.3506e-01,\n",
       "           -1.1383e-01,  7.3291e-01, -2.0459e-01,  5.9937e-02, -3.5718e-01,\n",
       "           -7.7576e-02,  1.6418e-01,  5.4504e-02,  1.3477e-01,  8.7891e-02,\n",
       "           -6.8970e-03,  1.4807e-01, -4.4141e-01, -1.1401e-01, -3.2031e-01,\n",
       "            1.1230e-01,  3.6670e-01,  3.2739e-01,  1.9812e-01, -5.4297e-01,\n",
       "           -4.1333e-01, -1.9763e-01, -1.5527e-01, -5.5145e-02,  1.2384e-01,\n",
       "           -9.3201e-02, -2.5177e-02,  1.5808e-01,  1.7383e-01,  9.7961e-02,\n",
       "           -4.0100e-02, -3.9331e-01,  1.0571e-01,  2.8091e-02,  8.9966e-02,\n",
       "           -4.7681e-01,  3.6084e-01,  6.8311e-01, -1.1572e-01,  2.1594e-01,\n",
       "            1.8945e-01, -2.9907e-01,  8.3780e-04,  1.0889e-01,  2.1606e-02,\n",
       "            3.2983e-01,  1.8286e-01,  6.1523e-01,  2.1265e-01, -5.5206e-02,\n",
       "            1.7847e-01, -5.3528e-02,  1.5332e-01,  4.6883e-03,  1.7041e-01,\n",
       "           -3.0273e-01, -1.3557e-02,  2.2729e-01,  5.6445e-01,  3.6572e-01,\n",
       "            2.5903e-01, -2.6929e-01,  3.8647e-01,  4.0186e-01,  3.8818e-01,\n",
       "            3.6041e-02,  2.6465e-01, -4.0527e-01,  2.3596e-01, -2.0178e-01,\n",
       "           -1.5283e-01, -5.4004e-01,  1.6966e-03, -1.0577e-01, -2.0056e-01,\n",
       "           -1.3184e-01, -3.7622e-01, -5.2246e-01,  2.1960e-01,  3.4229e-01,\n",
       "           -4.9097e-01,  5.4932e-01,  3.2324e-01,  1.3940e-01,  3.5010e-01,\n",
       "           -2.8687e-01,  7.9541e-01,  1.5244e+00,  1.2042e-01,  1.9067e-01,\n",
       "            3.6328e-01,  4.5142e-01, -4.0698e-01,  4.2053e-02, -9.9976e-02,\n",
       "            1.4514e-01,  1.2311e-01,  1.8945e-01, -7.4341e-02,  1.7822e-01,\n",
       "            3.3496e-01, -9.5581e-02, -2.2461e-01,  1.8372e-01, -2.3792e-01,\n",
       "            1.8616e-01,  1.4319e-01,  5.2441e-01, -5.8838e-01, -3.4943e-02,\n",
       "           -1.2756e-01,  3.7158e-01,  1.2744e-01,  2.0630e-02,  1.9678e-01,\n",
       "            4.2572e-02, -3.7085e-01,  3.9282e-01, -1.1066e-01,  5.6061e-02,\n",
       "           -5.0995e-02,  1.6089e-01,  3.0859e-01, -1.3818e-01, -1.3342e-01,\n",
       "            5.2637e-01,  2.5854e-01,  1.6968e-01,  4.7144e-01, -8.3862e-02,\n",
       "            4.8645e-02,  3.0371e-01, -7.3486e-02,  2.8290e-02,  1.3940e-01,\n",
       "            8.5388e-02,  1.0864e-01, -3.7769e-01,  8.5400e-01, -1.6162e-01,\n",
       "           -2.1997e-01, -1.7578e-01, -2.0227e-01,  7.4921e-03, -1.9324e-01,\n",
       "           -1.0345e-01, -1.6370e-01,  6.6284e-02,  4.0845e-01,  2.0935e-01,\n",
       "            3.2227e-01,  7.5012e-02, -3.8477e-01, -2.1045e-01,  4.7827e-01,\n",
       "           -1.5552e-01,  1.3562e-01, -1.2634e-01, -3.8818e-01,  9.2468e-02,\n",
       "            6.4011e-03, -2.2595e-01, -3.6743e-02,  2.4109e-01, -4.2847e-02,\n",
       "           -3.5327e-01, -1.5027e-01, -2.8125e-01,  2.5586e-01,  1.1102e-01,\n",
       "           -3.4180e-01, -3.7567e-02, -9.0881e-02, -2.4170e-01,  2.3499e-01,\n",
       "           -1.0999e-01, -5.4810e-02, -3.6560e-02,  2.4719e-02,  7.8430e-02,\n",
       "            6.9519e-02,  1.7969e-01, -5.3101e-02,  2.6929e-01, -1.0693e-01,\n",
       "            3.9331e-01,  7.4402e-02,  8.6182e-02, -2.7856e-01, -1.9714e-01,\n",
       "            1.6016e-01,  9.0393e-02, -2.1436e-01,  1.6626e-01, -1.0779e-01,\n",
       "            9.5337e-02, -8.4839e-02,  1.6809e-01,  5.7098e-02,  3.2544e-01,\n",
       "            4.1077e-02, -4.7217e-01,  5.4169e-03,  1.9812e-01, -6.5063e-02,\n",
       "           -1.3025e-01, -7.5439e-02,  3.3752e-02,  1.3940e-01,  1.4563e-01,\n",
       "            1.3318e-01, -2.9175e-01, -8.9294e-02, -5.6787e-01, -7.0251e-02,\n",
       "            2.7295e-01,  1.0887e-02, -1.7053e-01, -1.6833e-01,  3.9612e-02,\n",
       "            1.0785e-01, -3.1006e-01, -2.5464e-01, -1.1551e-02,  3.0249e-01,\n",
       "           -4.9463e-01,  8.6914e-02,  3.2153e-01, -1.0266e-01, -2.4744e-01,\n",
       "           -3.1812e-01,  2.1155e-01,  2.7588e-01, -5.0446e-02,  3.0469e-01,\n",
       "            5.6152e-01,  8.3557e-02,  2.9236e-02,  1.4392e-01,  1.1884e-01,\n",
       "            2.2717e-01, -4.1968e-01,  5.7182e-03, -5.2637e-01, -2.3438e-01,\n",
       "            2.4890e-01, -3.0838e-02,  4.8218e-01, -7.1228e-02,  3.6475e-01,\n",
       "           -4.0283e-02,  2.0935e-01, -3.3643e-01, -2.4121e-01,  3.5107e-01,\n",
       "           -1.9623e-02, -1.9336e-01,  1.4297e-02,  2.1680e-01, -8.0688e-02,\n",
       "            1.4380e-01, -1.9006e-01, -1.2744e-01,  1.8958e-01,  5.5115e-02,\n",
       "            3.8513e-02,  5.8136e-02,  2.5244e-01,  9.7510e-01, -3.1860e-02,\n",
       "           -3.1738e-01,  1.8994e-01,  4.7339e-01, -3.8892e-01,  2.0447e-01,\n",
       "            1.9763e-01, -4.6729e-01,  2.6489e-01,  2.0886e-01,  8.9478e-02,\n",
       "            4.5752e-01, -5.1709e-01,  5.1709e-01,  5.4993e-02,  2.7637e-01,\n",
       "           -5.8057e-01,  3.3905e-02]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1620, 0.4688, 0.1407, 0.2285]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class name\n",
       "  0  343.418030  159.000031  607.788513  390.645172    0.918609     16  dog\n",
       "  1   28.706940  104.689972  281.365479  232.965820    0.889498     16  dog\n",
       "  2    2.081696    0.042313  632.995117  474.521790    0.720626     59  bed,\n",
       "  'caption': ['black and brown dog with red collar laying under a blue blanket'],\n",
       "  'bbox_target': [27.33, 104.79, 255.12, 128.33]},\n",
       " 141: {'image_emb': tensor([[-0.1660,  0.5981, -0.2179,  ...,  0.6870,  0.0815, -0.0394],\n",
       "          [-0.1316,  0.4285, -0.0393,  ...,  0.9209, -0.1449, -0.1481],\n",
       "          [-0.5649,  0.4917, -0.4407,  ...,  0.6870,  0.1570,  0.0629],\n",
       "          [-0.3967,  0.3799, -0.2295,  ...,  0.6665,  0.0530, -0.1040]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.5254,  0.0528,  0.1835,  ...,  0.1155, -0.2234, -0.3069],\n",
       "          [-0.4043,  0.1145,  0.2009,  ...,  0.1174, -0.2705, -0.3518]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0019, 0.0035, 0.9160, 0.0788],\n",
       "          [0.0030, 0.0041, 0.9141, 0.0786]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   192.299637   16.194534  434.168823  249.544067    0.932645     45   \n",
       "  1    70.119263  215.661255  305.829498  456.891907    0.835840     45   \n",
       "  2   333.861542  208.042786  575.936829  450.973999    0.808767     45   \n",
       "  3     6.499603    2.267136  628.687744  470.685852    0.636199     60   \n",
       "  4   134.063705  271.765137  297.624023  363.115540    0.629425     50   \n",
       "  5   133.088837  335.620514  259.642609  434.148987    0.622507     50   \n",
       "  6   349.101105  314.617645  438.327179  362.851166    0.587734     51   \n",
       "  7   232.511261  260.287506  298.256195  343.929230    0.372947     50   \n",
       "  8    75.167343  216.265686  305.259888  449.439697    0.360459     50   \n",
       "  9   102.890190  319.101624  150.724823  387.435364    0.291132     50   \n",
       "  10  374.286255  266.321228  456.318359  320.849792    0.274824     51   \n",
       "  11  344.339691  216.418701  537.398315  380.828491    0.270512     51   \n",
       "  \n",
       "              name  \n",
       "  0           bowl  \n",
       "  1           bowl  \n",
       "  2           bowl  \n",
       "  3   dining table  \n",
       "  4       broccoli  \n",
       "  5       broccoli  \n",
       "  6         carrot  \n",
       "  7       broccoli  \n",
       "  8       broccoli  \n",
       "  9       broccoli  \n",
       "  10        carrot  \n",
       "  11        carrot  ,\n",
       "  'caption': ['A bowl of carrots and small tomatoes.',\n",
       "   'bowl of carrots and tomatoes'],\n",
       "  'bbox_target': [333.9, 205.96, 241.44, 244.67]},\n",
       " 142: {'image_emb': tensor([[ 0.2191,  0.2152, -0.2227,  ...,  0.5957,  0.2430,  0.1339],\n",
       "          [ 0.0312,  0.2607, -0.0149,  ...,  0.5835,  0.0071,  0.1782],\n",
       "          [ 0.0453,  0.1382, -0.1802,  ...,  0.5195,  0.0555,  0.1036],\n",
       "          [ 0.0974,  0.1212, -0.3809,  ...,  1.0049,  0.2810, -0.0048],\n",
       "          [ 0.1887,  0.1026, -0.6475,  ...,  0.6748,  0.2563, -0.1674]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2378,  0.1989,  0.0323,  ..., -0.0542, -0.2837, -0.0017],\n",
       "          [-0.0324, -0.0245,  0.0172,  ...,  0.4070,  0.0060, -0.4221]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7712e-01, 6.3818e-01, 1.7175e-01, 1.0475e-02, 2.5673e-03],\n",
       "          [3.2532e-02, 9.2139e-01, 5.9605e-04, 2.3329e-04, 4.5166e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   132.505432   36.703339  242.026703  183.610229    0.917208      0   \n",
       "  1   416.569580   38.739029  548.867432  202.377930    0.898406      0   \n",
       "  2   199.574173   40.048523  279.838867  135.113159    0.856911      0   \n",
       "  3   269.537781   96.657578  332.052979  164.055450    0.780020      0   \n",
       "  4   333.317230  211.401581  384.233002  236.339020    0.642719     54   \n",
       "  5   226.704407  243.003998  295.061218  283.582123    0.621964     54   \n",
       "  6   374.889618  224.386414  419.921906  249.814392    0.518492     54   \n",
       "  7   176.535294  222.525635  238.731522  258.060730    0.508999     54   \n",
       "  8   394.427826  220.875702  451.951263  243.690704    0.497418     54   \n",
       "  9   295.342590  252.896667  348.917358  278.837646    0.491543     54   \n",
       "  10  187.932693  204.721069  234.938858  227.595581    0.482126     54   \n",
       "  11  429.519775  212.514648  469.506348  235.825012    0.454206     54   \n",
       "  12  304.803101  196.817566  352.662903  225.630737    0.418310     54   \n",
       "  13  346.901764  231.009430  398.807892  258.662506    0.351710     54   \n",
       "  14   21.672638  167.718201  635.524902  416.280090    0.341090     60   \n",
       "  15  219.192459  217.104126  273.935852  245.818237    0.334282     54   \n",
       "  16  238.006210  207.358093  288.897491  234.820862    0.320230     54   \n",
       "  17  216.627121  179.063843  256.220551  201.830322    0.284674     54   \n",
       "  18  121.354836  201.590149  158.398468  232.302429    0.274320     54   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3         person  \n",
       "  4          donut  \n",
       "  5          donut  \n",
       "  6          donut  \n",
       "  7          donut  \n",
       "  8          donut  \n",
       "  9          donut  \n",
       "  10         donut  \n",
       "  11         donut  \n",
       "  12         donut  \n",
       "  13         donut  \n",
       "  14  dining table  \n",
       "  15         donut  \n",
       "  16         donut  \n",
       "  17         donut  \n",
       "  18         donut  ,\n",
       "  'caption': ['A woman in a hooded sweatshirt.',\n",
       "   'Lady in the back of a bakery with a white apron and light hooded shirt on.'],\n",
       "  'bbox_target': [124.06, 35.01, 116.8, 148.07]},\n",
       " 143: {'image_emb': tensor([[-0.2047,  0.0798,  0.3755,  ...,  0.8154,  0.1490, -0.0276],\n",
       "          [-0.3521,  0.5127, -0.2057,  ...,  1.2461,  0.0325, -0.0225],\n",
       "          [-0.1801,  0.1157, -0.5479,  ...,  0.5176, -0.2878, -0.2874],\n",
       "          [-0.1610,  0.0100, -0.5562,  ...,  0.5488, -0.2467, -0.2830]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2006,  0.2455, -0.0327,  ..., -0.1608, -0.2908, -0.0303],\n",
       "          [-0.2974, -0.0879,  0.0799,  ...,  0.1581, -0.1903,  0.0885]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0058, 0.9839, 0.0060, 0.0042],\n",
       "          [0.0039, 0.5410, 0.2678, 0.1870]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  595.705811    0.605453  639.885620   76.653976    0.814777     41   \n",
       "  1    2.428215    0.783203  226.748016  150.146027    0.777719     42   \n",
       "  2    0.000000    0.000000  634.843750  355.125244    0.707051     60   \n",
       "  3   26.235603  203.525024   44.562798  232.910400    0.489045     42   \n",
       "  4    0.098400  222.258148   46.238766  265.420135    0.304553     43   \n",
       "  5   26.150375  203.936249   44.552193  232.898590    0.263379     43   \n",
       "  \n",
       "             name  \n",
       "  0           cup  \n",
       "  1          fork  \n",
       "  2  dining table  \n",
       "  3          fork  \n",
       "  4         knife  \n",
       "  5         knife  ,\n",
       "  'caption': ['A placemat covered by a white plate.',\n",
       "   'A neutral-toned placement underneath a breakfast plate.'],\n",
       "  'bbox_target': [549.33, 84.3, 76.45, 270.22]},\n",
       " 144: {'image_emb': tensor([[ 0.0863,  0.4246,  0.0428,  ...,  0.3960,  0.6890, -0.0263],\n",
       "          [-0.4258,  0.4924,  0.1611,  ...,  0.2739,  0.3293, -0.0341],\n",
       "          [-0.3101,  0.5938, -0.0293,  ...,  0.4626,  0.5015, -0.1149],\n",
       "          ...,\n",
       "          [-0.1533,  0.7598, -0.2214,  ...,  0.5713,  0.3684, -0.2727],\n",
       "          [-0.7451,  0.2705,  0.0329,  ...,  0.3589,  0.5015,  0.1042],\n",
       "          [-0.2053,  0.1237,  0.1132,  ..., -0.1807, -0.2242,  0.2141]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-2.5024e-01, -5.7617e-02, -3.4302e-01,  5.1025e-02, -2.0911e-01,\n",
       "            1.8774e-01, -4.6704e-01, -1.1670e+00, -9.3689e-02, -7.7209e-02,\n",
       "           -1.6016e-01,  1.4185e-01,  2.0935e-01,  4.0649e-01,  1.2494e-01,\n",
       "            2.2546e-01,  2.0874e-01, -1.3293e-01, -1.7871e-01,  5.0342e-01,\n",
       "            2.1277e-01,  2.2803e-01, -9.2590e-02, -6.9580e-03, -1.3721e-01,\n",
       "           -1.1444e-02, -5.0879e-01,  1.3464e-01, -8.7585e-02, -1.0565e-01,\n",
       "            1.6016e-01,  1.8112e-02,  1.3000e-01,  1.0657e-01,  1.0577e-01,\n",
       "            2.2559e-01, -4.8157e-02,  2.2668e-01, -1.1387e-03,  3.8574e-01,\n",
       "            1.4514e-01,  1.4331e-01, -3.6206e-01,  2.4536e-01,  1.4526e-01,\n",
       "           -1.8616e-01, -2.3474e-01, -8.7219e-02, -8.4900e-02,  1.8726e-01,\n",
       "            1.1963e-01,  8.5510e-02, -5.3955e-02,  5.5078e-01, -6.2793e-01,\n",
       "           -2.0410e-01,  7.0801e-02,  4.1504e-02, -3.3630e-02, -4.3854e-02,\n",
       "            6.9519e-02, -4.0222e-02, -9.9182e-02, -2.8687e-02,  2.6465e-01,\n",
       "            4.7516e-02, -3.1396e-01,  5.9326e-01,  3.2715e-01,  2.0386e-01,\n",
       "           -1.8542e-01,  3.4729e-02,  1.7188e-01,  2.8345e-01,  3.1177e-01,\n",
       "           -3.7231e-01,  9.6741e-02,  8.0505e-02,  2.3438e-02,  1.1978e-03,\n",
       "            2.2614e-02,  2.7893e-02,  5.8502e-02,  4.8340e-01, -8.7769e-02,\n",
       "            2.5098e-01, -4.3799e-01,  2.3840e-01,  1.1959e-03, -2.2754e-01,\n",
       "           -1.9312e-01, -2.7222e-01, -1.3955e+00,  2.8400e-03,  1.8835e-01,\n",
       "           -1.6919e-01, -5.1056e-02, -4.0771e-01,  3.6768e-01, -1.5442e-01,\n",
       "           -1.4380e-01, -1.8799e-01, -2.1229e-03, -1.6858e-01,  5.0195e-01,\n",
       "           -2.4438e-01, -2.8247e-01,  3.1543e-01, -4.8103e-03, -4.0454e-01,\n",
       "            4.2450e-02,  4.9927e-01,  2.9956e-01,  2.9395e-01, -4.8157e-02,\n",
       "           -9.3460e-03, -4.3823e-01,  1.1208e-02,  1.2360e-01, -1.2009e-02,\n",
       "           -1.5564e-01, -9.1357e-01, -3.6816e-01, -5.1074e-01,  1.9507e-01,\n",
       "           -1.5405e-01,  1.6266e-02, -1.8079e-01, -7.0251e-02,  3.5126e-02,\n",
       "            3.7500e-01, -5.6982e-01, -1.1176e-01,  5.1602e+00, -4.8511e-01,\n",
       "           -2.8174e-01, -1.7737e-01,  1.5564e-01, -1.1810e-01, -3.2373e-01,\n",
       "            5.8868e-02, -1.1218e-01, -4.1382e-01,  3.3765e-01, -1.3481e-02,\n",
       "            2.4438e-01,  7.5684e-02, -2.2949e-01, -3.9111e-01, -2.0300e-01,\n",
       "           -5.2368e-02, -4.4189e-01,  1.8384e-01,  1.2561e-01, -1.3867e-01,\n",
       "           -4.5093e-01,  1.4381e-02, -2.4841e-01, -1.5210e-01,  4.0747e-01,\n",
       "            1.6602e-01,  1.0840e-01, -2.1497e-01,  1.6309e-01,  5.5359e-02,\n",
       "            1.7310e-01,  5.4199e-02, -1.7029e-01,  3.1812e-01, -7.0251e-02,\n",
       "            1.3586e-01, -1.9336e-01,  7.5500e-02,  3.5083e-01,  7.4707e-02,\n",
       "            5.4199e-01, -1.2537e-01,  2.0667e-01, -8.2397e-02, -1.1426e-01,\n",
       "           -1.8616e-01,  2.3279e-01,  6.8703e-03, -2.2485e-01, -7.8308e-02,\n",
       "           -2.4567e-02,  3.0981e-01, -2.3289e-03, -1.2439e-01, -7.7942e-02,\n",
       "           -1.8225e-01,  7.2021e-02, -1.3501e-01, -1.7041e-01, -2.7539e-01,\n",
       "            2.5903e-01, -6.0986e-01,  5.8984e-01, -6.6040e-02, -8.7830e-02,\n",
       "            1.0300e-02, -1.0010e-02, -3.9490e-02,  1.0852e-01, -1.2457e-01,\n",
       "            4.3018e-01,  1.5576e-01,  5.7411e-03, -8.1726e-02,  1.9363e-02,\n",
       "           -3.2544e-01,  1.1530e-01, -3.1647e-02,  4.9414e-01,  1.0547e-01,\n",
       "           -1.3306e-01, -2.5391e-01,  3.1799e-02,  5.3680e-02,  6.2927e-02,\n",
       "            2.5879e-01, -4.3335e-02,  1.3782e-01, -5.6006e-01, -1.0614e-01,\n",
       "            1.6223e-01,  1.8140e-01, -2.8760e-01, -2.3865e-01, -3.6060e-01,\n",
       "           -9.0103e-03, -1.5515e-01, -9.2041e-02, -2.2675e-02,  5.2490e-02,\n",
       "            1.1749e-01,  5.4248e-01, -2.4109e-01, -2.1533e-01,  6.7322e-02,\n",
       "           -5.2551e-02,  1.9458e-01, -1.1604e-02, -2.4463e-01, -9.6802e-02,\n",
       "           -4.0405e-01, -8.3923e-02, -4.3091e-01, -4.3365e-02, -1.4111e-01,\n",
       "            2.7100e-01,  4.8926e-01, -2.2742e-01,  2.8491e-01, -2.3840e-01,\n",
       "           -1.3342e-01,  3.1396e-01, -1.2549e-01,  6.0486e-02,  1.9702e-01,\n",
       "            2.0459e-01,  2.4475e-01, -1.2585e-01, -1.2964e-01,  2.4878e-01,\n",
       "            2.9297e-01, -7.9468e-02, -3.4277e-01, -6.1951e-02,  1.9080e-01,\n",
       "           -3.2300e-01,  2.2729e-01, -4.4922e-02, -2.4304e-01,  6.1676e-02,\n",
       "           -1.4111e-01,  3.2495e-01,  1.7444e-01,  6.9885e-02, -1.3542e-02,\n",
       "            2.8580e-02, -1.8213e-01,  2.3621e-01,  1.0303e-01,  2.3218e-01,\n",
       "            5.0830e-01,  1.3989e-01,  6.2561e-03, -4.0552e-01, -4.6118e-01,\n",
       "           -3.6890e-01,  3.3105e-01, -3.7207e-01, -5.0690e-02, -2.2815e-01,\n",
       "            1.9275e-01, -5.2368e-02, -4.3396e-02, -3.3252e-01, -1.8604e-01,\n",
       "           -9.6893e-03,  1.0419e-01,  2.1313e-01,  1.5662e-01, -8.9172e-02,\n",
       "           -7.8320e-01, -1.2863e-02,  7.2266e-02, -1.1725e-01, -4.0552e-01,\n",
       "            1.0828e-01,  3.8135e-01,  5.1406e+00,  1.3281e-01, -7.9956e-02,\n",
       "            1.9360e-01,  2.5024e-01, -2.4023e-01,  1.5793e-02,  1.0248e-01,\n",
       "            1.5625e-01, -1.4075e-01, -2.4536e-02,  1.1041e-01, -1.0933e-02,\n",
       "            2.8711e-01,  8.4167e-02,  2.9736e-01,  2.7197e-01, -2.0410e+00,\n",
       "            1.4481e-02, -1.1102e-01,  2.3145e-01,  1.4795e-01, -3.5547e-01,\n",
       "            1.6248e-01, -1.7505e-01, -2.0178e-01,  1.2878e-01, -2.0642e-01,\n",
       "           -1.4111e-01,  1.6064e-01, -3.4448e-01, -3.2617e-01,  2.9053e-01,\n",
       "           -3.5187e-02,  2.3376e-01,  4.1351e-02, -2.5366e-01,  2.3914e-01,\n",
       "           -1.8661e-02,  3.6694e-01,  3.4888e-01, -2.0837e-01,  5.0995e-02,\n",
       "            1.7712e-01,  7.3914e-02, -3.1470e-01,  4.9292e-01,  3.9185e-01,\n",
       "            4.4141e-01,  1.9519e-01,  3.1891e-02,  2.5330e-02,  4.6021e-01,\n",
       "            3.1433e-02, -2.1362e-01,  1.2927e-01, -1.9385e-01,  1.7749e-01,\n",
       "            2.0813e-02, -1.4075e-01,  2.0557e-01, -1.5979e-01, -3.6499e-01,\n",
       "            1.7883e-01, -2.3352e-01,  1.6980e-01,  3.8745e-01, -1.4563e-01,\n",
       "            9.4543e-02,  2.9449e-02, -1.6052e-02, -8.4351e-02,  1.4490e-01,\n",
       "            6.7566e-02,  4.0796e-01, -2.9028e-01, -3.8257e-01, -2.8052e-01,\n",
       "           -6.3354e-02,  9.4177e-02, -3.9795e-02, -1.2000e-01, -8.1116e-02,\n",
       "           -1.7603e-01, -8.2254e-04,  6.7993e-02, -2.5513e-01,  6.0242e-02,\n",
       "           -4.4287e-01, -1.4844e-01,  1.5454e-01, -4.1382e-01, -3.0640e-01,\n",
       "            7.4524e-02,  6.3232e-02,  3.1219e-02,  2.4084e-01,  9.1003e-02,\n",
       "            3.2642e-01,  5.3174e-01,  2.8427e-02,  4.3579e-01, -9.4482e-02,\n",
       "           -2.8003e-01, -1.5259e-01,  7.9102e-02, -1.6632e-02, -5.1367e-01,\n",
       "            2.7344e-01,  8.3130e-02, -3.5156e-02, -2.7759e-01,  5.2429e-02,\n",
       "           -5.0439e-01, -1.2976e-01,  1.1670e-01,  6.2598e-01, -7.1594e-02,\n",
       "            1.3708e-01, -3.2935e-01, -1.9849e-01, -1.6187e-01,  1.4587e-01,\n",
       "            3.6230e-01, -3.4619e-01, -1.4124e-01, -3.1274e-01,  2.7051e-01,\n",
       "           -8.9355e-02,  3.6938e-01, -1.1298e-01, -1.3696e-01,  2.2217e-01,\n",
       "            8.5144e-02,  9.4528e-03, -2.2827e-01,  2.6074e-01,  1.8677e-01,\n",
       "           -6.9531e-01,  3.8177e-02,  2.9761e-01,  4.1748e-02,  3.5742e-01,\n",
       "           -4.1504e-02, -1.6575e-03,  8.6609e-02,  9.2773e-02,  2.5366e-01,\n",
       "            3.7646e-01, -1.8143e-02, -2.2217e-01,  2.3376e-01,  2.6978e-01,\n",
       "           -4.0680e-02,  3.0469e-01, -5.9418e-02, -2.3560e-01,  2.8882e-01,\n",
       "           -2.9688e-01, -2.4490e-03, -8.9905e-02,  5.9570e-02,  1.7090e-01,\n",
       "            2.8351e-02,  2.1777e-01,  8.6365e-02,  3.6255e-02,  3.0933e-01,\n",
       "           -3.6279e-01, -4.6753e-01,  2.3669e-01, -1.0889e-01,  4.5166e-02,\n",
       "           -2.7588e-01, -1.5027e-01, -6.7177e-03, -1.4990e-01, -2.8662e-01,\n",
       "           -9.5947e-02,  1.1285e-01, -2.9932e-01,  4.7192e-01, -2.4792e-01,\n",
       "           -3.4088e-02,  1.0757e-02, -3.4351e-01, -2.5293e-01,  1.4587e-01,\n",
       "           -8.7646e-02,  1.2732e-01, -3.1055e-01,  2.9712e-01, -5.6702e-02,\n",
       "           -5.4901e-02, -2.3401e-01,  4.7144e-01, -1.3940e-01, -1.2970e-02,\n",
       "            9.8145e-02, -3.7695e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0690, 0.5957, 0.0090, 0.1578, 0.0081, 0.1229, 0.0231, 0.0034, 0.0107]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  438.316803   23.681213  565.456543  309.785583    0.913412      0  person\n",
       "  1  390.793274   88.850937  580.654297  451.265137    0.912842     17   horse\n",
       "  2  320.755493   53.363449  419.173767  303.609985    0.876252      0  person\n",
       "  3  229.682327  111.322693  415.706543  472.256592    0.866682     17   horse\n",
       "  4  160.218582  120.808167  304.046692  450.818481    0.865116     17   horse\n",
       "  5   44.215958  114.421936  214.844223  446.708679    0.832452     17   horse\n",
       "  6  101.235985   73.019409  202.346375  327.888000    0.826081      0  person\n",
       "  7  227.496506   60.684128  313.635803  194.057983    0.748500      0  person,\n",
       "  'caption': ['The man riding the spotted horse.'],\n",
       "  'bbox_target': [438.14, 25.64, 131.56, 284.29]},\n",
       " 145: {'image_emb': tensor([[-0.1191, -0.0617,  0.0562,  ...,  0.5557, -0.3928, -0.5576],\n",
       "          [ 0.1813,  0.1444,  0.2091,  ...,  1.1133, -0.0765, -0.1078],\n",
       "          [ 0.5278,  0.1135,  0.1198,  ...,  0.8354,  0.3176,  0.0535]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0146, -0.0007,  0.2137,  ..., -0.0962, -0.0205, -0.0891],\n",
       "          [-0.0957, -0.1760,  0.0468,  ..., -0.1272, -0.0110, -0.1870]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0753, 0.6406, 0.2842],\n",
       "          [0.8677, 0.0564, 0.0758]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  name\n",
       "  0   25.961971   19.692207  499.427124  186.446304    0.844429      8  boat\n",
       "  1  371.521545  136.401047  499.695190  248.825882    0.774653      8  boat\n",
       "  2   61.817966    0.000000  239.711288   32.061852    0.312934      8  boat,\n",
       "  'caption': ['A white boat on the water at the end of the line',\n",
       "   'The boat in the very back.'],\n",
       "  'bbox_target': [55.82, 0.09, 441.57, 36.24]},\n",
       " 146: {'image_emb': tensor([[ 0.1943,  0.4619, -0.0537,  ...,  1.0186, -0.5303,  0.1137],\n",
       "          [-0.0970,  0.2372, -0.2493,  ...,  0.5498, -0.2289, -0.0180],\n",
       "          [-0.1346,  0.3889, -0.0853,  ...,  0.6797, -0.2131, -0.2773],\n",
       "          [ 0.1361,  0.2177, -0.3118,  ...,  0.6240, -0.4106,  0.1240]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0683, -0.0434, -0.1530,  ...,  0.1460, -0.2321,  0.6592],\n",
       "          [-0.1279, -0.0547, -0.2126,  ...,  0.0520,  0.0847,  0.0136]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1071, 0.1510, 0.6558, 0.0861],\n",
       "          [0.1921, 0.2505, 0.4004, 0.1569]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0    0.726273    1.240997  356.109924  259.031097    0.916442      4  airplane\n",
       "  1  174.654221   71.801224  638.309509  473.691589    0.908842      4  airplane\n",
       "  2  266.687622   62.737793  411.216370  184.395630    0.763814      4  airplane\n",
       "  3  170.254395  278.803680  282.594971  474.509766    0.254255      7     truck,\n",
       "  'caption': ['the AA tail parked in the shade',\n",
       "   'The tail end of the airplane on the far left.'],\n",
       "  'bbox_target': [1.08, 2.43, 354.88, 256.72]},\n",
       " 147: {'image_emb': tensor([[-0.2395,  0.2837, -0.0529,  ...,  0.5786, -0.1415,  0.6284],\n",
       "          [-0.3008,  0.3916, -0.2891,  ...,  0.7461,  0.4236,  0.1248],\n",
       "          [-0.1523,  0.1165, -0.0814,  ...,  0.8794,  0.3186,  0.1576],\n",
       "          ...,\n",
       "          [-0.0428,  0.4265, -0.5259,  ...,  0.8921,  0.2991, -0.1924],\n",
       "          [-0.1099,  0.0684, -0.2913,  ...,  0.7725,  0.1209,  0.1472],\n",
       "          [-0.3904,  0.3076, -0.1617,  ...,  0.4165,  0.2013,  0.4307]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-2.2803e-01,  5.0598e-02, -3.4668e-01,  3.8916e-01, -3.7750e-02,\n",
       "            2.8027e-01, -4.7925e-01, -4.2261e-01,  2.1997e-01, -1.0394e-01,\n",
       "            1.1963e-01,  2.3148e-02,  4.5807e-02, -6.1371e-02, -1.4905e-01,\n",
       "           -2.4414e-01, -1.9336e-01,  8.4412e-02, -3.0957e-01,  2.5757e-01,\n",
       "           -3.1616e-01, -1.6687e-01, -1.9482e-01, -1.2115e-01,  2.0374e-01,\n",
       "           -4.7264e-03, -1.6479e-01, -5.5023e-02,  4.3872e-01,  2.2656e-01,\n",
       "           -6.5308e-02, -1.2732e-01, -3.6963e-01,  1.6797e-01, -3.8965e-01,\n",
       "           -6.1914e-01,  4.0308e-01,  4.0991e-01, -2.7002e-01, -6.5491e-02,\n",
       "           -4.6768e-03,  7.5684e-02, -2.8336e-02,  2.1985e-01,  1.2744e-01,\n",
       "           -1.9617e-01,  1.6846e-01,  2.5269e-02,  7.0679e-02, -3.2959e-02,\n",
       "           -2.4292e-01,  3.7975e-03,  1.8848e-01, -2.7368e-01,  6.0486e-02,\n",
       "            1.4172e-01, -1.9629e-01,  3.3301e-01,  5.2002e-02,  2.0471e-01,\n",
       "           -3.9355e-01,  1.8237e-01,  8.5297e-03, -2.9053e-01, -3.7939e-01,\n",
       "            3.2410e-02,  1.7981e-01,  2.2949e-01, -1.9360e-01, -6.0669e-02,\n",
       "            9.7595e-02, -1.5344e-01,  5.4565e-02, -1.2115e-01, -8.3740e-02,\n",
       "            3.2410e-02, -1.7566e-01,  1.7163e-01,  5.8197e-02,  3.6572e-01,\n",
       "           -7.0801e-01,  2.0789e-01,  1.3672e-01, -3.0981e-01, -1.4209e-01,\n",
       "            1.7822e-01, -4.3774e-01,  1.1316e-01, -4.3182e-02,  2.8946e-02,\n",
       "           -6.2714e-03, -4.2017e-01, -8.2178e-01,  3.1299e-01, -2.6465e-01,\n",
       "            6.0242e-02, -2.0776e-01, -2.7856e-01,  7.4463e-01,  4.6313e-01,\n",
       "           -1.8445e-01,  2.0947e-01, -4.1901e-02,  2.2437e-01,  7.0007e-02,\n",
       "           -3.1525e-02, -3.0273e-01,  4.4922e-02, -1.2476e-01,  2.8491e-01,\n",
       "            3.0746e-02, -2.0642e-01,  5.5469e-01,  1.0468e-01, -1.6333e-01,\n",
       "           -2.7295e-01,  2.3804e-02, -5.6274e-02, -3.0078e-01, -4.2261e-01,\n",
       "            4.9341e-01, -4.0918e-01, -1.1792e-01,  3.2568e-01,  2.2247e-02,\n",
       "           -1.2927e-01, -2.9907e-01, -1.5564e-01, -4.8401e-02,  2.5464e-01,\n",
       "            2.0068e-01, -1.0913e-01,  2.5659e-01,  3.9023e+00,  1.5601e-01,\n",
       "           -8.5068e-03, -1.2482e-01, -2.8052e-01,  3.2642e-01, -3.7427e-01,\n",
       "           -3.0688e-01,  5.3027e-01, -1.5234e-01,  2.3096e-01, -9.6619e-02,\n",
       "           -3.6011e-01, -7.1472e-02, -9.4873e-01, -2.3840e-01,  2.5464e-01,\n",
       "           -4.9316e-01, -1.6174e-01,  3.1323e-01, -2.2668e-01,  7.1167e-02,\n",
       "            1.1743e-01,  1.4795e-01, -4.1821e-01,  5.4657e-02,  3.6133e-02,\n",
       "            1.7441e-02,  1.7920e-01, -1.0565e-01, -1.0999e-01, -1.5442e-01,\n",
       "            2.3633e-01,  1.0016e-01, -4.4312e-02, -2.0032e-01, -1.7041e-01,\n",
       "           -8.5754e-03,  4.6806e-03,  1.8359e-01,  2.2571e-01, -9.6130e-03,\n",
       "            2.7124e-01, -4.7180e-02,  2.4622e-01,  3.2373e-01, -2.8955e-01,\n",
       "           -1.5015e-01, -2.4268e-01, -1.9714e-01,  3.1342e-02,  7.4463e-03,\n",
       "            5.3558e-02, -1.7981e-01, -1.9028e-02,  1.0992e-01,  2.6904e-01,\n",
       "            8.9966e-02,  6.8066e-01,  4.2920e-01,  5.0732e-01, -1.4880e-01,\n",
       "           -1.8604e-01, -1.6772e-01,  9.0698e-02, -4.2651e-01,  2.5415e-01,\n",
       "           -1.1493e-01, -1.5991e-01,  4.1718e-02,  4.5972e-01,  3.9673e-01,\n",
       "            1.1798e-01,  2.0837e-01, -8.9233e-02,  2.0532e-01, -1.8323e-01,\n",
       "            5.9082e-01,  5.2368e-02, -1.3953e-01, -2.4182e-01,  8.6914e-02,\n",
       "           -4.1431e-01, -7.8918e-02,  4.7729e-02,  4.5190e-01,  2.6880e-01,\n",
       "            5.7373e-01, -1.9360e-01, -2.6978e-01, -1.0754e-01,  1.4319e-01,\n",
       "            3.3752e-02, -9.2926e-03, -7.8491e-02, -2.7441e-01, -2.0203e-01,\n",
       "           -8.3557e-02, -3.4668e-01,  8.6975e-02, -1.3501e-01, -1.5381e-01,\n",
       "            2.8491e-01,  3.1189e-02, -1.8506e-01, -9.5520e-02, -3.7427e-01,\n",
       "           -2.1375e-01,  1.5906e-01,  2.0007e-01, -8.1360e-02, -1.6602e-01,\n",
       "            3.5718e-01,  2.2864e-01,  1.9836e-01,  1.3831e-01,  1.0254e-01,\n",
       "           -5.1221e-01, -1.5698e-01,  2.0605e-01, -7.5977e-01, -3.0249e-01,\n",
       "           -2.6782e-01, -8.0872e-02,  8.0933e-02,  3.6548e-01,  5.4047e-02,\n",
       "            5.8496e-01,  9.8877e-02, -3.6938e-01,  2.8906e-01,  6.3843e-02,\n",
       "            2.6123e-01, -1.0406e-02, -2.1948e-01, -1.1755e-01, -1.6418e-01,\n",
       "            1.3831e-01, -3.1836e-01,  5.7587e-02,  1.2244e-01,  1.2305e-01,\n",
       "            1.8042e-01,  1.0260e-01, -1.0327e-01, -2.9861e-02, -2.4048e-01,\n",
       "            5.9521e-01, -9.0820e-02, -2.5781e-01, -1.0175e-01,  2.1069e-01,\n",
       "           -7.7858e-03, -8.3984e-02, -6.2683e-02,  4.5312e-01, -3.1714e-01,\n",
       "           -4.9683e-02,  7.4097e-02,  5.5078e-01,  3.0884e-01, -2.3102e-02,\n",
       "            3.3545e-01, -6.8726e-02, -1.3452e-01, -3.9154e-02,  6.8213e-01,\n",
       "           -1.9397e-01,  2.5415e-01, -3.6304e-01,  5.1953e-01, -5.6801e-03,\n",
       "           -1.4038e-01,  1.5271e-01,  1.8433e-01,  2.0752e-01,  9.6497e-02,\n",
       "           -2.1497e-01,  7.2607e-01,  3.9004e+00, -1.1798e-01, -9.5703e-02,\n",
       "            4.6730e-03,  2.3987e-01, -5.4443e-01,  2.2009e-01,  4.8315e-01,\n",
       "            2.5098e-01, -3.5547e-01,  1.4563e-01,  3.5126e-02,  1.9287e-01,\n",
       "            1.6516e-01,  2.3608e-01,  5.9143e-02,  1.1719e-02, -9.1309e-01,\n",
       "            3.5248e-02,  2.0203e-01,  3.4106e-01, -3.7207e-01,  8.0444e-02,\n",
       "            3.0197e-02,  7.6416e-02, -3.5248e-02,  1.3885e-02, -8.9478e-02,\n",
       "           -3.2129e-01, -3.8818e-02,  1.5918e-01, -4.2871e-01,  2.3926e-01,\n",
       "            1.2384e-01,  2.6318e-01, -1.8005e-01,  3.3765e-01,  1.9116e-01,\n",
       "            7.1838e-02, -2.4536e-01,  3.7061e-01, -1.4136e-01, -1.5662e-01,\n",
       "           -7.5488e-01, -5.0732e-01, -2.2998e-01, -5.8838e-01,  2.4268e-01,\n",
       "           -9.5117e-01, -4.5044e-01,  9.8389e-02, -6.6299e-03,  5.0507e-02,\n",
       "            5.7106e-03, -2.1350e-01,  4.0283e-01,  3.2764e-01,  5.4047e-02,\n",
       "            6.5002e-02,  1.0400e-01, -1.7969e-01,  2.1167e-01, -1.4636e-01,\n",
       "            1.5991e-01, -3.6255e-01, -8.0078e-02,  1.4233e-01, -2.4146e-01,\n",
       "            2.7612e-01,  3.6072e-02,  1.6553e-01, -1.6089e-01, -8.1360e-02,\n",
       "           -4.8889e-02,  1.9666e-01, -1.3672e-01, -1.8481e-01, -2.5781e-01,\n",
       "           -2.5146e-01,  3.3936e-01, -1.3757e-01,  6.5918e-02, -4.6313e-01,\n",
       "            8.8257e-02,  4.4312e-02, -3.6816e-01, -1.2524e-01,  5.8691e-01,\n",
       "            6.3623e-01,  4.7852e-02, -1.4410e-03, -3.2422e-01, -1.8970e-01,\n",
       "           -3.0859e-01, -2.6685e-01, -2.6099e-01, -2.0584e-02, -2.6221e-01,\n",
       "            2.5586e-01,  2.1765e-01, -1.2213e-01,  1.7261e-01,  1.2122e-01,\n",
       "           -1.6772e-01, -5.1953e-01, -6.7529e-01,  4.3188e-01, -2.7100e-01,\n",
       "           -4.6899e-01,  1.0321e-01,  3.0777e-02,  4.8767e-02,  9.0149e-02,\n",
       "            1.4870e-02,  6.6345e-02, -2.3828e-01,  1.1090e-01,  3.7207e-01,\n",
       "           -1.4868e-01,  3.0075e-02,  1.9971e-01, -2.9419e-01,  9.7961e-02,\n",
       "           -1.1908e-01,  9.3079e-02, -1.4795e-01,  8.5754e-02,  2.8540e-01,\n",
       "           -5.0879e-01, -4.8047e-01, -1.3354e-01, -5.6213e-02,  1.7200e-01,\n",
       "           -5.0977e-01, -4.2310e-01,  4.0234e-01,  8.4900e-02,  6.2207e-01,\n",
       "           -8.6914e-01, -1.4392e-01,  1.5884e-02, -8.8989e-02,  2.1118e-01,\n",
       "           -1.0706e-01,  2.1594e-01, -1.7285e-01,  1.3367e-01, -1.7175e-01,\n",
       "           -9.9792e-02,  3.0664e-01,  3.4302e-01, -9.8511e-02,  3.3130e-01,\n",
       "           -1.4563e-01, -1.3928e-01, -2.5146e-01, -1.9580e-01, -1.3824e-02,\n",
       "            1.0162e-01,  2.5415e-01, -8.9905e-02, -2.0679e-01,  3.0688e-01,\n",
       "            3.2104e-01,  3.4155e-01, -1.1420e-01, -7.1777e-02,  3.8208e-01,\n",
       "            1.2067e-01,  9.7351e-02,  8.9417e-02, -3.5839e-03, -4.3640e-02,\n",
       "            1.2683e-01,  7.3853e-02, -4.7180e-02,  2.4316e-01, -2.6566e-02,\n",
       "            3.5083e-01, -3.3838e-01,  2.2717e-01,  1.0771e+00,  6.3818e-01,\n",
       "           -3.9819e-01,  2.4060e-01, -8.2581e-02, -7.3120e-02, -1.6968e-01,\n",
       "            4.0186e-01, -5.8472e-02,  3.8623e-01, -6.3232e-02,  1.2079e-01,\n",
       "           -1.9189e-01,  1.5955e-01,  7.2083e-02, -2.1936e-01,  3.7427e-01,\n",
       "           -2.4609e-01, -7.1640e-03]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.0381e-01, 1.6508e-03, 5.3048e-05, 2.8682e-04, 7.1289e-02, 7.7963e-04,\n",
       "           1.5354e-04, 5.3883e-05, 1.7881e-06, 2.4500e-01, 4.0054e-05, 4.7278e-04,\n",
       "           1.0653e-03, 3.5496e-03, 6.7368e-03, 4.0054e-05, 4.6849e-05, 8.0884e-05,\n",
       "           2.6489e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    14.961075  161.004333  274.453003  422.431824    0.934052      0   \n",
       "  1   386.892609  115.256195  546.835571  347.138519    0.930413      0   \n",
       "  2     2.741676   93.588593  138.057037  236.938019    0.925139      0   \n",
       "  3   284.424438  313.322327  316.967041  369.273010    0.916181     41   \n",
       "  4   304.280792  121.822723  412.170013  281.584808    0.911828      0   \n",
       "  5   535.273438  130.924637  639.750488  397.979126    0.911194      0   \n",
       "  6   308.531799  267.388733  333.366699  308.222107    0.903738     41   \n",
       "  7   427.047485  344.467621  467.811584  411.135498    0.887367     41   \n",
       "  8   338.391510  356.009949  388.206879  425.801514    0.885046     39   \n",
       "  9   249.989807  142.204666  331.229919  240.552673    0.883445      0   \n",
       "  10  148.221283   82.161880  217.629333  126.789810    0.882482     62   \n",
       "  11    0.207787  144.838455  111.138962  346.990845    0.860249      0   \n",
       "  12  196.505630  236.566528  216.568832  264.290649    0.835152     41   \n",
       "  13  212.614136  123.392532  284.041351  217.780243    0.834749      0   \n",
       "  14  493.979889  126.105652  569.520264  197.071716    0.813629      0   \n",
       "  15  498.509125   78.235168  550.570862  152.441711    0.767977      0   \n",
       "  16  378.066833  346.247162  403.086609  422.886047    0.749656     39   \n",
       "  17  368.724213   92.410751  396.228668  171.402496    0.709950      0   \n",
       "  18  252.512039  226.493866  270.065338  255.468475    0.696557     41   \n",
       "  19  158.172653  204.892578  637.244629  423.161194    0.643624     60   \n",
       "  20  169.443604  138.316833  220.815674  191.928894    0.625058      0   \n",
       "  21  522.044250  188.387268  537.078308  203.219666    0.579391     41   \n",
       "  22  396.877747  378.399170  433.773499  426.000000    0.489847     39   \n",
       "  23  159.198059  168.450867  214.465729  205.720154    0.479404     60   \n",
       "  24  546.832947  190.364838  561.459290  206.109528    0.466456     41   \n",
       "  25  164.353683  144.819885  188.733292  178.570618    0.414093      0   \n",
       "  26  205.629410  211.746429  225.247177  248.862152    0.396124     39   \n",
       "  27  476.283356  128.666473  509.391144  174.829620    0.387410      0   \n",
       "  28  495.503326  177.005859  585.480835  241.413025    0.376233     60   \n",
       "  29    0.186390  322.911743   20.180820  416.942566    0.318784     13   \n",
       "  30  296.173584  300.714325  321.446350  322.445343    0.303477     44   \n",
       "  31  351.912262  317.518829  374.406647  367.367096    0.289469     39   \n",
       "  32  325.297516  338.614624  368.749237  417.195435    0.283926     41   \n",
       "  33  279.062439  250.604736  338.196838  272.494446    0.281578     45   \n",
       "  34  490.133545   75.111725  517.146851  137.077667    0.274052      0   \n",
       "  35  160.040375  167.776031  176.892822  186.587555    0.271635     41   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3            cup  \n",
       "  4         person  \n",
       "  5         person  \n",
       "  6            cup  \n",
       "  7            cup  \n",
       "  8         bottle  \n",
       "  9         person  \n",
       "  10            tv  \n",
       "  11        person  \n",
       "  12           cup  \n",
       "  13        person  \n",
       "  14        person  \n",
       "  15        person  \n",
       "  16        bottle  \n",
       "  17        person  \n",
       "  18           cup  \n",
       "  19  dining table  \n",
       "  20        person  \n",
       "  21           cup  \n",
       "  22        bottle  \n",
       "  23  dining table  \n",
       "  24           cup  \n",
       "  25        person  \n",
       "  26        bottle  \n",
       "  27        person  \n",
       "  28  dining table  \n",
       "  29         bench  \n",
       "  30         spoon  \n",
       "  31        bottle  \n",
       "  32           cup  \n",
       "  33          bowl  \n",
       "  34        person  \n",
       "  35           cup  ,\n",
       "  'caption': ['A table full of food trays at which officers are seated.'],\n",
       "  'bbox_target': [152.5, 203.43, 487.5, 222.57]},\n",
       " 148: {'image_emb': tensor([[ 0.0150,  0.2080,  0.1310,  ...,  0.3918, -0.1079,  0.3162],\n",
       "          [-0.3325,  0.3621,  0.1001,  ...,  0.2003, -0.1113, -0.2734],\n",
       "          [-0.0895,  0.1185, -0.2119,  ...,  0.8169,  0.0403, -0.2413],\n",
       "          [-0.1418, -0.0287, -0.0640,  ...,  0.2385,  0.1865,  0.0655]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0356,  0.0502, -0.0147,  ...,  0.0179, -0.2971, -0.0725],\n",
       "          [ 0.2196, -0.2474,  0.4548,  ..., -0.1146, -0.3115,  0.0857]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9805e-01, 3.2127e-05, 5.9605e-08, 1.7824e-03],\n",
       "          [6.1328e-01, 2.9129e-02, 9.4055e-02, 2.6367e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  273.995178  109.845718  573.531006  315.600342    0.941764      5     bus\n",
       "  1  100.693008  162.491180  245.425705  252.634796    0.898817      7   truck\n",
       "  2  317.139191  183.536377  364.611603  221.455627    0.781242      0  person\n",
       "  3  584.901306  186.822784  609.757629  247.201874    0.689067      5     bus\n",
       "  4  598.169312  182.980347  640.000000  311.411804    0.526549      5     bus\n",
       "  5  243.941925  179.184875  281.590240  246.925415    0.507156      7   truck\n",
       "  6  598.110474  183.952332  639.780029  312.931335    0.344597      7   truck,\n",
       "  'caption': ['A passenger bus with the words Waterloo 521 on the front of it.',\n",
       "   'a bus was rod'],\n",
       "  'bbox_target': [279.37, 103.55, 296.63, 204.94]},\n",
       " 149: {'image_emb': tensor([[-0.0621, -0.0382, -0.2250,  ...,  0.8975,  0.5347,  0.1498],\n",
       "          [ 0.1934,  0.0027, -0.2678,  ...,  1.1465,  0.4697, -0.0320],\n",
       "          [-0.3040,  0.5278,  0.0638,  ...,  1.0342,  0.3740, -0.0051],\n",
       "          ...,\n",
       "          [ 0.1583,  0.1376, -0.0483,  ...,  1.2637,  0.0641, -0.3623],\n",
       "          [-0.0984,  0.3433, -0.0399,  ...,  1.2520, -0.3096, -0.4741],\n",
       "          [-0.0965, -0.0884, -0.2181,  ...,  0.8364,  0.5190, -0.0503]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2025, -0.0952, -0.1847,  ..., -0.3538,  0.0807, -0.0182],\n",
       "          [-0.1163, -0.0453, -0.3691,  ..., -0.0737,  0.0483, -0.1877]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7822e-02, 9.5850e-01, 6.6340e-05, 2.2186e-02, 1.3053e-05, 7.2122e-06,\n",
       "           1.3292e-05, 8.0466e-06, 1.5583e-03],\n",
       "          [9.2822e-01, 1.6479e-02, 2.9802e-07, 3.0956e-03, 9.2983e-04, 1.3471e-05,\n",
       "           2.5630e-06, 4.1914e-04, 5.0751e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   289.985168   61.623520  561.073303  466.447205    0.951152      0   \n",
       "  1   257.399048   29.465500  414.878235  396.213440    0.919610      0   \n",
       "  2   462.942078  415.757416  510.472412  486.337280    0.910896     41   \n",
       "  3   336.315033  136.652008  359.486176  188.841522    0.816716     27   \n",
       "  4   251.753235  336.669006  308.238525  404.501831    0.808702     43   \n",
       "  5   244.785309  426.211151  423.378571  502.736450    0.804948     55   \n",
       "  6     0.077862  173.149170   41.877621  208.154907    0.786305     58   \n",
       "  7    99.752579  390.690643  280.060730  502.146851    0.758294     55   \n",
       "  8   126.986351  150.089462  201.076813  209.427948    0.655347     58   \n",
       "  9     7.010071  381.992096  607.138123  501.266663    0.508056     60   \n",
       "  10  538.214905  247.696808  639.644958  485.482910    0.408699     60   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2            cup  \n",
       "  3            tie  \n",
       "  4          knife  \n",
       "  5           cake  \n",
       "  6   potted plant  \n",
       "  7           cake  \n",
       "  8   potted plant  \n",
       "  9   dining table  \n",
       "  10  dining table  ,\n",
       "  'caption': ['A man in a suit.',\n",
       "   'Man in white and black standing holding knife with woman.'],\n",
       "  'bbox_target': [259.19, 31.04, 161.98, 364.79]},\n",
       " 150: {'image_emb': tensor([[ 0.1665,  0.2394, -0.0838,  ...,  0.6992, -0.0988, -0.0113],\n",
       "          [-0.5693,  0.5786, -0.3535,  ...,  0.5815,  0.1289, -0.0751],\n",
       "          [ 0.0718,  0.0266, -0.2081,  ...,  0.3931,  0.0934,  0.0530],\n",
       "          [ 0.1747,  0.1899, -0.4290,  ...,  0.7515, -0.1641, -0.0640],\n",
       "          [-0.1846,  0.7632, -0.2018,  ...,  0.7397, -0.1085,  0.0618],\n",
       "          [-0.2046,  0.5991, -0.0656,  ...,  0.3818, -0.0687,  0.0429]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2079, -0.1029, -0.4622,  ...,  0.2007, -0.3135, -0.1646],\n",
       "          [-0.2681,  0.2446, -0.4890,  ..., -0.0429, -0.6011, -0.2581]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.7217e-01, 1.4763e-02, 1.0014e-05, 1.8768e-03, 8.5449e-03, 2.7733e-03],\n",
       "          [9.9268e-01, 2.3117e-03, 5.3644e-07, 2.8491e-04, 1.9372e-05, 4.5280e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.238240    1.159012  275.096619  413.539459    0.902332      0   \n",
       "  1  140.538940   44.333626  499.001160  416.516449    0.890587      0   \n",
       "  2  168.022018  249.130920  221.388123  333.615112    0.846995     67   \n",
       "  3    0.022179    0.496870   80.723114  142.939865    0.814594      0   \n",
       "  4   39.986343  229.321579  115.306641  311.018311    0.796497     67   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2  cell phone  \n",
       "  3      person  \n",
       "  4  cell phone  ,\n",
       "  'caption': ['woman with ring staring into phone',\n",
       "   'A woman without a head covering looking at a phone.'],\n",
       "  'bbox_target': [138.45, 36.98, 361.55, 383.12]},\n",
       " 151: {'image_emb': tensor([[-0.0991,  0.4148, -0.4346,  ...,  1.0244,  0.5156,  0.0186],\n",
       "          [ 0.0544,  0.3574, -0.1989,  ...,  1.2812,  0.5308,  0.0209],\n",
       "          [-0.0400,  0.1201, -0.3757,  ...,  0.7700,  0.4702, -0.3467],\n",
       "          [ 0.1777,  0.0747, -0.1798,  ...,  1.1074,  0.3555,  0.2712]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0836,  0.6777, -0.2196,  ...,  0.0069,  0.0930, -0.6367],\n",
       "          [-0.1035,  0.3074, -0.4053,  ..., -0.0995,  0.1577, -0.6323]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2419, 0.5894, 0.1637, 0.0047],\n",
       "          [0.1127, 0.7236, 0.0209, 0.1426]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  113.482002  231.020721  284.788269  495.921875    0.923043     75   \n",
       "  1  432.064148  336.653687  568.424377  520.137573    0.900547     75   \n",
       "  2  301.651733  344.821259  410.129272  527.743774    0.862811     75   \n",
       "  3  395.625732  343.004425  418.356567  442.695709    0.375299     75   \n",
       "  4   13.133438    0.000000  368.537781  506.337769    0.271602     58   \n",
       "  \n",
       "             name  \n",
       "  0          vase  \n",
       "  1          vase  \n",
       "  2          vase  \n",
       "  3          vase  \n",
       "  4  potted plant  ,\n",
       "  'caption': ['The largest vase', 'The large vase on the left.'],\n",
       "  'bbox_target': [115.36, 234.53, 177.39, 262.97]},\n",
       " 152: {'image_emb': tensor([[-2.5726e-02,  2.4670e-01,  5.5664e-01,  ...,  4.8877e-01,\n",
       "           -4.0942e-01,  1.7773e-01],\n",
       "          [-7.1228e-02,  4.4189e-01,  2.7856e-01,  ...,  6.7383e-01,\n",
       "           -4.9487e-01,  6.2037e-04],\n",
       "          [ 2.8979e-01, -1.1084e-01, -4.4019e-01,  ...,  7.4756e-01,\n",
       "            1.9617e-01, -4.2285e-01],\n",
       "          [-2.8906e-01,  3.3716e-01,  2.5952e-01,  ...,  1.0830e+00,\n",
       "           -1.1389e-01, -1.3892e-01],\n",
       "          [-1.3025e-01,  6.5479e-01,  3.1519e-01,  ...,  8.7842e-01,\n",
       "           -1.5540e-01, -3.3783e-02],\n",
       "          [-1.5381e-01,  2.3389e-01,  6.0205e-01,  ...,  7.2461e-01,\n",
       "           -2.6099e-01, -8.9294e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0725, -0.0813,  0.0637,  ...,  0.5850, -0.2155,  0.1141],\n",
       "          [-0.0249,  0.1691,  0.1288,  ...,  0.8135, -0.4041, -0.3567]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.2607e-01, 9.5215e-02, 4.9973e-04, 2.4078e-02, 3.1891e-02, 1.2225e-01],\n",
       "          [8.2080e-01, 7.5195e-02, 3.8242e-04, 5.7648e-02, 1.8143e-02, 2.7664e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  391.538727  176.330200  636.483826  452.848450    0.950109      0   \n",
       "  1  224.484985  231.920776  420.437073  453.795227    0.922048      0   \n",
       "  2  197.987961  289.813477  215.641800  307.419128    0.896340     32   \n",
       "  3  214.088257  264.858124  313.634338  370.195282    0.856068     34   \n",
       "  4  328.192505  107.183258  482.567017  267.271576    0.744511      0   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2   sports ball  \n",
       "  3  baseball bat  \n",
       "  4        person  ,\n",
       "  'caption': ['An umpire that is behind the catcher and batter.',\n",
       "   'an umpire for a baseball game'],\n",
       "  'bbox_target': [394.22, 176.0, 243.99, 274.23]},\n",
       " 153: {'image_emb': tensor([[-0.2791,  0.3708,  0.1107,  ...,  0.6685,  0.0117, -0.0101],\n",
       "          [ 0.3333,  0.1991, -0.3459,  ...,  1.2402,  0.2605, -0.2271],\n",
       "          [ 0.2158,  0.2637, -0.1698,  ...,  0.5273, -0.0450, -0.1595],\n",
       "          [-0.0021,  0.1620, -0.0138,  ...,  0.3005, -0.0515, -0.2693]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-1.4783e-01,  1.8188e-01, -2.7295e-01,  ..., -3.5767e-01,\n",
       "            2.3425e-01,  2.0886e-01],\n",
       "          [ 1.7834e-01,  4.3848e-01, -1.9592e-01,  ..., -5.6299e-01,\n",
       "            5.5122e-04, -8.4961e-02]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.2432e-01, 4.0054e-05, 2.5024e-02, 5.0537e-02],\n",
       "          [9.8682e-01, 1.2767e-04, 1.4381e-03, 1.1490e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    0.443130  113.070663  306.355652  421.426697    0.955546      0  person\n",
       "  1  438.841217  201.548584  472.110809  253.126892    0.857266     33    kite\n",
       "  2  527.629883  316.361816  555.530273  353.084229    0.851051      0  person\n",
       "  3  191.741959  179.120697  252.372849  287.802338    0.605699      0  person,\n",
       "  'caption': ['The boy with the blue jacket standing behind a girl.',\n",
       "   'Two people standing together facing away from the viewer, one of whom has a blue jacket and glasses and the other has long hair'],\n",
       "  'bbox_target': [0.0, 112.27, 286.91, 309.93]},\n",
       " 154: {'image_emb': tensor([[ 0.2659, -0.1456,  0.1617,  ...,  0.1746,  0.0014, -0.0883],\n",
       "          [-0.0397,  0.1100,  0.1278,  ...,  1.2578, -0.1293, -0.2076],\n",
       "          [ 0.3083, -0.2247,  0.2052,  ...,  0.0299, -0.0594, -0.1454]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1766,  0.0182, -0.2155,  ...,  0.4980, -0.4753, -0.1115],\n",
       "          [ 0.4526, -0.2074, -0.0321,  ..., -0.1564, -0.2827,  0.0062]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2230, 0.4722, 0.3049],\n",
       "          [0.2429, 0.0089, 0.7480]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class name\n",
       "  0    6.312990  64.485443  439.375458  477.593719    0.937439     15  cat\n",
       "  1  150.165649  64.365974  456.403870  211.791534    0.863515     15  cat,\n",
       "  'caption': ['a white and brown coloured cat is sitting in front of a mirror',\n",
       "   'a cat was seeing out side cat'],\n",
       "  'bbox_target': [14.45, 57.81, 432.52, 416.0]},\n",
       " 155: {'image_emb': tensor([[ 0.0195,  0.2871,  0.5732,  ...,  0.3696, -0.5078, -0.2690],\n",
       "          [ 0.1956, -0.0767,  0.2452,  ...,  0.5625,  0.0829, -0.1799],\n",
       "          [-0.1836,  0.0199,  0.1560,  ...,  0.0261, -0.4597, -0.2634]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.5962, -0.1758,  0.4390,  ...,  0.0439, -0.0424, -0.0181],\n",
       "          [-0.0334,  0.1272,  0.2124,  ...,  0.6211, -0.1274, -0.2262]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.1080e-04, 9.9902e-01, 4.1080e-04],\n",
       "          [1.3220e-01, 7.4902e-01, 1.1853e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  331.562500   80.713928  625.696899  368.146606    0.943573     17  horse\n",
       "  1   87.652473  103.750412  260.805603  367.800354    0.921105     17  horse,\n",
       "  'caption': ['A baby mule', 'The baby horse is dark brown and tan.'],\n",
       "  'bbox_target': [92.51, 102.93, 171.64, 259.02]},\n",
       " 156: {'image_emb': tensor([[ 0.3142,  0.3269, -0.0889,  ...,  0.9053,  0.2181,  0.1077],\n",
       "          [-0.2354,  0.7983, -0.2382,  ...,  0.8701,  0.1503,  0.3289],\n",
       "          [ 0.0757,  0.3152, -0.2058,  ...,  0.6250,  0.4683, -0.0874],\n",
       "          ...,\n",
       "          [ 0.0726, -0.1458,  0.1162,  ...,  0.8159,  0.2668,  0.0125],\n",
       "          [ 0.2013,  0.0695, -0.2179,  ...,  1.0625, -0.0839, -0.0703],\n",
       "          [ 0.3010,  0.3093, -0.0577,  ...,  0.5771,  0.7310, -0.0387]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0603, -0.0612,  0.1981,  ...,  0.2881,  0.1285, -0.2042],\n",
       "          [-0.1525, -0.1267,  0.2605,  ...,  0.2454,  0.1434, -0.1221]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.0689e-03, 3.1299e-01, 5.5811e-01, 8.0872e-03, 3.4428e-04, 2.0447e-03,\n",
       "           1.1517e-01],\n",
       "          [6.4373e-04, 8.6475e-01, 1.2659e-01, 9.8705e-05, 1.3590e-05, 4.4250e-04,\n",
       "           7.2517e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   136.951736  118.426941  287.639862  273.381470    0.946574      0   \n",
       "  1     0.777739  225.218231  183.696457  484.708679    0.928047      0   \n",
       "  2    82.248764  227.260590  332.372192  499.371246    0.900605      1   \n",
       "  3   251.544769   61.527695  318.333923  138.038498    0.879629      0   \n",
       "  4   199.106415  111.781487  226.054520  136.469620    0.807466      0   \n",
       "  5    81.473885  217.927216  116.438202  252.501633    0.719199      0   \n",
       "  6   303.532990  125.000656  325.611267  146.045059    0.659771     36   \n",
       "  7    34.489124  251.613831   56.009548  268.312897    0.544902      0   \n",
       "  8   201.072815  133.212296  252.757568  175.785004    0.527727     26   \n",
       "  9    56.547363  238.574768   74.852112  255.868561    0.515662      0   \n",
       "  10   58.402710   88.263367   83.018356  117.841484    0.413598      9   \n",
       "  11   99.876610  210.406189  117.925522  235.256607    0.341631      0   \n",
       "  12   73.602943  225.926712   93.805054  252.720047    0.305904      0   \n",
       "  13   56.485252  238.947800   83.763962  262.815033    0.286014      1   \n",
       "  14   76.328018  236.589172  102.449440  259.724701    0.268709      1   \n",
       "  \n",
       "               name  \n",
       "  0          person  \n",
       "  1          person  \n",
       "  2         bicycle  \n",
       "  3          person  \n",
       "  4          person  \n",
       "  5          person  \n",
       "  6      skateboard  \n",
       "  7          person  \n",
       "  8         handbag  \n",
       "  9          person  \n",
       "  10  traffic light  \n",
       "  11         person  \n",
       "  12         person  \n",
       "  13        bicycle  \n",
       "  14        bicycle  ,\n",
       "  'caption': ['the bicycle that is closest to the camera',\n",
       "   'A silver bicycle.'],\n",
       "  'bbox_target': [52.27, 225.09, 280.73, 274.91]},\n",
       " 157: {'image_emb': tensor([[ 0.0565,  0.6602, -0.0635,  ...,  1.1650, -0.2031, -0.0470],\n",
       "          [-0.1737,  0.7822, -0.4294,  ...,  0.4944, -0.2411,  0.1798],\n",
       "          [-0.1753,  0.6699, -0.5142,  ...,  0.5527,  0.0866,  0.1694],\n",
       "          [ 0.0945,  0.6387,  0.1389,  ...,  1.4053, -0.0077, -0.0218],\n",
       "          [-0.0090,  0.4795, -0.1017,  ...,  1.1914, -0.2900, -0.0268]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2118,  0.1415, -0.0139,  ...,  0.1913,  0.3093, -0.1711],\n",
       "          [-0.1409,  0.2812,  0.0780,  ..., -0.3359, -0.0225, -0.0225]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.1594e-02, 5.5552e-04, 2.1286e-03, 9.0039e-01, 2.5543e-02],\n",
       "          [1.1102e-01, 4.0436e-03, 1.5030e-02, 7.5879e-01, 1.1102e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   27.646212   28.559475  538.475037  296.444824    0.919157      0   \n",
       "  1  364.355377   98.480728  470.393982  237.176071    0.907336     67   \n",
       "  2  281.171295  423.430695  335.192139  491.948181    0.884376     67   \n",
       "  3   25.088648  310.741882  401.961578  596.764526    0.875031      0   \n",
       "  4  483.535004  323.978180  506.840790  356.597900    0.680777     67   \n",
       "  5  325.142700  541.011292  377.491791  595.034058    0.608379     39   \n",
       "  6  321.253693  303.057526  504.364197  480.657501    0.536566      0   \n",
       "  7  247.929169  305.916779  505.194061  565.568054    0.403115     56   \n",
       "  8  286.934570  490.250214  376.654785  594.678284    0.274782     24   \n",
       "  9  141.297287  470.906494  377.780579  595.696228    0.259574     26   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1  cell phone  \n",
       "  2  cell phone  \n",
       "  3      person  \n",
       "  4  cell phone  \n",
       "  5      bottle  \n",
       "  6      person  \n",
       "  7       chair  \n",
       "  8    backpack  \n",
       "  9     handbag  ,\n",
       "  'caption': ['this is a man with glasses and a plaid shirt',\n",
       "   'A man wearing glasses'],\n",
       "  'bbox_target': [23.23, 311.49, 392.28, 284.18]},\n",
       " 158: {'image_emb': tensor([[-0.0220,  0.6157, -0.0425,  ...,  0.9395, -0.2267, -0.0973],\n",
       "          [-0.2394,  0.8882, -0.3545,  ...,  1.0049, -0.0488,  0.3049],\n",
       "          [-0.1691,  0.3066,  0.0527,  ...,  0.9263, -0.1947,  0.1609],\n",
       "          ...,\n",
       "          [ 0.0649,  0.3313, -0.3103,  ...,  0.8730, -0.3418, -0.1337],\n",
       "          [ 0.0501,  0.4373, -0.1755,  ...,  0.9312,  0.1108, -0.3440],\n",
       "          [ 0.3491,  0.2137, -0.2115,  ...,  0.5542, -0.1169, -0.3394]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-3.0249e-01, -2.1095e-03,  1.9763e-01,  1.8079e-01, -5.3589e-02,\n",
       "            1.0803e-01, -1.5930e-02, -7.0898e-01, -1.1066e-01,  1.0132e-01,\n",
       "            1.9092e-01, -1.9852e-02,  1.6150e-01, -1.9699e-02, -2.9251e-02,\n",
       "           -5.3864e-02,  7.2083e-02, -3.9215e-02, -6.6956e-02,  3.2788e-01,\n",
       "           -2.0276e-01,  2.6953e-01,  2.7368e-01,  3.5767e-01,  1.9006e-01,\n",
       "            9.6924e-02, -1.7786e-01, -4.4360e-01,  2.3901e-01, -1.7554e-01,\n",
       "           -1.7224e-01,  1.1292e-01,  8.1543e-02, -5.7520e-01, -3.3984e-01,\n",
       "           -2.8784e-01,  4.0210e-01, -3.7207e-01, -8.1055e-02,  1.4661e-01,\n",
       "            1.7834e-01,  2.0862e-01,  3.2715e-01,  2.1591e-02, -3.3032e-01,\n",
       "            4.1931e-02,  5.8350e-01,  3.3789e-01,  1.0918e-02, -3.6035e-01,\n",
       "           -6.5479e-01,  1.3817e-02, -1.4929e-01, -1.7297e-01, -2.1164e-02,\n",
       "           -8.7463e-02, -7.3853e-02, -1.2457e-01, -3.7524e-01,  1.4087e-01,\n",
       "           -1.1340e-01,  2.9761e-01,  2.4890e-01, -3.5034e-02, -7.8369e-02,\n",
       "            1.3062e-01,  5.2100e-01,  5.3418e-01, -5.5206e-02,  1.7432e-01,\n",
       "            4.9634e-01, -1.7993e-01,  3.8208e-01, -9.3689e-02, -5.1758e-02,\n",
       "           -5.1765e-03, -2.8711e-01,  1.2207e-01,  3.9307e-01,  5.1758e-01,\n",
       "           -8.3435e-02,  8.5388e-02,  6.5308e-02,  1.6638e-01, -7.2449e-02,\n",
       "            3.3105e-01,  1.3245e-01, -2.3169e-01, -6.7993e-02,  1.7261e-01,\n",
       "           -5.6580e-02,  1.3867e-01, -1.1182e+00,  1.8188e-01, -4.3848e-01,\n",
       "           -3.7207e-01, -2.2888e-01, -3.6157e-01,  3.0786e-01,  2.5406e-03,\n",
       "           -1.3538e-01,  2.9565e-01,  2.9419e-02, -1.6113e-01, -5.3857e-01,\n",
       "           -2.4646e-01,  2.3303e-01, -4.2163e-01,  7.9407e-02,  1.6675e-01,\n",
       "           -1.5442e-01,  3.5669e-01,  4.1040e-01,  1.6626e-01, -6.0840e-01,\n",
       "           -5.6519e-02,  3.0151e-01,  9.0698e-02, -3.1647e-02, -3.3252e-01,\n",
       "            3.0930e-02, -6.3721e-01,  1.5234e-01,  2.4634e-01, -3.3862e-01,\n",
       "            3.6060e-01, -3.8232e-01,  8.8379e-02, -2.3865e-01,  1.6125e-01,\n",
       "           -3.5815e-01,  2.2974e-01, -4.1821e-01,  4.4727e+00, -3.0054e-01,\n",
       "           -3.7866e-01, -2.2412e-01,  8.8379e-02, -3.7280e-01, -4.1309e-01,\n",
       "            4.0503e-01,  3.8452e-01, -5.0888e-03,  1.5234e-01,  2.2522e-01,\n",
       "           -5.3711e-02,  4.8187e-02, -3.0542e-01, -3.2495e-01,  2.2980e-02,\n",
       "           -4.2334e-01,  4.7388e-01,  1.4734e-01,  1.6479e-01,  3.2495e-01,\n",
       "           -3.0591e-01, -2.7359e-02,  8.0200e-02,  1.0828e-01,  5.7190e-02,\n",
       "            2.2205e-01, -8.0811e-01,  1.8787e-01,  4.2310e-01,  9.9731e-02,\n",
       "           -2.1387e-01,  2.0215e-01,  2.1423e-02,  1.8616e-01,  3.3716e-01,\n",
       "            7.6843e-02, -3.4937e-01,  3.2739e-01, -1.0443e-01, -3.2257e-02,\n",
       "            9.5154e-02, -2.8174e-01,  1.2451e-01,  6.2744e-01,  1.6125e-01,\n",
       "            4.2816e-02,  4.9561e-02,  8.9417e-03, -1.8726e-01, -1.2000e-01,\n",
       "            1.2927e-01, -4.8409e-03,  1.0437e-01,  2.9761e-01,  1.9031e-01,\n",
       "            2.9980e-01,  2.1387e-01,  2.3010e-01,  1.8323e-01,  2.1387e-01,\n",
       "            2.9150e-01, -4.4287e-01, -6.3538e-02, -2.7271e-01, -1.6876e-02,\n",
       "           -2.0776e-01,  2.2351e-01,  4.9854e-01,  6.9031e-02, -2.5269e-01,\n",
       "            3.3057e-01,  4.2285e-01, -2.7466e-01, -7.8552e-02,  7.6218e-03,\n",
       "            5.0323e-02, -1.1703e-02, -1.3599e-01, -3.0005e-01,  3.4576e-02,\n",
       "           -1.4122e-02, -1.6895e-01, -1.9119e-02,  6.2939e-01,  8.4900e-02,\n",
       "            3.2520e-01,  2.2595e-01,  1.4282e-01,  9.8816e-02,  5.9509e-02,\n",
       "            7.7881e-02,  1.5640e-02, -2.1545e-02,  8.3557e-02, -1.5601e-01,\n",
       "           -1.8127e-01,  3.0933e-01,  2.3059e-01, -4.0210e-01, -1.1005e-01,\n",
       "           -3.2074e-02,  1.8042e-01,  1.1096e-01, -2.3914e-01, -1.3440e-01,\n",
       "            2.9126e-01,  9.9365e-02, -3.1665e-01, -3.8422e-02, -2.6978e-01,\n",
       "           -2.4597e-01,  2.3328e-01, -8.8318e-02,  6.4502e-01, -2.5146e-01,\n",
       "           -1.7212e-01,  2.1179e-02, -1.0077e-01, -6.1768e-01, -1.8372e-01,\n",
       "           -5.3070e-02,  3.7573e-01,  3.6304e-01, -1.1621e-01,  2.0166e-01,\n",
       "           -7.7248e-03, -4.3750e-01, -1.3269e-01, -9.6619e-02,  1.0724e-01,\n",
       "           -6.9971e-01, -1.6565e-01, -1.0748e-01, -2.7271e-01,  3.8666e-02,\n",
       "           -2.6306e-02,  4.6191e-01, -5.4785e-01,  6.7993e-02, -1.0242e-01,\n",
       "            7.6294e-02,  2.8320e-01,  5.3320e-01, -9.0210e-02,  3.4521e-01,\n",
       "           -2.5952e-01, -4.2969e-01,  3.1567e-01,  4.9585e-01, -3.6682e-02,\n",
       "           -1.8750e-01,  4.4739e-02,  8.7402e-02, -3.7720e-02, -1.0962e-01,\n",
       "            1.3708e-01, -3.0664e-01, -8.9172e-02,  8.0811e-02,  9.8450e-02,\n",
       "           -2.8076e-02, -1.1230e-01, -1.9556e-01, -3.9014e-01, -5.0537e-02,\n",
       "           -3.5010e-01, -3.5571e-01,  1.7639e-01,  1.4709e-01,  2.9419e-01,\n",
       "            2.6392e-01,  8.5632e-02,  2.2009e-01, -7.2632e-02,  1.4465e-01,\n",
       "           -2.8735e-01,  4.5825e-01,  4.4688e+00, -1.3062e-01,  1.6724e-02,\n",
       "            2.0117e-01,  1.6650e-01, -4.8141e-03,  2.8442e-01,  2.4829e-01,\n",
       "            6.0498e-01,  7.4463e-02,  1.7773e-01,  9.9243e-02, -6.2195e-02,\n",
       "            2.0459e-01,  1.7944e-01,  2.3315e-01, -1.6846e-01, -1.5088e+00,\n",
       "           -5.5664e-01, -5.4016e-02,  1.1401e-01,  3.2715e-02,  2.6855e-01,\n",
       "           -2.5589e-02, -1.2189e-01,  1.6223e-01,  2.5024e-01, -1.1420e-01,\n",
       "           -4.6924e-01, -1.4734e-01,  4.1797e-01,  3.2104e-02,  4.3068e-03,\n",
       "           -7.4402e-02,  3.3057e-01, -9.1125e-02, -4.8096e-01,  2.9761e-01,\n",
       "           -3.4058e-02, -6.2012e-01,  4.7119e-01,  3.9978e-02, -4.2480e-02,\n",
       "           -3.4106e-01, -9.9854e-02, -1.3013e-01,  6.4795e-01,  4.7681e-01,\n",
       "           -2.6221e-01, -3.1787e-01,  7.3853e-02,  4.2041e-01,  3.4790e-01,\n",
       "            7.5256e-02,  1.6321e-01,  1.9910e-01,  2.5415e-01, -8.0200e-02,\n",
       "           -3.3984e-01, -1.5723e-01,  2.4097e-01,  2.0593e-01, -1.8713e-01,\n",
       "           -2.6611e-01, -2.4646e-01, -6.6284e-02, -1.6577e-01, -3.3105e-01,\n",
       "           -4.5929e-02, -5.6836e-01,  4.1771e-03, -1.2915e-01,  2.3120e-01,\n",
       "           -1.2128e-01,  7.6721e-02,  1.4221e-01, -3.9893e-01, -1.9629e-01,\n",
       "           -3.9746e-01,  1.5601e-01, -8.1055e-02, -1.7126e-01,  1.6724e-01,\n",
       "           -1.3489e-01,  4.2603e-02, -2.8296e-01, -8.0200e-02,  4.1968e-01,\n",
       "            1.4331e-01, -3.6890e-01,  1.7432e-01, -1.7542e-01, -3.6108e-01,\n",
       "            3.5815e-01,  1.4854e-02,  1.8652e-01,  6.0498e-01, -2.7979e-01,\n",
       "           -1.7175e-01, -1.0260e-01,  2.9688e-01,  5.5054e-02, -4.7379e-03,\n",
       "            3.3667e-01,  3.0228e-02, -3.0566e-01,  6.1328e-01,  1.5015e-02,\n",
       "            4.6777e-01, -5.5127e-01, -1.4807e-01, -1.3403e-01,  1.3403e-01,\n",
       "            2.5513e-02, -2.6880e-01,  6.4392e-02,  1.3770e-01, -1.5381e-02,\n",
       "           -3.6694e-01, -3.0640e-02,  1.2213e-01, -9.5764e-02,  1.7271e-03,\n",
       "           -6.6147e-03,  1.1353e-01,  2.9053e-01, -6.3049e-02, -2.5806e-01,\n",
       "            2.8992e-02, -2.7612e-01,  2.8174e-01,  4.2053e-02,  3.8452e-01,\n",
       "            1.4282e-01, -3.3154e-01, -2.8711e-01,  8.7952e-02,  1.2915e-01,\n",
       "           -9.0210e-02, -5.8319e-02,  6.5002e-02,  3.2166e-02, -4.8920e-02,\n",
       "            4.8157e-02,  9.4299e-02, -2.5366e-01,  2.2070e-01, -3.1665e-01,\n",
       "           -2.9614e-01, -1.8066e-01, -7.1533e-02,  4.9896e-02,  1.7746e-02,\n",
       "           -1.7676e-01, -9.1553e-03, -1.1938e-01, -9.7656e-02, -3.3276e-01,\n",
       "           -3.5400e-01,  2.3511e-01, -1.7532e-02,  7.3242e-02,  1.1121e-01,\n",
       "            1.8066e-01, -7.8064e-02,  2.9150e-01, -1.0187e-01,  4.4092e-01,\n",
       "            8.8120e-03, -3.1250e-01,  3.8910e-03,  1.1017e-01, -2.3544e-02,\n",
       "            1.5198e-01, -1.6431e-01, -4.9829e-01, -4.5239e-01, -1.2903e-01,\n",
       "           -3.3521e-01, -9.4666e-02,  2.4988e-01,  8.6426e-01,  4.5044e-01,\n",
       "           -3.4676e-03,  2.0410e-01,  5.4834e-01, -2.1265e-01, -9.3445e-02,\n",
       "            1.6052e-01,  1.8677e-02,  3.6694e-01, -4.0039e-02,  1.1377e-01,\n",
       "           -2.2034e-01,  3.7183e-01,  5.0415e-02,  2.5317e-01,  2.6831e-01,\n",
       "           -1.0565e-01, -5.5127e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.3915e-05, 2.2437e-01, 1.4126e-05, 2.0752e-01, 1.9073e-06, 1.7405e-03,\n",
       "           1.9608e-02, 5.4688e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  164.707458    2.499329  310.914124  132.446472    0.932569     41   \n",
       "  1    1.752602  193.172150  195.926392  320.353607    0.903140     45   \n",
       "  2  411.201202   69.175537  530.850464  163.465973    0.875964     41   \n",
       "  3  267.921509  242.427490  625.236633  475.606018    0.870808     48   \n",
       "  4   28.149227   11.878876  105.834213   51.307617    0.869798     67   \n",
       "  5  144.405258  306.677460  412.591919  478.053528    0.862607     48   \n",
       "  6    1.390884  302.291931  224.161469  478.429504    0.821433     48   \n",
       "  7    0.560364    0.000000  639.402832  482.000000    0.683056     60   \n",
       "  8   46.125122   91.951477  264.288086  141.755188    0.502498     44   \n",
       "  9    0.309498  331.018005   62.156788  479.118408    0.428926     48   \n",
       "  \n",
       "             name  \n",
       "  0           cup  \n",
       "  1          bowl  \n",
       "  2           cup  \n",
       "  3      sandwich  \n",
       "  4    cell phone  \n",
       "  5      sandwich  \n",
       "  6      sandwich  \n",
       "  7  dining table  \n",
       "  8         spoon  \n",
       "  9      sandwich  ,\n",
       "  'caption': ['The quesadilla directly in front of the bowl of jam.'],\n",
       "  'bbox_target': [1.08, 303.11, 209.19, 172.33]},\n",
       " 159: {'image_emb': tensor([[-0.3389, -0.0052,  0.1400,  ...,  0.7710,  0.3450, -0.2976],\n",
       "          [-0.3271,  0.0512, -0.0416,  ...,  0.6860,  0.4524, -0.2776]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0450, -0.2380, -0.3853,  ...,  0.1176,  0.0454, -0.3386],\n",
       "          [-0.0191, -0.0821, -0.1209,  ..., -0.1586, -0.1326, -0.2747]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5889, 0.4111],\n",
       "          [0.4495, 0.5508]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  224.922623   50.761154  498.416626  404.247559    0.951339      0  person\n",
       "  1    0.920670    0.206070  411.705505  401.703247    0.576133     69    oven\n",
       "  2    0.501129    2.499847  173.127441  399.964111    0.404011     69    oven\n",
       "  3  533.497314  292.485107  629.215576  312.313965    0.334024     71    sink,\n",
       "  'caption': ['the racks in the oven', 'An open oven.'],\n",
       "  'bbox_target': [167.44, 5.12, 54.05, 297.79]},\n",
       " 160: {'image_emb': tensor([[-5.4108e-02,  1.4478e-01,  4.4922e-01,  ...,  9.8877e-01,\n",
       "           -1.9547e-02,  5.2704e-02],\n",
       "          [-1.1615e-01, -3.3844e-02,  2.4988e-01,  ...,  1.2354e+00,\n",
       "            2.3169e-01, -1.0297e-01],\n",
       "          [-1.1548e-01,  3.4839e-01, -4.5728e-01,  ...,  1.2627e+00,\n",
       "           -5.1422e-02, -2.0630e-01],\n",
       "          [-3.1494e-01,  4.8389e-01, -4.7073e-03,  ...,  8.8135e-01,\n",
       "            2.4878e-01, -2.2131e-01],\n",
       "          [-1.7163e-01,  2.6611e-01, -3.9526e-01,  ...,  1.3359e+00,\n",
       "           -2.4939e-04, -2.4139e-02],\n",
       "          [ 1.1011e-01, -1.9495e-01,  3.7158e-01,  ...,  9.0625e-01,\n",
       "           -2.2754e-01, -2.3621e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0078,  0.0467, -0.0682,  ...,  0.3462, -0.1144,  0.0396],\n",
       "          [ 0.0925,  0.3569, -0.2991,  ..., -0.1116, -0.2900, -0.4417]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.4734e-02, 6.1218e-02, 5.9426e-05, 6.5565e-07, 3.7789e-05, 9.1406e-01],\n",
       "          [9.3445e-02, 4.5090e-03, 8.0713e-01, 7.8344e-04, 9.3445e-02, 7.0238e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  277.175476  160.332092  639.015991  423.375183    0.953841      0   \n",
       "  1    0.659531    0.000000  298.098999  421.144409    0.944269     19   \n",
       "  2  552.138428  100.624435  598.232300  231.740051    0.884804      0   \n",
       "  3  341.443268  171.674988  433.319489  237.743652    0.884520      3   \n",
       "  4  602.166016   68.149506  640.000000  243.547211    0.846315      0   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1         cow  \n",
       "  2      person  \n",
       "  3  motorcycle  \n",
       "  4      person  ,\n",
       "  'caption': ['The woman wearing a stethoscope, that is measuring the heart rate of the cow.',\n",
       "   'A woman wearing a baseball cap.'],\n",
       "  'bbox_target': [278.37, 160.2, 361.24, 266.8]},\n",
       " 161: {'image_emb': tensor([[-0.3354,  0.7285, -0.0328,  ...,  0.6196, -0.2949,  0.0344],\n",
       "          [-0.0038,  0.1763, -0.4167,  ...,  0.6016, -0.2993, -0.2247],\n",
       "          [-0.1759,  0.0091, -0.0488,  ...,  0.9639, -0.2620, -0.1254],\n",
       "          ...,\n",
       "          [-0.0531,  0.2881, -0.1218,  ...,  1.3818, -0.2155,  0.2126],\n",
       "          [-0.2004,  0.0102, -0.2239,  ...,  0.8149, -0.1970,  0.1503],\n",
       "          [-0.1859,  0.3164, -0.2284,  ...,  0.6235, -0.0764, -0.0929]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0020,  0.1445, -0.1606,  ..., -0.1556, -0.2678,  0.2537],\n",
       "          [-0.2073,  0.2812, -0.0061,  ...,  0.1382, -0.2314,  0.2449]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.5244e-01, 4.7363e-02, 3.2562e-02, 4.3121e-02, 5.0415e-02, 5.3139e-03,\n",
       "           2.7847e-02, 3.5187e-02, 5.6572e-03],\n",
       "          [9.7705e-01, 2.5387e-03, 2.0279e-02, 3.5763e-07, 2.0623e-05, 5.3644e-06,\n",
       "           1.1921e-07, 0.0000e+00, 2.3973e-04]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   315.473022  244.349670  459.594177  443.713989    0.932565     63   \n",
       "  1    28.911385  162.625916  185.709442  307.843018    0.918944     63   \n",
       "  2   337.714783   79.096024  530.700073  239.613831    0.901279     63   \n",
       "  3     0.474792    0.173744  131.400955  156.308868    0.844289      0   \n",
       "  4   506.275421    0.193954  639.961792  273.563293    0.831755      0   \n",
       "  5   192.743134    0.000000  431.924408  164.603516    0.806061      0   \n",
       "  6   468.636993    0.021042  543.516174   81.453415    0.747004     56   \n",
       "  7   588.010315  258.283478  639.875183  475.077759    0.707311     56   \n",
       "  8     0.023392  387.045288   24.455181  459.014954    0.474514      0   \n",
       "  9   320.311340  345.875824  451.076477  401.918854    0.359769     66   \n",
       "  10  119.312653    0.594437  178.878418  125.251663    0.330992      0   \n",
       "  11  422.702698   57.395935  459.097595  114.869141    0.318127     56   \n",
       "  \n",
       "          name  \n",
       "  0     laptop  \n",
       "  1     laptop  \n",
       "  2     laptop  \n",
       "  3     person  \n",
       "  4     person  \n",
       "  5     person  \n",
       "  6      chair  \n",
       "  7      chair  \n",
       "  8     person  \n",
       "  9   keyboard  \n",
       "  10    person  \n",
       "  11     chair  ,\n",
       "  'caption': ['Close laptop.', 'A closed green and white laptop.'],\n",
       "  'bbox_target': [29.84, 165.53, 160.42, 141.77]},\n",
       " 162: {'image_emb': tensor([[ 0.0716, -0.2164,  0.0068,  ...,  0.9023, -0.0215,  0.1097],\n",
       "          [-0.1425,  0.1586, -0.0109,  ...,  0.4990, -0.0367,  0.2966],\n",
       "          [-0.4934,  0.4109,  0.0492,  ...,  0.8301, -0.2234,  0.1047],\n",
       "          ...,\n",
       "          [-0.1736,  0.3606, -0.1357,  ...,  1.2178,  0.2690, -0.1826],\n",
       "          [-0.2920,  0.0114, -0.2754,  ...,  1.0439, -0.2173,  0.0245],\n",
       "          [ 0.0626, -0.0300, -0.1428,  ...,  0.5186,  0.2717, -0.2395]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 6.7139e-02,  8.6670e-02, -4.7207e-04,  ..., -1.5601e-01,\n",
       "           -8.1665e-02, -4.2505e-01],\n",
       "          [-2.8784e-01,  7.5439e-02,  1.2732e-01,  ..., -1.7365e-02,\n",
       "            1.6333e-01, -5.3906e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.9504e-05, 4.0126e-04, 2.1935e-05, 3.4308e-04, 1.9252e-04, 9.3689e-02,\n",
       "           7.1436e-01, 2.9938e-02, 5.3978e-04, 1.0550e-05, 4.2236e-02, 1.1841e-01],\n",
       "          [5.4777e-05, 1.0500e-03, 1.4436e-04, 1.5366e-04, 3.4094e-04, 7.6818e-04,\n",
       "           4.4385e-01, 7.4863e-05, 2.7823e-04, 2.5868e-05, 8.8406e-04, 5.5225e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   158.284851   65.923904  407.029541  434.869141    0.924868      0   \n",
       "  1     0.052536   17.624359  159.995041  355.307648    0.924300      0   \n",
       "  2   183.400696  445.812225  304.977600  618.881775    0.906861     41   \n",
       "  3   145.066437    0.562134  294.809845  147.657455    0.906760      0   \n",
       "  4   115.978119    1.507484  185.841827  146.001068    0.851287      0   \n",
       "  5     0.486076  561.165039  186.424377  639.681396    0.825815     53   \n",
       "  6     1.050354  348.375549  476.147888  636.549622    0.789683     60   \n",
       "  7     0.000000  356.681274  266.040161  492.897034    0.773492     53   \n",
       "  8     1.546921  387.758179  171.168930  419.091187    0.731683     42   \n",
       "  9   319.589966   30.961823  371.336487  176.532867    0.720917      0   \n",
       "  10  397.764862  317.934967  479.609558  393.839020    0.702546     13   \n",
       "  11    0.109283  256.112854  122.514038  358.584167    0.626506     13   \n",
       "  12  376.550415   28.127613  479.785950  109.267075    0.593162      3   \n",
       "  13  334.333832  168.132965  479.635986  248.370026    0.585194     60   \n",
       "  14  131.492935  143.030746  208.540771  182.011490    0.329062     60   \n",
       "  15  148.759537  143.136536  165.270187  165.643524    0.274735     41   \n",
       "  16   90.476059  353.024109  211.419312  408.976807    0.258809     53   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2            cup  \n",
       "  3         person  \n",
       "  4         person  \n",
       "  5          pizza  \n",
       "  6   dining table  \n",
       "  7          pizza  \n",
       "  8           fork  \n",
       "  9         person  \n",
       "  10         bench  \n",
       "  11         bench  \n",
       "  12    motorcycle  \n",
       "  13  dining table  \n",
       "  14  dining table  \n",
       "  15           cup  \n",
       "  16         pizza  ,\n",
       "  'caption': ['A table with some pizza on it.',\n",
       "   'white table with pizza, cups and plates on it being used by a young boy'],\n",
       "  'bbox_target': [1.43, 331.48, 478.57, 301.35]},\n",
       " 163: {'image_emb': tensor([[ 0.4146,  0.3987,  0.1451,  ...,  0.0668,  0.0840,  0.4341],\n",
       "          [-0.2622,  0.3550, -0.2822,  ...,  0.9429, -0.1520, -0.2393],\n",
       "          [ 0.3315,  0.3633, -0.1332,  ...,  0.4351, -0.1713, -0.1266],\n",
       "          [-0.2225,  0.2220, -0.3872,  ...,  0.9478,  0.0953, -0.1061],\n",
       "          [ 0.2379,  0.4104,  0.1213,  ..., -0.2456,  0.2744,  0.0942]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2520,  0.2173, -0.1294,  ...,  0.7583, -0.4834, -0.4395],\n",
       "          [-0.1818,  0.1008, -0.2108,  ...,  0.8438, -0.6431, -0.3047]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2217, 0.1987, 0.2019, 0.2473, 0.1304],\n",
       "          [0.4131, 0.1776, 0.0951, 0.0752, 0.2390]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  103.108536    7.570557  525.644287  635.335327    0.948758      0   \n",
       "  1   22.748856  461.325012  497.432739  579.771301    0.869355     34   \n",
       "  2  241.690918    0.000000  397.519043  157.931488    0.724106      0   \n",
       "  3  400.366699  391.213257  476.236145  483.864868    0.701054     35   \n",
       "  4  241.119080  452.364014  326.989075  532.809143    0.475797     35   \n",
       "  5  119.116196  452.849487  434.988953  569.055847    0.450779     35   \n",
       "  \n",
       "               name  \n",
       "  0          person  \n",
       "  1    baseball bat  \n",
       "  2          person  \n",
       "  3  baseball glove  \n",
       "  4  baseball glove  \n",
       "  5  baseball glove  ,\n",
       "  'caption': ['BASEBALL PLAYER UP CLSOE', 'baseball player'],\n",
       "  'bbox_target': [97.58, 6.6, 429.06, 633.4]},\n",
       " 164: {'image_emb': tensor([[ 0.0768,  0.0637,  0.5073,  ...,  0.4241, -0.0945, -0.0471],\n",
       "          [-0.2025,  0.2983, -0.0152,  ...,  1.1035, -0.1978, -0.0920],\n",
       "          [-0.1271,  0.3125,  0.1434,  ...,  0.9512, -0.0640, -0.2307],\n",
       "          [ 0.1437,  0.0226, -0.3418,  ...,  1.0029, -0.0406, -0.0594],\n",
       "          [-0.3794,  0.6343, -0.0080,  ...,  1.3516, -0.2192, -0.1099],\n",
       "          [-0.0403,  0.0128,  0.4775,  ...,  0.4465, -0.0858, -0.1210]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0656, -0.0999,  0.0479,  ..., -0.2612, -0.3220,  0.0363],\n",
       "          [-0.1686, -0.2196, -0.0249,  ...,  0.2167, -0.4531,  0.0439]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.2482e-02, 1.0025e-02, 7.0312e-01, 2.5464e-01, 2.7132e-04, 1.9638e-02],\n",
       "          [5.4207e-03, 5.4749e-02, 8.7012e-01, 6.4026e-02, 1.4448e-04, 5.5923e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   164.841568   21.627472  527.313171  405.181641    0.958933      0   \n",
       "  1     0.719231   16.708008   70.657509   84.566284    0.912803      0   \n",
       "  2     0.196224  140.667526   82.934647  398.221313    0.881971      0   \n",
       "  3    11.461479  296.100433   71.579163  354.321503    0.826641     35   \n",
       "  4    18.726944  121.341278  195.719345  183.697357    0.796715     34   \n",
       "  5   225.893066   61.402863  282.013306   83.121246    0.681431     56   \n",
       "  6   107.105125   61.935928  162.584885   83.382980    0.668591     56   \n",
       "  7   166.328384   61.848862  220.950974   82.925369    0.665354     56   \n",
       "  8   589.204407   61.658752  639.305847   81.312073    0.629663     56   \n",
       "  9    64.101974   61.928619  103.225143   82.595062    0.597014     56   \n",
       "  10  538.640564   61.645340  586.252136   81.890762    0.503207     56   \n",
       "  11  485.643799   61.631226  532.833740   82.018677    0.458188     56   \n",
       "  12  427.007324   62.030441  482.966187   82.904587    0.332592     56   \n",
       "  13    0.000000  288.334045   14.162592  330.147278    0.269520     35   \n",
       "  \n",
       "                name  \n",
       "  0           person  \n",
       "  1           person  \n",
       "  2           person  \n",
       "  3   baseball glove  \n",
       "  4     baseball bat  \n",
       "  5            chair  \n",
       "  6            chair  \n",
       "  7            chair  \n",
       "  8            chair  \n",
       "  9            chair  \n",
       "  10           chair  \n",
       "  11           chair  \n",
       "  12           chair  \n",
       "  13  baseball glove  ,\n",
       "  'caption': ['The catcher.', 'a catcher kneeling behind the batter'],\n",
       "  'bbox_target': [0.0, 141.03, 80.38, 256.36]},\n",
       " 165: {'image_emb': tensor([[ 0.1324,  0.7466, -0.0815,  ...,  0.5649, -0.1550,  0.0252],\n",
       "          [ 0.1597,  0.6128,  0.0319,  ...,  0.3916,  0.0024, -0.1998]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2031,  0.1974, -0.0187,  ...,  0.3665, -0.1542, -0.3738],\n",
       "          [-0.0564, -0.0317, -0.2240,  ...,  0.0696, -0.0209, -0.1184]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.7090, 0.2910],\n",
       "          [0.4883, 0.5117]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0    8.291321   10.541718  626.673279  555.552246    0.749897     45      bowl\n",
       "  1   85.545753  291.508972  177.264465  381.891052    0.696413     51    carrot\n",
       "  2   10.780128  155.096954  155.516815  300.140900    0.612077     51    carrot\n",
       "  3  426.136292  236.157471  556.590942  440.206299    0.606859     50  broccoli\n",
       "  4  449.994476  183.787766  530.317078  248.548996    0.383046     51    carrot\n",
       "  5  488.051331  170.298584  585.281677  288.703949    0.303919     50  broccoli\n",
       "  6   10.193214  372.140015   74.433495  439.205078    0.301423     51    carrot\n",
       "  7  188.762161  408.178345  268.966797  484.910889    0.251505     51    carrot,\n",
       "  'caption': ['A grilled red tomato piece sits atop a bowl of vegetables',\n",
       "   'chunk of tomatoe on left'],\n",
       "  'bbox_target': [8.1, 163.53, 168.74, 135.9]},\n",
       " 166: {'image_emb': tensor([[ 0.0785,  0.2253, -0.2700,  ...,  0.7212, -0.0187,  0.0967],\n",
       "          [ 0.1312,  0.2568, -0.3008,  ...,  0.6133, -0.0410,  0.1072]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2004, -0.0648,  0.0035,  ...,  0.1500,  0.2191,  0.1837],\n",
       "          [ 0.1545, -0.1938,  0.1709,  ...,  0.3948, -0.2500, -0.1183]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.7124, 0.2878],\n",
       "          [0.3850, 0.6152]], dtype=torch.float16),\n",
       "  'df_boxes':         xmin        ymin   xmax        ymax  confidence  class    name\n",
       "  0  22.984909  131.632736  427.0  492.109924    0.890632     46  banana,\n",
       "  'caption': ['THERE ARE A BUNCH OF FOUR YELLOW BANANAS',\n",
       "   'A group of ripe bananas, and some strawberries.'],\n",
       "  'bbox_target': [24.45, 132.31, 402.55, 360.99]},\n",
       " 167: {'image_emb': tensor([[-0.1581,  0.1635, -0.2362,  ...,  0.4153, -0.3801, -0.2189],\n",
       "          [-0.0430,  0.5171, -0.0860,  ...,  0.6006, -0.2166, -0.0461],\n",
       "          [ 0.0635,  0.2788,  0.1860,  ...,  0.5010, -0.2292, -0.1061],\n",
       "          [ 0.0381,  0.1388, -0.0959,  ...,  0.6899, -0.0213, -0.1080],\n",
       "          [ 0.2360, -0.1390,  0.1405,  ...,  0.4429, -0.2593,  0.2050]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0465,  0.0151,  0.0389,  ...,  0.4021, -0.2181,  0.0160],\n",
       "          [-0.1080,  0.0565,  0.1218,  ...,  0.3813, -0.2451, -0.2012]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.8552e-04, 9.9805e-01, 9.2459e-04, 1.0610e-05, 4.3678e-04],\n",
       "          [1.1925e-02, 9.6191e-01, 1.2299e-02, 3.3283e-04, 1.3512e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  237.042465  207.419189  387.691895  423.662537    0.940112      0   \n",
       "  1   77.619080   70.285629  258.187531  423.212708    0.937549      0   \n",
       "  2  291.650299  109.635712  447.568756  414.812744    0.932829      0   \n",
       "  3  272.293823   63.817368  373.973206  187.519684    0.828556     34   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2        person  \n",
       "  3  baseball bat  ,\n",
       "  'caption': ['an umpire in a blue shirt',\n",
       "   'An umpire standing before home plate while the catcher and batter prepare for an at bat'],\n",
       "  'bbox_target': [76.76, 72.93, 182.32, 349.27]},\n",
       " 168: {'image_emb': tensor([[-0.2764,  0.1342, -0.8037,  ...,  0.5747,  0.2471, -0.1117],\n",
       "          [-0.2722,  0.4619, -0.4778,  ...,  0.0960, -0.1450,  0.0778],\n",
       "          [-0.1621,  0.3347, -0.1849,  ...,  1.1816, -0.2861, -0.3574],\n",
       "          ...,\n",
       "          [ 0.2874, -0.1549, -0.5098,  ...,  0.3826,  0.2656, -0.1354],\n",
       "          [-0.1550,  0.0359, -0.5132,  ...,  0.9761, -0.0837, -0.0709],\n",
       "          [-0.3547,  0.4351, -0.7441,  ...,  0.4397, -0.0778, -0.0030]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1331,  0.0480, -0.4878,  ..., -0.3464,  0.1234, -0.1110],\n",
       "          [ 0.2487, -0.1294, -0.3840,  ...,  0.3652,  0.0111, -0.1631]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.2705e-01, 4.1016e-02, 1.5140e-05, 2.2876e-01, 5.9605e-08, 1.4901e-06,\n",
       "           3.0193e-03],\n",
       "          [4.8755e-01, 1.7487e-02, 1.2720e-01, 3.0493e-01, 4.5156e-04, 4.3082e-04,\n",
       "           6.1981e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   228.528595   14.722610  498.243042  369.855499    0.890633      0   \n",
       "  1    27.489769   52.580738  244.336700  367.586792    0.851767      0   \n",
       "  2    34.081448  277.139374  286.347290  371.584229    0.842983     26   \n",
       "  3   270.251129  204.685394  500.000000  370.341003    0.833602     26   \n",
       "  4   139.338989  201.316208  167.669601  224.615814    0.809840     67   \n",
       "  5   132.005798  172.958084  152.295868  219.599533    0.762727     27   \n",
       "  6   409.927185    0.905627  499.652466  185.892624    0.665814      0   \n",
       "  7     0.136957  175.462433   38.078941  356.367065    0.423039      0   \n",
       "  8   138.919830  200.987701  168.124039  225.141739    0.404232     41   \n",
       "  9     4.821062    1.337159  487.730164  367.321075    0.351244      6   \n",
       "  10  116.165329  157.818298  167.790314  238.038940    0.252899     27   \n",
       "  \n",
       "            name  \n",
       "  0       person  \n",
       "  1       person  \n",
       "  2      handbag  \n",
       "  3      handbag  \n",
       "  4   cell phone  \n",
       "  5          tie  \n",
       "  6       person  \n",
       "  7       person  \n",
       "  8          cup  \n",
       "  9        train  \n",
       "  10         tie  ,\n",
       "  'caption': ['a man in suit holding a black bag',\n",
       "   'A black color bag which is on the lap on right side man'],\n",
       "  'bbox_target': [266.54, 204.82, 233.46, 164.8]},\n",
       " 169: {'image_emb': tensor([[ 0.1189,  0.2153, -0.2057,  ...,  0.6533, -0.0699, -0.2009],\n",
       "          [-0.1243,  0.0059,  0.2283,  ...,  0.6621,  0.0506, -0.2142],\n",
       "          [-0.2014,  0.1180, -0.2242,  ...,  0.1089,  0.0741, -0.3418],\n",
       "          [-0.0673,  0.3096, -0.0288,  ...,  0.7378, -0.0133, -0.2644],\n",
       "          [ 0.0753,  0.1161, -0.2256,  ...,  0.5337, -0.0839, -0.3506]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0965,  0.8145,  0.0343,  ..., -0.0635,  0.1979, -0.2751],\n",
       "          [-0.1760, -0.1255,  0.0859,  ..., -0.5063,  0.0024, -0.4895]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0939, 0.1987, 0.1283, 0.4622, 0.1168],\n",
       "          [0.1965, 0.1790, 0.1483, 0.2859, 0.1904]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    0.000000    4.975250  545.482178  473.163696    0.950861      0  person\n",
       "  1  329.659546  200.576965  539.381653  424.303589    0.907277     49  orange\n",
       "  2  210.259979   11.395813  434.559601  241.522400    0.897116     49  orange\n",
       "  3   96.533981  215.026123  339.930847  461.848022    0.896816     49  orange,\n",
       "  'caption': ['An orange being held between a palm and pinky finger.',\n",
       "   'The bottom left orange'],\n",
       "  'bbox_target': [93.74, 210.07, 247.34, 251.86]},\n",
       " 170: {'image_emb': tensor([[-0.0738, -0.3953, -0.1087,  ...,  0.5850,  0.1521, -0.4907],\n",
       "          [-0.0778,  0.2139, -0.3215,  ...,  1.2080,  0.0453,  0.3237],\n",
       "          [ 0.0507, -0.5923, -0.1572,  ...,  0.2986, -0.1210,  0.0173]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2749, -0.3447, -0.2773,  ...,  0.5571, -0.0608, -0.0618],\n",
       "          [ 0.1687, -0.0480, -0.3318,  ...,  0.3606, -0.1159,  0.1575]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.2712e-02, 9.5367e-07, 9.0723e-01],\n",
       "          [2.4643e-03, 9.3408e-01, 6.3538e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  178.464111  263.308960  389.473145  401.732361    0.911965     23  giraffe\n",
       "  1    0.000000  316.969055   57.203735  631.765320    0.778995      0   person\n",
       "  2   27.429520  194.132660  389.901611  601.323975    0.699232     23  giraffe\n",
       "  3   29.892426  340.734131  114.433075  620.217102    0.676205      0   person\n",
       "  4   27.888382  193.846497  386.825012  333.032349    0.585926     23  giraffe,\n",
       "  'caption': ['blonde girl feeding giraffe', 'girl with blond ponytail'],\n",
       "  'bbox_target': [23.92, 340.62, 75.07, 267.56]},\n",
       " 171: {'image_emb': tensor([[-0.2229,  0.6978,  0.0040,  ...,  0.9785,  0.1626,  0.2744],\n",
       "          [-0.0175,  0.2303, -0.3687,  ...,  0.7651,  0.0930,  0.0145],\n",
       "          [-0.1057,  0.6411,  0.0435,  ...,  0.9722,  0.6108,  0.3933],\n",
       "          ...,\n",
       "          [-0.0984,  0.5342, -0.2485,  ...,  1.1211,  0.0641, -0.3547],\n",
       "          [ 0.1514, -0.1687, -0.3435,  ...,  0.8403, -0.0748, -0.2186],\n",
       "          [-0.2233,  0.3938, -0.1077,  ...,  0.6172,  0.5513,  0.3579]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.4346,  0.2043, -0.1912,  ...,  0.0836,  0.1642, -0.4128],\n",
       "          [ 0.0969, -0.4314, -0.3320,  ...,  0.1643, -0.0458, -0.2544]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.9605e-08, 9.1162e-01, 1.2093e-03, 8.2153e-02, 5.3644e-07, 2.8276e-04,\n",
       "           8.3447e-07, 1.5736e-05, 4.7836e-03],\n",
       "          [1.4901e-06, 6.2109e-01, 4.4107e-04, 3.3765e-01, 8.7619e-06, 2.3246e-06,\n",
       "           1.0133e-06, 4.7684e-06, 4.0955e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   351.552887  262.190918  423.793365  389.636963    0.936425     41   \n",
       "  1   182.275970   54.450714  339.179657  217.232727    0.936308      0   \n",
       "  2     0.000000  208.353699  640.000000  474.079651    0.918785     60   \n",
       "  3   491.549194  146.550079  607.292603  319.875580    0.896349      0   \n",
       "  4   190.675751  186.491364  219.672302  238.236481    0.842457     41   \n",
       "  5   102.653999  384.734955  322.216858  478.773254    0.838964     53   \n",
       "  6   312.000702   65.331512  347.872223  123.262726    0.734143     75   \n",
       "  7   486.536987  222.597626  524.305847  234.383331    0.704941     53   \n",
       "  8   253.525955  207.173523  305.643005  222.930176    0.673507     53   \n",
       "  9   406.350677  202.277130  441.152557  255.390900    0.615684     41   \n",
       "  10  585.423706  242.727631  623.816650  325.835785    0.612123     56   \n",
       "  11  249.434799  187.191254  274.220215  208.447723    0.488911     53   \n",
       "  12  226.405457  193.190491  256.028320  245.978333    0.438159     41   \n",
       "  13  410.686707  256.520447  461.155029  275.443054    0.416058     53   \n",
       "  14  332.661743  166.392548  395.213196  220.802826    0.400285     56   \n",
       "  15  572.699036  116.131866  639.977844  334.210297    0.374510     56   \n",
       "  16    0.000000  147.276367   84.594543  190.425446    0.290639     24   \n",
       "  17   22.188324  221.412109  178.903397  267.252502    0.252755     73   \n",
       "  \n",
       "              name  \n",
       "  0            cup  \n",
       "  1         person  \n",
       "  2   dining table  \n",
       "  3         person  \n",
       "  4            cup  \n",
       "  5          pizza  \n",
       "  6           vase  \n",
       "  7          pizza  \n",
       "  8          pizza  \n",
       "  9            cup  \n",
       "  10         chair  \n",
       "  11         pizza  \n",
       "  12           cup  \n",
       "  13         pizza  \n",
       "  14         chair  \n",
       "  15         chair  \n",
       "  16      backpack  \n",
       "  17          book  ,\n",
       "  'caption': ['a woman in a black shirt holding pizza',\n",
       "   'A GIRL EATTING  THE FOOD AND BLACK COLOR DRESS'],\n",
       "  'bbox_target': [181.21, 50.7, 156.41, 169.34]},\n",
       " 172: {'image_emb': tensor([[ 0.3555,  0.2153, -0.1428,  ...,  0.8721, -0.8164, -0.0034],\n",
       "          [-0.5029,  0.6328, -0.0226,  ...,  1.1562, -0.1725, -0.0505],\n",
       "          [-0.0734,  0.4321, -0.2505,  ...,  1.1641,  0.0222,  0.1768],\n",
       "          [-0.0937,  0.2673, -0.2795,  ...,  1.0850, -0.0297, -0.0918],\n",
       "          [ 0.3152,  0.2632, -0.3213,  ...,  0.7065, -0.6445,  0.0787]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0972, -0.0712, -0.1294,  ..., -0.0792, -0.2617, -0.3025],\n",
       "          [ 0.1504,  0.0912, -0.3167,  ..., -0.3086,  0.0839, -0.2434]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.7607e-03, 3.8457e-04, 8.2568e-01, 1.0333e-01, 6.5674e-02],\n",
       "          [3.7670e-03, 1.3153e-02, 7.8857e-01, 1.3916e-01, 5.5359e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  217.616531  110.924942  595.982056  417.114868    0.934313     55   \n",
       "  1    0.810303  305.093750  252.456848  420.982666    0.919731     44   \n",
       "  2   33.574165   18.681282  223.963196  165.732208    0.891035     41   \n",
       "  3  377.322388    1.583656  552.725281  133.777283    0.887431     41   \n",
       "  4    3.744049    2.138474  632.218140  419.943542    0.578007     60   \n",
       "  5  317.309723  352.945190  605.024902  423.660889    0.573138     42   \n",
       "  6  226.330826    0.760529  335.501465   97.662262    0.522785     41   \n",
       "  7  578.406677    0.541534  640.000000  127.202606    0.296372     41   \n",
       "  \n",
       "             name  \n",
       "  0          cake  \n",
       "  1         spoon  \n",
       "  2           cup  \n",
       "  3           cup  \n",
       "  4  dining table  \n",
       "  5          fork  \n",
       "  6           cup  \n",
       "  7           cup  ,\n",
       "  'caption': ['A cup of coffee to the right of another cup of coffee.',\n",
       "   'The coffee cup on the right.'],\n",
       "  'bbox_target': [378.03, 0.48, 173.66, 133.36]},\n",
       " 173: {'image_emb': tensor([[-0.3503,  0.1251, -0.2139,  ...,  0.9731,  0.1687, -0.4417],\n",
       "          [ 0.1646,  0.4670, -0.2317,  ...,  0.7847, -0.1575,  0.2402],\n",
       "          [ 0.0499, -0.0932, -0.1110,  ...,  0.7324,  0.0246,  0.2416],\n",
       "          ...,\n",
       "          [ 0.0412,  0.1027, -0.4138,  ...,  0.8198,  0.1864, -0.1677],\n",
       "          [ 0.1655, -0.1180, -0.2244,  ...,  0.6260,  0.0318,  0.0390],\n",
       "          [-0.3899,  0.1337, -0.3564,  ...,  0.9976, -0.0696,  0.3716]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1470, -0.1403, -0.4082,  ...,  0.5054,  0.1301,  0.0784],\n",
       "          [-0.1334,  0.3367, -0.6313,  ...,  0.0276, -0.1207,  0.3074]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9072e-01, 1.3771e-03, 1.3561e-03, 2.0351e-03, 1.1063e-03, 1.3709e-05,\n",
       "           1.8826e-03, 1.1244e-03, 9.8228e-05, 8.5354e-05, 3.7074e-04],\n",
       "          [9.7607e-01, 8.6260e-04, 2.2161e-04, 2.1561e-02, 1.8179e-05, 7.4506e-06,\n",
       "           3.0398e-06, 7.8535e-04, 7.0333e-06, 1.7285e-06, 4.3368e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   501.218140   77.264526  640.000000  352.990662    0.952619      0   \n",
       "  1   320.701416  106.111404  395.176514  279.679443    0.910695      0   \n",
       "  2     2.932236  184.139435  217.437378  456.799683    0.901825      0   \n",
       "  3   401.583466  174.411316  532.810913  330.718750    0.899558     69   \n",
       "  4   175.060547  168.061127  296.724731  303.473663    0.847911      0   \n",
       "  5   125.208298  283.958557  628.204407  476.182495    0.826838     60   \n",
       "  6   262.801819  286.744934  282.369263  342.806702    0.815608     39   \n",
       "  7     0.000000  323.756165  164.007233  477.198425    0.745314     56   \n",
       "  8   509.364319  347.760254  544.167908  367.079285    0.730119     67   \n",
       "  9   181.353577  295.151581  206.470612  336.414154    0.724769     41   \n",
       "  10  416.650513    4.345757  472.812744   32.930214    0.657314     45   \n",
       "  11  411.414581  185.072540  450.753754  213.644073    0.648043     70   \n",
       "  12  486.561615  341.327698  519.525269  360.172791    0.640177     67   \n",
       "  13  138.956696  362.951355  233.915314  476.911621    0.629676     56   \n",
       "  14    0.257367  179.509766   23.366360  245.115173    0.586691     68   \n",
       "  15  349.776550  135.389145  364.665344  154.694748    0.522629     41   \n",
       "  16  394.611633  420.797882  450.843018  458.961670    0.491233     67   \n",
       "  17  554.590576  376.046570  639.660400  478.537964    0.453760     56   \n",
       "  18  274.531494  178.254669  286.678955  208.772247    0.431187     39   \n",
       "  19  440.402466  338.676086  461.519348  357.985840    0.427442     67   \n",
       "  20  136.733948  361.810486  342.644928  479.244873    0.330937     56   \n",
       "  21  182.976700  211.270599  237.268936  227.136871    0.303936     71   \n",
       "  22  411.130157  185.203430  450.985077  213.597046    0.301012     26   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3           oven  \n",
       "  4         person  \n",
       "  5   dining table  \n",
       "  6         bottle  \n",
       "  7          chair  \n",
       "  8     cell phone  \n",
       "  9            cup  \n",
       "  10          bowl  \n",
       "  11       toaster  \n",
       "  12    cell phone  \n",
       "  13         chair  \n",
       "  14     microwave  \n",
       "  15           cup  \n",
       "  16    cell phone  \n",
       "  17         chair  \n",
       "  18        bottle  \n",
       "  19    cell phone  \n",
       "  20         chair  \n",
       "  21          sink  \n",
       "  22       handbag  ,\n",
       "  'caption': ['An old man with goatee and grey tshirt and black pants.',\n",
       "   'a man wearing a gray tshirt leaning against a counter'],\n",
       "  'bbox_target': [494.02, 82.05, 145.98, 276.13]},\n",
       " 174: {'image_emb': tensor([[ 0.1044, -0.1360,  0.2668,  ...,  0.8765,  0.0365,  0.3630],\n",
       "          [ 0.4429,  0.0923, -0.3159,  ...,  0.7925,  0.0641,  0.0406],\n",
       "          [ 0.0111, -0.2754,  0.0276,  ...,  1.2002,  0.0966, -0.1450],\n",
       "          [ 0.0608, -0.0282, -0.1032,  ...,  1.0928,  0.2457, -0.2363],\n",
       "          [ 0.2048,  0.3328, -0.2045,  ...,  1.0029,  0.0878, -0.0966],\n",
       "          [ 0.2339,  0.1194,  0.0357,  ...,  1.0547,  0.1744,  0.1938]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2469,  0.5947, -0.2659,  ..., -0.4941, -0.2808,  0.2340],\n",
       "          [ 0.2778,  0.0077,  0.0380,  ..., -0.8989, -0.2241,  0.0326]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.9085e-04, 9.6777e-01, 3.1113e-02, 1.7381e-04, 7.7868e-04, 6.7353e-06],\n",
       "          [1.1032e-02, 3.5400e-01, 3.3276e-01, 1.7529e-01, 1.1859e-01, 8.3313e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  318.539551   49.283752  590.126831  432.851807    0.937160      0   \n",
       "  1  166.574951   38.755096  419.080872  433.505371    0.924334      0   \n",
       "  2    0.000000  208.193909  582.749023  432.403137    0.853164     13   \n",
       "  3  545.296631  200.962341  574.307861  276.271423    0.836152     39   \n",
       "  4  156.425308  290.799591  216.828049  379.348541    0.754002     26   \n",
       "  5  562.198181   38.012085  613.078308  208.384277    0.454515     10   \n",
       "  6  159.246613  144.194595  223.269928  382.744568    0.301096     26   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2         bench  \n",
       "  3        bottle  \n",
       "  4       handbag  \n",
       "  5  fire hydrant  \n",
       "  6       handbag  ,\n",
       "  'caption': ['a woman wearing black coloured top and white coloured trouser',\n",
       "   'A woman'],\n",
       "  'bbox_target': [179.43, 35.77, 238.4, 398.82]},\n",
       " 175: {'image_emb': tensor([[ 0.1462,  0.2256, -0.1865,  ...,  1.5420,  0.3145, -0.2086],\n",
       "          [ 0.0199,  0.1798,  0.0668,  ...,  0.9263, -0.0926, -0.1072],\n",
       "          [-0.2742,  0.2117, -0.0994,  ...,  0.6890, -0.4214, -0.0457],\n",
       "          [ 0.3513, -0.0750,  0.3450,  ...,  0.3855, -0.1345, -0.0899],\n",
       "          [ 0.3467, -0.0334,  0.3113,  ...,  0.5708, -0.0047,  0.0392]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 9.3750e-02, -1.9373e-01, -2.3071e-01, -3.8843e-01,  1.3757e-01,\n",
       "           -1.7737e-01, -4.1064e-01, -6.2891e-01, -8.0566e-02,  2.1460e-01,\n",
       "            7.2449e-02, -1.7242e-02, -1.0284e-01, -2.1851e-01, -4.3213e-01,\n",
       "            3.5254e-01, -5.2147e-03, -6.6040e-02, -5.4492e-01,  1.3806e-01,\n",
       "            8.7036e-02,  1.1371e-01,  8.8196e-03,  4.2700e-01,  1.1108e-01,\n",
       "            2.3364e-01, -7.0572e-03, -8.1909e-02, -1.4087e-01, -2.7319e-01,\n",
       "           -2.4988e-01,  2.7832e-01,  2.5513e-02,  1.9202e-01, -6.8506e-01,\n",
       "            1.1273e-01, -1.3196e-01,  1.5491e-01,  3.9673e-01,  1.2830e-01,\n",
       "            3.3643e-01,  1.0400e-01,  7.4402e-02, -3.3472e-01,  1.2659e-01,\n",
       "            5.3009e-02,  1.1702e-03,  1.4526e-01, -5.0720e-02,  2.3889e-01,\n",
       "            1.0010e-01, -6.0840e-01,  1.8115e-01, -4.5947e-01, -2.2171e-02,\n",
       "           -7.4890e-02, -5.5603e-02, -2.1167e-01, -1.1310e-01,  3.3374e-01,\n",
       "            8.6609e-02,  5.5847e-02, -2.2327e-01, -1.7871e-01, -1.4990e-01,\n",
       "           -3.5034e-01,  6.3037e-01,  1.0413e-01, -1.4490e-01,  5.7556e-02,\n",
       "           -4.1333e-01, -1.3013e-01,  4.3068e-03,  2.1460e-01,  1.8677e-01,\n",
       "            8.4351e-02,  2.4805e-01,  2.2754e-01, -2.0557e-01, -6.4941e-02,\n",
       "           -5.1904e-01,  5.3613e-01, -2.0798e-02,  1.4465e-01, -1.6150e-01,\n",
       "            1.3708e-01,  6.0501e-03, -2.6123e-01, -2.6904e-01, -1.3232e-01,\n",
       "            5.0049e-02, -3.6670e-01, -1.0938e+00, -8.1360e-02, -1.4209e-01,\n",
       "            1.8811e-01, -9.2163e-02, -3.1372e-02, -2.0813e-01,  1.0364e-01,\n",
       "            2.6782e-01,  7.0312e-02,  6.1493e-02, -6.0272e-02,  1.6602e-01,\n",
       "            1.7838e-02,  5.5023e-02,  3.9282e-01,  1.9641e-01,  1.7456e-02,\n",
       "           -6.0547e-02,  3.9453e-01,  1.2634e-02, -1.1578e-01, -3.6304e-01,\n",
       "            6.5369e-02,  3.8110e-01,  1.4099e-01, -7.0984e-02,  4.7217e-01,\n",
       "           -2.8271e-01, -5.4883e-01, -1.9031e-01,  9.1003e-02,  1.5088e-01,\n",
       "            2.5009e-02,  6.8848e-01, -2.2192e-01, -5.2551e-02,  4.1602e-01,\n",
       "           -3.5815e-01, -2.5537e-01, -2.3645e-01,  2.7305e+00, -4.8975e-01,\n",
       "            8.8654e-03,  2.2742e-01, -2.6147e-01,  4.1797e-01,  6.5430e-02,\n",
       "           -4.6899e-01,  2.1606e-01,  1.8274e-01, -2.2632e-01, -2.6221e-01,\n",
       "           -4.2163e-01,  2.1716e-01, -7.5977e-01,  1.3672e-01,  7.4036e-02,\n",
       "            2.4927e-01,  7.5928e-02,  2.1423e-01, -3.2056e-01, -3.0298e-01,\n",
       "            2.6489e-01, -4.3066e-01, -1.4978e-01, -1.0626e-01, -3.1982e-01,\n",
       "            4.1211e-01, -1.1462e-01, -4.7607e-03, -1.5686e-02, -4.2267e-02,\n",
       "            1.4368e-01,  5.7617e-01, -2.2253e-01,  4.0430e-01, -5.6580e-02,\n",
       "           -4.6997e-01, -2.8345e-01,  4.7656e-01,  9.5764e-02, -2.8271e-01,\n",
       "           -1.5503e-01,  8.7158e-02,  4.6021e-02, -2.0288e-01, -1.0065e-01,\n",
       "           -5.5957e-01, -2.7496e-02, -2.8519e-02, -6.9031e-02, -2.8198e-01,\n",
       "           -2.5269e-01,  2.1765e-01, -2.1069e-01,  5.9723e-02, -4.0802e-02,\n",
       "            1.6138e-01,  1.3831e-01,  1.8640e-01, -1.6373e-02,  6.1554e-02,\n",
       "           -3.0899e-02, -6.8555e-01,  3.9819e-01, -1.0577e-01,  7.0068e-02,\n",
       "           -1.1932e-01,  3.4973e-02,  1.7444e-01,  1.5991e-02, -3.4790e-01,\n",
       "            5.2539e-01,  1.2024e-01,  2.1194e-02, -5.3589e-02, -4.5825e-01,\n",
       "           -2.4951e-01,  2.1643e-01, -5.3711e-01, -3.8269e-02, -4.3237e-01,\n",
       "           -1.8689e-01, -2.5244e-01, -1.0162e-01,  8.4082e-01, -4.4983e-02,\n",
       "            5.1074e-01, -7.8506e-03, -5.1074e-01,  3.1812e-01, -8.6792e-02,\n",
       "           -4.0436e-02,  9.9487e-02, -2.3462e-01, -1.5100e-01,  1.0199e-01,\n",
       "           -2.7637e-01,  1.9714e-01,  9.5520e-02, -5.4297e-01, -3.9917e-01,\n",
       "            1.0632e-01,  1.7615e-01, -2.3206e-01,  1.2830e-01,  1.4160e-01,\n",
       "           -5.5170e-04, -1.3989e-01,  2.4048e-01,  4.6118e-01,  1.0339e-01,\n",
       "           -2.3938e-01, -5.2783e-01, -5.3589e-02, -2.6733e-01,  1.9226e-01,\n",
       "           -1.8701e-01, -1.4722e-01,  4.6143e-01,  1.2585e-01, -3.2129e-01,\n",
       "           -1.8433e-01,  1.6309e-01, -1.7236e-01,  8.1909e-02, -2.1179e-01,\n",
       "            1.0944e-01,  1.2097e-01,  1.6675e-01, -8.3191e-02,  3.5962e-01,\n",
       "           -8.3801e-02, -5.7617e-01, -1.2866e-01, -1.3464e-01,  9.1858e-02,\n",
       "            1.6199e-01,  2.8174e-01, -2.1835e-02, -1.7908e-01, -1.3708e-01,\n",
       "           -1.7139e-01, -3.7988e-01, -9.6313e-02, -4.6570e-02, -6.8359e-02,\n",
       "           -4.5264e-01,  3.4521e-01,  5.3320e-01,  5.6250e-01,  4.0747e-01,\n",
       "            3.0960e-02,  5.4230e-02,  2.1924e-01, -9.3811e-02,  1.7212e-02,\n",
       "           -3.2544e-01, -5.5939e-02, -3.0029e-02,  7.7362e-03, -2.0642e-01,\n",
       "            3.4277e-01, -2.4866e-01, -2.0410e-01,  4.3311e-01,  6.9641e-02,\n",
       "           -4.8730e-01, -5.7178e-01,  4.8767e-02,  2.3853e-01,  3.2812e-01,\n",
       "           -1.4246e-01,  1.1871e-01,  9.3689e-02,  4.2542e-02,  2.6318e-01,\n",
       "            1.0754e-01,  4.9341e-01,  2.7188e+00,  7.2754e-02, -6.4697e-02,\n",
       "            3.5156e-01,  2.9712e-01, -2.4673e-02,  1.9238e-01,  1.2195e-01,\n",
       "           -3.0640e-02,  2.2571e-01,  5.6305e-02,  2.5708e-01, -3.7158e-01,\n",
       "            9.1431e-02,  3.9111e-01, -4.5557e-01, -1.3501e-01, -1.7578e+00,\n",
       "            3.2275e-01, -3.0591e-01,  3.5400e-01, -3.5449e-01, -1.4954e-01,\n",
       "            2.7563e-01,  2.3163e-02, -1.6956e-01,  3.1348e-01, -1.4026e-01,\n",
       "           -3.6328e-01, -6.6846e-01,  3.7183e-01, -1.3342e-01,  1.6687e-01,\n",
       "            2.9419e-01,  3.0469e-01,  9.6863e-02,  1.0925e-01,  4.1846e-01,\n",
       "            1.8115e-01, -1.4294e-01,  2.0764e-01,  4.0015e-01,  1.0156e-01,\n",
       "           -3.2739e-01, -2.7344e-01,  4.5654e-02, -1.1316e-01,  1.7502e-02,\n",
       "           -1.8616e-01,  5.5267e-02,  2.3669e-01,  3.0786e-01, -1.6150e-01,\n",
       "            2.0605e-01,  1.8689e-01, -7.1259e-03, -4.0344e-02,  1.6907e-01,\n",
       "           -1.6281e-02, -5.8008e-01, -9.7717e-02, -8.7891e-02, -9.9182e-02,\n",
       "            9.6741e-02, -4.3518e-02,  1.9006e-01,  2.1973e-01,  1.3599e-01,\n",
       "            3.8422e-02,  5.3314e-02,  9.5596e-03,  2.8320e-01, -4.6802e-01,\n",
       "           -4.9133e-02,  2.7368e-01,  3.4058e-01, -4.6204e-02,  1.6394e-01,\n",
       "           -8.0762e-01,  2.3206e-01, -5.4492e-01, -5.2357e-04, -4.4189e-02,\n",
       "           -1.0760e-01, -4.1382e-02,  2.7832e-01, -2.7344e-01, -1.3281e-01,\n",
       "            1.8738e-01, -1.5356e-01,  3.0811e-01, -1.9861e-01, -5.6836e-01,\n",
       "           -2.8369e-01, -4.4365e-03,  3.0228e-02,  7.3120e-02,  5.0262e-02,\n",
       "            1.1841e-01,  5.5634e-02, -2.6880e-01, -1.0645e-01,  6.4697e-02,\n",
       "           -6.0498e-01, -2.9395e-01, -1.6882e-01,  1.7224e-01, -1.0657e-01,\n",
       "           -3.0957e-01, -2.2778e-01,  4.6436e-01, -3.9502e-01,  1.7593e-02,\n",
       "           -3.3539e-02, -1.4185e-01, -1.1359e-01,  1.5271e-01,  8.3801e-02,\n",
       "           -1.7517e-02, -9.2651e-02, -2.3340e-01,  1.3647e-01, -7.6172e-02,\n",
       "            3.8361e-02, -1.4478e-01, -3.2007e-01, -2.8540e-01, -1.8201e-01,\n",
       "            7.8186e-02, -1.2598e-01, -2.9816e-02, -3.0792e-02,  2.0764e-01,\n",
       "            9.7322e-04, -2.7515e-01, -6.4941e-02, -3.4277e-01,  2.5537e-01,\n",
       "            2.5513e-02,  3.2562e-02,  9.5642e-02, -3.1665e-01,  2.8784e-01,\n",
       "           -2.1814e-01, -2.3254e-02,  6.7480e-01,  3.1909e-01,  4.0619e-02,\n",
       "           -1.3306e-01,  6.4148e-02,  3.0737e-01,  3.2104e-01,  1.9751e-01,\n",
       "           -3.7866e-01,  1.5869e-01, -1.8835e-01,  6.6833e-02,  6.1401e-02,\n",
       "           -1.5961e-02, -8.3542e-03,  1.6260e-01,  1.4417e-01,  1.2439e-01,\n",
       "           -2.6050e-01,  3.6182e-01, -2.2144e-01,  3.2227e-01,  3.3838e-01,\n",
       "            1.2842e-01, -5.2783e-01,  1.1116e-02,  3.8062e-01,  7.6477e-02,\n",
       "           -1.6846e-01, -3.3032e-01,  3.4741e-01, -1.5823e-02, -3.6407e-02,\n",
       "           -7.1411e-02, -6.4941e-01, -1.9031e-01,  6.2891e-01,  2.2620e-01,\n",
       "           -2.8149e-01,  2.8540e-01, -4.3140e-01, -2.6855e-01, -4.5319e-02,\n",
       "           -2.2937e-01, -2.7271e-01,  3.0556e-03, -2.7771e-02,  7.3633e-01,\n",
       "            1.6418e-01, -5.2100e-01,  3.2288e-02,  4.7656e-01,  2.1753e-01,\n",
       "           -3.2593e-01,  9.8633e-02]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.1738e-01, 5.9326e-01, 5.4419e-05, 5.2643e-02, 3.6743e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0    0.342834  182.766785  138.147339  355.739563    0.924161     56  chair\n",
       "  1  117.910995  243.540848  448.042236  469.176025    0.921649     14   bird\n",
       "  2    0.671585  342.192352  289.438049  515.370972    0.913086     73   book\n",
       "  3    5.097687  172.712601  478.000000  636.026794    0.744115     56  chair\n",
       "  4  371.665253  154.863922  396.299652  177.605164    0.636565     14   bird\n",
       "  5    0.000000  235.959076  376.143829  601.268799    0.334039     73   book,\n",
       "  'caption': ['black chair cushion behind the green and red bird'],\n",
       "  'bbox_target': [178.92, 173.42, 295.92, 459.7]},\n",
       " 176: {'image_emb': tensor([[-0.0609,  0.1497, -0.4114,  ...,  0.8975,  0.2468,  0.1367],\n",
       "          [-0.1102,  0.2681, -0.3281,  ...,  0.8735,  0.4600, -0.0187],\n",
       "          [ 0.1434,  0.3330, -0.0836,  ...,  1.1914,  0.1848, -0.3201],\n",
       "          [ 0.0020,  0.2893, -0.3359,  ...,  1.3066, -0.0890, -0.1130],\n",
       "          [ 0.2554,  0.3616, -0.3735,  ...,  1.2949, -0.1433, -0.0507],\n",
       "          [-0.2407,  0.2048, -0.3618,  ...,  0.6177,  0.5679,  0.2966]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2319, -0.1428, -0.4426,  ...,  0.4189, -0.0851,  0.2634],\n",
       "          [ 0.1351, -0.2734, -0.5317,  ...,  0.2003, -0.0467,  0.2708]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.4315e-03, 1.3530e-04, 9.9805e-01, 4.7505e-05, 4.7505e-05, 2.6488e-04],\n",
       "          [4.1199e-03, 9.1934e-04, 9.9268e-01, 7.6246e-04, 3.9554e-04, 1.0099e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  260.617188   92.186035  489.135254  478.908813    0.932039      0    person\n",
       "  1    0.455200    2.710495  565.234863  478.355225    0.909001      7     truck\n",
       "  2  142.288712  200.982391  329.814209  379.487823    0.865404     16       dog\n",
       "  3  138.161255  154.478256  282.262573  220.764099    0.863063     28  suitcase\n",
       "  4  379.722290  123.884094  500.308411  372.976318    0.831162     16       dog\n",
       "  5    0.459404   45.515945  127.700584  129.070908    0.463007     28  suitcase,\n",
       "  'caption': ['A black dog with green collar who is looking behind itself.',\n",
       "   'The black dog with the green collar looking backwards.'],\n",
       "  'bbox_target': [141.29, 197.42, 187.74, 187.74]},\n",
       " 177: {'image_emb': tensor([[ 0.0092,  0.2218, -0.1624,  ...,  0.4968,  0.1846,  0.0439],\n",
       "          [-0.1254,  0.1296, -0.1168,  ...,  0.8398,  0.1687,  0.2520],\n",
       "          [-0.3442,  0.0811, -0.1411,  ...,  0.7822,  0.1221,  0.1453],\n",
       "          ...,\n",
       "          [-0.2800,  0.7412, -0.0876,  ...,  1.1250, -0.1461,  0.0353],\n",
       "          [-0.1194, -0.2335, -0.1521,  ...,  0.7681,  0.2325, -0.0080],\n",
       "          [ 0.3516,  0.0848, -0.0355,  ...,  0.8154,  0.2900,  0.1632]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2471,  0.3389, -0.7178,  ...,  0.3416,  0.1931,  0.0015],\n",
       "          [ 0.0274,  0.0497, -0.3660,  ...,  0.4385, -0.0504,  0.0528]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.2224e-02, 1.2213e-01, 9.1791e-04, 1.0405e-03, 6.5088e-04, 7.8857e-02,\n",
       "           1.2951e-03, 7.0312e-01],\n",
       "          [2.7930e-01, 7.6355e-02, 2.0981e-03, 8.6606e-05, 1.3901e-02, 1.3818e-01,\n",
       "           1.9217e-04, 4.8999e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    1.157837   18.716919  343.444305  420.525452    0.952525      0  person\n",
       "  1  183.329330   82.011765  590.053711  421.218567    0.939707      0  person\n",
       "  2  407.250153  316.600037  563.784424  424.824524    0.900789      0  person\n",
       "  3  516.706177  134.603149  590.743164  195.615479    0.866902     65  remote\n",
       "  4  269.786896  184.433472  349.519318  241.373352    0.834122     65  remote\n",
       "  5  117.667603  105.959229  210.999298  199.394043    0.812592     65  remote\n",
       "  6  304.809113  351.720337  417.478729  425.471252    0.799332      0  person\n",
       "  7  528.968567  371.996429  639.297791  424.644531    0.601509     57   couch,\n",
       "  'caption': ['a girl in a yellow shirt, holding a wii controller',\n",
       "   'A girl in a yellow shirt playing a video game with a white joystick.'],\n",
       "  'bbox_target': [189.99, 83.48, 418.37, 338.72]},\n",
       " 178: {'image_emb': tensor([[-0.1163, -0.4023, -0.1599,  ...,  0.6685,  0.2484, -0.0901],\n",
       "          [-0.2600, -0.2617, -0.0150,  ...,  0.6431,  0.0762, -0.1019],\n",
       "          [-0.1824, -0.3931, -0.1677,  ...,  0.3945,  0.1530,  0.0711]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2185, -0.1338, -0.2830,  ..., -0.2305, -0.2773, -0.5610],\n",
       "          [ 0.1172, -0.3311, -0.3894,  ..., -0.1423, -0.1120, -0.2233]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1738, 0.2197, 0.6064],\n",
       "          [0.1440, 0.0649, 0.7910]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  319.279175   83.492752  541.149353  423.622559    0.941037     23  giraffe\n",
       "  1  166.222870  199.171997  332.961792  424.382812    0.896998     23  giraffe\n",
       "  2  286.583832  135.800949  378.043427  422.924316    0.523834     23  giraffe\n",
       "  3  286.818878  137.116272  359.325165  282.053162    0.312005     23  giraffe,\n",
       "  'caption': ['The giraffe in the middle.',\n",
       "   'The giraffe in the middle is looking at the camera.'],\n",
       "  'bbox_target': [287.87, 138.02, 94.99, 283.06]},\n",
       " 179: {'image_emb': tensor([[ 3.2288e-02,  3.1738e-01, -3.2520e-01,  ...,  8.7451e-01,\n",
       "            2.1655e-01, -2.1179e-01],\n",
       "          [ 8.9050e-02,  6.2109e-01, -2.3401e-01,  ...,  8.3545e-01,\n",
       "           -1.0718e-01, -1.7542e-01],\n",
       "          [ 3.5059e-01,  2.8516e-01, -7.1883e-05,  ...,  1.1035e+00,\n",
       "           -7.7026e-02, -9.3872e-02],\n",
       "          ...,\n",
       "          [-2.4548e-01,  1.8140e-01, -2.3730e-01,  ...,  9.5264e-01,\n",
       "           -3.4637e-02, -2.4643e-02],\n",
       "          [ 3.5693e-01, -2.3389e-01, -5.6445e-01,  ...,  9.7314e-01,\n",
       "           -1.1549e-03, -1.3342e-01],\n",
       "          [ 5.4932e-02,  5.4932e-01, -1.0345e-01,  ...,  8.3594e-01,\n",
       "            1.2512e-01,  1.5662e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3120,  0.2725, -0.0428,  ..., -0.0766,  0.0831, -0.2856],\n",
       "          [-0.3367, -0.2028, -0.0248,  ..., -0.1017,  0.3721, -0.4131]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.4893e-01, 2.7206e-02, 3.2104e-01, 2.0981e-03, 8.6546e-05, 1.4782e-05,\n",
       "           6.8092e-04],\n",
       "          [8.0713e-01, 3.1281e-02, 1.4465e-01, 4.3941e-04, 4.1544e-05, 1.9312e-05,\n",
       "           1.6495e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  193.004150    8.743864  393.374725  330.890717    0.950603      0   \n",
       "  1  338.161591   17.987638  484.378723  331.020294    0.941459      0   \n",
       "  2   47.877205   16.643936  197.593842  329.848572    0.905312      0   \n",
       "  3    0.000000  215.253098  159.635300  270.290527    0.892287     36   \n",
       "  4    0.000000    0.000000  218.359512   36.981697    0.862742      2   \n",
       "  5  425.962677  130.983826  441.273560  140.393829    0.782480     67   \n",
       "  6   84.008904   86.883171  180.578201  199.866058    0.492365     24   \n",
       "  7  361.687531   74.725052  450.729126  141.874619    0.468876     24   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2      person  \n",
       "  3  skateboard  \n",
       "  4         car  \n",
       "  5  cell phone  \n",
       "  6    backpack  \n",
       "  7    backpack  ,\n",
       "  'caption': ['A man in dark clothing holding a skateboard',\n",
       "   'A guy walking with his skates on his right hand'],\n",
       "  'bbox_target': [46.53, 15.76, 150.12, 314.49]},\n",
       " 180: {'image_emb': tensor([[-0.0207,  0.4473, -0.2861,  ...,  1.2529, -0.2993, -0.5029],\n",
       "          [-0.5781,  0.1913, -0.3591,  ...,  0.9009, -0.2043, -0.6401],\n",
       "          [-0.1638,  0.4028, -0.1530,  ...,  0.9424, -0.1748, -0.8584],\n",
       "          [-0.1406,  0.0290,  0.0225,  ...,  1.1416,  0.2306, -0.3545],\n",
       "          [ 0.0223,  0.2341, -0.0634,  ...,  0.8022,  0.0353, -0.4546]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0083, -0.3079, -0.3635,  ...,  0.5747,  0.1014, -0.4309],\n",
       "          [-0.0170, -0.0479, -0.1091,  ..., -0.3794,  0.3892, -0.0154]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0712e-01, 6.3324e-03, 8.1329e-03, 9.3613e-03, 8.6914e-01],\n",
       "          [1.7920e-01, 2.0542e-03, 1.3110e-01, 4.5133e-04, 6.8701e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  120.051743   17.639191  339.909546  549.246704    0.936096      0   \n",
       "  1  145.741394  538.813721  259.291565  577.296631    0.831833     36   \n",
       "  2   66.350677  170.231171  277.102814  547.772339    0.787522      0   \n",
       "  3  239.714508  544.209656  303.998749  577.167053    0.707408     36   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1  skateboard  \n",
       "  2      person  \n",
       "  3  skateboard  ,\n",
       "  'caption': ['Skateboarder behind bearded guy',\n",
       "   'The person on the skateboard behind another person'],\n",
       "  'bbox_target': [63.15, 175.59, 238.26, 375.12]},\n",
       " 181: {'image_emb': tensor([[-0.2057, -0.3284,  0.0017,  ...,  0.5127,  0.1957, -0.0554],\n",
       "          [-0.2384, -0.5376, -0.0264,  ..., -0.0442,  0.1564, -0.1045]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0909, -0.2428, -0.0592,  ...,  0.0475,  0.1451,  0.4587],\n",
       "          [-0.0834, -0.3801, -0.1024,  ...,  0.4541, -0.0129,  0.2922]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.4072, 0.5928],\n",
       "          [0.1801, 0.8198]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class    name\n",
       "  0  262.631348  66.693878  468.443787  277.394135    0.899644     22   zebra\n",
       "  1  581.442200   0.115219  639.998230   97.703323    0.676489      0  person\n",
       "  2  390.434082  76.196625  561.901062  262.809418    0.567476     22   zebra\n",
       "  3  430.056335   0.414970  447.685547   56.254684    0.307055      0  person,\n",
       "  'caption': ['The zebra in front of all the other zebras.',\n",
       "   'a zebra grazing while standing in front of two other zebras'],\n",
       "  'bbox_target': [300.67, 86.11, 229.8, 220.13]},\n",
       " 182: {'image_emb': tensor([[-0.3115,  0.4795, -0.0025,  ...,  1.0625, -0.0677,  0.0082],\n",
       "          [ 0.1863,  0.5171, -0.2155,  ...,  0.7729, -0.0998,  0.0240],\n",
       "          [-0.2014,  0.6924,  0.0279,  ...,  1.0371,  0.0795,  0.0484],\n",
       "          [ 0.1373,  0.5908, -0.2595,  ...,  1.1094, -0.0092, -0.0123],\n",
       "          [-0.0659,  0.4055, -0.1118,  ...,  1.2920,  0.1538, -0.0748],\n",
       "          [-0.0349,  0.3450, -0.1710,  ...,  0.7437, -0.0925, -0.0839]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.4548,  0.0692, -0.0019,  ..., -0.1288, -0.2336, -0.2744],\n",
       "          [-0.1974,  0.0042, -0.0701,  ..., -0.2871, -0.1802, -0.4990]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.4470, 0.0753, 0.3591, 0.0009, 0.0010, 0.1166],\n",
       "          [0.6006, 0.0313, 0.3423, 0.0007, 0.0012, 0.0240]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   394.300232  262.032898  597.622864  418.909119    0.935774     45   \n",
       "  1     2.736755   58.338043  636.583435  421.179077    0.892349     60   \n",
       "  2   182.101868  252.483276  366.828186  397.849731    0.890043     45   \n",
       "  3   341.905212  189.226868  439.373047  246.909912    0.776141     54   \n",
       "  4    29.292461    0.000000  109.196701   99.085678    0.775289     58   \n",
       "  5   159.877472  103.837265  309.001587  171.198792    0.616053     45   \n",
       "  6   355.363007    0.589325  503.523407   90.259033    0.614753     56   \n",
       "  7   266.968750  163.638336  383.027405  214.153717    0.578121     54   \n",
       "  8   187.923615  117.132812  233.787872  154.911621    0.573616     49   \n",
       "  9   248.259293  110.530014  298.410522  148.946487    0.526837     49   \n",
       "  10  243.821075    0.000000  368.783905   77.943481    0.504558     56   \n",
       "  11  218.182114  104.063126  245.357956  135.848557    0.373888     49   \n",
       "  12   54.731174  186.847015  140.636368  249.696442    0.336091     54   \n",
       "  13  228.114716  131.256714  251.221985  153.600159    0.308188     49   \n",
       "  14  244.432709  104.961334  261.273224  131.390228    0.252804     49   \n",
       "  \n",
       "              name  \n",
       "  0           bowl  \n",
       "  1   dining table  \n",
       "  2           bowl  \n",
       "  3          donut  \n",
       "  4   potted plant  \n",
       "  5           bowl  \n",
       "  6          chair  \n",
       "  7          donut  \n",
       "  8         orange  \n",
       "  9         orange  \n",
       "  10         chair  \n",
       "  11        orange  \n",
       "  12         donut  \n",
       "  13        orange  \n",
       "  14        orange  ,\n",
       "  'caption': ['The leftmost bowl of soup.', 'The bowl of soup on the left.'],\n",
       "  'bbox_target': [182.79, 252.76, 181.35, 141.06]},\n",
       " 183: {'image_emb': tensor([[ 0.4675, -0.4568, -0.3933,  ...,  0.4226, -0.3433,  0.2271],\n",
       "          [ 0.0452, -0.0579, -0.4797,  ...,  0.8477, -0.2025, -0.2605],\n",
       "          [ 0.6514, -0.2883, -0.2703,  ...,  0.2559, -0.2883,  0.2576]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2864,  0.1197,  0.1664,  ...,  0.0146, -0.2720, -0.2642],\n",
       "          [-0.0478,  0.0229,  0.1190,  ...,  0.1126, -0.5425, -0.0598]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2622, 0.0360, 0.7017],\n",
       "          [0.3252, 0.0072, 0.6675]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0   20.001709   28.977768  612.614136  435.523315    0.935755      7  truck\n",
       "  1  600.383789  121.219421  639.746826  242.852112    0.705080      7  truck,\n",
       "  'caption': ['A red truck with a blue light on the roof.',\n",
       "   'A large red and white vintage fire truck parked.'],\n",
       "  'bbox_target': [27.92, 41.98, 584.16, 383.36]},\n",
       " 184: {'image_emb': tensor([[ 0.1730,  0.4846,  0.0402,  ...,  0.4529, -0.0729,  0.3152],\n",
       "          [-0.2661,  0.0655, -0.4473,  ...,  0.4067,  0.0230,  0.1235],\n",
       "          [-0.2335, -0.3237,  0.0028,  ...,  1.2705,  0.1653,  0.0504],\n",
       "          [ 0.1903,  0.0238,  0.0675,  ...,  0.2798,  0.1792,  0.1643]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1008, -0.3064,  0.1774,  ...,  0.1160,  0.0588, -0.2336],\n",
       "          [ 0.3149, -0.0595, -0.6294,  ...,  0.1528,  0.2350,  0.0207]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.3350e-01, 8.2612e-05, 1.7881e-07, 1.6663e-01],\n",
       "          [9.8193e-01, 6.1560e-04, 8.4639e-05, 1.7166e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0   64.900757  101.800774  166.977432  255.418259    0.943929      0   person\n",
       "  1  349.598206  153.921890  409.616577  309.652832    0.906530      0   person\n",
       "  2  397.668030  207.212112  445.155182  227.045258    0.856093     29  frisbee,\n",
       "  'caption': ['A woman in an orange shirt kneels down watching a child play with a frisbee',\n",
       "   'Woman in a yellow top'],\n",
       "  'bbox_target': [66.17, 102.32, 100.28, 153.13]},\n",
       " 185: {'image_emb': tensor([[-0.2002,  0.4473,  0.0307,  ...,  0.5444,  0.0988,  0.1960],\n",
       "          [ 0.2729,  0.4485,  0.1011,  ...,  0.7026,  0.3333,  0.3979],\n",
       "          [-0.2034,  0.4370, -0.1617,  ...,  0.6494, -0.1125,  0.1443],\n",
       "          [ 0.1362,  0.3440, -0.0269,  ...,  0.5259,  0.1937,  0.0667],\n",
       "          [ 0.0346,  0.1785, -0.1025,  ...,  0.3088,  0.5371, -0.0450],\n",
       "          [ 0.3862,  0.1131, -0.1536,  ...,  0.5923,  0.1255,  0.2793]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1090,  0.3694, -0.0953,  ..., -0.1548,  0.1260, -0.0947],\n",
       "          [-0.2460,  0.4282,  0.0642,  ..., -0.1270,  0.1620,  0.2250]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.4336e-01, 8.6021e-04, 1.2457e-05, 8.8215e-06, 2.4368e-02, 3.1281e-02],\n",
       "          [1.2952e-01, 6.5125e-02, 4.8676e-03, 4.5991e-04, 7.9346e-01, 6.3515e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   369.831238  232.202042  588.890442  469.546844    0.946420     41   \n",
       "  1    13.676143   90.383842  177.377380  461.356415    0.885733     41   \n",
       "  2     0.177652  392.557678   41.842316  481.429382    0.796210     67   \n",
       "  3   218.553009  209.032867  434.436584  362.909637    0.784673     55   \n",
       "  4   499.446198  154.191696  607.529724  355.600616    0.769638     41   \n",
       "  5     0.580118  267.151154  610.266907  609.579224    0.612690     60   \n",
       "  6    66.331474    0.000000   84.738800   51.408211    0.452291     73   \n",
       "  7    54.481106    0.000000   72.716873   51.478630    0.442150     73   \n",
       "  8    17.737394    0.000000   37.219608   54.492714    0.421546     73   \n",
       "  9    30.677004    0.000000   52.450409   53.197426    0.410559     73   \n",
       "  10    0.525670    0.000000   21.478613   54.858879    0.383407     73   \n",
       "  11   79.942253    0.062233   96.487411   50.303341    0.357443     73   \n",
       "  12   41.057831    0.001514   61.592346   52.741508    0.347484     73   \n",
       "  13   95.118073    0.000000  113.231071   49.950420    0.331331     73   \n",
       "  14    0.115500  139.895645   29.473957  332.763733    0.303205     39   \n",
       "  \n",
       "              name  \n",
       "  0            cup  \n",
       "  1            cup  \n",
       "  2     cell phone  \n",
       "  3           cake  \n",
       "  4            cup  \n",
       "  5   dining table  \n",
       "  6           book  \n",
       "  7           book  \n",
       "  8           book  \n",
       "  9           book  \n",
       "  10          book  \n",
       "  11          book  \n",
       "  12          book  \n",
       "  13          book  \n",
       "  14        bottle  ,\n",
       "  'caption': ['A clear cup behind the brown mug.', 'A clear glass of water.'],\n",
       "  'bbox_target': [499.65, 156.36, 106.33, 206.92]},\n",
       " 186: {'image_emb': tensor([[ 0.0044,  0.0132,  0.0089,  ...,  0.2820,  0.3364,  0.3276],\n",
       "          [-0.3430,  0.0584,  0.0614,  ...,  0.5137,  0.2766,  0.0509],\n",
       "          [-0.0065,  0.3376, -0.1305,  ...,  1.0625, -0.2937, -0.4875],\n",
       "          ...,\n",
       "          [ 0.0929,  0.3286, -0.4329,  ...,  0.9092,  0.2206, -0.1917],\n",
       "          [ 0.0295,  0.1045, -0.2106,  ...,  1.0273,  0.0989, -0.1344],\n",
       "          [-0.3718,  0.1284, -0.0892,  ...,  0.3425,  0.3408,  0.4116]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-1.8774e-01,  1.2903e-01, -2.9883e-01, -8.9172e-02,  4.7144e-01,\n",
       "            2.1753e-01, -1.5247e-01, -1.6968e-01, -1.0419e-01, -1.9592e-01,\n",
       "            4.3750e-01, -4.3237e-01,  6.9336e-01, -4.8523e-02,  3.2910e-01,\n",
       "           -2.9956e-01, -4.9194e-01, -3.6108e-01, -1.9470e-02,  1.3684e-01,\n",
       "           -2.4463e-01,  5.5664e-01,  1.2372e-01, -1.0248e-01, -2.0386e-02,\n",
       "            2.2827e-01,  6.5613e-02,  1.1420e-01, -5.8868e-02, -1.8982e-02,\n",
       "            1.6464e-02, -1.3013e-01,  6.3574e-01,  1.0046e-01, -5.8398e-01,\n",
       "            9.2651e-02,  7.2083e-02,  2.2229e-01, -8.9783e-02,  1.2378e-01,\n",
       "           -1.9385e-01, -1.4307e-01, -3.1689e-01, -8.1558e-03,  2.6001e-02,\n",
       "           -7.0007e-02, -3.5522e-01, -4.0649e-02, -5.3040e-02, -8.7524e-02,\n",
       "            5.7129e-02, -7.9956e-02, -2.0081e-01, -1.5430e-01, -1.1945e-01,\n",
       "            1.5344e-01,  3.9722e-01, -1.3790e-03,  8.3374e-02,  3.2935e-01,\n",
       "            2.9150e-01,  4.1699e-01,  1.7297e-01, -2.1570e-01, -1.7188e-01,\n",
       "           -2.4414e-01, -1.3281e-01,  1.5845e-01, -1.8616e-01, -2.1594e-01,\n",
       "           -9.9548e-02, -4.4434e-01,  1.0339e-01, -5.2368e-02,  2.7612e-01,\n",
       "           -6.6345e-02,  2.3499e-01, -1.8262e-01, -1.8994e-01, -1.3623e-01,\n",
       "           -4.2554e-01, -2.3657e-01, -8.8379e-02,  2.6831e-01,  4.5349e-02,\n",
       "            3.0640e-01, -2.1948e-01, -3.0396e-01, -6.7078e-02,  1.6919e-01,\n",
       "           -8.4351e-02, -1.6003e-01, -8.2129e-01,  1.3171e-01, -1.2817e-01,\n",
       "           -1.4111e-01, -2.2290e-01,  7.3914e-02, -9.9640e-03,  1.0229e-01,\n",
       "            2.5928e-01,  4.1235e-01, -2.2705e-01,  3.1787e-01,  1.7700e-01,\n",
       "           -5.9296e-02, -3.4912e-01, -6.2408e-02, -8.6853e-02, -3.0640e-01,\n",
       "           -1.4539e-01, -3.0493e-01,  1.1993e-01,  2.9150e-01,  2.2095e-01,\n",
       "           -1.6174e-02, -4.4092e-01, -5.8777e-02, -3.9966e-01,  5.1904e-01,\n",
       "            2.0276e-01, -5.7910e-01, -1.5576e-01,  4.6948e-01,  2.0068e-01,\n",
       "            2.7905e-03, -3.2739e-01,  1.2732e-01, -2.9810e-01,  1.5076e-01,\n",
       "            5.9277e-01, -8.6304e-02, -2.7612e-01,  2.6309e+00, -2.8027e-01,\n",
       "           -3.1201e-01,  6.1737e-02, -1.4270e-01,  5.5176e-02,  1.6284e-01,\n",
       "           -3.1104e-01,  4.4678e-01, -5.7526e-02,  2.6685e-01,  2.7197e-01,\n",
       "           -6.6589e-02,  1.7627e-01, -3.7329e-01,  3.0566e-01, -2.9883e-01,\n",
       "           -3.6816e-01,  8.3435e-02, -5.5145e-02,  1.9934e-01,  4.7607e-01,\n",
       "           -1.9873e-01, -8.4473e-02, -4.7290e-01,  2.8882e-01,  4.1333e-01,\n",
       "            4.2163e-01,  6.2012e-02, -4.2847e-02,  1.5845e-01,  3.3472e-01,\n",
       "            3.0542e-01,  3.1226e-01,  4.5752e-01,  5.9814e-01,  2.9297e-02,\n",
       "            1.6748e-01, -1.1200e-01,  3.5840e-01,  2.4158e-01, -6.2109e-01,\n",
       "           -3.0225e-01,  2.8882e-01,  3.8721e-01, -3.3447e-01, -4.0741e-02,\n",
       "           -3.0273e-01, -3.2616e-03, -2.5977e-01, -4.1772e-01,  4.6600e-02,\n",
       "           -3.4717e-01,  5.5054e-02, -4.7095e-01,  1.7627e-01,  2.1228e-01,\n",
       "            5.0195e-01,  4.4531e-01, -5.7080e-01, -8.3923e-02, -7.1899e-02,\n",
       "           -2.0508e-01, -5.5469e-01,  9.4629e-01, -3.2593e-02,  2.5488e-01,\n",
       "            3.2178e-01, -5.6305e-02,  2.6416e-01, -3.5645e-02,  2.1606e-02,\n",
       "           -5.1709e-01,  6.8481e-02, -2.6123e-01,  1.0504e-01,  2.7344e-01,\n",
       "            1.7334e-01,  2.3950e-01, -2.5269e-01, -2.6880e-01, -8.6731e-02,\n",
       "           -1.1574e-02,  1.3464e-01, -1.4075e-01, -2.7002e-01, -2.6831e-01,\n",
       "            4.0625e-01, -2.4646e-01, -2.2559e-01,  1.1121e-01, -4.8169e-01,\n",
       "            2.6123e-02, -1.2659e-01, -5.0781e-02,  2.8833e-01, -3.2349e-01,\n",
       "            2.2375e-01,  6.3354e-02,  4.4800e-01,  1.0291e-01, -9.0027e-02,\n",
       "           -4.7058e-02,  7.2327e-02,  2.6660e-01,  4.4250e-02, -7.6050e-02,\n",
       "            5.0977e-01, -1.4297e-02, -1.1255e-01,  3.2520e-01, -3.8232e-01,\n",
       "            3.4637e-03, -1.1841e-01,  4.7681e-01,  1.0809e-01,  1.9946e-01,\n",
       "           -3.9868e-01,  7.7637e-02,  1.3477e-01,  2.0630e-02, -6.2500e-02,\n",
       "            5.1880e-02, -3.6987e-01, -4.9438e-01,  1.0315e-01,  4.0601e-01,\n",
       "            3.4497e-01, -3.9764e-02,  5.7715e-01,  3.1860e-02,  2.5928e-01,\n",
       "           -4.7217e-01, -4.1577e-01, -2.4084e-01,  6.0486e-02,  2.0312e-01,\n",
       "           -1.4124e-01, -2.5513e-01,  3.4595e-01,  2.4792e-01, -3.7793e-01,\n",
       "           -1.4172e-01, -7.4646e-02,  1.9922e-01,  3.9258e-01, -2.4866e-01,\n",
       "            2.5000e-01,  6.2354e-01,  3.1909e-01,  3.1714e-01,  3.3600e-02,\n",
       "            2.9980e-01, -1.3843e-01, -1.7847e-01, -6.1096e-02, -2.9028e-01,\n",
       "            1.4636e-01,  1.8579e-01,  3.6353e-01,  1.1096e-01, -1.5723e-01,\n",
       "            1.6284e-01, -6.6699e-01,  3.3173e-02, -2.2058e-01, -9.7839e-02,\n",
       "            3.8477e-01,  9.1309e-02,  2.4597e-01,  8.5083e-02, -2.3853e-01,\n",
       "           -6.2598e-01,  2.3022e-01, -1.3770e-01, -4.2633e-02, -1.5723e-01,\n",
       "            1.2856e-02,  7.9248e-01,  2.6289e+00,  1.5930e-01, -1.1432e-01,\n",
       "            4.0308e-01, -1.5710e-01, -2.7759e-01, -1.5320e-01,  9.1553e-02,\n",
       "           -1.4807e-01,  2.2375e-01, -1.0974e-01,  2.9663e-01, -4.5361e-01,\n",
       "            1.7224e-01,  3.0640e-01,  1.6614e-01,  9.1125e-02, -5.4541e-01,\n",
       "            4.4214e-01,  2.0593e-01,  1.9238e-01,  1.0535e-01,  8.5510e-02,\n",
       "           -1.3757e-01,  3.4271e-02,  1.6638e-01,  6.5857e-02,  3.3911e-01,\n",
       "           -1.1865e-01, -6.1829e-02,  4.1290e-02, -4.7949e-01, -1.3855e-01,\n",
       "           -6.5374e-04,  3.3618e-01, -7.2708e-03,  3.5205e-01, -2.8589e-01,\n",
       "            6.0498e-01, -1.7151e-01, -1.7664e-01, -1.7603e-01, -6.3574e-01,\n",
       "           -1.6138e-01,  4.7168e-01,  1.1731e-01, -1.5039e-01,  1.6724e-01,\n",
       "            7.2754e-01, -3.8257e-01, -9.1187e-02, -4.0283e-01,  1.0089e-01,\n",
       "           -2.4097e-01, -1.0315e-01,  1.1133e-01, -3.3740e-01, -1.5259e-02,\n",
       "            1.3330e-01, -1.4270e-01, -7.2517e-03,  1.8250e-01,  3.4717e-01,\n",
       "            6.4026e-02,  2.1619e-01, -3.6499e-02,  3.4351e-01, -1.8811e-01,\n",
       "           -2.9614e-01,  4.0375e-02,  3.1763e-01, -8.5107e-01,  1.3855e-01,\n",
       "            7.4768e-02, -3.1689e-01,  6.3599e-02,  1.0248e-01, -2.0203e-02,\n",
       "           -1.0029e+00, -3.6060e-01, -1.5906e-01, -4.1406e-01,  5.1575e-02,\n",
       "            3.9551e-01,  3.0908e-01, -3.6890e-01,  2.3010e-01,  6.8457e-01,\n",
       "            1.6199e-01, -3.0273e-01,  2.3621e-02, -2.7637e-01,  1.6235e-01,\n",
       "            1.2482e-01,  2.3132e-01,  1.4832e-01,  3.6670e-01, -4.1077e-02,\n",
       "            2.0215e-01,  6.7993e-02,  6.3037e-01,  1.5186e-01, -6.4941e-02,\n",
       "            2.9639e-01,  1.3519e-02, -4.9927e-01,  8.4717e-02,  4.9683e-02,\n",
       "           -8.5938e-02, -2.0630e-01, -8.6548e-02, -1.7166e-02,  1.4258e-01,\n",
       "           -3.2031e-01,  2.6199e-02, -1.0931e-01,  5.9473e-01, -3.4985e-01,\n",
       "           -2.3938e-01,  1.3306e-01,  3.4448e-01,  7.4036e-02, -1.5381e-01,\n",
       "           -1.4160e-01, -2.4939e-01,  2.4792e-01, -5.2673e-02,  4.0137e-01,\n",
       "            6.7993e-02, -1.2262e-01, -2.3071e-02,  2.2949e-01, -1.5869e-01,\n",
       "            3.7720e-01, -2.8394e-01, -1.4343e-01,  3.5059e-01, -2.6245e-01,\n",
       "           -6.0303e-01,  2.1106e-01, -2.0837e-01,  2.1973e-01, -2.9419e-02,\n",
       "           -4.9365e-01,  5.3009e-02, -7.8247e-02, -3.4570e-01,  1.3931e-02,\n",
       "            2.5415e-01,  1.7188e-01, -1.5503e-01,  4.7546e-02,  4.3970e-01,\n",
       "           -3.5132e-01,  3.7451e-01, -1.9836e-01, -9.5749e-03, -5.8014e-02,\n",
       "            1.2756e-01, -2.8833e-01,  7.9834e-02,  7.8491e-02,  2.2473e-01,\n",
       "            2.3718e-01,  1.0246e-02, -2.4182e-01,  4.5996e-01, -8.2703e-02,\n",
       "            1.8921e-02,  7.2060e-03,  1.5369e-01, -3.1787e-01,  1.9495e-01,\n",
       "           -1.9897e-01, -6.6992e-01,  2.6443e-02, -1.5186e-01, -2.0251e-01,\n",
       "            4.6655e-01,  2.3132e-01, -2.0544e-01,  5.9570e-01,  2.9590e-01,\n",
       "           -3.4229e-01,  1.3538e-01, -1.2405e-02, -2.8760e-01, -2.5049e-01,\n",
       "           -2.0691e-01,  9.7961e-02, -3.0542e-01,  5.2246e-01,  7.6953e-01,\n",
       "            1.3268e-02, -1.1060e-01, -2.1655e-01, -4.3762e-02,  1.2695e-02,\n",
       "            3.6987e-01, -3.0762e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.2021e-01, 1.7371e-01, 3.0696e-05, 9.7084e-04, 1.6344e-04, 5.9795e-04,\n",
       "           5.1165e-04, 6.3181e-06, 1.0376e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin       ymin        xmax        ymax  confidence  class      name\n",
       "  0   296.847473  53.910599  508.820679  433.174072    0.928231      0    person\n",
       "  1    43.613800  71.377579  303.698639  433.045288    0.911431      0    person\n",
       "  2     9.408646   0.407959   82.148102  160.463013    0.873526      0    person\n",
       "  3   584.499695  23.377777  621.959045  127.865967    0.858455      0    person\n",
       "  4   533.490295  19.824753  566.858582   85.700089    0.837444      0    person\n",
       "  5   451.592163   0.000000  487.041260  113.773895    0.767809      0    person\n",
       "  6   352.312256   0.000000  391.735718   82.544281    0.757299      0    person\n",
       "  7   472.870514  23.498039  497.216034   61.149261    0.737097     24  backpack\n",
       "  8   112.379723  19.117599  159.719940   98.958145    0.647422      0    person\n",
       "  9   291.334686  81.465363  386.900970  435.261963    0.619062     30      skis\n",
       "  10   70.271790  49.132217  171.834900  438.000000    0.618152     30      skis\n",
       "  11  489.835510  21.528412  510.545654   88.504639    0.611549      0    person\n",
       "  12  315.758575  29.826508  341.766266   92.162109    0.597624      0    person\n",
       "  13  195.061798  21.102837  222.106964   81.073715    0.571029      0    person\n",
       "  14  167.158707  16.922592  191.908401   72.335556    0.553755      0    person\n",
       "  15  217.647705  13.759399  252.422394   96.440369    0.540896      0    person\n",
       "  16  398.514801  24.145370  420.430084   55.254868    0.472613      0    person\n",
       "  17  264.109955  15.770721  305.470734  104.239044    0.437282      0    person\n",
       "  18   42.180702   0.000000   88.124123  124.501022    0.434810      0    person\n",
       "  19  518.804871  36.209335  533.761169   79.921921    0.431549      0    person\n",
       "  20  333.059204  28.427261  363.468872   98.167892    0.427648      0    person\n",
       "  21  147.129395  16.176300  168.926727   75.169495    0.410687      0    person\n",
       "  22  251.521194  23.720314  279.831024  104.466003    0.401852      0    person\n",
       "  23  297.179993  23.926445  317.055420   72.599045    0.276634      0    person,\n",
       "  'caption': ['Skis held by the child in the black and white coat.'],\n",
       "  'bbox_target': [290.58, 86.4, 110.47, 351.6]},\n",
       " 187: {'image_emb': tensor([[ 2.3975e-01,  2.8760e-01, -2.1130e-01,  ...,  6.9238e-01,\n",
       "            3.4424e-01, -3.3997e-02],\n",
       "          [ 8.8562e-02,  4.0454e-01, -3.3789e-01,  ...,  1.1738e+00,\n",
       "           -1.1969e-01, -6.2256e-02],\n",
       "          [ 1.2245e-02,  1.8945e-01, -3.1274e-01,  ...,  1.0518e+00,\n",
       "            1.7908e-01,  6.3721e-02],\n",
       "          ...,\n",
       "          [ 2.8613e-01,  2.4243e-01, -2.9712e-01,  ...,  7.5635e-01,\n",
       "           -1.1377e-03,  2.2375e-01],\n",
       "          [ 3.5278e-01, -1.7944e-01, -4.4653e-01,  ...,  7.5000e-01,\n",
       "           -1.1243e-01, -1.0901e-01],\n",
       "          [-4.6802e-01,  1.5930e-01, -4.7943e-02,  ...,  3.9978e-02,\n",
       "            3.1494e-01, -1.7212e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1576, -0.1218,  0.0814,  ...,  0.3206, -0.1595, -0.1127],\n",
       "          [ 0.0808, -0.0765, -0.0241,  ...,  0.2610,  0.1459, -0.2515]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.0270e-03, 1.5497e-06, 4.3511e-06, 5.3644e-07, 2.2054e-06, 2.9802e-07,\n",
       "           5.9605e-07, 6.5565e-07, 9.9707e-01],\n",
       "          [4.0833e-02, 2.1577e-05, 1.3840e-04, 9.5367e-06, 1.8418e-05, 2.9802e-06,\n",
       "           8.7023e-06, 7.8082e-06, 9.5898e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0   235.753830  193.057404  319.157257  296.182709    0.913627     14     bird\n",
       "  1   357.524323  157.898499  420.563629  240.605957    0.886664      0   person\n",
       "  2   333.567108  181.296509  393.691010  255.126709    0.832969     13    bench\n",
       "  3   406.138824  318.605957  453.631195  349.125427    0.774343     14     bird\n",
       "  4   611.345337  357.315918  640.000000  382.651245    0.769883     14     bird\n",
       "  5   591.530823  322.194733  620.577454  348.258942    0.756356     14     bird\n",
       "  6   446.118774  341.410126  491.528503  378.847076    0.753715     14     bird\n",
       "  7   590.219055  292.181519  624.712830  314.490295    0.710358     14     bird\n",
       "  8   175.738022  261.808746  552.520386  423.498779    0.674705     13    bench\n",
       "  9   455.627869  302.079651  495.808411  336.677307    0.650604     14     bird\n",
       "  10  408.783905  294.961182  428.222443  318.979309    0.615603     14     bird\n",
       "  11  524.811707  297.367279  545.084412  339.987152    0.613440     14     bird\n",
       "  12  353.482819  368.869598  393.553558  396.733551    0.542325     14     bird\n",
       "  13  344.517365  317.943176  386.052765  357.291321    0.509256     14     bird\n",
       "  14    0.000000  176.458008   34.630951  225.534546    0.506493     13    bench\n",
       "  15  314.791077  262.886749  344.992737  296.810944    0.465354     14     bird\n",
       "  16  569.585144  286.499817  599.564758  302.935364    0.454990     14     bird\n",
       "  17  533.599731  283.215210  562.154053  314.264832    0.453937     14     bird\n",
       "  18  360.120819  287.980499  387.619354  313.749481    0.421640     14     bird\n",
       "  19  280.926300  136.166809  368.758514  177.451050    0.414228      2      car\n",
       "  20  467.888458  280.144257  498.999847  314.741791    0.404983     14     bird\n",
       "  21  446.239685  140.559235  504.356689  177.770660    0.380957      2      car\n",
       "  22  591.816956  172.412476  640.000000  226.544250    0.357991      1  bicycle\n",
       "  23  344.934174  317.869415  368.516022  348.369598    0.345163     14     bird\n",
       "  24  328.178680  349.751160  361.910065  382.616638    0.339411     14     bird\n",
       "  25  531.763550  163.660248  640.000000  227.830231    0.253423      1  bicycle,\n",
       "  'caption': ['A park bench with a pigeon sitting on it.',\n",
       "   'Wood bench with a bird perched on the back of it.'],\n",
       "  'bbox_target': [175.36, 261.22, 373.58, 159.15]},\n",
       " 188: {'image_emb': tensor([[-0.1272,  0.5566, -0.0364,  ...,  0.8335,  0.1541,  0.4570],\n",
       "          [-0.0601,  0.2656,  0.0999,  ...,  0.4121,  0.2920,  0.0984],\n",
       "          [ 0.0131,  0.5479,  0.3313,  ...,  0.7744,  0.2085,  0.2107],\n",
       "          [ 0.2705,  0.1001, -0.1406,  ...,  1.2900,  0.2891, -0.0721],\n",
       "          [-0.2529,  0.2003,  0.0239,  ...,  0.9951,  0.3188, -0.0124],\n",
       "          [-0.5649,  0.6006,  0.1671,  ...,  0.6509,  0.2096,  0.0221]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3235, -0.3435, -0.0096,  ...,  0.9507,  0.1639, -0.1763],\n",
       "          [ 0.4226, -0.1166,  0.1989,  ...,  0.5259,  0.0893, -0.4265]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.4766e-01, 2.6245e-01, 3.6450e-01, 6.6090e-04, 1.4809e-02, 9.8648e-03],\n",
       "          [4.3373e-03, 8.3936e-01, 1.5527e-01, 3.8683e-05, 7.7724e-04, 2.0278e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  151.654266   20.615311  268.925537  195.510864    0.936663      0   \n",
       "  1  469.417786   46.173340  565.456726  128.652466    0.929450     31   \n",
       "  2  490.943298   25.851562  556.414734   95.899658    0.869218      0   \n",
       "  3  111.840942  122.427307  164.602661  164.313049    0.849230      0   \n",
       "  4  116.258163  156.645508  239.605270  209.512817    0.752020     31   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1  snowboard  \n",
       "  2     person  \n",
       "  3     person  \n",
       "  4  snowboard  ,\n",
       "  'caption': ['The blue snowboard of a snowboarder attempting a jump.',\n",
       "   'A blue and red snowboard in the air'],\n",
       "  'bbox_target': [469.85, 46.35, 95.39, 82.44]},\n",
       " 189: {'image_emb': tensor([[ 0.1644,  0.2086,  0.1661,  ...,  0.9468,  0.2465,  0.3438],\n",
       "          [ 0.0263,  0.7661,  0.0514,  ...,  1.1641,  0.1531,  0.2003],\n",
       "          [-0.1606,  0.1409,  0.0051,  ...,  1.0723, -0.0617, -0.2830],\n",
       "          [ 0.1570,  0.6343, -0.2024,  ...,  0.5571,  0.2078,  0.0590]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-1.6614e-01,  3.3936e-02, -2.5854e-01, -1.9324e-01, -4.5923e-01,\n",
       "           -1.3562e-01, -8.8806e-02, -4.0894e-01, -1.3965e-01, -4.8431e-02,\n",
       "           -2.1680e-01, -2.6531e-03, -9.1125e-02, -1.6736e-01,  2.1753e-01,\n",
       "            3.1787e-01,  8.3191e-02, -2.6807e-01, -4.8682e-01,  4.9487e-01,\n",
       "           -2.7100e-01,  1.0901e-01,  2.5854e-01,  4.1924e-03,  2.2095e-01,\n",
       "            1.7664e-01, -1.7322e-01,  6.6040e-02, -3.4668e-01,  6.2158e-01,\n",
       "           -7.7881e-02, -5.1208e-02, -3.9062e-01,  3.2251e-01, -1.0480e-01,\n",
       "           -4.9414e-01,  4.3066e-01, -2.1936e-01,  4.2969e-02,  4.9957e-02,\n",
       "            3.0469e-01,  1.8933e-01, -5.9891e-03, -1.6922e-02, -5.9204e-02,\n",
       "           -8.3847e-03, -3.4644e-01,  2.3499e-02, -1.7383e-01,  1.1041e-01,\n",
       "           -2.3889e-01, -5.1807e-01,  6.9962e-03, -6.8359e-02, -2.1423e-02,\n",
       "            1.5295e-01, -3.6987e-01,  5.1971e-02,  1.9031e-01,  3.6450e-01,\n",
       "           -8.4534e-02, -1.0846e-01, -9.4788e-02, -3.6890e-01,  1.4001e-01,\n",
       "            1.9263e-01, -4.5715e-02,  4.5361e-01, -1.9043e-01,  8.5571e-02,\n",
       "            2.6123e-01, -3.1299e-01,  1.9861e-01,  2.5830e-01,  4.3335e-01,\n",
       "           -1.9824e-01,  8.4076e-03, -3.9209e-01, -4.7089e-02, -2.5415e-01,\n",
       "           -1.2610e-01,  2.9224e-01, -5.7678e-02, -1.0205e-01, -8.8013e-02,\n",
       "           -3.7451e-01, -1.3196e-01,  1.8262e-01,  3.8319e-03, -1.0406e-01,\n",
       "           -2.6154e-02, -1.4465e-01, -8.5791e-01, -7.7820e-02, -3.5181e-01,\n",
       "           -2.3450e-01, -3.7134e-01,  2.5101e-02, -3.7012e-01,  2.7051e-01,\n",
       "            4.5947e-01,  7.7820e-02,  2.4500e-01, -3.7256e-01, -1.0681e-01,\n",
       "            1.0950e-01, -2.4207e-01, -3.6035e-01,  8.8120e-03, -2.9150e-01,\n",
       "           -2.6611e-01,  1.8726e-01,  4.0222e-02,  5.3040e-02,  1.1969e-01,\n",
       "           -1.2085e-01,  1.7969e-01, -1.4990e-01, -2.1021e-01,  1.2286e-01,\n",
       "            1.3098e-01, -1.0303e+00, -3.8965e-01,  9.3445e-02,  1.5344e-01,\n",
       "            3.6507e-03,  2.5098e-01, -6.9397e-02,  4.3066e-01,  2.8271e-01,\n",
       "            1.2488e-01, -4.1235e-01,  4.8364e-01,  3.5547e+00,  4.8096e-02,\n",
       "           -3.6499e-01, -6.1035e-01, -4.9658e-01, -1.4587e-01, -4.7998e-01,\n",
       "           -1.0236e-01,  1.8152e-01, -3.4961e-01, -1.4136e-01, -2.1118e-01,\n",
       "           -2.7417e-01, -1.0889e-01, -3.8965e-01, -2.0972e-01,  1.0791e-01,\n",
       "           -1.8286e-01,  2.4524e-01, -1.6980e-01,  2.2717e-01, -3.1421e-01,\n",
       "           -2.9053e-01,  2.2781e-02, -4.5117e-01, -1.3403e-01, -4.1016e-02,\n",
       "            1.3623e-01, -6.4209e-01, -1.2817e-01,  1.7297e-01, -4.4727e-01,\n",
       "           -7.1533e-02,  1.3623e-01, -2.2876e-01,  1.9666e-01,  4.1687e-02,\n",
       "           -3.8849e-02,  8.4167e-02,  2.6636e-01,  1.9080e-01, -2.5952e-01,\n",
       "            5.3320e-01, -1.2573e-01,  2.6221e-01, -1.2732e-01, -3.5919e-02,\n",
       "            7.6027e-03, -1.1194e-01,  6.4516e-04,  1.7441e-02, -3.7402e-01,\n",
       "           -1.1920e-01, -3.7305e-01, -2.5098e-01,  4.0039e-01,  1.9971e-01,\n",
       "            6.6211e-01,  6.7578e-01,  3.7231e-01, -7.9163e-02,  1.3086e-01,\n",
       "            2.6465e-01, -1.1682e-01,  4.6729e-01, -3.9429e-02, -2.9938e-02,\n",
       "            4.6082e-02, -3.2397e-01, -1.5369e-01,  7.2876e-02, -7.4524e-02,\n",
       "            1.3501e-01,  3.9276e-02, -1.5454e-01, -1.3098e-01, -2.2388e-01,\n",
       "           -2.2388e-01, -1.1778e-03,  7.3814e-03,  2.3926e-01, -1.0223e-01,\n",
       "            8.6792e-02, -5.5713e-01, -4.3408e-01, -1.2428e-02, -1.2177e-02,\n",
       "            4.7314e-01, -8.7585e-02,  7.2815e-02,  6.1584e-02,  1.4404e-01,\n",
       "           -1.9141e-01, -1.8872e-01, -1.3049e-01, -1.1578e-01, -5.9180e-01,\n",
       "           -2.1838e-01,  5.0049e-01,  4.5239e-01, -6.6113e-01, -1.4172e-01,\n",
       "            4.5264e-01,  2.1753e-01,  6.5735e-02, -1.7749e-01, -2.4719e-01,\n",
       "            2.4670e-01,  4.0063e-01, -1.0846e-01,  2.4768e-01, -8.4412e-02,\n",
       "            2.6270e-01, -2.9617e-02,  3.4473e-01,  3.1323e-01, -1.5491e-01,\n",
       "           -5.4834e-01,  5.4443e-02,  2.6392e-01,  1.6861e-02, -2.2437e-01,\n",
       "           -2.2278e-01,  1.3855e-01,  1.2683e-01, -8.3374e-02,  6.3525e-01,\n",
       "            3.4119e-02,  4.1473e-02,  4.6387e-01,  2.2510e-01, -8.6792e-02,\n",
       "           -1.3538e-01, -3.3417e-02, -1.0956e-01, -2.3462e-01, -2.5024e-01,\n",
       "            1.8689e-01, -1.5625e-01, -6.5247e-02, -1.4551e-01, -3.3105e-01,\n",
       "            3.7451e-01,  7.0923e-02, -7.4036e-02,  1.6406e-01, -2.2559e-01,\n",
       "            1.7505e-01, -4.6143e-02,  3.7183e-01,  3.5815e-01,  4.4678e-02,\n",
       "           -1.9409e-01, -7.3975e-02,  1.2500e-01,  2.8760e-01, -3.6792e-01,\n",
       "            2.5293e-01, -3.4302e-02, -6.7322e-02,  8.7891e-02, -7.5562e-02,\n",
       "            1.3513e-01, -4.0466e-02,  6.6042e-04,  4.4128e-02, -1.6919e-01,\n",
       "           -5.5957e-01, -6.9775e-01, -5.2051e-01, -1.6467e-01, -1.0760e-01,\n",
       "            1.3232e-01, -2.3621e-02,  1.2756e-01, -1.1009e-02, -3.4082e-01,\n",
       "           -2.5610e-01,  4.3457e-01,  3.5508e+00,  3.6279e-01,  1.8652e-01,\n",
       "            1.0138e-01,  3.8257e-01,  1.8726e-01,  4.0063e-01,  1.1322e-01,\n",
       "           -2.3914e-01,  3.3154e-01,  9.9792e-02,  1.7395e-01, -4.9683e-01,\n",
       "            1.3049e-01,  8.2764e-02,  7.6538e-02, -1.4048e-03, -1.1045e+00,\n",
       "           -1.8567e-01, -3.8062e-01,  1.8494e-01, -7.1594e-02, -2.8107e-02,\n",
       "           -1.8079e-01,  1.8750e-01, -1.4868e-01, -4.4824e-01,  3.3398e-01,\n",
       "           -4.9194e-01, -1.1060e-01,  4.8859e-02,  2.1375e-01,  2.0129e-01,\n",
       "           -3.4523e-03, -2.5253e-02,  9.1370e-02,  7.0557e-02,  1.8591e-01,\n",
       "            2.7100e-01, -9.4299e-02,  2.4585e-01,  2.5284e-02, -3.2330e-04,\n",
       "           -7.4561e-01, -3.6084e-01, -3.5534e-03, -1.3928e-01, -2.3636e-02,\n",
       "           -5.3925e-02, -1.6077e-01,  2.2461e-01, -3.1543e-01, -3.0493e-01,\n",
       "           -3.9253e-03, -1.4148e-01,  2.3242e-01, -1.3977e-01, -2.3779e-01,\n",
       "           -4.8096e-02, -1.0107e-01,  1.1987e-01,  5.3345e-02,  2.0459e-01,\n",
       "            8.0444e-02, -1.1475e-01,  4.8767e-02,  5.0995e-02,  1.1499e-01,\n",
       "           -7.4890e-02, -3.1494e-01,  4.0894e-01, -2.1338e-01, -4.7339e-01,\n",
       "            9.9731e-02,  2.4048e-01,  9.0820e-02, -4.6069e-01, -9.4788e-02,\n",
       "           -6.9531e-01, -5.2826e-02, -5.4590e-01, -2.0386e-01,  1.2451e-01,\n",
       "            1.8884e-01, -2.4734e-02,  4.4067e-01, -6.7932e-02,  4.2993e-01,\n",
       "            1.5930e-01, -4.8920e-02, -1.0297e-01, -9.4147e-03, -1.8494e-01,\n",
       "           -2.0984e-01, -4.2578e-01,  2.4011e-01,  1.2140e-01, -2.6703e-02,\n",
       "           -2.3413e-01,  6.2927e-02, -4.6094e-01,  1.3184e-01,  6.8237e-02,\n",
       "            8.3130e-02, -3.4668e-01, -2.8662e-01, -2.9932e-01, -2.5415e-01,\n",
       "           -4.1455e-01, -4.5288e-01,  3.2544e-01, -6.5283e-01, -8.9355e-02,\n",
       "            1.0846e-01, -2.5977e-01, -2.7173e-01, -6.5063e-02, -6.9275e-02,\n",
       "           -5.7031e-01,  2.3779e-01,  1.8970e-01,  1.2769e-01, -1.3748e-02,\n",
       "            1.4417e-01, -6.9971e-01,  2.3926e-02, -3.4760e-02,  4.1040e-01,\n",
       "            8.0872e-02,  5.3619e-02,  1.7319e-02,  8.3389e-03, -3.2251e-01,\n",
       "            1.5466e-01,  6.9519e-02,  2.2168e-01, -3.7506e-02,  9.7290e-02,\n",
       "           -2.2241e-01, -6.6223e-02, -1.8799e-01, -1.4246e-01,  1.7273e-01,\n",
       "            4.4189e-01, -3.3496e-01,  1.7407e-01, -1.4807e-01, -1.3904e-01,\n",
       "           -6.9504e-03, -1.1377e-01, -2.2595e-01, -1.9617e-01,  5.3125e-01,\n",
       "            8.3374e-02,  2.4643e-02, -2.6587e-01,  8.8013e-02, -2.2778e-01,\n",
       "           -1.4030e-02, -3.9978e-02,  2.1167e-01, -3.7012e-01,  2.9761e-01,\n",
       "            8.0444e-02, -4.7852e-02, -1.4746e-01,  2.4927e-01,  7.1240e-01,\n",
       "            3.5693e-01, -2.0691e-01, -1.0950e-01, -2.2485e-01, -4.6777e-01,\n",
       "           -2.7930e-01, -1.2854e-01,  3.4961e-01, -2.2720e-02, -1.1664e-01,\n",
       "            3.8403e-01, -3.6475e-01,  9.9182e-02,  1.1699e+00,  1.8152e-01,\n",
       "           -2.1313e-01, -9.7656e-02, -3.3264e-02, -2.4451e-01,  6.7871e-02,\n",
       "            2.1741e-01, -6.2317e-02,  3.2422e-01,  1.2250e-01,  6.6699e-01,\n",
       "           -5.0232e-02, -2.9395e-01, -3.5950e-02,  5.3467e-01,  4.2969e-01,\n",
       "            3.7671e-01,  1.7993e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.5010e-01, 2.0950e-02, 6.6280e-04, 1.2830e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  239.081787   16.300392  456.492706  215.762405    0.811513     46  banana\n",
       "  1   63.585411   30.216705  248.705292  213.246155    0.751229     49  orange\n",
       "  2  142.630600    3.335318  278.909149   78.827301    0.735132     49  orange\n",
       "  3  134.916260  222.392136  387.302399  360.479889    0.623594     49  orange\n",
       "  4   62.982971  203.073715  156.050690  322.443420    0.611795     39  bottle\n",
       "  5    4.789519  299.680542  243.779587  499.315887    0.589574     49  orange\n",
       "  6   21.288227    3.215337  487.108337  242.755859    0.567363     45    bowl\n",
       "  7  391.286621  140.740784  499.700317  348.611511    0.299074     46  banana,\n",
       "  'caption': ['In the bottom fruit basket, the green shiny item beside the end of the bananas.'],\n",
       "  'bbox_target': [342.43, 279.78, 152.51, 214.21]},\n",
       " 190: {'image_emb': tensor([[ 0.1066,  0.7183, -0.0877,  ...,  1.0137, -0.0046, -0.1395],\n",
       "          [-0.3745,  0.6855,  0.0298,  ...,  0.9170,  0.2605, -0.1125],\n",
       "          [-0.3499,  0.3855,  0.0845,  ...,  1.1309,  0.1604, -0.2495],\n",
       "          [ 0.0497, -0.3782, -0.3296,  ...,  0.8218, -0.1396, -0.1229],\n",
       "          [-0.4553,  0.5645,  0.1553,  ...,  0.6040,  0.1851,  0.0232]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0307,  0.5815, -0.0735,  ...,  0.6611,  0.1000, -0.0818],\n",
       "          [ 0.0123, -0.2832, -0.4307,  ..., -0.4922,  0.3110, -0.1588]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.0471e-01, 3.8843e-01, 5.8655e-02, 5.9664e-05, 3.4814e-01],\n",
       "          [8.2617e-01, 4.5898e-02, 5.8014e-02, 3.5763e-05, 6.9946e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0   92.374207  135.927505  199.873840  314.520325    0.885971      0    person\n",
       "  1   87.049507   72.696365  151.475586  192.939240    0.873321      0    person\n",
       "  2  115.412598   28.993618  190.569748  164.428299    0.731026      0    person\n",
       "  3  135.881760  164.624725  143.350159  192.371628    0.718121     27       tie\n",
       "  4    3.274257    2.952890  349.059326  306.708160    0.641768      4  airplane\n",
       "  5  117.573776   98.547623  124.334793  125.858414    0.573254     27       tie\n",
       "  6  139.273514   57.214245  147.245026  100.335342    0.259700     27       tie,\n",
       "  'caption': ['President Bush waving from an airplane',\n",
       "   'The man leading the others down the stairs.'],\n",
       "  'bbox_target': [89.1, 140.01, 110.28, 176.01]},\n",
       " 191: {'image_emb': tensor([[ 0.0839,  0.4922, -0.2466,  ...,  1.0244, -0.2061,  0.1046],\n",
       "          [ 0.0024,  0.4500, -0.1980,  ...,  0.9053, -0.0176,  0.1567],\n",
       "          [-0.0324,  0.6548, -0.4238,  ...,  0.9951, -0.0357,  0.1403],\n",
       "          [ 0.4758,  0.0597,  0.2126,  ...,  0.6465, -0.1459,  0.2380]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2242,  0.3579, -0.0822,  ..., -0.1890, -0.5195, -0.2194],\n",
       "          [ 0.1105,  0.2471, -0.3357,  ...,  0.2008, -0.2766,  0.0439]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.0862e-06, 1.3428e-03, 9.8047e-01, 1.7960e-02],\n",
       "          [1.3113e-06, 9.1455e-01, 1.1513e-02, 7.3914e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  355.282990   16.355614  467.375793  335.083496    0.920187      0  person\n",
       "  1   78.291550  236.184555  157.886627  320.338440    0.915825     63  laptop\n",
       "  2   18.476915  281.499237  453.897339  386.916840    0.762599      0  person\n",
       "  3   15.338087  283.531158  477.084869  392.655457    0.560260     59     bed,\n",
       "  'caption': ['A man laying on a bed.',\n",
       "   'A man laying down on a bed while typing away on a laptop that is on his chest.'],\n",
       "  'bbox_target': [15.93, 280.08, 432.63, 85.1]},\n",
       " 192: {'image_emb': tensor([[-0.0139,  0.4915,  0.1605,  ...,  1.2832,  0.3181, -0.0702],\n",
       "          [-0.4316,  0.6729,  0.2225,  ...,  1.1807,  0.2944, -0.2720],\n",
       "          [-0.4207,  0.2854, -0.1646,  ...,  0.1350, -0.3337,  0.1992]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2852,  0.2256,  0.2118,  ...,  0.3169, -0.0607, -0.1979],\n",
       "          [-0.3689,  0.5605, -0.0529,  ...,  0.0770,  0.2515, -0.5713]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.6321e-04, 8.8965e-01, 1.0962e-01],\n",
       "          [1.0595e-03, 9.9414e-01, 4.9019e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    0.096817   92.887733  115.639145  203.601929    0.891506     45    bowl\n",
       "  1  115.564957   31.648148  286.068970  117.731567    0.870772     45    bowl\n",
       "  2    0.274429   33.421310   53.023331   85.293098    0.665549     49  orange\n",
       "  3  272.293945   88.858398  359.735718  225.232056    0.662296     45    bowl\n",
       "  4    6.241684   58.983978   91.405899  103.270035    0.604149     49  orange\n",
       "  5  305.593811   18.846100  360.000000   85.566040    0.530513     49  orange\n",
       "  6  269.752747  154.380005  309.871521  178.051117    0.522532     44   spoon\n",
       "  7   60.113876   35.999897  152.440918   70.190109    0.403613     43   knife\n",
       "  8  332.360687   24.410147  360.000000   82.727142    0.335576     49  orange\n",
       "  9   68.732559  225.533844  142.462891  285.560822    0.316059     47   apple,\n",
       "  'caption': ['The bowl of orange marmalade that has a butter knife in it.',\n",
       "   'An image of an lemon marmalade drink in a cup with a stirring stick.'],\n",
       "  'bbox_target': [113.93, 28.93, 171.89, 86.21]},\n",
       " 193: {'image_emb': tensor([[ 0.1213,  0.8496,  0.0524,  ...,  0.6924,  0.1984,  0.2849],\n",
       "          [-0.1186,  0.1821,  0.1392,  ...,  0.8838,  0.0892,  0.4578],\n",
       "          [ 0.0764,  0.4641, -0.0602,  ...,  0.7397,  0.2920, -0.0191]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1520, -0.0228, -0.1998,  ..., -0.2532, -0.1260, -0.0547],\n",
       "          [-0.1339,  0.1428, -0.1661,  ..., -0.1108, -0.0325,  0.0294]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0247, 0.9429, 0.0323],\n",
       "          [0.0242, 0.9507, 0.0253]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   10.437424  109.944702  235.247269  468.482056    0.957877      0  person\n",
       "  1  268.760071   61.349457  640.000000  474.928711    0.956472      0  person,\n",
       "  'caption': ['Woman using a blue towel on a silver ring',\n",
       "   'a woman drying her hands'],\n",
       "  'bbox_target': [266.91, 66.37, 373.09, 407.89]},\n",
       " 194: {'image_emb': tensor([[ 4.1931e-02, -5.2032e-02, -1.4502e-01,  2.5366e-01,  1.4441e-01,\n",
       "            2.2595e-01,  4.8157e-02, -1.3513e-01, -1.7590e-01,  4.9957e-02,\n",
       "            3.9917e-01,  1.8225e-01, -4.3384e-01, -1.3318e-01, -9.4421e-02,\n",
       "           -2.5024e-02,  5.3271e-01, -3.6768e-01,  4.8267e-01, -3.7183e-01,\n",
       "           -2.1562e+00, -2.4805e-01,  9.6375e-02, -5.8057e-01,  2.7563e-01,\n",
       "            8.9600e-02,  1.6174e-01,  2.3132e-01, -1.3342e-01, -4.0332e-01,\n",
       "            5.1605e-02,  1.8884e-01, -3.9490e-02,  1.3135e-01,  3.3643e-01,\n",
       "           -3.3276e-01, -1.4267e-02, -5.2295e-01, -3.5327e-01,  1.0322e+00,\n",
       "            2.4463e-01,  2.7588e-01, -6.8750e-01, -2.9736e-01, -1.0010e-01,\n",
       "           -6.0028e-02, -8.2153e-02,  2.7881e-01,  2.0599e-02,  4.2261e-01,\n",
       "           -3.9307e-01, -4.9805e-01, -2.4628e-02,  1.4795e-01, -4.0308e-01,\n",
       "            2.2461e-01, -3.1738e-01, -4.1284e-01, -7.1289e-02,  1.2659e-01,\n",
       "           -7.0007e-02,  4.2023e-02, -5.5957e-01, -8.0948e-03,  4.7424e-02,\n",
       "            6.0498e-01, -2.0703e-01,  8.6914e-01, -2.2437e-01, -7.7686e-01,\n",
       "            8.6670e-03, -3.2446e-01, -6.7480e-01,  4.4092e-01,  4.4212e-03,\n",
       "           -2.3523e-01,  2.3218e-01, -2.7417e-01, -4.1211e-01,  2.7786e-02,\n",
       "            3.1909e-01, -4.4629e-01,  2.4695e-01, -1.2939e-01, -6.6748e-01,\n",
       "           -3.3228e-01,  9.9854e-01, -2.2144e-01,  7.5244e-01,  6.7368e-03,\n",
       "            5.5176e-01,  4.0161e-01, -6.7578e+00, -4.1235e-01,  4.2676e-01,\n",
       "           -1.7615e-01, -5.1636e-02, -7.9773e-02, -9.2285e-01, -2.5317e-01,\n",
       "           -7.1472e-02,  5.0140e-02, -2.9321e-01, -1.9028e-02, -6.3477e-01,\n",
       "            6.8909e-02, -9.4788e-02,  3.1470e-01, -1.0596e-01,  5.2637e-01,\n",
       "            1.4221e-01, -5.5127e-01, -2.4939e-01, -3.6530e-02,  2.1472e-01,\n",
       "           -7.2266e-02,  7.2705e-01,  3.2520e-01,  3.3105e-01, -6.3574e-01,\n",
       "            2.1240e-01,  5.3516e-01, -1.4172e-01, -1.4136e-01,  3.3569e-01,\n",
       "           -1.9495e-01,  1.3232e-01, -4.4891e-02,  3.8672e-01,  9.7107e-02,\n",
       "            1.0117e+00, -3.1226e-01, -2.2217e-01,  9.0088e-01,  6.5674e-02,\n",
       "            2.4548e-01,  7.8857e-02, -3.3740e-01, -1.5796e-01,  3.6353e-01,\n",
       "           -1.8481e-01,  6.0043e-03, -1.6980e-01,  1.7761e-02, -3.5913e-01,\n",
       "            8.1726e-02, -2.7783e-01, -3.0151e-02, -2.4585e-01, -3.6230e-01,\n",
       "            9.0515e-02,  6.5369e-02,  5.8984e-01, -4.4385e-01, -1.3208e-01,\n",
       "           -6.4331e-02,  5.8838e-01, -1.6846e-01,  1.9519e-01,  1.5698e-01,\n",
       "           -5.4492e-01, -1.8347e-01, -4.3555e-01, -3.6206e-01,  3.4644e-01,\n",
       "           -9.2773e-02, -2.6514e-01,  4.0039e-01, -7.2266e-02,  2.9248e-01,\n",
       "            7.4829e-02,  7.1240e-01,  2.1802e-01, -6.0181e-02,  2.0483e-01,\n",
       "           -3.4760e-02, -1.8193e+00, -3.0371e-01, -1.4929e-01, -4.1479e-01,\n",
       "            9.1675e-02, -4.2383e-01,  1.2671e-01, -5.5481e-02,  3.0615e-01,\n",
       "           -5.3125e-01,  3.1421e-01,  3.8110e-01, -3.6914e-01,  8.1055e-02,\n",
       "           -2.4976e-01,  6.2744e-02,  3.7524e-01,  2.7515e-01,  5.5127e-01,\n",
       "            1.2140e-01, -3.6646e-01,  2.3987e-01,  3.7256e-01,  4.9756e-01,\n",
       "            5.5127e-01, -3.6865e-02,  1.9263e-01,  6.5857e-02,  2.2156e-01,\n",
       "           -5.0146e-01, -3.4619e-01,  1.2917e-02, -1.5881e-01, -9.6252e-02,\n",
       "           -4.8523e-02,  2.5244e-01,  8.9294e-02,  5.4535e-02,  4.2358e-01,\n",
       "           -2.4829e-01, -2.3999e-01,  4.3921e-01,  6.6504e-01,  2.0850e-01,\n",
       "            2.9321e-01,  1.7929e-02, -2.4268e-01, -5.6299e-01, -5.1117e-02,\n",
       "            2.9688e-01,  3.1677e-02, -4.2334e-01, -1.6772e-01, -1.5671e-02,\n",
       "           -2.4939e-01,  3.1372e-01,  1.2549e-01, -8.0762e-01, -3.9624e-01,\n",
       "            6.6113e-01, -1.0422e-02, -1.3684e-01,  4.4580e-01,  4.5020e-01,\n",
       "            2.1033e-01, -5.6946e-02,  6.3135e-01, -1.4294e-01,  9.7229e-02,\n",
       "           -3.0786e-01,  7.4272e-03,  3.1714e-01,  6.8359e-02, -9.1602e-01,\n",
       "           -1.3818e-01,  2.8223e-01,  2.3328e-01,  5.0928e-01,  1.9299e-01,\n",
       "            1.5491e-01,  4.1357e-01,  5.0732e-01, -1.2246e+00,  8.3008e-01,\n",
       "           -4.8676e-02,  3.3960e-01, -1.4549e-02,  8.4180e-01, -5.0537e-02,\n",
       "            3.5596e-01,  2.3596e-01,  2.0654e-01,  5.1361e-02, -2.5610e-01,\n",
       "            2.1313e-01, -2.4207e-01, -1.5588e-01, -1.3562e-01,  3.8300e-02,\n",
       "            6.2256e-02, -1.9238e-01, -4.5801e-01, -6.3599e-02,  9.7595e-02,\n",
       "            4.6973e-01,  3.5522e-01, -1.4099e-01,  1.8848e-01, -2.2754e-01,\n",
       "            4.9683e-01, -7.5830e-01, -1.2482e-01,  2.9785e-01, -3.7085e-01,\n",
       "           -2.7985e-02, -1.8506e-01,  1.5540e-01, -1.6187e-01, -1.9031e-01,\n",
       "            4.6387e-01,  9.4055e-02, -1.3879e-01,  5.6006e-01, -3.4644e-01,\n",
       "           -6.1066e-02, -6.5491e-02, -3.7329e-01,  5.3857e-01,  7.5439e-02,\n",
       "            2.3145e-01, -6.9885e-02,  5.1660e-01,  1.1627e-01, -2.7344e-01,\n",
       "            3.1567e-01,  2.2827e-01,  9.0137e-01, -2.6831e-01, -3.2690e-01,\n",
       "            6.2061e-01,  6.6260e-01,  2.3145e-01,  2.7393e-01, -3.6865e-02,\n",
       "           -2.6904e-01,  2.1504e+00,  2.4768e-01, -1.7969e-01, -3.2788e-01,\n",
       "            4.6924e-01, -3.3105e-01, -4.1333e-01, -1.8262e-01, -4.6478e-02,\n",
       "           -2.2803e-01, -1.4990e-01,  2.1362e-01, -2.4988e-01, -4.3237e-01,\n",
       "            1.4197e-01, -3.1006e-01,  5.4834e-01,  1.6345e-01,  2.6660e-01,\n",
       "           -1.0455e-01, -2.5684e-01,  1.1761e-01, -2.5000e-01,  1.4380e-01,\n",
       "            1.8286e-01, -2.1118e-01,  9.8633e-02,  3.9209e-01,  1.8295e-02,\n",
       "            6.5234e-01, -8.4106e-02, -1.2260e-02,  1.2891e-01,  1.3405e-02,\n",
       "            2.9785e-01, -1.5222e-01, -3.9624e-01,  2.0340e-02, -2.2949e-01,\n",
       "           -7.4316e-01, -6.0005e-03,  2.1790e-01,  1.1578e-01,  4.6704e-01,\n",
       "            4.0967e-01, -4.3896e-01, -2.2131e-01, -4.7531e-03, -4.8877e-01,\n",
       "           -2.3346e-02,  7.7942e-02,  5.5371e-01,  3.7402e-01,  1.8726e-01,\n",
       "           -1.3098e-01,  7.1631e-01, -2.6050e-01, -4.7778e-01, -2.4951e-01,\n",
       "            1.1420e-01, -7.6416e-01,  5.0732e-01, -6.2805e-02,  1.8506e-01,\n",
       "           -3.7354e-01, -8.9722e-02, -7.0435e-02, -5.4102e-01, -1.3350e+00,\n",
       "            2.9922e-02, -4.6216e-01,  4.5215e-01,  1.9153e-01, -1.9043e-02,\n",
       "            3.4009e-01, -7.6538e-02, -2.2998e-01,  1.8237e-01, -1.9250e-01,\n",
       "            1.2134e-01, -3.1757e-03, -6.9727e-01, -1.8469e-01, -2.7832e-01,\n",
       "           -2.1948e-01,  1.4966e-01, -2.3560e-01,  1.4542e-02, -3.4839e-01,\n",
       "           -5.9668e-01,  1.8054e-01,  8.7463e-02,  2.4719e-01, -2.6245e-01,\n",
       "            4.2114e-02, -2.5854e-01,  1.4807e-01,  1.4514e-01, -5.8365e-03,\n",
       "           -2.3605e-02, -5.2783e-01,  2.3853e-01,  3.4717e-01, -6.6211e-01,\n",
       "           -2.8345e-01,  2.6416e-01, -2.6855e-01, -2.4980e+00, -8.4351e-02,\n",
       "            1.6678e-02,  5.2832e-01,  4.5117e-01, -1.1517e-01,  3.5840e-01,\n",
       "           -1.9373e-01,  2.2681e-01,  3.5400e-01,  7.3425e-02, -7.6416e-02,\n",
       "            1.1591e-01,  1.9226e-01, -9.6741e-02,  5.7678e-02, -1.1090e-01,\n",
       "           -5.0964e-02,  1.8604e-01, -2.3743e-01, -4.4629e-01,  3.7622e-01,\n",
       "           -9.6680e-02,  8.8989e-02, -2.7515e-01, -1.0199e-01,  3.3234e-02,\n",
       "           -3.4033e-01, -6.9458e-02, -2.4146e-01, -2.0227e-01,  1.0767e-01,\n",
       "            8.8501e-02,  4.2383e-01, -4.2163e-01, -1.7834e-01,  2.1680e-01,\n",
       "            1.3159e-01,  1.5039e-01, -2.9199e-01, -7.4268e-01,  1.2854e-01,\n",
       "           -1.2280e-01,  2.3096e-01, -3.9331e-01, -4.3823e-01, -1.7529e-01,\n",
       "           -1.3550e-01, -4.2749e-01, -2.4207e-01,  7.3682e-01, -2.5903e-01,\n",
       "            7.0740e-02,  2.0557e-01,  2.9556e-02,  1.6199e-01, -5.0964e-02,\n",
       "            5.0140e-02, -1.4111e-01,  2.9126e-01,  1.2793e-01, -6.2842e-01,\n",
       "            2.1765e-01, -3.4546e-01,  2.2781e-02, -3.1616e-01,  1.1176e-01,\n",
       "            2.2644e-01,  4.4739e-02, -1.7834e-01, -5.5273e-01, -2.3083e-01,\n",
       "            9.5093e-02,  4.0674e-01,  2.5610e-01,  5.0098e-01,  2.4585e-01,\n",
       "           -4.7266e-01, -2.4811e-02,  2.2232e-02, -3.9404e-01,  4.6533e-01,\n",
       "           -2.5391e-01,  2.8027e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0358,  0.0675, -0.0530,  ..., -0.3098, -0.0340, -0.2405],\n",
       "          [-0.1830, -0.2188, -0.0864,  ..., -0.2571, -0.0524, -0.3479]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.],\n",
       "          [1.]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0    0.000000   40.332916  433.000000  623.234619    0.539085     45      bowl\n",
       "  1    0.000000  157.046890  433.000000  622.007324    0.537248     50  broccoli\n",
       "  2  278.816589  337.297150  432.756653  524.962585    0.410439     50  broccoli,\n",
       "  'caption': ['broccoli touching the edge of the photograph on the bottom right',\n",
       "   'The broccoli spear facing down on the bottom right.'],\n",
       "  'bbox_target': [256.0, 335.46, 177.0, 198.47]},\n",
       " 195: {'image_emb': tensor([[-3.5400e-01,  6.8652e-01, -9.9609e-02,  ...,  1.1543e+00,\n",
       "            2.3120e-01,  1.2683e-01],\n",
       "          [-6.3135e-01,  3.8599e-01, -2.2607e-01,  ...,  1.0371e+00,\n",
       "            2.9160e-02,  1.7041e-01],\n",
       "          [-1.7236e-01,  4.8242e-01,  3.8147e-05,  ...,  1.0771e+00,\n",
       "            4.9902e-01, -7.1716e-02],\n",
       "          [-4.6704e-01,  6.1279e-01, -4.3243e-02,  ...,  7.6367e-01,\n",
       "            5.6592e-01, -4.3884e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 2.2437e-01,  1.0101e-01, -1.5674e-01, -8.5449e-02,  1.2061e-01,\n",
       "           -1.3586e-01, -4.0576e-01, -1.1514e+00, -1.2756e-01,  2.3340e-01,\n",
       "           -4.6783e-02,  3.8879e-02, -1.0419e-01, -2.1545e-01,  3.5547e-01,\n",
       "           -2.4939e-04, -1.1755e-01, -9.8572e-02, -2.7856e-01,  1.5210e-01,\n",
       "            5.1660e-01,  6.5796e-02,  3.1616e-02,  7.3425e-02,  6.1676e-02,\n",
       "            1.7493e-01,  1.8921e-01,  3.1323e-01, -6.2500e-02,  1.4771e-01,\n",
       "            1.1192e-02,  1.8579e-01, -1.8616e-01,  6.9946e-02, -1.4490e-01,\n",
       "           -7.8247e-02,  4.7144e-01,  2.2498e-01, -1.0101e-01,  2.2571e-01,\n",
       "           -6.3843e-02, -6.3293e-02, -2.0740e-01,  2.2925e-01,  1.3367e-01,\n",
       "           -2.2324e-02,  3.0716e-02,  1.6309e-01,  1.6235e-01, -1.3145e-02,\n",
       "           -4.0894e-02, -1.8396e-01,  6.1859e-02, -4.6753e-01, -6.0150e-02,\n",
       "           -1.0278e-01,  9.3567e-02, -1.3062e-01,  1.9629e-01,  1.5186e-01,\n",
       "           -2.0752e-01, -1.5125e-01,  1.6846e-01,  6.0501e-03,  2.0496e-01,\n",
       "           -2.1863e-01,  4.2084e-02, -7.2937e-02,  1.6992e-01, -1.7029e-01,\n",
       "            3.6060e-01,  9.2285e-02,  1.2402e-01,  2.4573e-01, -1.6675e-01,\n",
       "           -2.8931e-02,  2.6398e-02,  2.3511e-01,  3.5950e-02, -1.9128e-01,\n",
       "           -1.3062e-01,  3.4424e-01, -1.1725e-01,  3.8184e-01, -3.9868e-01,\n",
       "           -9.7351e-02,  2.1851e-01, -5.5725e-02,  2.1582e-01,  7.3914e-02,\n",
       "            2.9272e-01,  1.2720e-01, -1.5244e+00,  5.0586e-01, -4.4751e-01,\n",
       "            1.5601e-01,  2.2119e-01, -2.0972e-01, -2.6123e-01, -2.6221e-01,\n",
       "           -7.5436e-04,  1.4368e-01,  3.2886e-01, -2.9517e-01, -3.3447e-01,\n",
       "           -7.7881e-02,  1.1237e-01,  1.3599e-01,  5.6366e-02,  4.1187e-01,\n",
       "           -3.2397e-01,  6.1523e-02, -8.6609e-02,  2.1851e-01, -1.0193e-01,\n",
       "            4.4159e-02, -2.9590e-01, -4.0222e-02, -5.8496e-01,  1.2030e-01,\n",
       "            1.6003e-01, -4.9512e-01,  6.1554e-02, -2.5208e-02,  4.4897e-01,\n",
       "            1.5332e-01, -1.6370e-01,  1.7578e-02,  1.6174e-01,  4.6191e-01,\n",
       "           -3.5950e-02, -9.4299e-02, -1.2817e-01,  5.8906e+00,  1.1920e-01,\n",
       "            4.1237e-03, -1.3196e-01, -3.0029e-01, -3.9673e-01, -1.6846e-01,\n",
       "           -5.8624e-02,  4.0015e-01, -3.1714e-01, -1.9946e-01, -4.7388e-01,\n",
       "            7.0068e-02, -1.3100e-02, -1.9177e-01, -2.6367e-01,  1.1292e-01,\n",
       "            3.5034e-02, -9.7046e-02,  3.3057e-01, -1.1310e-01,  4.4678e-01,\n",
       "           -4.9170e-01, -5.9692e-02, -3.3838e-01,  9.5642e-02, -2.6074e-01,\n",
       "           -1.6479e-01,  1.1115e-01,  4.0100e-02,  3.1348e-01, -1.5625e-01,\n",
       "           -2.0178e-01,  5.1025e-01, -1.4931e-02,  2.3193e-02,  2.4573e-01,\n",
       "           -5.3711e-02, -4.0674e-01,  2.2644e-01, -1.1292e-01, -4.5142e-01,\n",
       "            4.0747e-01, -4.0356e-01, -2.0496e-01, -5.5518e-01, -1.0718e-01,\n",
       "            2.8516e-01, -2.3773e-02, -3.2275e-01,  2.6880e-01,  1.2573e-01,\n",
       "            1.3989e-01, -1.8481e-01, -4.6448e-02, -2.5073e-01, -3.4698e-02,\n",
       "            6.8726e-02,  1.1053e-01,  3.7628e-02, -3.2886e-01, -1.6699e-01,\n",
       "            2.5171e-01, -2.2266e-01,  4.9585e-01,  7.3364e-02,  2.6436e-03,\n",
       "            2.9785e-01, -1.5906e-01,  3.6407e-02, -6.6211e-01, -1.0883e-01,\n",
       "            1.6663e-01,  7.9041e-02, -5.2441e-01, -1.8640e-01, -2.3462e-01,\n",
       "           -4.5752e-01,  3.3984e-01,  4.5679e-01,  1.4526e-01, -8.8623e-02,\n",
       "           -9.7961e-02, -1.3892e-01, -6.2598e-01,  3.2764e-01, -8.2886e-02,\n",
       "            3.0472e-02,  1.1163e-01, -1.2634e-01,  1.0516e-01,  1.5125e-01,\n",
       "           -4.3555e-01, -2.3911e-02, -3.9459e-02,  8.0017e-02, -2.9834e-01,\n",
       "           -6.4880e-02, -3.3417e-02,  6.6338e-03, -3.2520e-01, -9.9854e-02,\n",
       "            4.6338e-01,  2.8735e-01,  7.2266e-02, -2.6343e-01, -3.0103e-01,\n",
       "            2.3242e-01,  3.2349e-01, -1.8469e-01,  1.4935e-03, -3.8147e-02,\n",
       "           -5.0934e-02,  3.3600e-02,  1.8707e-02,  2.8735e-01,  6.8604e-02,\n",
       "           -2.6929e-01,  3.4912e-01,  2.1753e-01, -2.6733e-01, -1.8799e-01,\n",
       "           -2.3209e-02, -2.3340e-01,  2.2656e-01,  4.4336e-01,  2.2876e-01,\n",
       "           -2.3267e-01, -2.2125e-02,  3.0981e-01,  1.4380e-01,  4.2432e-01,\n",
       "           -3.8672e-01,  6.9763e-02, -3.3386e-02, -1.8640e-01, -4.9866e-02,\n",
       "           -7.7759e-02, -5.6335e-02, -3.9087e-01, -2.7441e-01,  9.2957e-02,\n",
       "            1.9348e-01, -4.0314e-02,  1.8909e-01,  1.5271e-01, -1.5343e-02,\n",
       "            3.6646e-01,  5.1849e-02,  2.8247e-01,  4.0894e-01, -1.4221e-01,\n",
       "           -1.0297e-01,  6.7200e-02,  5.3076e-01,  1.4648e-01, -2.8735e-01,\n",
       "            1.4453e-01,  3.5742e-01, -2.5317e-01,  1.9580e-01,  1.2817e-02,\n",
       "           -9.1797e-02, -9.5764e-02,  2.3792e-01,  4.7394e-02, -1.5454e-01,\n",
       "           -2.3743e-02, -1.0870e-01, -1.9873e-01, -7.6843e-02, -3.1372e-01,\n",
       "           -2.7197e-01, -1.7297e-01, -3.4155e-01,  2.0349e-01,  1.3649e-02,\n",
       "           -1.0559e-01,  5.3558e-02,  5.8828e+00, -1.1230e-01,  2.8223e-01,\n",
       "           -8.4106e-02,  3.2422e-01,  2.9761e-01,  2.3267e-01,  6.2549e-01,\n",
       "            6.4026e-02,  2.3120e-01,  3.5782e-03, -1.2708e-01, -8.3679e-02,\n",
       "            3.1592e-01,  2.9114e-02, -2.1802e-01, -2.6343e-01, -2.5527e+00,\n",
       "            1.4294e-01, -2.1680e-01, -5.7495e-02, -2.4963e-01, -2.3938e-01,\n",
       "           -4.9591e-02, -1.7334e-01,  3.7866e-01,  6.6345e-02, -1.7627e-01,\n",
       "           -5.1807e-01,  1.1467e-02,  1.4929e-01, -2.5464e-01, -1.4481e-02,\n",
       "            1.6394e-01,  1.6394e-01,  9.3140e-02, -1.7908e-01,  1.3245e-01,\n",
       "            4.4165e-01, -1.2140e-01,  3.1714e-01,  3.9502e-01, -6.2891e-01,\n",
       "            1.3306e-01,  2.3145e-01, -1.3953e-01,  4.3884e-02,  2.5317e-01,\n",
       "            2.1643e-01, -1.7468e-01, -3.5950e-02, -5.9753e-02, -1.8042e-01,\n",
       "           -9.7046e-03, -3.1592e-01,  5.1453e-02,  2.0911e-01, -3.1543e-01,\n",
       "            3.7964e-01,  2.0508e-01,  2.3743e-01, -7.0007e-02,  6.0608e-02,\n",
       "            2.1692e-01, -1.2354e-01, -1.9775e-02,  1.1700e-01, -1.3199e-03,\n",
       "            2.0715e-01,  2.2961e-01,  4.2755e-02,  1.2585e-01,  1.4709e-01,\n",
       "            4.5593e-02, -2.2083e-01, -9.9182e-02, -1.8469e-01, -1.7053e-01,\n",
       "           -7.5977e-01,  4.9072e-01,  2.2888e-02,  7.0740e-02, -2.4500e-01,\n",
       "           -6.1920e-02, -3.2082e-03,  2.5781e-01, -9.1553e-02,  4.5581e-01,\n",
       "            5.8624e-02,  2.5952e-01,  1.4136e-01,  5.1910e-02, -1.9897e-01,\n",
       "           -3.9154e-02, -2.0422e-01, -7.1472e-02,  3.4595e-01,  1.1450e-01,\n",
       "            9.2651e-02, -1.5894e-01, -6.1426e-01, -1.2314e-02,  2.8442e-01,\n",
       "           -3.8525e-01,  2.1228e-01, -7.6111e-02,  3.3521e-01,  5.2588e-01,\n",
       "            1.7053e-01, -7.7637e-01,  1.7651e-01, -4.1528e-01,  2.9541e-01,\n",
       "           -1.5857e-01, -2.9639e-01, -1.0297e-01,  3.1055e-01,  1.2866e-01,\n",
       "           -1.4392e-01,  4.4434e-02, -1.6602e-01,  1.8970e-01, -1.9788e-01,\n",
       "            9.7717e-02,  1.4026e-01,  3.4229e-01, -8.2031e-02,  2.5732e-01,\n",
       "           -2.7734e-01, -3.5083e-01, -3.2446e-01,  2.5955e-02, -2.2241e-01,\n",
       "            3.8818e-01,  8.1909e-02, -1.6028e-01, -2.4304e-01, -3.6438e-02,\n",
       "            7.5989e-02, -3.3630e-02, -2.2083e-01,  8.1329e-03,  4.0186e-01,\n",
       "            5.2307e-02,  2.4915e-01, -2.3035e-01, -6.9092e-02, -5.4053e-01,\n",
       "           -1.8091e-01, -2.6392e-01, -2.6489e-01,  1.5955e-01, -5.9753e-02,\n",
       "            1.0913e-01,  4.7638e-02,  2.2681e-01,  1.2439e-01,  2.7026e-01,\n",
       "            1.0223e-01,  4.6509e-02, -4.3274e-02, -2.0654e-01, -1.1368e-02,\n",
       "            1.3379e-01,  7.9712e-02, -1.0605e-03, -2.7206e-02,  3.1543e-01,\n",
       "            1.0010e-01, -7.4316e-01,  4.3433e-01, -1.6809e-01,  5.9418e-02,\n",
       "            4.4952e-02, -1.1768e-01, -5.3436e-02, -8.3618e-02,  3.0701e-02,\n",
       "            4.6680e-01, -1.0925e-01, -7.5562e-02,  8.4814e-01,  1.9592e-01,\n",
       "           -2.3596e-01,  2.8198e-01, -5.4474e-02,  1.7297e-01,  1.1493e-01,\n",
       "            2.7542e-03, -2.1484e-01, -5.3040e-02,  3.0078e-01,  2.0129e-01,\n",
       "           -2.5781e-01,  1.0052e-01, -5.8868e-02,  3.2776e-02, -5.1172e-01,\n",
       "           -2.1558e-01,  6.3934e-03]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3879, 0.1891, 0.3645, 0.0586]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class  \\\n",
       "  0  429.252563  61.258633  500.000000  223.009415    0.925705      0   \n",
       "  1    0.000000   0.069088  238.830399  253.154709    0.919672      0   \n",
       "  2  326.905212   6.391340  439.236176  231.332214    0.919548     40   \n",
       "  3   25.297482  37.802773  125.814774  158.427551    0.642427     58   \n",
       "  4  267.495392  51.035614  326.054626  145.042969    0.484677     58   \n",
       "  5  232.420990  36.547512  275.519501  216.771454    0.442118     58   \n",
       "  6  112.078163   1.575458  209.397583   96.493347    0.386092     58   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2    wine glass  \n",
       "  3  potted plant  \n",
       "  4  potted plant  \n",
       "  5  potted plant  \n",
       "  6  potted plant  ,\n",
       "  'caption': ['the tree on the left'],\n",
       "  'bbox_target': [24.31, 33.05, 104.66, 129.74]},\n",
       " 196: {'image_emb': tensor([[ 0.2019,  0.4604,  0.2147,  ...,  0.3474, -0.1105, -0.1356],\n",
       "          [-0.0355,  0.5776,  0.2227,  ...,  0.5815, -0.4260, -0.2135],\n",
       "          [ 0.1534,  0.5312,  0.1987,  ...,  0.5498, -0.3113,  0.0356],\n",
       "          [-0.0390,  0.2998, -0.0182,  ...,  1.1816,  0.1831, -0.4595],\n",
       "          [-0.2384,  0.0870, -0.1306,  ...,  0.5527, -0.2499,  0.0861],\n",
       "          [-0.0252, -0.1547,  0.3328,  ...,  0.5889, -0.5088,  0.0746]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2734,  0.1559, -0.0755,  ...,  0.5161, -0.3755, -0.5811],\n",
       "          [-0.0197,  0.0031, -0.1991,  ...,  0.8950, -0.4490, -0.1917]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.3105e-01, 5.3444e-03, 3.0441e-03, 3.3379e-06, 4.4975e-03, 1.5613e-01],\n",
       "          [4.0845e-01, 1.1346e-01, 9.8572e-02, 2.8563e-04, 9.4055e-02, 2.8516e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  219.751770   70.476715  342.147522  224.220428    0.945574      0   \n",
       "  1  357.817230  141.508118  488.248993  243.599976    0.919078      0   \n",
       "  2  466.688660  100.942520  558.849304  237.979309    0.918071      0   \n",
       "  3  356.779236  205.103851  380.722778  231.333832    0.856327     35   \n",
       "  4  158.992950   91.369629  222.000732  129.883240    0.732961     34   \n",
       "  \n",
       "               name  \n",
       "  0          person  \n",
       "  1          person  \n",
       "  2          person  \n",
       "  3  baseball glove  \n",
       "  4    baseball bat  ,\n",
       "  'caption': ['A baseball player, dressed in white, who is swinging the bat.',\n",
       "   'A baseball player that had just swung the bat.'],\n",
       "  'bbox_target': [221.01, 62.13, 126.63, 169.11]},\n",
       " 197: {'image_emb': tensor([[-0.4041,  0.2135, -0.0093,  ...,  1.4727,  0.0133, -0.4604],\n",
       "          [-0.0563, -0.0953, -0.2937,  ...,  0.9746,  0.0538, -0.1688],\n",
       "          [-0.0875, -0.0383, -0.4651,  ...,  0.9185, -0.0296,  0.0903],\n",
       "          ...,\n",
       "          [-0.1869,  0.3887, -0.1638,  ...,  1.3418,  0.0587,  0.0273],\n",
       "          [-0.1689, -0.0531, -0.3040,  ...,  1.2979,  0.0017, -0.3108],\n",
       "          [-0.1741, -0.1316, -0.0834,  ...,  0.5347,  0.3110, -0.2705]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0937, -0.0533, -0.4272,  ..., -0.0828, -0.1949, -0.3862],\n",
       "          [-0.0733,  0.1642, -0.5293,  ..., -0.0261, -0.4324, -0.3259]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.4100e-04, 1.5295e-04, 9.5367e-07, 1.0550e-05, 1.0133e-06, 3.6120e-04,\n",
       "           1.7881e-07, 2.0087e-05, 7.6485e-04, 1.6193e-03, 1.7333e-04, 2.5606e-04,\n",
       "           9.9609e-01],\n",
       "          [7.3357e-03, 6.5369e-02, 1.1134e-04, 6.8893e-03, 3.2735e-04, 2.1225e-02,\n",
       "           2.6464e-05, 1.5950e-04, 3.9429e-01, 1.2871e-02, 2.3210e-04, 5.5695e-04,\n",
       "           4.9072e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   160.579941  413.850189  359.763611  501.637909    0.901207     55   \n",
       "  1   295.132324  324.894684  442.050964  406.562103    0.862184     55   \n",
       "  2    80.130859  138.219818  109.167908  209.776154    0.852399      0   \n",
       "  3   391.637329  165.168411  423.865479  224.220200    0.841488      0   \n",
       "  4   111.606964  140.779541  148.266357  209.986481    0.839797      0   \n",
       "  5   178.863464  267.294739  274.385559  321.390320    0.810673     55   \n",
       "  6     0.052597  146.762085   26.995255  229.483826    0.791844      0   \n",
       "  7   134.524994  199.732437  172.380432  226.577072    0.748635     56   \n",
       "  8    90.937744  221.709259  186.965454  293.648712    0.748252     55   \n",
       "  9   242.561707  278.427246  461.180908  376.922668    0.705679     60   \n",
       "  10   19.053612  205.878784   64.032990  264.927856    0.702656     56   \n",
       "  11    0.000000  235.187195   26.930923  329.820129    0.701520     56   \n",
       "  12   35.685394  226.588852   87.648254  329.376282    0.679274     56   \n",
       "  13   12.092667  501.788452  471.074341  638.679688    0.675685     60   \n",
       "  14  186.909393  224.957031  224.919464  269.432526    0.668459     56   \n",
       "  15  147.364182  546.437988  222.747620  582.677246    0.661185     44   \n",
       "  16  167.534912  197.821823  210.058411  234.295273    0.605663     56   \n",
       "  17  390.282379  362.814911  472.041626  507.185394    0.553563     60   \n",
       "  18  118.948929  542.630737  190.408142  576.109497    0.516289     44   \n",
       "  19  268.329742  238.994949  283.019379  291.054108    0.407172     39   \n",
       "  20  285.142944  267.353699  310.257446  291.151672    0.364124     41   \n",
       "  21    0.029343  286.476410   25.359909  381.475311    0.312897     60   \n",
       "  22  283.014557  267.932037  311.391754  292.225311    0.257660     55   \n",
       "  23  138.229584  147.990646  151.064423  178.848892    0.251825      0   \n",
       "  \n",
       "              name  \n",
       "  0           cake  \n",
       "  1           cake  \n",
       "  2         person  \n",
       "  3         person  \n",
       "  4         person  \n",
       "  5           cake  \n",
       "  6         person  \n",
       "  7          chair  \n",
       "  8           cake  \n",
       "  9   dining table  \n",
       "  10         chair  \n",
       "  11         chair  \n",
       "  12         chair  \n",
       "  13  dining table  \n",
       "  14         chair  \n",
       "  15         spoon  \n",
       "  16         chair  \n",
       "  17  dining table  \n",
       "  18         spoon  \n",
       "  19        bottle  \n",
       "  20           cup  \n",
       "  21  dining table  \n",
       "  22          cake  \n",
       "  23        person  ,\n",
       "  'caption': ['A table holding silverware and cakes on wooden holders',\n",
       "   'the table with the cake'],\n",
       "  'bbox_target': [0.0, 509.23, 471.89, 130.77]},\n",
       " 198: {'image_emb': tensor([[-0.1512,  0.5234, -0.0745,  ...,  0.7178,  0.4202, -0.2878],\n",
       "          [-0.0696,  0.3066, -0.0268,  ...,  0.9214,  0.2107, -0.2363],\n",
       "          [-0.0930,  0.4331, -0.0475,  ...,  0.8052,  0.4670, -0.3049]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0165,  0.1294, -0.3511,  ...,  0.0136,  0.5142,  0.3154],\n",
       "          [ 0.1182,  0.0932, -0.2256,  ..., -0.0484,  0.4585,  0.3496]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6372, 0.0218, 0.3411],\n",
       "          [0.4648, 0.1121, 0.4231]], dtype=torch.float16),\n",
       "  'df_boxes':        xmin        ymin        xmax        ymax  confidence  class        name\n",
       "  0  0.000000   32.445358  332.040314  497.608368    0.958042      0      person\n",
       "  1  0.075591  132.474380  155.456726  230.767197    0.889976     40  wine glass\n",
       "  2  0.458598  397.759308   85.037140  499.679047    0.364801      0      person,\n",
       "  'caption': ['the pink shirt', 'A pink sleeve.'],\n",
       "  'bbox_target': [0.0, 392.72, 86.75, 107.28]},\n",
       " 199: {'image_emb': tensor([[-0.2404, -0.0311,  0.0251,  ...,  0.6377, -0.0141, -0.0020],\n",
       "          [ 0.0532, -0.4014, -0.2710,  ...,  1.2070, -0.5078,  0.0337],\n",
       "          [ 0.1823, -0.1176, -0.3755,  ...,  0.8379, -0.0036,  0.1012],\n",
       "          ...,\n",
       "          [ 0.3455,  0.2739,  0.0821,  ...,  1.3975, -0.3591,  0.2942],\n",
       "          [-0.0460,  0.3943, -0.1985,  ...,  1.1875, -0.3396, -0.3687],\n",
       "          [-0.0104, -0.1997,  0.0687,  ...,  0.6196, -0.8022, -0.4546]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0688, -0.1871,  0.0787,  ...,  0.1761, -0.2092, -0.0359],\n",
       "          [-0.0522, -0.3455,  0.1167,  ...,  0.0090, -0.0462,  0.0361]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.2095e-02, 2.3376e-01, 3.5309e-02, 4.7241e-01, 2.1408e-02, 1.7932e-01,\n",
       "           3.1815e-03, 3.2654e-02],\n",
       "          [1.2846e-03, 1.9360e-01, 1.7834e-03, 8.1970e-02, 3.0994e-04, 1.2772e-02,\n",
       "           3.8803e-05, 7.0801e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   128.530975  167.333099  174.651733  313.187469    0.852936      0   \n",
       "  1   157.821121  116.063110  417.208069  224.730591    0.845140      7   \n",
       "  2   369.520172  157.151794  390.661469  220.690369    0.806784      0   \n",
       "  3   530.205017  175.832001  561.715027  200.594879    0.740331      2   \n",
       "  4    50.752884  156.123718   93.745789  278.600952    0.736280      0   \n",
       "  5   455.799011  178.307159  529.420288  230.833527    0.733710      2   \n",
       "  6   270.371063  183.449799  345.171356  310.143097    0.702339      0   \n",
       "  7    54.806465  133.603210  153.537476  190.449890    0.687616      7   \n",
       "  8    89.107895  167.593262  137.842270  301.744385    0.670135      0   \n",
       "  9   520.875000  167.468170  543.242920  189.877533    0.597655      2   \n",
       "  10  441.475952  167.944031  465.173523  182.705444    0.496464      2   \n",
       "  11  423.817352  171.424500  446.027679  183.070312    0.477438      2   \n",
       "  12  134.119324  120.757355  150.215912  155.250763    0.363083      0   \n",
       "  13  393.879974  153.382507  425.210724  186.446899    0.355244      7   \n",
       "  14  480.490936  171.742493  498.364655  180.776245    0.253361      2   \n",
       "  15  567.832092  174.932281  636.807922  257.592194    0.252574     58   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1          truck  \n",
       "  2         person  \n",
       "  3            car  \n",
       "  4         person  \n",
       "  5            car  \n",
       "  6         person  \n",
       "  7          truck  \n",
       "  8         person  \n",
       "  9            car  \n",
       "  10           car  \n",
       "  11           car  \n",
       "  12        person  \n",
       "  13         truck  \n",
       "  14           car  \n",
       "  15  potted plant  ,\n",
       "  'caption': ['A white special truck that is moving towards three men',\n",
       "   'The fire truck closest to the group of people.'],\n",
       "  'bbox_target': [161.76, 118.38, 232.45, 100.64]},\n",
       " 200: {'image_emb': tensor([[-0.2615,  0.2964, -0.4158,  ...,  0.8101,  0.0280,  0.0754],\n",
       "          [-0.0355,  0.4048, -0.1317,  ...,  0.8809,  0.0916,  0.2025],\n",
       "          [-0.0330,  0.4058, -0.2664,  ...,  0.6475,  0.1192,  0.5225]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0753,  0.1154,  0.0076,  ...,  0.4937, -0.0435,  0.0633],\n",
       "          [-0.0836,  0.2394,  0.1128,  ...,  0.3809,  0.0911, -0.1088]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1160, 0.1512, 0.7329],\n",
       "          [0.5190, 0.0888, 0.3918]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  263.630829  114.954575  590.306763  266.462891    0.921000      3   \n",
       "  1  307.591583   95.940109  485.158173  201.631256    0.842241      0   \n",
       "  \n",
       "           name  \n",
       "  0  motorcycle  \n",
       "  1      person  ,\n",
       "  'caption': ['The motorcycle rider.', 'A man in black riding a motorcycle.'],\n",
       "  'bbox_target': [308.81, 96.93, 171.04, 116.64]},\n",
       " 201: {'image_emb': tensor([[ 0.0111,  0.4541, -0.2346,  ...,  1.1006,  0.0502, -0.4993],\n",
       "          [-0.3413,  0.4788, -0.0163,  ...,  0.6411,  0.1644,  0.0373]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1223,  0.2291, -0.1865,  ..., -0.1428, -0.0022, -0.6733],\n",
       "          [ 0.0596,  0.3350, -0.3684,  ..., -0.0036,  0.1855, -0.3982]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.9956, 0.0044],\n",
       "          [0.9990, 0.0012]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    79.973717  353.219116  268.915894  478.183472    0.891938     25   \n",
       "  1   290.121826   31.865311  514.198303  124.910355    0.618396     25   \n",
       "  2   145.060699  114.081192  362.876465  168.634598    0.603183     25   \n",
       "  3   336.832703  380.055695  358.966125  432.075867    0.398379     75   \n",
       "  4   406.387207  194.929352  444.811401  276.396942    0.391641     58   \n",
       "  5   216.086502  230.553528  319.418396  431.374329    0.379513     58   \n",
       "  6   425.405884  369.943665  458.499329  417.450867    0.374247     58   \n",
       "  7   271.432037  396.683075  294.060699  430.647552    0.357211     75   \n",
       "  8   144.607941  112.658234  362.967773  233.744385    0.354604     25   \n",
       "  9    18.466198  313.836945   88.103989  399.133087    0.332080     58   \n",
       "  10  505.494843  377.323303  517.990845  408.622681    0.323672     75   \n",
       "  11  533.486023  298.427795  550.280457  343.695251    0.299351     75   \n",
       "  12  313.221832  385.250153  333.336517  431.852753    0.279648     75   \n",
       "  13  304.030151  207.422668  423.587952  374.493103    0.276799     58   \n",
       "  14  494.596588  376.266632  506.544098  408.673737    0.261062     75   \n",
       "  15  488.300781  181.271210  524.054810  276.014923    0.258672     58   \n",
       "  16  445.602234  197.049347  488.848389  276.264313    0.256793     58   \n",
       "  17  470.162292  367.772552  492.087830  408.212006    0.251475     75   \n",
       "  \n",
       "              name  \n",
       "  0       umbrella  \n",
       "  1       umbrella  \n",
       "  2       umbrella  \n",
       "  3           vase  \n",
       "  4   potted plant  \n",
       "  5   potted plant  \n",
       "  6   potted plant  \n",
       "  7           vase  \n",
       "  8       umbrella  \n",
       "  9   potted plant  \n",
       "  10          vase  \n",
       "  11          vase  \n",
       "  12          vase  \n",
       "  13  potted plant  \n",
       "  14          vase  \n",
       "  15  potted plant  \n",
       "  16  potted plant  \n",
       "  17          vase  ,\n",
       "  'caption': ['white umbrella on the ground',\n",
       "   'white umbrella with a yellow center lying on floor with bottom edge blurred out'],\n",
       "  'bbox_target': [72.66, 354.0, 198.13, 125.36]},\n",
       " 202: {'image_emb': tensor([[ 0.0331,  0.7563, -0.3257,  ...,  0.9233, -0.0392, -0.1501],\n",
       "          [ 0.0883,  0.3948, -0.1356,  ...,  1.0117,  0.3186, -0.0793],\n",
       "          [ 0.3945, -0.1094, -0.3689,  ...,  0.3831, -0.1360, -0.0580],\n",
       "          ...,\n",
       "          [-0.2581,  0.3098, -0.2590,  ...,  1.0410, -0.0151, -0.5415],\n",
       "          [-0.4082,  0.3726, -0.0764,  ...,  1.2070, -0.3049, -0.4360],\n",
       "          [ 0.4028, -0.1171, -0.3020,  ...,  0.4714, -0.1963, -0.0428]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0350,  0.2703, -0.1653,  ...,  0.1036, -0.0746, -0.5063],\n",
       "          [-0.0855,  0.5806, -0.2563,  ..., -0.3601, -0.0577, -0.1611]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.3208e-05, 2.9206e-06, 7.5623e-02, 1.7881e-07, 6.7432e-01, 2.0161e-03,\n",
       "           2.4805e-01],\n",
       "          [1.1587e-03, 3.3736e-04, 4.6295e-02, 1.7583e-05, 8.7354e-01, 8.0414e-03,\n",
       "           7.0618e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  507.765869  172.244080  639.791260  423.280823    0.944108      0   \n",
       "  1    0.000000  121.028290  135.680420  423.301025    0.877186      0   \n",
       "  2  110.940460    0.000000  541.605835  421.207825    0.870230      0   \n",
       "  3   18.971830  292.499542   56.529297  340.644562    0.817148     41   \n",
       "  4  175.091583  253.952209  516.867554  423.051147    0.790035     24   \n",
       "  5  265.669403  310.768372  363.279205  423.636414    0.717493     27   \n",
       "  6   72.342735  173.705688  237.572083  423.478333    0.681719      0   \n",
       "  7   27.671062  384.505310   68.900208  404.571899    0.373615     41   \n",
       "  8  175.403854  253.866028  516.118958  424.815552    0.339605     27   \n",
       "  9   27.654661  384.754883   68.414230  404.405762    0.307308     67   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2      person  \n",
       "  3         cup  \n",
       "  4    backpack  \n",
       "  5         tie  \n",
       "  6      person  \n",
       "  7         cup  \n",
       "  8         tie  \n",
       "  9  cell phone  ,\n",
       "  'caption': ['A blonde-haired man with facial hair who is wearing a white shirt, black vest, and black tie.',\n",
       "   'A man with longer curly hair, wearing a loose tie.'],\n",
       "  'bbox_target': [113.23, 3.68, 422.2, 419.32]},\n",
       " 203: {'image_emb': tensor([[-0.2683, -0.5254,  0.1131,  ...,  0.6577,  0.0465,  0.3267],\n",
       "          [-0.4219, -0.2827,  0.0367,  ...,  0.5435,  0.1283,  0.4260],\n",
       "          [-0.4658, -0.2891,  0.0776,  ...,  0.7725, -0.0030,  0.3640]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2634,  0.1366, -0.1758,  ...,  0.0790, -0.0640,  0.1384],\n",
       "          [ 0.0432,  0.2891,  0.1080,  ...,  0.1458, -0.1395, -0.0773]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1958, 0.5239, 0.2805],\n",
       "          [0.0558, 0.8999, 0.0441]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin       xmax        ymax  confidence  class      name\n",
       "  0    0.052368  41.143829  446.42807  340.634247    0.955819     20  elephant\n",
       "  1  381.360352   0.625595  640.00000  329.034546    0.922278     20  elephant,\n",
       "  'caption': [\"the big elephant's trunk\",\n",
       "   'taller elephant whose face cannot be seen'],\n",
       "  'bbox_target': [371.14, 0.63, 268.86, 330.07]},\n",
       " 204: {'image_emb': tensor([[-0.0272,  0.2891, -0.2627,  ...,  0.9644,  0.2362, -0.1666],\n",
       "          [-0.1990,  0.4768, -0.0704,  ...,  0.8477,  0.0598, -0.0930],\n",
       "          [-0.2913,  0.8428, -0.1266,  ...,  0.8623,  0.1162, -0.1014],\n",
       "          ...,\n",
       "          [-0.1947,  0.2786, -0.2157,  ...,  0.4919,  0.3159, -0.0803],\n",
       "          [ 0.0892,  0.3386, -0.4736,  ...,  1.0020,  0.2754,  0.1848],\n",
       "          [ 0.0422,  0.4602, -0.0323,  ...,  0.5244,  0.2913, -0.3628]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3447, -0.0053, -0.3005,  ...,  0.1710, -0.1290, -0.2949],\n",
       "          [-0.1219, -0.0153, -0.1013,  ...,  0.4536, -0.2961, -0.1611]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.5522e-02, 4.6777e-01, 1.0338e-02, 2.2449e-01, 2.1082e-01, 1.3423e-04,\n",
       "           5.0873e-02],\n",
       "          [5.6190e-03, 9.4482e-01, 2.2217e-02, 2.0233e-02, 3.2005e-03, 1.8418e-05,\n",
       "           4.0474e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   459.026184  184.198669  510.110474  217.839233    0.914390     64   \n",
       "  1   233.248657  179.276733  439.351929  307.448303    0.898519     66   \n",
       "  2   294.155579  267.014526  507.874695  405.342224    0.864192     63   \n",
       "  3    71.007309  225.082428  350.859863  475.805603    0.851099     63   \n",
       "  4   225.954926    0.400055  405.565643  223.648071    0.825816     62   \n",
       "  5   128.654892   44.741226  182.846024  153.360733    0.739319     41   \n",
       "  6   162.642197    6.678329  232.577560  131.817581    0.665804     39   \n",
       "  7   110.716576   59.000473  143.618286  114.308578    0.591882     76   \n",
       "  8   163.040390  365.821167  350.044128  478.345886    0.567294     66   \n",
       "  9   506.844879   83.584137  531.409912  135.053101    0.544876     67   \n",
       "  10    0.000000  279.933777   40.504066  344.500671    0.462671     67   \n",
       "  11  568.258606  134.117920  600.118713  172.087585    0.434325     65   \n",
       "  12   75.619324  110.081573  131.747910  177.659729    0.410186     41   \n",
       "  \n",
       "            name  \n",
       "  0        mouse  \n",
       "  1     keyboard  \n",
       "  2       laptop  \n",
       "  3       laptop  \n",
       "  4           tv  \n",
       "  5          cup  \n",
       "  6       bottle  \n",
       "  7     scissors  \n",
       "  8     keyboard  \n",
       "  9   cell phone  \n",
       "  10  cell phone  \n",
       "  11      remote  \n",
       "  12         cup  ,\n",
       "  'caption': ['The keyboard closest to the camera.', 'keyboard of laptop'],\n",
       "  'bbox_target': [161.23, 364.88, 189.34, 115.12]},\n",
       " 205: {'image_emb': tensor([[-0.7363,  0.1937,  0.0607,  ...,  0.7725, -0.1404, -0.3135],\n",
       "          [-0.2949, -0.1179, -0.4846,  ...,  0.6343, -0.1555, -0.2242],\n",
       "          [-0.6040,  0.1510,  0.0512,  ...,  0.8843,  0.0767, -0.1477],\n",
       "          [-0.1831,  0.1010, -0.3479,  ...,  1.2275, -0.1353, -0.0571],\n",
       "          [-0.4724,  0.3665, -0.0547,  ...,  0.9106, -0.1411, -0.1176],\n",
       "          [-0.6812, -0.0146, -0.0947,  ...,  0.4785, -0.2629, -0.1266]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3884,  0.0094, -0.1150,  ...,  0.4573, -0.2488, -0.3486],\n",
       "          [-0.3215, -0.3623, -0.1608,  ...,  0.2073, -0.2859, -0.3860]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.4084e-01, 8.4991e-03, 5.1575e-03, 9.1028e-04, 2.5772e-02, 7.1875e-01],\n",
       "          [4.3311e-01, 5.5351e-03, 5.8937e-03, 3.1233e-04, 5.6793e-02, 4.9854e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   33.986267  143.051910  454.883820  419.091431    0.932022      0   \n",
       "  1  549.260742   19.112259  573.429932   42.645096    0.912956     32   \n",
       "  2  415.868500    0.000000  546.277710  162.787903    0.857709     38   \n",
       "  3  388.126556  258.110413  427.849518  310.812683    0.759601     38   \n",
       "  4  328.205750  135.073471  426.523499  419.025085    0.728462      0   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1    sports ball  \n",
       "  2  tennis racket  \n",
       "  3  tennis racket  \n",
       "  4         person  ,\n",
       "  'caption': ['Woman in a blue and white tennis outfit standing on a tennis court.',\n",
       "   'A tennis player in a blue and white dress awaiting the ball.'],\n",
       "  'bbox_target': [326.61, 135.24, 102.54, 285.49]},\n",
       " 206: {'image_emb': tensor([[-0.3030,  0.4075, -0.2261,  ...,  1.1348, -0.1418, -0.3743],\n",
       "          [-0.0648, -0.0979, -0.2642,  ...,  0.4072,  0.0268, -0.2876],\n",
       "          [-0.1644, -0.1300,  0.1148,  ...,  1.2080, -0.0398, -0.2573]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0865,  0.2429, -0.2751,  ..., -0.2062, -0.1923, -0.2983],\n",
       "          [-0.3079,  0.2891, -0.2859,  ..., -0.2788, -0.1906, -0.0787]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0000e+00, 1.2732e-04, 3.9458e-05],\n",
       "          [9.9463e-01, 4.6043e-03, 8.9264e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  461.356995  226.549652  528.043579  456.597839    0.920331      0   \n",
       "  1  257.565704  174.086700  292.882843  248.952606    0.833422      9   \n",
       "  2  520.514160  225.964783  559.457397  365.530334    0.660861      0   \n",
       "  3  513.023438  262.169678  548.049072  352.076416    0.617565     26   \n",
       "  4  556.157166  264.598450  616.555969  358.587708    0.507810     58   \n",
       "  5  583.988525  243.976257  600.748169  278.006165    0.362943      0   \n",
       "  6  383.047974    0.032387  407.002075   40.900429    0.333166     74   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1  traffic light  \n",
       "  2         person  \n",
       "  3        handbag  \n",
       "  4   potted plant  \n",
       "  5         person  \n",
       "  6          clock  ,\n",
       "  'caption': ['A woman wearing a white top and a yellowish skirt',\n",
       "   'A woman wearing a skirt.'],\n",
       "  'bbox_target': [460.67, 227.75, 67.65, 229.8]},\n",
       " 207: {'image_emb': tensor([[-0.3174, -0.0402, -0.0772,  ...,  0.8174,  0.1812,  0.1882],\n",
       "          [ 0.3608,  0.3052, -0.0101,  ...,  1.1729, -0.1377, -0.0714],\n",
       "          [-0.1150,  0.0878, -0.4541,  ...,  0.5762,  0.0974,  0.2059],\n",
       "          [-0.2917, -0.0082, -0.0458,  ...,  0.8208,  0.0939,  0.0560]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0773,  0.0517, -0.1987,  ..., -0.3091,  0.2632,  0.0723],\n",
       "          [ 0.0938, -0.1796, -0.1840,  ..., -0.3223,  0.1960, -0.0549]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.3076e-01, 4.0770e-04, 1.5557e-05, 4.6851e-01],\n",
       "          [7.8809e-01, 6.7890e-05, 1.4901e-06, 2.1204e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0    0.479630  142.960815  478.130493  638.416016    0.924277      0    person\n",
       "  1    0.604950  217.560242  183.017822  556.734985    0.793031      0    person\n",
       "  2  278.172180   29.980820  478.907715  524.910645    0.757288      0    person\n",
       "  3   42.057945  214.778687  159.983185  319.221375    0.380165      0    person\n",
       "  4    1.906380  137.980042  172.022034  416.712341    0.361011      0    person\n",
       "  5  123.380402  140.734940  172.358490  186.234695    0.331080     76  scissors,\n",
       "  'caption': ['The child getting a haircut being held by the man in the red shirt.',\n",
       "   'A child getting a haircut.'],\n",
       "  'bbox_target': [0.0, 145.45, 477.43, 484.28]},\n",
       " 208: {'image_emb': tensor([[-0.1153,  0.1771,  0.1549,  ...,  0.6152, -0.0551,  0.0928],\n",
       "          [-0.3623, -0.1617, -0.0167,  ...,  0.4512, -0.2834, -0.3254],\n",
       "          [-0.5527,  0.0043, -0.3291,  ...,  0.2988, -0.0472, -0.7183],\n",
       "          ...,\n",
       "          [ 0.0059,  0.0632, -0.4072,  ...,  1.4463, -0.2131,  0.1030],\n",
       "          [-0.3049, -0.2090, -0.2435,  ...,  0.7241,  0.1130, -0.0270],\n",
       "          [ 0.1039, -0.0067,  0.0428,  ...,  0.5234, -0.1683, -0.1851]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.4336, -0.1863,  0.1094,  ..., -0.2438, -0.3574,  0.2247],\n",
       "          [ 0.2195, -0.1859, -0.4402,  ...,  0.0445,  0.0323,  0.1870]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.3115e-01, 1.2100e-02, 2.4080e-05, 3.6526e-04, 1.3232e-04, 6.2466e-05,\n",
       "           5.5939e-02],\n",
       "          [6.2646e-01, 3.5693e-01, 4.6134e-05, 4.2877e-03, 6.5756e-04, 1.5049e-03,\n",
       "           1.0124e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   21.190786   48.361134  424.186279  312.746033    0.944555     15   \n",
       "  1   70.316383  197.298859  212.489105  285.982239    0.869312     73   \n",
       "  2   55.327415  264.498413  283.951721  372.521820    0.829594     73   \n",
       "  3  322.975525  236.620834  407.532776  336.114960    0.829207     41   \n",
       "  4    0.281870   12.490779  121.783195  122.487854    0.818881     56   \n",
       "  5    0.051416  114.470764   36.928165  369.908386    0.786137     56   \n",
       "  6   32.392788  158.183594  498.781403  371.342285    0.617003     60   \n",
       "  7  284.637543  239.400864  448.707062  355.566223    0.578154     45   \n",
       "  8  323.099365  237.251862  407.801727  337.438721    0.427987     45   \n",
       "  9   89.889015  209.746170  167.348892  257.900238    0.336117     73   \n",
       "  \n",
       "             name  \n",
       "  0           cat  \n",
       "  1          book  \n",
       "  2          book  \n",
       "  3           cup  \n",
       "  4         chair  \n",
       "  5         chair  \n",
       "  6  dining table  \n",
       "  7          bowl  \n",
       "  8          bowl  \n",
       "  9          book  ,\n",
       "  'caption': [\"A book which is near the cat's hind legs\",\n",
       "   'Yellow book partially under cat.'],\n",
       "  'bbox_target': [71.38, 197.45, 139.38, 87.67]},\n",
       " 209: {'image_emb': tensor([[ 0.2249,  0.1362,  0.0282,  ...,  1.1299, -0.1990,  0.0647],\n",
       "          [-0.0132,  0.1241, -0.2157,  ...,  1.1191, -0.3530,  0.1060],\n",
       "          [ 0.0918,  0.1876, -0.1073,  ...,  1.1367, -0.1876, -0.0493],\n",
       "          ...,\n",
       "          [ 0.1169,  0.4370,  0.0709,  ...,  0.9634, -0.1576, -0.2854],\n",
       "          [ 0.1569,  0.4907, -0.1383,  ...,  1.0469,  0.1765, -0.2905],\n",
       "          [-0.2109,  0.2844,  0.3110,  ...,  0.9453, -0.1610, -0.0408]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-4.2212e-01, -1.7798e-01,  8.5510e-02,  3.9673e-01,  2.4734e-02,\n",
       "            2.3633e-01,  3.3936e-01, -1.1543e+00, -3.1470e-01, -3.2471e-01,\n",
       "            1.2549e-01,  2.6953e-01,  4.2725e-01,  3.5431e-02,  1.0858e-01,\n",
       "            1.6345e-01,  1.5366e-02, -2.4438e-01, -2.0764e-01, -5.9784e-02,\n",
       "            5.7764e-01,  2.2864e-01,  1.9678e-01, -8.3252e-02, -1.3977e-01,\n",
       "           -4.5074e-02, -2.1439e-02,  2.2324e-02,  3.8910e-02,  3.5522e-01,\n",
       "            3.1396e-01, -4.5166e-01,  2.1045e-01,  2.1313e-01, -3.3594e-01,\n",
       "            4.2480e-02, -1.3342e-01,  1.5906e-01,  2.3401e-01, -1.8701e-01,\n",
       "           -6.6528e-02,  2.1179e-02, -2.7271e-01,  1.4061e-02,  3.5596e-01,\n",
       "            4.0527e-01, -3.3032e-01,  9.6191e-02, -7.1350e-02, -6.6101e-02,\n",
       "           -1.3257e-01,  2.7246e-01, -7.8735e-02,  1.9727e-01,  3.8666e-02,\n",
       "           -3.2642e-01, -1.2657e-02,  2.5952e-01, -1.8600e-02,  2.7417e-01,\n",
       "           -1.5625e-01,  1.3306e-01,  2.1875e-01, -4.2432e-01, -1.6382e-01,\n",
       "            1.1835e-01, -9.8511e-02,  2.8931e-01, -1.0968e-01,  3.3496e-01,\n",
       "           -2.4390e-01, -9.0637e-02,  8.6288e-03,  1.3245e-01, -9.3994e-02,\n",
       "           -3.7170e-02, -3.0640e-01, -4.3457e-02,  5.1117e-02, -2.8271e-01,\n",
       "           -2.0679e-01,  1.1261e-01, -1.5051e-01,  4.3311e-01,  4.2212e-01,\n",
       "            1.7932e-01, -4.0771e-02,  5.7259e-03, -1.9885e-01, -1.7822e-01,\n",
       "           -1.6693e-02,  2.4854e-01, -1.4922e+00,  3.5278e-01, -1.1299e-02,\n",
       "            9.1125e-02,  1.5869e-01, -3.8788e-02,  1.6125e-01,  1.9482e-01,\n",
       "            4.8169e-01, -2.3181e-01,  1.5053e-02, -1.7615e-01, -5.0000e-01,\n",
       "           -9.7778e-02, -2.8638e-01, -1.6797e-01, -3.1299e-01,  3.0957e-01,\n",
       "            2.2180e-01,  3.1714e-01,  2.7441e-01, -5.5656e-03, -5.7983e-03,\n",
       "            1.5552e-01, -4.5502e-02,  3.8550e-01, -3.7537e-02,  2.1375e-01,\n",
       "           -7.8735e-02, -1.9409e-01, -1.0449e-01, -1.0596e+00, -8.9905e-02,\n",
       "            4.8004e-02, -1.2274e-01,  4.0723e-01,  2.0740e-01,  4.2529e-01,\n",
       "            2.9395e-01, -7.3975e-02,  1.4069e-02,  6.5234e+00, -2.0740e-01,\n",
       "           -6.8604e-02, -1.5906e-01, -6.7383e-01, -5.8861e-03,  2.3535e-01,\n",
       "           -3.2935e-01,  2.1606e-02, -2.4744e-01, -9.3994e-02, -2.3279e-01,\n",
       "           -1.5857e-01, -1.1029e-01,  1.2283e-02, -5.1904e-01, -4.6631e-02,\n",
       "            3.8513e-02,  4.8584e-02,  2.1106e-01, -6.5735e-02, -2.4612e-02,\n",
       "           -1.9421e-01,  1.3440e-01, -1.9238e-01, -3.7793e-01, -6.1646e-02,\n",
       "           -1.8042e-01,  8.8806e-02, -4.4263e-01,  1.9791e-02, -1.2512e-01,\n",
       "            2.5317e-01,  1.7529e-01,  1.3184e-01,  6.4758e-02,  5.6671e-02,\n",
       "           -1.2952e-01,  1.9897e-01,  3.1757e-04, -8.9966e-02, -5.8350e-01,\n",
       "            4.6509e-01, -2.3718e-01, -8.6853e-02, -1.5234e-01,  2.2339e-01,\n",
       "            8.1848e-02, -1.0712e-01, -2.8418e-01, -1.1688e-01,  3.3057e-01,\n",
       "            1.4114e-02,  7.4997e-03, -1.5002e-01,  1.2140e-01,  2.3181e-01,\n",
       "           -2.2632e-01, -2.4927e-01,  2.5977e-01, -2.6221e-01, -1.3208e-01,\n",
       "           -4.5947e-01, -4.1040e-01, -1.2915e-01,  1.1200e-01, -1.9666e-01,\n",
       "            1.6199e-01, -2.1606e-01,  1.4502e-01,  6.1798e-02, -1.3330e-01,\n",
       "           -2.1057e-01,  5.4932e-02, -1.3306e-01,  1.3283e-02,  3.3478e-02,\n",
       "           -6.8481e-02,  5.0244e-01,  1.8872e-01,  2.3010e-01, -3.1543e-01,\n",
       "            3.2153e-01, -1.5540e-01, -1.4319e-01, -3.5742e-01, -4.7583e-01,\n",
       "           -2.0325e-01, -2.8149e-01, -1.7358e-01,  2.6514e-01,  2.7344e-02,\n",
       "           -1.0529e-01, -3.5547e-01,  5.5615e-01, -1.1737e-01,  1.6321e-01,\n",
       "            1.4844e-01,  2.8516e-01,  5.2223e-03, -4.2285e-01, -1.7261e-01,\n",
       "            7.4219e-02,  5.4352e-02, -1.5820e-01, -1.0010e-01, -1.2866e-01,\n",
       "           -7.1289e-01,  6.4819e-02,  2.3657e-01,  2.0544e-01, -1.7969e-01,\n",
       "            6.7322e-02, -4.2175e-02,  7.8552e-02, -4.3793e-02,  1.8518e-01,\n",
       "            2.3303e-01, -6.8665e-02, -3.4766e-01, -9.3628e-02, -4.4165e-01,\n",
       "           -3.6865e-01,  8.5999e-02, -3.6621e-01,  3.4210e-02, -1.2291e-02,\n",
       "           -3.2642e-01,  1.2805e-01,  1.2408e-01,  7.3730e-02,  1.0687e-01,\n",
       "           -3.6646e-01, -2.5513e-01,  1.5430e-01, -1.1041e-01,  9.1553e-02,\n",
       "            3.6987e-01, -7.7637e-02, -7.4097e-02, -5.2917e-02, -3.4424e-01,\n",
       "            9.5398e-02,  1.5210e-01,  8.2581e-02, -4.2877e-02,  4.8981e-02,\n",
       "           -1.1810e-01, -2.3193e-01,  6.6699e-01,  2.7856e-01,  1.2732e-01,\n",
       "            1.7395e-01,  2.1753e-01,  1.7383e-01,  2.2571e-01, -1.0699e-01,\n",
       "            1.8665e-01, -4.1064e-01,  2.2351e-01,  9.8343e-03, -3.7915e-01,\n",
       "            2.4805e-01, -1.4880e-01, -4.6875e-02, -2.3840e-01,  9.4360e-02,\n",
       "           -2.4963e-01,  1.1212e-01, -4.6967e-02, -2.9617e-02, -2.4390e-01,\n",
       "           -3.3496e-01,  1.2524e-01, -2.0349e-01, -2.2583e-01,  1.3940e-01,\n",
       "           -2.1118e-01, -8.0261e-02,  6.5156e+00, -1.7517e-01,  2.5452e-02,\n",
       "            1.9019e-01,  1.5112e-01,  6.9727e-01,  1.3379e-01,  5.3320e-01,\n",
       "           -1.8152e-01,  1.2891e-01,  5.5078e-01,  1.2671e-01, -4.6875e-01,\n",
       "           -8.7708e-02, -9.7595e-02,  1.0602e-01, -2.8946e-02, -2.3535e+00,\n",
       "            4.9500e-02,  9.8022e-02,  2.1069e-01, -5.8472e-02,  5.5267e-02,\n",
       "           -2.8906e-01, -2.7539e-01,  2.5049e-01,  2.0129e-01, -4.2310e-01,\n",
       "            1.0669e-01, -1.1780e-01, -1.2073e-01,  2.6807e-01,  1.1548e-01,\n",
       "           -3.3765e-01,  2.2083e-01,  1.2335e-01, -6.6162e-02,  2.8857e-01,\n",
       "            2.7661e-01, -5.7812e-01,  2.4670e-01,  4.0466e-02,  8.2153e-02,\n",
       "            3.8354e-01, -1.3855e-01, -1.7957e-01,  2.3926e-02, -8.7524e-02,\n",
       "            8.4412e-02, -1.1658e-01,  4.5581e-01,  1.3135e-01, -1.5656e-02,\n",
       "            1.4233e-01, -1.9495e-01,  1.1456e-01, -3.7598e-01,  1.6260e-01,\n",
       "            8.3313e-02, -2.2507e-02, -1.1603e-01, -3.5889e-01,  6.0974e-02,\n",
       "            3.6084e-01,  8.7769e-02, -6.8665e-02, -2.3206e-01, -4.1479e-01,\n",
       "            1.0849e-02,  2.3389e-01, -2.0593e-01,  6.9885e-02, -2.2314e-01,\n",
       "           -2.1875e-01,  1.7297e-01,  5.9937e-02, -1.8054e-01,  1.2808e-03,\n",
       "            3.6890e-01,  5.5298e-02, -2.7100e-01,  3.2623e-02,  1.2024e-01,\n",
       "           -2.3483e-02, -2.8397e-02,  7.8430e-02,  1.2732e-01,  1.6724e-01,\n",
       "            1.6760e-01, -1.6553e-01, -2.8351e-02,  1.9214e-01,  7.1594e-02,\n",
       "           -2.6154e-02,  1.3354e-01, -2.1252e-01, -3.4692e-01, -2.1777e-01,\n",
       "            3.5303e-01,  1.0486e-01, -2.4390e-01,  5.7983e-02,  3.4937e-01,\n",
       "            3.5004e-02,  1.6187e-01,  6.7368e-03,  1.1316e-01, -1.9379e-02,\n",
       "            1.8384e-01,  7.4387e-03, -2.7808e-01,  3.4729e-02, -1.0461e-01,\n",
       "            4.1687e-02, -7.8674e-02,  1.3062e-01,  2.4829e-01, -3.0786e-01,\n",
       "            2.6343e-01, -3.5547e-01, -6.3281e-01,  5.1544e-02,  1.9495e-01,\n",
       "            1.2244e-01, -3.3667e-01, -6.1920e-02,  1.9177e-01, -2.2778e-01,\n",
       "            3.5352e-01, -9.3079e-02, -4.3506e-01, -1.2201e-01,  3.3887e-01,\n",
       "            7.7209e-02, -3.4863e-01, -1.8408e-01,  5.5756e-02,  1.8591e-01,\n",
       "           -1.6589e-01, -4.4281e-02, -2.4207e-01,  1.7578e-02,  2.4207e-01,\n",
       "           -1.0931e-01, -2.1936e-01, -3.2227e-02,  2.7313e-02, -7.8186e-02,\n",
       "            3.5913e-01, -4.9835e-02, -8.2336e-02, -2.4704e-02,  2.2949e-01,\n",
       "           -1.8713e-01, -4.3762e-02, -9.3506e-02, -1.4624e-01,  2.6465e-01,\n",
       "            2.6953e-01,  4.4482e-01,  3.6304e-01, -1.6882e-01, -2.0654e-01,\n",
       "            1.7261e-01,  1.6876e-02,  1.9141e-01,  1.3049e-01,  3.0005e-01,\n",
       "           -2.0361e-01, -8.9160e-01,  1.1316e-01,  1.0162e-01, -1.9678e-01,\n",
       "           -2.0776e-01,  1.9763e-01, -2.2461e-01,  8.8348e-03,  1.8896e-01,\n",
       "            7.9163e-02,  1.2158e-01,  3.4155e-01,  6.3037e-01,  2.3254e-01,\n",
       "           -8.5632e-02,  7.5111e-03, -1.9226e-03, -5.5115e-02, -4.6851e-01,\n",
       "           -1.9568e-01,  1.2451e-01, -1.7969e-01,  1.2891e-01,  3.1274e-01,\n",
       "            9.8206e-02, -1.1371e-01,  6.1073e-03, -6.2354e-01, -3.5864e-01,\n",
       "           -1.9623e-02, -1.1401e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.1810e-02, 6.2218e-03, 6.9427e-03, 1.3176e-02, 4.6515e-04, 1.8682e-03,\n",
       "           7.3195e-04, 2.9392e-03, 6.0654e-04, 2.2888e-03, 9.5312e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   124.114944  287.697479  220.471878  424.622803    0.882090     33    kite\n",
       "  1   132.114731   53.443207  227.868393  191.085205    0.872430     33    kite\n",
       "  2   472.382111  288.566010  568.380432  421.056030    0.866406     33    kite\n",
       "  3   467.363647   55.325439  563.960083  182.442780    0.848079     33    kite\n",
       "  4    21.238361  277.927856  125.178711  422.972473    0.772105     33    kite\n",
       "  5    81.506004  236.605896  165.290344  440.512085    0.762678      0  person\n",
       "  6    29.650745   46.167206  132.559128  201.025055    0.757975     33    kite\n",
       "  7   427.918060  240.721344  509.747894  438.194580    0.744730      0  person\n",
       "  8   364.369995   43.220673  466.840759  199.340454    0.718527     33    kite\n",
       "  9   435.892944    4.029938  511.554016  208.541809    0.706529      0  person\n",
       "  10   39.176243   39.644333  100.429535  100.448410    0.687434     33    kite\n",
       "  11   95.114197    4.218666  174.531403  210.441284    0.678721      0  person\n",
       "  12  369.452576  281.358337  472.325623  424.745728    0.662763     33    kite\n",
       "  13  374.623413   41.239090  434.885010   98.257767    0.622367     33    kite\n",
       "  14   30.540276  274.610046   87.158638  333.641174    0.572623     33    kite\n",
       "  15  379.218384  279.127777  441.466370  335.703522    0.480786     33    kite\n",
       "  16  365.995422   41.499176  468.320129  201.664124    0.422898      0  person\n",
       "  17  454.542480   55.209625  560.666992  182.783783    0.325910      0  person\n",
       "  18  372.431946  277.201874  482.106689  433.154175    0.319553      0  person,\n",
       "  'caption': ['the man in the last frame'],\n",
       "  'bbox_target': [423.05, 240.46, 87.18, 194.86]},\n",
       " 210: {'image_emb': tensor([[-0.4873, -0.1038, -0.1114,  ...,  0.6851,  0.0619,  0.2678],\n",
       "          [-0.1085,  0.0930, -0.2581,  ...,  0.6030, -0.0169,  0.2231],\n",
       "          [ 0.2319,  0.3770, -0.2411,  ...,  1.2139, -0.3223,  0.1890],\n",
       "          ...,\n",
       "          [-0.4504,  0.2166, -0.0948,  ...,  1.3867, -0.1692, -0.4551],\n",
       "          [ 0.0239,  0.3137, -0.4204,  ...,  1.0811,  0.0471,  0.1844],\n",
       "          [-0.3176, -0.0706,  0.3167,  ...,  0.3235, -0.0363,  0.3655]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3960, -0.0611, -0.4226,  ..., -0.2036, -0.2566,  0.2954],\n",
       "          [-0.3054, -0.0660, -0.1658,  ..., -0.2615, -0.2444,  0.1781]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0124, 0.0039, 0.0033, 0.0013, 0.9380, 0.0370, 0.0041],\n",
       "          [0.1747, 0.0516, 0.0045, 0.0068, 0.6592, 0.0181, 0.0851]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   23.385315  192.592819  407.349915  565.353455    0.953696      0   \n",
       "  1    5.408173  331.267761  479.789185  637.817993    0.924794     60   \n",
       "  2  317.743042  552.143311  438.646851  639.291260    0.921012     54   \n",
       "  3  307.856689  426.797180  417.445129  487.364685    0.895403     45   \n",
       "  4  397.829834  293.891663  479.802124  347.784241    0.839749     56   \n",
       "  5  437.118164  571.317932  479.688232  639.894226    0.796791     54   \n",
       "  6    0.098900  285.671875   69.372955  561.980286    0.588073     56   \n",
       "  7  217.001251  416.043671  308.911346  452.851471    0.518878     45   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1  dining table  \n",
       "  2         donut  \n",
       "  3          bowl  \n",
       "  4         chair  \n",
       "  5         donut  \n",
       "  6         chair  \n",
       "  7          bowl  ,\n",
       "  'caption': ['The back of the chair in which the girl is seated on.',\n",
       "   'The chair that the young girl is sitting on.'],\n",
       "  'bbox_target': [0.0, 290.29, 77.7, 271.94]},\n",
       " 211: {'image_emb': tensor([[-0.2317,  0.1247,  0.1538,  ...,  1.2686, -0.0396, -0.2386],\n",
       "          [-0.0359,  0.5171, -0.1105,  ...,  1.0039,  0.0920, -0.4521],\n",
       "          [-0.0073,  0.4888,  0.2644,  ...,  0.9614, -0.1339, -0.0445],\n",
       "          [-0.1242,  0.4304, -0.2622,  ...,  1.2656,  0.1059,  0.0271],\n",
       "          [-0.0238,  0.5122,  0.1102,  ...,  0.6460, -0.3853, -0.2332]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2646,  0.1227,  0.0850,  ...,  0.1188, -0.6509, -0.0731],\n",
       "          [ 0.0172, -0.0323,  0.0646,  ...,  0.1908, -0.3384, -0.5664]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.7782e-03, 4.0970e-03, 9.7217e-01, 4.8923e-04, 1.5717e-02],\n",
       "          [1.7868e-02, 1.0614e-01, 1.8628e-01, 6.8115e-01, 8.5754e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  305.029724   12.727141  457.486572  310.394897    0.920710     72   \n",
       "  1   72.854439   72.477470  312.059875  170.668930    0.872697      2   \n",
       "  2    0.423646   68.006004   92.832932  143.268555    0.840031      2   \n",
       "  3  436.212646   79.717514  499.637115  209.227936    0.730646      2   \n",
       "  4  236.258698   71.199348  309.056366  107.441116    0.690341      2   \n",
       "  5   47.948898  110.042503  293.065887  338.986450    0.606196     72   \n",
       "  \n",
       "             name  \n",
       "  0  refrigerator  \n",
       "  1           car  \n",
       "  2           car  \n",
       "  3           car  \n",
       "  4           car  \n",
       "  5  refrigerator  ,\n",
       "  'caption': ['A white car behind a tree with two trunks and a white wooden structure',\n",
       "   'A white car parked in front of a truck.'],\n",
       "  'bbox_target': [73.21, 72.62, 239.36, 100.46]},\n",
       " 212: {'image_emb': tensor([[ 0.5332,  0.3486, -0.5425,  ...,  1.4766, -0.2236,  0.3447],\n",
       "          [ 0.2666,  0.4707, -0.6128,  ...,  1.0938, -0.2642,  0.5034],\n",
       "          [ 0.2788,  0.3423, -0.5400,  ...,  1.8115, -0.0632, -0.0733],\n",
       "          [-0.0315,  0.4077,  0.2812,  ...,  1.3730, -0.0487, -0.2727],\n",
       "          [ 0.1216,  0.2554, -0.0718,  ...,  1.1348, -0.4497,  0.2002]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1370, -0.0730, -0.2563,  ..., -0.3508,  0.0350, -0.0226],\n",
       "          [ 0.0056,  0.1604,  0.0583,  ...,  0.1252,  0.1162, -0.0760]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.2783e-01, 3.7427e-01, 6.6040e-02, 3.3593e-04, 3.1677e-02],\n",
       "          [3.6194e-02, 3.9978e-03, 1.8384e-01, 2.1744e-03, 7.7393e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  367.967590  102.653030  520.074524  420.758606    0.932485      0  person\n",
       "  1  242.053421  109.989929  356.178406  324.015076    0.927208      0  person\n",
       "  2   49.773346  251.273041  637.448242  420.266846    0.901909     17   horse\n",
       "  3   29.490540  196.869934  201.736725  367.123596    0.880388     17   horse,\n",
       "  'caption': ['The officer facing the camera.',\n",
       "   'A mounted police officer wearing sunglasses.'],\n",
       "  'bbox_target': [243.24, 108.9, 113.32, 214.24]},\n",
       " 213: {'image_emb': tensor([[ 0.2715,  0.1700,  0.2079,  ...,  1.1484,  0.2556, -0.0503],\n",
       "          [ 0.1976,  0.9102, -0.0658,  ...,  1.1045,  0.0756, -0.1594],\n",
       "          [-0.0849,  0.4224, -0.0244,  ...,  1.1289,  0.2389,  0.1526],\n",
       "          [ 0.3489,  0.2380, -0.1230,  ...,  0.9043, -0.1694, -0.0669],\n",
       "          [-0.0020,  0.0528,  0.2061,  ...,  0.6982,  0.0678,  0.0352]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1880,  0.0585,  0.0504,  ..., -0.1395,  0.3789, -0.2170],\n",
       "          [ 0.3096, -0.0722,  0.0959,  ..., -0.1531,  0.2335,  0.0513]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9902e-01, 8.2922e-04, 2.8372e-05, 8.3387e-05, 2.0647e-04],\n",
       "          [9.9951e-01, 1.2732e-04, 1.1683e-05, 3.2663e-05, 1.8525e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  448.056946   59.299988  617.888733  225.539307    0.950758     11   \n",
       "  1    0.464775  335.987213  158.037842  476.104492    0.931628      2   \n",
       "  2  122.939651  237.218567  433.857056  370.136780    0.911162      7   \n",
       "  3  446.567230  301.415833  486.064117  332.946594    0.872636      2   \n",
       "  4   45.843403  266.060455   58.402004  279.673859    0.650044     11   \n",
       "  5  467.962982  287.202637  490.657867  310.618286    0.334558      2   \n",
       "  6  114.086166  281.720825  122.816086  305.307678    0.262671      0   \n",
       "  \n",
       "          name  \n",
       "  0  stop sign  \n",
       "  1        car  \n",
       "  2      truck  \n",
       "  3        car  \n",
       "  4  stop sign  \n",
       "  5        car  \n",
       "  6     person  ,\n",
       "  'caption': ['There is a singnal board showing the stop sign.',\n",
       "   'The stop sign'],\n",
       "  'bbox_target': [450.94, 58.12, 168.97, 167.89]},\n",
       " 214: {'image_emb': tensor([[-0.0938,  0.3303, -0.1147,  ...,  0.8130,  0.4302,  0.2732],\n",
       "          [-0.1461,  0.2878, -0.0659,  ...,  0.8091,  0.5322,  0.2610],\n",
       "          [-0.1055,  0.2708,  0.0058,  ...,  0.6470,  0.4312,  0.2073],\n",
       "          ...,\n",
       "          [-0.2341,  0.5708,  0.0548,  ...,  0.9492,  0.5088,  0.0867],\n",
       "          [-0.3254,  0.2272,  0.0292,  ...,  1.1934,  0.2520, -0.2283],\n",
       "          [-0.0704,  0.1189, -0.0582,  ...,  0.5000,  0.3794, -0.1844]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-5.3375e-02,  3.0957e-01, -2.7563e-01, -1.9080e-01,  3.1226e-01,\n",
       "           -5.0201e-02,  1.2561e-01, -1.0216e-02, -1.1603e-01,  2.1252e-01,\n",
       "           -1.9641e-01,  1.5430e-01,  1.5320e-01, -3.8892e-01,  4.6313e-01,\n",
       "           -1.1945e-01,  6.5430e-01,  1.6772e-01,  1.6602e-01, -2.0068e-01,\n",
       "            1.6980e-01, -1.0986e-01, -3.6597e-01,  1.1589e-02, -6.2256e-01,\n",
       "           -9.0881e-02, -9.9915e-02,  1.1604e-02,  2.4170e-01, -1.4824e-02,\n",
       "            3.0347e-01,  5.2765e-02, -3.2617e-01, -2.7197e-01, -3.1104e-01,\n",
       "           -6.6455e-01, -3.4515e-02, -3.1104e-01, -1.9446e-01,  1.6064e-01,\n",
       "            1.7441e-02, -1.7993e-01,  7.0068e-02, -4.5898e-02, -4.2798e-01,\n",
       "           -2.9938e-02,  2.0312e-01,  1.8628e-01,  2.8882e-01, -1.7627e-01,\n",
       "            1.4969e-02,  1.0504e-01, -3.6774e-02, -5.5518e-01, -1.6541e-01,\n",
       "           -2.3853e-01, -5.7861e-01, -5.0774e-03, -2.0300e-01,  4.2676e-01,\n",
       "           -9.9243e-02, -4.6484e-01, -7.9956e-02, -2.2552e-02,  2.6733e-01,\n",
       "           -2.3877e-01,  3.8719e-03,  2.3547e-01, -2.8711e-01,  4.1351e-03,\n",
       "            3.0469e-01,  6.5308e-03,  3.3618e-01,  2.1228e-01,  1.1401e-01,\n",
       "            1.3965e-01, -1.0754e-01, -7.6050e-02, -2.2620e-01, -5.6396e-01,\n",
       "           -1.5857e-01,  1.3330e-01,  1.5613e-01,  2.1582e-01, -2.2229e-01,\n",
       "            1.0199e-01, -2.0923e-01, -9.6863e-02,  8.7219e-02,  3.3130e-01,\n",
       "           -1.0559e-01,  1.9507e-01, -7.7832e-01,  1.8225e-01, -2.9077e-01,\n",
       "           -4.1718e-02, -3.9490e-02, -1.0858e-01,  7.4756e-01,  2.4915e-01,\n",
       "            2.3828e-01, -3.2749e-03,  4.1724e-01,  1.2634e-01,  1.8494e-01,\n",
       "           -2.2095e-01, -5.6152e-02,  1.2720e-01,  9.6283e-03,  1.5149e-01,\n",
       "           -4.1797e-01, -5.0586e-01,  2.7344e-01,  3.8745e-01, -5.5811e-01,\n",
       "           -3.6346e-02, -4.0894e-01, -1.4294e-01, -5.2441e-01,  4.4312e-01,\n",
       "            1.1646e-01, -7.0605e-01, -1.7371e-01, -2.9770e-02,  2.6440e-01,\n",
       "           -3.6060e-01, -1.9104e-01,  1.4917e-01, -2.3352e-01, -1.3684e-01,\n",
       "            2.4878e-01, -1.5515e-01, -3.4692e-01,  4.1289e+00,  2.0764e-01,\n",
       "           -1.5466e-01, -6.0645e-01, -4.8804e-01, -2.4695e-01, -3.4131e-01,\n",
       "           -5.4102e-01,  9.7595e-02, -2.0959e-01,  8.9417e-03, -2.3865e-01,\n",
       "           -2.1594e-01, -8.6914e-02, -6.3379e-01, -4.1309e-01, -4.3384e-01,\n",
       "           -4.4238e-01,  1.7029e-01,  5.0537e-01,  1.1298e-01,  1.8713e-01,\n",
       "           -2.0837e-01,  3.2837e-02, -4.0576e-01,  5.1123e-01, -8.0017e-02,\n",
       "            1.3879e-01,  2.7930e-01, -4.1064e-01, -3.7842e-01, -2.1277e-01,\n",
       "           -4.9133e-03,  2.5415e-01, -1.8311e-02,  4.4995e-01,  3.2153e-01,\n",
       "            2.4128e-03, -2.8711e-01,  4.4556e-01, -9.3628e-02, -3.9160e-01,\n",
       "           -2.5558e-02, -1.3733e-01, -2.4829e-01,  3.4839e-01, -8.7952e-02,\n",
       "           -1.1786e-01,  2.5171e-01, -1.7859e-01, -5.3528e-02, -3.5547e-01,\n",
       "            3.2080e-01,  7.7637e-02, -2.6123e-01, -1.4183e-02,  1.2457e-01,\n",
       "            3.0469e-01, -1.5613e-01,  3.8477e-01,  1.0175e-01,  1.4832e-01,\n",
       "           -6.5735e-02,  1.0815e-01,  9.0149e-02,  7.4219e-02,  2.3047e-01,\n",
       "           -1.6174e-01, -2.5772e-02, -1.8127e-01,  2.3291e-01,  1.1725e-01,\n",
       "           -3.3887e-01,  2.8223e-01, -2.0203e-01,  5.1086e-02, -8.5083e-02,\n",
       "            7.8918e-02,  6.9482e-01, -9.6680e-02,  2.9224e-01,  8.3008e-03,\n",
       "           -1.9580e-01,  5.7520e-01,  1.5045e-02,  1.8469e-01,  1.2390e-01,\n",
       "            4.4214e-01, -9.7351e-03, -9.2529e-02, -8.0383e-02, -3.5065e-02,\n",
       "           -2.9004e-01, -2.2339e-01,  2.9077e-01,  2.7539e-01,  1.3013e-01,\n",
       "           -2.3962e-01,  6.6016e-01, -4.0375e-02,  1.5039e-01, -1.4062e-01,\n",
       "            1.3466e-02,  7.4921e-03, -1.2622e-01, -1.9043e-01, -3.2080e-01,\n",
       "           -1.7798e-01, -3.5352e-01, -1.1823e-01, -1.7090e-01, -2.6807e-01,\n",
       "           -8.4991e-03,  3.2593e-01,  5.6982e-01, -3.9062e-01, -2.4231e-01,\n",
       "           -5.1123e-01,  2.3535e-01, -4.3396e-02, -5.0977e-01, -1.0156e-01,\n",
       "           -2.9526e-02, -8.8379e-02, -2.0874e-01, -2.0166e-01,  5.4346e-01,\n",
       "           -2.2754e-01, -4.2676e-01, -1.2512e-01,  5.9296e-02,  2.2046e-01,\n",
       "           -2.5000e-01,  2.4243e-01, -5.4736e-01, -1.9250e-01, -2.3483e-02,\n",
       "            3.0884e-01,  1.9922e-01,  4.0479e-01, -1.2039e-02, -9.0515e-02,\n",
       "           -7.7271e-02,  1.3580e-02,  2.6215e-02, -6.4819e-02, -3.6597e-01,\n",
       "           -1.0513e-02,  3.8501e-01, -2.2180e-01,  1.5894e-01, -2.0093e-01,\n",
       "            2.1118e-01, -1.9958e-01, -9.8114e-03,  2.8833e-01, -4.0576e-01,\n",
       "            2.6099e-01, -8.8196e-02,  2.8223e-01,  5.3076e-01,  3.6865e-01,\n",
       "           -1.4990e-01, -1.9165e-01,  2.9785e-01, -7.8888e-03,  5.2216e-02,\n",
       "           -8.1299e-02, -2.1521e-01,  1.7761e-01,  2.0520e-01, -1.5405e-01,\n",
       "            6.9885e-02,  9.6375e-02,  7.9224e-02,  2.3816e-01, -1.4966e-01,\n",
       "            1.5833e-01,  5.2588e-01,  4.1250e+00, -4.1565e-02,  2.1948e-01,\n",
       "            7.3242e-02,  1.5662e-01,  2.2583e-01,  3.3545e-01, -9.2102e-02,\n",
       "           -1.6833e-01,  2.4390e-01, -4.6936e-02,  1.7407e-01, -2.6807e-01,\n",
       "            4.8027e-03,  2.4854e-01, -2.4878e-01,  1.6309e-01, -4.7144e-01,\n",
       "            1.5698e-01, -1.9189e-01, -2.4084e-01, -1.4014e-01, -2.7563e-01,\n",
       "           -4.9658e-01, -9.7961e-03, -9.0637e-02, -8.3374e-02,  4.0137e-01,\n",
       "           -6.5369e-02,  2.2327e-01,  4.3286e-01,  8.6212e-03,  4.5837e-02,\n",
       "           -3.2471e-01,  2.7490e-01,  4.0039e-01, -1.6406e-01,  8.4595e-02,\n",
       "            2.2595e-01, -5.5389e-02,  6.9885e-02,  4.7876e-01, -3.5400e-01,\n",
       "           -4.5386e-01,  2.4524e-01, -1.9458e-01, -2.8439e-03, -8.6426e-02,\n",
       "           -4.0820e-01, -8.4877e-04,  1.4624e-01, -3.8525e-01, -2.1497e-01,\n",
       "           -1.0022e-01,  6.2927e-02,  6.2218e-03,  8.6609e-02,  1.8713e-01,\n",
       "            8.6487e-02,  1.8140e-01, -3.9624e-01,  3.0225e-01, -8.4778e-02,\n",
       "           -8.6487e-02, -1.4209e-01, -1.2756e-01, -8.6243e-02,  1.7554e-01,\n",
       "           -3.1958e-01, -7.1631e-01,  2.5488e-01, -1.3840e-02,  2.9614e-01,\n",
       "            1.9031e-01, -1.7322e-01,  6.8604e-02,  4.0497e-02,  2.2900e-01,\n",
       "           -3.6572e-01, -4.8004e-02, -4.8218e-01, -3.4033e-01,  1.3135e-01,\n",
       "            1.6919e-01, -3.7231e-02, -1.5698e-01,  1.1017e-01,  3.7329e-01,\n",
       "            3.0563e-02,  3.4009e-01,  5.0635e-01, -1.3403e-01, -1.5137e-01,\n",
       "            2.2412e-01,  1.8762e-01, -1.1316e-01,  5.6738e-01,  4.4556e-02,\n",
       "           -3.8159e-01, -4.0991e-01, -2.8491e-01,  8.0566e-02, -1.0449e-01,\n",
       "           -2.7490e-01,  1.6003e-01, -6.3525e-01,  8.5266e-02, -4.6234e-02,\n",
       "           -2.7417e-01, -4.8926e-01,  4.0625e-01, -6.7334e-01, -3.1104e-01,\n",
       "           -2.5903e-01,  2.6199e-02, -2.0068e-01, -2.0227e-01,  2.3669e-01,\n",
       "           -1.0254e-01,  1.3928e-01,  2.5098e-01, -2.2003e-02, -9.4604e-02,\n",
       "           -1.8323e-01, -2.4963e-01,  3.3838e-01,  2.3950e-01,  1.8250e-01,\n",
       "           -2.9688e-01, -3.9478e-01,  4.2084e-02, -1.0406e-01,  1.9226e-01,\n",
       "           -2.2998e-01, -4.4434e-01, -8.9966e-02, -1.6187e-01,  2.0349e-01,\n",
       "            5.5713e-01, -1.8530e-01,  1.4624e-01, -3.0078e-01,  4.8767e-02,\n",
       "           -6.6040e-02, -4.8462e-02, -2.6352e-02, -8.0505e-02, -3.2642e-01,\n",
       "            1.9580e-01, -2.1347e-02, -2.0203e-01, -1.4221e-01, -1.9226e-01,\n",
       "            3.6279e-01,  1.2482e-01, -2.1045e-01, -2.1594e-01, -9.4788e-02,\n",
       "           -5.7910e-01,  1.8420e-01,  4.0161e-01, -6.1279e-01, -7.7026e-02,\n",
       "           -1.8457e-01,  6.3354e-02, -6.5137e-01, -1.7114e-01,  5.5518e-01,\n",
       "            4.7803e-01, -2.6953e-01, -5.5817e-02,  1.3147e-01, -1.4771e-01,\n",
       "           -2.8589e-01,  2.5513e-01,  7.7454e-02,  1.2976e-01, -2.3694e-01,\n",
       "           -2.2083e-01,  5.0537e-02,  3.2104e-01,  1.1123e+00,  3.3838e-01,\n",
       "            1.1353e-01,  8.8379e-02, -1.4893e-01, -2.6709e-01,  3.2654e-02,\n",
       "           -4.4189e-02,  9.8328e-02, -7.2021e-02,  2.6050e-01,  5.6348e-01,\n",
       "           -1.6895e-01,  2.6337e-02,  7.3181e-02, -8.0872e-02,  5.2216e-02,\n",
       "            2.8296e-01, -1.5784e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.1423e-01, 1.3196e-01, 7.4036e-02, 5.2571e-05, 2.2107e-01, 2.9297e-01,\n",
       "           2.6298e-04, 6.5369e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  105.463181  268.860779  233.078995  520.484192    0.946878     40   \n",
       "  1  433.885773  264.715088  556.663452  509.609253    0.936563     40   \n",
       "  2  258.345245  261.260590  378.988556  507.214081    0.888238     40   \n",
       "  3  242.517059   91.727997  343.833832  148.580780    0.881280     45   \n",
       "  4    0.000000   63.051697  634.330811  640.000000    0.768173     60   \n",
       "  5  430.679626  171.742081  516.492004  285.254700    0.763482     40   \n",
       "  6  205.933990  233.024811  437.350250  300.030121    0.733680     73   \n",
       "  7  490.508575   40.916565  639.617188  184.426193    0.474924     56   \n",
       "  \n",
       "             name  \n",
       "  0    wine glass  \n",
       "  1    wine glass  \n",
       "  2    wine glass  \n",
       "  3          bowl  \n",
       "  4  dining table  \n",
       "  5    wine glass  \n",
       "  6          book  \n",
       "  7         chair  ,\n",
       "  'caption': ['A wine glass which is seen on the left side of other three glasses.'],\n",
       "  'bbox_target': [109.3, 267.51, 120.81, 253.12]},\n",
       " 215: {'image_emb': tensor([[-0.1490,  0.0491, -0.1517,  ...,  1.3350, -0.1274, -0.2007],\n",
       "          [ 0.0724,  0.3015, -0.2184,  ...,  1.0469,  0.1192,  0.2054],\n",
       "          [ 0.1111,  0.3118, -0.0820,  ...,  0.7837,  0.1306, -0.0450],\n",
       "          ...,\n",
       "          [-0.2949,  0.1976,  0.0325,  ...,  0.6851, -0.0588,  0.4629],\n",
       "          [ 0.3069,  0.4001,  0.0961,  ...,  0.5005, -0.0753, -0.0211],\n",
       "          [-0.1586,  0.2500, -0.2761,  ...,  0.4805, -0.2622, -0.1133]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0876, -0.2625, -0.0697,  ..., -0.1106, -0.0812, -0.0370],\n",
       "          [ 0.2856, -0.1127, -0.5049,  ..., -0.1926, -0.3625,  0.0638]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.6226e-06, 4.1723e-07, 0.0000e+00, 1.1921e-06, 8.3447e-07, 0.0000e+00,\n",
       "           2.7418e-06, 6.6161e-06, 4.1723e-07, 1.2517e-06, 1.2529e-04, 2.3544e-05,\n",
       "           1.0000e+00, 1.2517e-06, 1.4305e-05],\n",
       "          [8.8990e-05, 1.0133e-05, 1.7881e-07, 5.9247e-05, 5.0724e-05, 0.0000e+00,\n",
       "           4.6082e-02, 1.2159e-04, 5.8353e-05, 3.5405e-05, 3.0112e-04, 4.5624e-03,\n",
       "           9.1162e-01, 9.6588e-03, 2.7527e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    57.476578  428.135223  144.267670  478.976501    0.909036     41   \n",
       "  1   433.587311  384.794037  483.711823  451.881653    0.900130     41   \n",
       "  2   164.096802  371.864441  207.869080  428.229065    0.891948     41   \n",
       "  3   150.888855  439.333130  204.278351  479.287231    0.887874     41   \n",
       "  4   517.599792  436.840576  567.216003  479.224365    0.877784     41   \n",
       "  5   150.905487  201.196411  209.106567  286.847412    0.867689     58   \n",
       "  6   218.919891  386.560852  388.960724  464.666565    0.859292     55   \n",
       "  7   379.338562  296.129822  402.029419  322.807312    0.858207     41   \n",
       "  8   396.447937  318.472565  424.913635  355.194061    0.856631     41   \n",
       "  9   239.002609  299.730499  262.016357  322.625153    0.847471     41   \n",
       "  10   61.136154  373.376282  101.884949  431.794250    0.777166     41   \n",
       "  11   56.710865  124.921204  136.488510  365.865417    0.769143      0   \n",
       "  12    0.682976  175.935959  143.359833  458.303467    0.754320      0   \n",
       "  13    0.000000  214.814392  628.166626  474.922424    0.748861     60   \n",
       "  14  267.800781  201.618378  353.922974  262.771271    0.647122     58   \n",
       "  15  416.919983  224.662842  467.262085  291.412292    0.612201     58   \n",
       "  16  294.026367  272.374084  327.768555  343.822937    0.602719     75   \n",
       "  17  205.047653  328.042419  230.450516  366.660828    0.476724     41   \n",
       "  18  381.119293  435.570557  513.141602  476.492798    0.425461     44   \n",
       "  19  217.541504  329.555481  261.816956  385.630859    0.387590     41   \n",
       "  20  523.262756  226.268494  584.671570  302.050476    0.329538      3   \n",
       "  21  269.365387  222.840240  345.754730  343.993866    0.304157     58   \n",
       "  22  482.959167  410.507080  589.770203  447.221558    0.276177     45   \n",
       "  23  523.529236  228.929199  580.783020  303.114197    0.254305     56   \n",
       "  \n",
       "              name  \n",
       "  0            cup  \n",
       "  1            cup  \n",
       "  2            cup  \n",
       "  3            cup  \n",
       "  4            cup  \n",
       "  5   potted plant  \n",
       "  6           cake  \n",
       "  7            cup  \n",
       "  8            cup  \n",
       "  9            cup  \n",
       "  10           cup  \n",
       "  11        person  \n",
       "  12        person  \n",
       "  13  dining table  \n",
       "  14  potted plant  \n",
       "  15  potted plant  \n",
       "  16          vase  \n",
       "  17           cup  \n",
       "  18         spoon  \n",
       "  19           cup  \n",
       "  20    motorcycle  \n",
       "  21  potted plant  \n",
       "  22          bowl  \n",
       "  23         chair  ,\n",
       "  'caption': ['A young child wearing a blue sweater decorated with stars.',\n",
       "   'A boy in a blue shirt looking at a birthday cake.'],\n",
       "  'bbox_target': [0.0, 180.15, 90.07, 264.29]},\n",
       " 216: {'image_emb': tensor([[-0.1196,  0.0330, -0.0827,  ...,  0.4905,  0.0738, -0.4448],\n",
       "          [-0.3159,  0.3359, -0.2017,  ...,  0.8457,  0.1677, -0.2515],\n",
       "          [-0.2913,  0.3816, -0.0237,  ...,  0.5137,  0.0897, -0.2194],\n",
       "          [ 0.0563,  0.1600, -0.0442,  ...,  0.8145, -0.2141,  0.3621],\n",
       "          [-0.1113,  0.2040, -0.3167,  ...,  0.5576,  0.1318, -0.0362],\n",
       "          [-0.1151,  0.1808, -0.5684,  ...,  0.3354, -0.3567,  0.2332]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0953, -0.1532, -0.0883,  ..., -0.6372,  0.1823, -0.1492],\n",
       "          [-0.1619,  0.1869, -0.1339,  ..., -0.2158, -0.1501,  0.1614]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.2068e-03, 1.3390e-03, 5.3263e-04, 9.6289e-01, 1.8015e-03, 3.1433e-02],\n",
       "          [0.0000e+00, 1.1921e-07, 0.0000e+00, 9.9805e-01, 2.2769e-05, 2.0199e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  242.841110   49.751740  277.987701  119.833466    0.916225     41   \n",
       "  1  343.474091   64.162750  384.717194  142.282043    0.907287     41   \n",
       "  2  309.178497   26.833549  339.958038   90.195694    0.875118     41   \n",
       "  3  425.481934   21.040382  631.921509  276.867157    0.860529      0   \n",
       "  4  163.260605   62.293274  485.017639  244.416809    0.825358     60   \n",
       "  5    0.000000  129.535385  228.640808  445.997925    0.660301      0   \n",
       "  6  419.685913    4.277023  639.657471  462.953613    0.575057     57   \n",
       "  7  294.057373  160.065781  353.415710  201.036865    0.384412     55   \n",
       "  8  269.638031   14.845505  287.780975   57.138306    0.371345     44   \n",
       "  9    0.000000    0.891693  222.146484  458.510986    0.283553     56   \n",
       "  \n",
       "             name  \n",
       "  0           cup  \n",
       "  1           cup  \n",
       "  2           cup  \n",
       "  3        person  \n",
       "  4  dining table  \n",
       "  5        person  \n",
       "  6         couch  \n",
       "  7          cake  \n",
       "  8         spoon  \n",
       "  9         chair  ,\n",
       "  'caption': ['A child in red kicking his feet.',\n",
       "   'A child in camouflage shorts with his feet on the wall, lying on a booth seat.'],\n",
       "  'bbox_target': [427.51, 27.93, 199.8, 242.76]},\n",
       " 217: {'image_emb': tensor([[ 0.2213,  0.3167, -0.0291,  ...,  0.9951, -0.0060, -0.1888],\n",
       "          [-0.2534,  0.4832, -0.1293,  ...,  0.8354,  0.0934,  0.3977],\n",
       "          [ 0.1154,  0.1674,  0.0291,  ...,  1.4443,  0.1873, -0.2141],\n",
       "          ...,\n",
       "          [-0.2749,  0.1328,  0.2228,  ...,  1.2529, -0.1630,  0.2327],\n",
       "          [-0.2988, -0.0444, -0.1073,  ...,  1.4111, -0.1743, -0.1971],\n",
       "          [ 0.1199,  0.3137,  0.0772,  ...,  0.7261, -0.0501, -0.0226]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 2.2021e-01, -9.9670e-02,  3.4607e-02, -3.8330e-02, -5.3320e-01,\n",
       "           -1.2122e-01, -1.9495e-01, -4.4141e-01,  1.2335e-01, -2.4011e-01,\n",
       "            9.7473e-02,  2.5806e-01,  4.0845e-01, -1.3074e-01, -2.7039e-02,\n",
       "           -1.0437e-02,  4.7729e-01,  7.2632e-02, -2.6709e-01,  2.8247e-01,\n",
       "            2.5854e-01,  3.0981e-01, -2.5488e-01,  2.0508e-01, -1.2073e-01,\n",
       "           -2.4329e-01,  1.0461e-01, -9.1187e-02, -1.1063e-02,  1.2891e-01,\n",
       "           -1.1841e-01, -4.8584e-01, -2.6489e-01,  2.7328e-02, -3.8330e-01,\n",
       "           -2.0728e-01,  5.9570e-02,  2.0410e-01,  3.6304e-01,  2.8857e-01,\n",
       "            1.0479e+00,  1.0321e-01, -5.1221e-01,  1.0498e-01, -2.7657e-03,\n",
       "            2.5732e-01,  1.3293e-01, -1.1664e-01, -2.0081e-01,  2.2781e-02,\n",
       "            1.9019e-01, -1.2238e-01, -1.9470e-01, -2.2559e-01, -3.3374e-01,\n",
       "           -5.3125e-01,  2.6025e-01, -1.1578e-01, -9.2041e-02, -2.9175e-01,\n",
       "           -1.0132e-01, -2.7979e-01, -2.7271e-01,  1.3745e-01, -2.7441e-01,\n",
       "            3.6182e-01,  2.6587e-01,  2.0422e-01,  3.6230e-01,  2.1240e-01,\n",
       "           -1.5088e-01, -9.1248e-03,  9.1553e-02,  1.1163e-01, -2.1286e-02,\n",
       "           -3.6426e-01,  3.4521e-01,  7.4219e-02,  1.8530e-01,  1.1853e-01,\n",
       "           -5.3369e-01, -2.6025e-01, -2.8906e-01, -3.2861e-01,  2.3022e-01,\n",
       "           -1.0809e-01, -1.7749e-01, -3.4644e-01,  2.7417e-01, -6.2103e-02,\n",
       "           -1.0712e-01, -3.9380e-01, -7.6074e-01,  2.2656e-01,  1.7896e-01,\n",
       "            1.4966e-01, -1.4905e-01, -1.5594e-02,  4.6826e-01, -1.1676e-01,\n",
       "           -2.1313e-01, -4.8364e-01,  2.8955e-01, -6.8703e-03,  2.9492e-01,\n",
       "            1.3354e-01, -8.3191e-02, -1.2103e-01,  2.1271e-02, -3.0151e-01,\n",
       "            4.9512e-01, -4.2383e-01, -9.9792e-03,  1.3757e-01, -1.7834e-01,\n",
       "            6.3721e-01, -1.3269e-01, -5.1483e-02, -2.9443e-01, -5.8740e-01,\n",
       "           -1.5735e-01, -7.4756e-01, -4.9048e-01, -7.7332e-02,  7.3486e-02,\n",
       "           -1.3452e-01,  9.3750e-02,  1.4148e-01,  1.1646e-01, -2.1777e-01,\n",
       "            2.0068e-01, -6.9629e-01,  4.1724e-01,  3.4355e+00, -1.0510e-01,\n",
       "           -1.2188e-03, -3.3911e-01, -2.5171e-01, -3.4277e-01, -2.1277e-01,\n",
       "           -3.2275e-01,  3.5791e-01, -7.7095e-03,  1.4624e-01, -3.8037e-01,\n",
       "           -4.2664e-02, -3.2928e-02,  5.1953e-01, -1.6943e-01,  2.4365e-01,\n",
       "            2.4017e-02, -2.7783e-01, -6.8542e-02,  3.1934e-01, -2.5806e-01,\n",
       "           -2.5903e-01, -3.5706e-02, -5.0146e-01, -1.4539e-01, -6.0028e-02,\n",
       "            4.5288e-02, -3.7085e-01,  1.5564e-01,  2.1935e-03, -1.7090e-01,\n",
       "            1.1938e-01,  8.7952e-02,  1.1780e-02,  6.8787e-02,  1.2878e-01,\n",
       "           -3.5498e-01, -2.2253e-01,  1.6125e-01,  3.1555e-02,  1.1530e-01,\n",
       "           -4.8859e-02,  1.5625e-01, -4.9463e-01, -3.8184e-01,  1.2207e-01,\n",
       "            1.1890e-01,  6.3037e-01, -5.5127e-01, -2.6440e-01, -2.2083e-01,\n",
       "           -2.4768e-01, -2.3291e-01,  8.0872e-02, -5.2686e-01,  1.3916e-01,\n",
       "            2.0386e-02,  3.3301e-01,  9.9945e-03, -1.9263e-01, -3.1689e-01,\n",
       "            1.4197e-01, -1.9312e-01,  2.1802e-01, -2.3010e-01, -6.3049e-02,\n",
       "           -7.6782e-02,  1.0632e-01, -1.7273e-01,  1.3013e-01, -2.0996e-01,\n",
       "            2.5806e-01,  3.0640e-01, -8.5815e-02, -1.7505e-01, -3.1372e-01,\n",
       "            4.7046e-01,  1.2915e-01,  9.9365e-02,  2.2363e-01,  4.1943e-01,\n",
       "            1.2360e-01,  2.6587e-01, -5.0049e-01,  2.9633e-02, -1.3098e-01,\n",
       "            1.2091e-01, -3.1836e-01, -9.0332e-02, -1.0242e-01,  5.1660e-01,\n",
       "           -4.0649e-01, -2.5520e-03, -5.6104e-01, -1.6699e-01, -2.3901e-01,\n",
       "           -2.1838e-01, -1.2100e-02,  6.7200e-02,  2.4304e-01,  2.1582e-01,\n",
       "            2.9150e-01,  2.4323e-02, -1.9690e-01, -8.6304e-02, -2.8809e-02,\n",
       "            4.0924e-02,  5.2930e-01,  1.8774e-01, -2.0789e-01, -1.7590e-01,\n",
       "           -1.9678e-01, -5.2393e-01, -2.2131e-01,  4.4604e-01, -5.1994e-03,\n",
       "           -2.4988e-01,  4.9536e-01,  3.1836e-01,  1.7456e-01, -4.5142e-01,\n",
       "            8.0032e-03, -3.0469e-01, -1.0297e-01,  3.5010e-01, -8.2458e-02,\n",
       "            5.5237e-02,  3.2684e-02, -1.6068e-02,  2.3792e-01,  5.6152e-01,\n",
       "            1.8140e-01,  4.5837e-02,  1.4136e-01, -9.9304e-02,  8.4229e-02,\n",
       "            6.3553e-03,  8.3435e-02,  2.6074e-01,  1.1182e-01,  1.7310e-01,\n",
       "           -1.6589e-01, -1.6833e-01,  1.2024e-02, -1.2305e-01,  9.7107e-02,\n",
       "            7.9041e-02,  7.7087e-02, -4.6167e-01,  4.0015e-01, -3.5152e-03,\n",
       "           -1.5710e-01,  7.0374e-02,  6.2073e-02,  4.5752e-01,  1.2537e-01,\n",
       "           -4.2511e-02,  2.6685e-01,  2.1399e-01, -9.0759e-02,  3.5278e-01,\n",
       "            1.1169e-01, -3.9673e-01, -3.3667e-01, -6.2622e-02,  3.3496e-01,\n",
       "           -2.5024e-01, -9.2102e-02,  2.2729e-01,  2.1826e-01,  5.3955e-02,\n",
       "            1.8604e-01,  3.6816e-01,  3.0688e-01,  1.5991e-01,  5.7678e-02,\n",
       "            2.6138e-02, -5.9448e-02,  3.4375e+00,  2.6688e-02,  1.0211e-01,\n",
       "            1.9800e-01,  1.7249e-01, -6.9775e-01, -2.2595e-01,  1.1700e-01,\n",
       "           -6.2622e-02,  2.3059e-01,  3.1030e-01, -1.8054e-01, -3.1128e-02,\n",
       "           -3.6768e-01,  2.6221e-01, -3.8879e-02, -1.2793e-01, -6.3525e-01,\n",
       "            4.1412e-02, -6.6719e-03,  3.3350e-01,  1.6858e-01,  3.1445e-01,\n",
       "           -3.5400e-01, -7.7858e-03, -7.0381e-03,  2.7686e-01,  3.2788e-01,\n",
       "            3.1097e-02, -2.7319e-01,  6.5689e-03, -4.8267e-01,  3.4644e-01,\n",
       "            3.3423e-01,  3.3154e-01, -2.5131e-02, -1.0474e-01, -1.8140e-01,\n",
       "           -1.7948e-03,  1.1188e-01,  3.0566e-01,  8.9294e-02, -4.1431e-01,\n",
       "           -9.5581e-02, -3.9282e-01,  4.8755e-01,  9.4116e-02,  8.1299e-02,\n",
       "            4.5074e-02, -2.7759e-01, -1.2433e-01, -1.6678e-02,  2.1338e-01,\n",
       "            2.2498e-01,  1.6711e-01, -5.8594e-02, -2.1619e-01,  2.0496e-01,\n",
       "           -3.8452e-01,  9.8816e-02, -2.6413e-02,  2.8638e-01, -3.6133e-02,\n",
       "           -1.4600e-01, -1.8286e-01, -1.8201e-01,  5.3406e-02,  2.0557e-01,\n",
       "            2.1985e-01,  7.0703e-01,  7.1655e-02, -6.2256e-01, -2.9272e-01,\n",
       "            4.4824e-01, -9.4223e-03, -9.0942e-02,  1.1304e-01,  3.5889e-02,\n",
       "            4.8218e-02,  2.4072e-01, -2.7954e-01,  1.0376e-01,  2.8744e-03,\n",
       "            5.0171e-02, -1.5088e-01, -1.7480e-01,  2.9468e-01,  4.7217e-01,\n",
       "            4.4708e-02,  4.5532e-02, -3.8892e-01, -1.0327e-01, -4.6216e-01,\n",
       "            3.3838e-01,  2.3340e-01,  9.9609e-02,  2.7124e-01, -1.8372e-01,\n",
       "           -1.0156e-01,  5.2979e-01,  6.0010e-01, -2.6514e-01, -8.3130e-02,\n",
       "           -3.8281e-01,  8.0566e-03,  1.5833e-01,  3.1982e-02,  4.8676e-02,\n",
       "           -4.3030e-02, -1.3879e-01,  3.6060e-01, -7.3096e-01, -1.4294e-01,\n",
       "           -2.0361e-01, -1.7017e-01, -1.0437e-02,  3.4180e-01,  1.9397e-01,\n",
       "           -4.1626e-01, -1.5274e-02, -5.9277e-01,  7.0862e-02,  7.4646e-02,\n",
       "            3.0835e-01, -2.7466e-01,  2.2498e-01, -2.3279e-01, -2.0984e-01,\n",
       "           -2.8979e-01, -3.3765e-01, -2.0056e-01, -2.8662e-01, -5.2051e-01,\n",
       "           -8.5999e-02, -4.6948e-01, -2.1082e-01, -1.4734e-01, -2.6001e-01,\n",
       "           -9.1895e-01, -2.8735e-01, -1.2646e-01,  3.9185e-01, -1.2964e-01,\n",
       "           -5.0586e-01, -3.0957e-01,  1.6589e-01,  2.4872e-02,  1.2646e-01,\n",
       "           -7.9895e-02,  3.5962e-01,  2.9816e-02,  3.6499e-01, -1.7334e-01,\n",
       "            1.1163e-01,  4.8126e-02, -2.2168e-01, -8.5327e-02,  5.2765e-02,\n",
       "            2.9224e-01,  2.7420e-02, -1.2070e-02, -2.5317e-01,  2.2327e-01,\n",
       "           -3.3386e-02, -8.7585e-02,  1.8823e-01,  1.6992e-01,  6.1914e-01,\n",
       "           -1.5942e-01, -2.5488e-01, -5.0507e-02, -3.0542e-01, -4.1779e-02,\n",
       "            1.7297e-01,  1.9250e-01,  1.7776e-02, -2.0691e-01, -2.6538e-01,\n",
       "           -2.9602e-02,  3.4863e-01, -2.4243e-01,  1.0439e+00, -3.3032e-01,\n",
       "           -8.9539e-02,  1.0614e-01, -2.6172e-01, -9.8511e-02,  7.7148e-02,\n",
       "           -2.7979e-01, -5.1318e-01, -1.4539e-01, -2.2583e-03,  1.2952e-01,\n",
       "           -4.1260e-02, -2.1912e-02,  3.9551e-02, -2.4805e-01,  6.4111e-01,\n",
       "           -2.3816e-01,  2.9224e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.9888e-03, 2.4487e-01, 9.0088e-02, 1.9678e-01, 3.5083e-01, 1.4198e-04,\n",
       "           1.6663e-02, 2.4152e-04, 9.7412e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   375.585632  235.240936  444.316528  339.261017    0.869059      0   \n",
       "  1   491.694550  212.849213  560.715271  337.913971    0.827975      0   \n",
       "  2     0.063187  240.114746   36.835873  315.656250    0.759419      2   \n",
       "  3   124.474564  254.924164  239.839493  339.278717    0.755481      3   \n",
       "  4    54.771149  226.197784  120.670929  380.549164    0.747608      0   \n",
       "  5   472.295929  216.516602  498.539825  277.112305    0.719324      0   \n",
       "  6   546.373474  212.506409  600.714661  296.628662    0.712480      0   \n",
       "  7   183.194946  231.215729  341.883423  279.654144    0.704266      2   \n",
       "  8   487.419983  257.061829  596.012512  335.727112    0.681831      3   \n",
       "  9    29.729816  265.394989  150.831177  380.283051    0.677304      3   \n",
       "  10  145.800140  223.255890  205.893524  343.013947    0.673450      0   \n",
       "  11   15.059731  231.632446   71.122459  256.856689    0.637071      2   \n",
       "  12  567.728760  243.806793  622.887451  298.275421    0.558166      3   \n",
       "  13  245.879745  234.887238  309.047546  301.575348    0.486918      0   \n",
       "  14   17.071930  230.692017  221.062225  285.659058    0.320300      2   \n",
       "  15  570.532715  219.456360  609.871948  296.748474    0.302070      0   \n",
       "  \n",
       "            name  \n",
       "  0       person  \n",
       "  1       person  \n",
       "  2          car  \n",
       "  3   motorcycle  \n",
       "  4       person  \n",
       "  5       person  \n",
       "  6       person  \n",
       "  7          car  \n",
       "  8   motorcycle  \n",
       "  9   motorcycle  \n",
       "  10      person  \n",
       "  11         car  \n",
       "  12  motorcycle  \n",
       "  13      person  \n",
       "  14         car  \n",
       "  15      person  ,\n",
       "  'caption': ['Blue motorcycle behind taxi.'],\n",
       "  'bbox_target': [26.89, 257.32, 125.84, 119.0]},\n",
       " 218: {'image_emb': tensor([[-0.0339,  0.2063,  0.1246,  ...,  1.4287,  0.2749, -0.1220],\n",
       "          [-0.0243,  0.1936, -0.2119,  ...,  1.3701,  0.0571,  0.0838],\n",
       "          [-0.2800,  0.5210,  0.0775,  ...,  1.0342,  0.2786, -0.3662],\n",
       "          [-0.2167, -0.1796,  0.1440,  ...,  1.1064,  0.0455, -0.4875]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1187,  0.2756, -0.3948,  ...,  0.1116, -0.1890, -0.4138],\n",
       "          [ 0.1882, -0.3542, -0.2374,  ..., -0.0273,  0.0139,  0.0817]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.7227e-04, 9.3408e-01, 6.5613e-02, 4.8876e-06],\n",
       "          [6.0654e-03, 1.6739e-02, 1.8286e-01, 7.9443e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0     0.645233  134.344681  238.081635  474.516663    0.940648      0  person\n",
       "  1   514.185242  200.953827  640.000000  477.225708    0.912856      0  person\n",
       "  2   184.942383  172.593338  316.487915  477.559021    0.909737      0  person\n",
       "  3   355.677765  296.961334  373.485443  352.290070    0.658794     39  bottle\n",
       "  4   449.759796  374.950897  508.744659  413.988800    0.657541     45    bowl\n",
       "  5   608.820984  285.240509  618.556458  321.699738    0.452392     39  bottle\n",
       "  6   389.634705  299.868103  416.098877  327.845642    0.341128     44   spoon\n",
       "  7   421.072052  140.403778  466.112091  158.997467    0.290836     45    bowl\n",
       "  8   601.287292  336.272980  640.000000  361.083038    0.284869     71    sink\n",
       "  9   380.490540  388.772919  459.822266  431.555511    0.284755     45    bowl\n",
       "  10  469.763916  303.028473  483.286194  342.189240    0.264554     39  bottle,\n",
       "  'caption': ['A woman in a floral dress holding a cell phone.',\n",
       "   'A woman reads a document in a kitchen with her friend looking on.'],\n",
       "  'bbox_target': [189.19, 169.19, 125.4, 304.86]},\n",
       " 219: {'image_emb': tensor([[ 0.1329,  0.2898, -0.2742,  ...,  0.6929, -0.4331,  0.0559],\n",
       "          [-0.1992,  0.1093, -0.1628,  ...,  1.4180, -0.0912, -0.0537],\n",
       "          [-0.3655,  0.3564, -0.1752,  ...,  1.3086, -0.0565,  0.0835],\n",
       "          [ 0.2598,  0.3545, -0.1560,  ...,  0.7051, -0.4382,  0.0610]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3354, -0.2654,  0.0039,  ..., -0.0698,  0.1034, -0.0502],\n",
       "          [ 0.0157,  0.1450, -0.2449,  ..., -0.2422, -0.4084,  0.2671]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0070, 0.3152, 0.6074, 0.0703],\n",
       "          [0.0952, 0.4836, 0.1185, 0.3027]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   64.958939  206.592972  388.456909  559.921631    0.924810     58   \n",
       "  1  342.145111  245.392929  426.315735  470.498230    0.835050     15   \n",
       "  2  203.093079    0.000000  388.812256  149.832764    0.797919     58   \n",
       "  3    0.000000   21.757622  141.505005  161.727966    0.563260     15   \n",
       "  4    0.000000   22.667213  141.603485  163.143433    0.490142     16   \n",
       "  \n",
       "             name  \n",
       "  0  potted plant  \n",
       "  1           cat  \n",
       "  2  potted plant  \n",
       "  3           cat  \n",
       "  4           dog  ,\n",
       "  'caption': ['Blurry animal sitting on a step.', 'A dog laying godnw.'],\n",
       "  'bbox_target': [0.0, 26.69, 140.61, 125.15]},\n",
       " 220: {'image_emb': tensor([[ 0.0798,  0.3391, -0.2673,  ...,  0.6553,  0.1627, -0.5024],\n",
       "          [-0.0588,  0.0407, -0.1135,  ...,  0.7827, -0.1710,  0.0301]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1247, -0.2866,  0.2766,  ...,  0.4897,  0.4004,  0.1010],\n",
       "          [ 0.3047, -0.0266, -0.0865,  ..., -0.0343,  0.3066, -0.2360]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6860, 0.3140],\n",
       "          [0.6406, 0.3594]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0   42.178131  104.949585  612.334351  251.321594    0.926441      4  airplane\n",
       "  1  390.776550  340.192047  637.829407  392.315704    0.598183      4  airplane\n",
       "  2  310.836700  371.206146  370.261383  390.522980    0.482036      7     truck,\n",
       "  'caption': ['A China Airlines plane with a pink flower painted on it.',\n",
       "   'An airplane ascending into the sky.'],\n",
       "  'bbox_target': [46.59, 103.32, 564.76, 147.37]},\n",
       " 221: {'image_emb': tensor([[-0.2722, -0.2405, -0.1196,  ...,  0.3989,  0.0123, -0.0048],\n",
       "          [-0.3833,  0.1290, -0.2500,  ...,  1.2646,  0.1083, -0.0023],\n",
       "          [-0.2213,  0.0338, -0.1688,  ...,  0.4597, -0.1610,  0.0867],\n",
       "          [-0.3469, -0.0021, -0.3000,  ...,  0.7788,  0.0892, -0.0254],\n",
       "          [-0.1635, -0.1849, -0.0835,  ...,  0.2224, -0.1537,  0.0255]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1885, -0.3542, -0.0980,  ...,  0.2913,  0.1349,  0.4255],\n",
       "          [-0.4417, -0.4631, -0.1587,  ...,  0.2107,  0.1389,  0.4529]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1226, 0.0095, 0.1730, 0.6631, 0.0320],\n",
       "          [0.2576, 0.0053, 0.2135, 0.4663, 0.0575]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0    1.248860   52.431065  339.999237  328.820526    0.935579     22  zebra\n",
       "  1    0.618815    0.164562  212.289642   71.585281    0.897955     22  zebra\n",
       "  2  257.027344  106.798767  499.256042  328.858765    0.858398     22  zebra\n",
       "  3    1.208591    0.683206  471.149536  149.877548    0.857570     22  zebra,\n",
       "  'caption': [\"A zebra's face.\", 'A zebra looking directly into the camera.'],\n",
       "  'bbox_target': [0.75, 50.01, 341.98, 279.12]},\n",
       " 222: {'image_emb': tensor([[ 7.5806e-02,  9.1553e-02,  2.8394e-01,  ...,  1.6418e-01,\n",
       "            5.5371e-01,  3.0365e-02],\n",
       "          [-3.3716e-01,  4.3164e-01, -2.9834e-01,  ...,  1.1445e+00,\n",
       "           -5.5695e-02,  6.3538e-02],\n",
       "          [-5.2500e-04, -3.8184e-01, -4.2969e-01,  ...,  9.4141e-01,\n",
       "           -7.7820e-02, -9.8450e-02],\n",
       "          ...,\n",
       "          [ 5.5328e-02,  6.9641e-02, -3.6591e-02,  ...,  1.0518e+00,\n",
       "            3.0176e-01, -4.0845e-01],\n",
       "          [-1.4893e-01,  2.3694e-01, -4.5605e-01,  ...,  1.2539e+00,\n",
       "           -4.2407e-01,  2.2229e-01],\n",
       "          [ 2.9755e-02,  1.0400e-01,  5.6055e-01,  ...,  2.6474e-03,\n",
       "            2.6392e-01,  1.6373e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0807,  0.0533, -0.7397,  ...,  0.5137,  0.1631, -0.1595],\n",
       "          [ 0.2261, -0.2401, -0.5190,  ...,  0.0785, -0.2986,  0.3420]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.1782e-03, 7.0715e-04, 1.2217e-03, 9.9561e-01, 7.9334e-05, 4.3154e-05,\n",
       "           1.4818e-04],\n",
       "          [4.4975e-03, 9.3765e-03, 2.9964e-03, 9.2676e-01, 8.7214e-04, 4.8370e-02,\n",
       "           7.0763e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  215.668274  147.581024  414.969299  424.224487    0.943157      0  person\n",
       "  1    0.132057   33.472870  109.485062  211.400238    0.911230     62      tv\n",
       "  2  552.204712  194.812134  603.804443  208.947510    0.812561     65  remote\n",
       "  3  540.277100   41.662689  639.615479  250.710663    0.804312      0  person\n",
       "  4  379.311554  197.410461  446.355377  228.929382    0.749634     65  remote\n",
       "  5  557.296753  352.055634  639.508667  426.032410    0.731428      0  person\n",
       "  6  133.942917  352.601776  149.204788  372.735382    0.662547     65  remote\n",
       "  7  146.817352  221.260101  262.741608  304.576935    0.662320     56   chair\n",
       "  8  505.412964  246.072144  639.208374  423.967041    0.624845     57   couch\n",
       "  9  378.141663  193.396210  423.824158  212.951935    0.436666     65  remote,\n",
       "  'caption': ['young girl in a white shirt holding a Wii remote',\n",
       "   'The girl standing beside the couch.'],\n",
       "  'bbox_target': [538.67, 34.41, 101.23, 220.46]},\n",
       " 223: {'image_emb': tensor([[-0.2847,  0.6675,  0.1169,  ...,  1.0918,  0.3276, -0.2527],\n",
       "          [-0.2444,  0.7358,  0.0478,  ...,  0.8428, -0.0128, -0.0248],\n",
       "          [-0.3054,  0.8037,  0.2070,  ...,  0.9229, -0.0458, -0.1726],\n",
       "          [-0.1663,  0.5718,  0.1058,  ...,  0.3894,  0.0304, -0.2457]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.4641,  0.3062,  0.3501,  ..., -0.1565, -0.3083, -0.1448],\n",
       "          [-0.0879,  0.3877,  0.3337,  ..., -0.1699, -0.2815, -0.2388]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0247, 0.5522, 0.3979, 0.0250],\n",
       "          [0.0065, 0.3369, 0.5137, 0.1427]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  260.786255  277.746094  421.325134  381.002319    0.860062     71    sink\n",
       "  1  193.577942  341.923889  328.939087  472.576172    0.851035     61  toilet\n",
       "  2  298.651184  486.312988  402.161560  640.000000    0.774979     61  toilet,\n",
       "  'caption': ['Toilet bowl with no seat or cover.',\n",
       "   'The western type toilet is open'],\n",
       "  'bbox_target': [297.93, 484.95, 102.77, 155.05]},\n",
       " 224: {'image_emb': tensor([[-0.0225,  0.3123, -0.1896,  ...,  1.1836, -0.0684, -0.2739],\n",
       "          [ 0.1024,  0.0754, -0.0111,  ...,  0.9897,  0.1599, -0.1115],\n",
       "          [-0.4934,  0.4646,  0.1699,  ...,  0.7930,  0.2059,  0.0367],\n",
       "          ...,\n",
       "          [-0.0861,  0.2651,  0.0246,  ...,  0.8701, -0.0338, -0.1544],\n",
       "          [ 0.0555,  0.4817, -0.2520,  ...,  0.2871, -0.1730, -0.1949],\n",
       "          [ 0.2074,  0.2358, -0.0707,  ...,  0.3286, -0.0657,  0.1617]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0304,  0.3447,  0.1276,  ..., -0.3523,  0.1968, -0.4275],\n",
       "          [-0.0875,  0.3940, -0.1174,  ..., -0.1360,  0.0989, -0.2546]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.1586e-02, 5.6648e-03, 8.8086e-01, 5.8105e-02, 1.1261e-02, 1.2375e-02,\n",
       "           2.3901e-05],\n",
       "          [1.8604e-01, 8.7891e-02, 1.4038e-01, 5.5518e-01, 2.2217e-02, 7.3280e-03,\n",
       "           7.3671e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  114.594055  195.287399  171.912460  291.964233    0.893497     54   donut\n",
       "  1  229.450821   70.853584  264.176208  116.512070    0.880552     41     cup\n",
       "  2  181.876831    0.433338  366.104340  205.788422    0.877652      0  person\n",
       "  3  117.880753  294.837341  180.168457  393.493164    0.869274     54   donut\n",
       "  4    0.706840   88.510559  131.635696  500.000000    0.866838      0  person\n",
       "  5  133.912354   99.228249  366.328735  499.147125    0.819747      0  person\n",
       "  6  140.699478  287.883698  182.144928  325.916534    0.271165     54   donut,\n",
       "  'caption': ['A person holding an orange paper cup',\n",
       "   'A hand holding an orange cup'],\n",
       "  'bbox_target': [184.27, 1.12, 183.73, 213.49]},\n",
       " 225: {'image_emb': tensor([[ 0.0022,  0.3118, -0.2108,  ...,  0.4272, -0.2383, -0.1881],\n",
       "          [-0.0674,  0.3777, -0.1636,  ...,  1.1895, -0.3337, -0.1161],\n",
       "          [-0.1050,  0.1511, -0.2246,  ...,  1.3535, -0.0219, -0.3840],\n",
       "          [-0.2404,  0.8237, -0.2321,  ...,  0.2991,  0.1090,  0.0925]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3291,  0.1837,  0.1646,  ..., -0.1716,  0.1026, -0.0208],\n",
       "          [-0.2612,  0.0494, -0.1630,  ...,  0.2184,  0.1003, -0.2634]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.4165, 0.0157, 0.0072, 0.5605],\n",
       "          [0.0078, 0.0050, 0.0011, 0.9863]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   98.658279   21.054550  380.738586  519.237671    0.956535      0   \n",
       "  1  130.019928  252.763885  206.705170  504.677460    0.846657      0   \n",
       "  2  190.481201  504.530365  254.486877  549.075928    0.775637     36   \n",
       "  3  160.318024  523.907776  205.977936  546.702087    0.383544     36   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2  skateboard  \n",
       "  3  skateboard  ,\n",
       "  'caption': ['man doing trick',\n",
       "   'A man in a red checkered shirt grinding a skateboard'],\n",
       "  'bbox_target': [103.27, 24.74, 277.92, 483.05]},\n",
       " 226: {'image_emb': tensor([[-0.3171,  0.3403, -0.2163,  ...,  1.3838,  0.2744, -0.1879],\n",
       "          [ 0.0222,  0.5259, -0.3181,  ...,  1.4717, -0.1326,  0.0432],\n",
       "          [ 0.0957,  0.1658,  0.1510,  ...,  0.4983, -0.3875,  0.1360],\n",
       "          ...,\n",
       "          [ 0.3638, -0.2551,  0.0071,  ...,  0.5576, -0.3701,  0.1963],\n",
       "          [-0.1008,  0.1779, -0.3950,  ...,  1.2949, -0.3440, -0.0715],\n",
       "          [ 0.1710, -0.0680, -0.0790,  ...,  0.7310, -0.1006,  0.0539]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-1.5027e-01,  2.3853e-01,  1.2915e-01, -3.2410e-02,  1.8616e-02,\n",
       "            1.4697e-01,  3.6545e-03, -6.5234e-01,  1.0266e-01, -1.0638e-01,\n",
       "           -1.7395e-02,  1.5601e-01,  3.8605e-03, -1.3599e-01, -1.1108e-01,\n",
       "            1.5454e-01,  1.6821e-01, -8.9478e-02,  4.3091e-02,  4.3994e-01,\n",
       "            1.4885e-02,  1.9080e-01,  3.2764e-01,  4.0674e-01,  1.6016e-01,\n",
       "            6.3232e-02, -1.5503e-01, -1.5186e-01,  6.1829e-02,  9.5520e-02,\n",
       "            3.6316e-02,  7.8003e-02, -2.1387e-01, -4.8608e-01,  4.1351e-02,\n",
       "           -3.7402e-01,  8.9111e-02, -2.8540e-01,  2.7962e-03,  2.7271e-01,\n",
       "            1.4624e-01, -6.2988e-02,  3.6353e-01, -1.5881e-01,  1.2415e-01,\n",
       "            5.3162e-02,  2.3682e-01,  4.0527e-01, -4.0802e-02, -1.3367e-01,\n",
       "           -2.6367e-01, -2.8516e-01, -1.5271e-01, -3.0127e-01, -9.3750e-02,\n",
       "           -5.9662e-03, -2.0667e-01, -1.4417e-01, -5.2197e-01,  3.3252e-01,\n",
       "            4.3121e-02, -6.4453e-02, -6.1066e-02, -3.9160e-01, -2.0203e-01,\n",
       "           -6.6345e-02,  7.4280e-02,  2.7637e-01,  1.2201e-01, -3.4912e-01,\n",
       "            2.1631e-01, -1.3586e-01,  4.9286e-02, -7.3914e-02, -1.7493e-01,\n",
       "           -6.1768e-02,  1.3391e-01,  2.5391e-01,  3.2275e-01,  3.0420e-01,\n",
       "            2.0056e-01,  1.7712e-01,  3.4717e-01,  5.6671e-02, -2.2491e-02,\n",
       "            1.7773e-01,  5.0977e-01,  4.6362e-01, -3.3911e-01,  4.1290e-02,\n",
       "           -8.2520e-02, -2.1899e-01, -1.2021e+00,  3.7769e-01, -2.3853e-01,\n",
       "           -3.1372e-01, -2.2864e-01, -3.3252e-01, -2.2729e-01,  8.9844e-02,\n",
       "           -1.2695e-01,  4.1772e-01,  2.9297e-01, -1.3046e-02, -4.3042e-01,\n",
       "            7.5195e-02,  2.8052e-01, -3.8843e-01, -1.7114e-01,  1.5930e-01,\n",
       "            1.4099e-01,  2.0081e-01,  4.7241e-01,  2.9102e-01, -3.5596e-01,\n",
       "           -9.2163e-02,  1.8408e-01, -5.9540e-02,  4.8096e-02, -3.2935e-01,\n",
       "           -4.4165e-01, -8.0078e-01,  1.3611e-01, -9.0759e-02,  1.8274e-01,\n",
       "            3.8818e-01, -2.1448e-01,  5.0087e-03,  1.2769e-01,  2.8857e-01,\n",
       "            8.4686e-03, -1.9922e-01,  4.8413e-01,  5.0156e+00,  6.2164e-02,\n",
       "           -2.7390e-02, -4.4873e-01, -1.3293e-01, -4.5801e-01, -2.3300e-02,\n",
       "           -3.3862e-01,  1.6931e-01, -4.9829e-01,  2.3743e-01,  2.8091e-02,\n",
       "           -1.2146e-01, -1.7944e-01, -2.7515e-01, -3.9380e-01, -1.5796e-01,\n",
       "            8.0261e-02,  2.3828e-01, -5.2185e-02, -1.4282e-01, -4.0186e-01,\n",
       "           -1.6541e-01,  1.2720e-01, -2.8149e-01,  7.3669e-02,  5.8960e-02,\n",
       "           -4.5776e-02, -5.7275e-01,  8.8318e-02, -4.4800e-02, -2.3596e-01,\n",
       "           -2.3853e-01, -1.4381e-02,  7.9651e-02, -8.6136e-03,  1.0089e-01,\n",
       "            4.2529e-01, -4.0308e-01, -9.5581e-02,  2.7661e-01, -1.6235e-01,\n",
       "            2.6807e-01, -6.0693e-01,  3.6835e-02,  2.5806e-01, -9.2896e-02,\n",
       "           -1.8652e-01, -3.5248e-02,  6.5186e-02,  5.0697e-03,  4.7272e-02,\n",
       "            1.2347e-01,  6.3330e-01,  1.1627e-01,  5.8936e-01,  6.4209e-02,\n",
       "            9.4238e-02,  2.2240e-03,  3.3691e-01,  1.8115e-01,  4.1577e-01,\n",
       "           -1.6769e-02, -4.1943e-01,  6.9580e-02, -1.9653e-01, -1.6467e-01,\n",
       "           -2.2961e-01,  4.4800e-01,  4.7559e-01, -4.9042e-02,  6.8604e-02,\n",
       "            4.4739e-02,  1.1407e-01, -6.5247e-02,  1.3696e-01, -2.0789e-01,\n",
       "           -3.8185e-03, -7.9834e-02,  6.0455e-02,  5.6641e-02,  1.2549e-01,\n",
       "           -2.5049e-01,  1.8799e-01, -2.8076e-01,  2.5146e-01,  1.8115e-01,\n",
       "            3.2788e-01,  1.3123e-01,  2.6904e-01, -3.9917e-02,  1.1162e-02,\n",
       "            1.4062e-01, -1.5900e-02, -1.7114e-01, -1.7969e-01, -6.3037e-01,\n",
       "           -3.8965e-01,  8.6060e-02,  3.6548e-01, -2.6807e-01,  9.0210e-02,\n",
       "           -1.1206e-01,  1.2573e-01,  1.8579e-01, -1.3977e-01, -2.6538e-01,\n",
       "            5.7080e-01,  7.6050e-02, -2.1497e-01,  3.5181e-01, -5.3760e-01,\n",
       "            1.6370e-01,  2.8015e-02, -4.4043e-01,  1.2219e-01, -1.1780e-01,\n",
       "           -6.7383e-02, -2.9614e-01,  3.4912e-01, -2.3938e-01,  2.2534e-01,\n",
       "           -6.9092e-02,  2.4988e-01,  3.3765e-01, -4.2236e-02,  1.9202e-01,\n",
       "            3.5474e-01,  4.2694e-02, -3.2568e-01, -2.6025e-01, -1.5173e-01,\n",
       "           -1.2061e+00, -2.8125e-01, -3.0371e-01, -2.2168e-01, -1.8958e-01,\n",
       "           -3.7842e-01, -4.7852e-02, -6.8115e-02,  6.3133e-03, -6.2469e-02,\n",
       "            9.1858e-02,  3.0200e-01,  3.0396e-01,  1.7102e-01,  7.4951e-02,\n",
       "           -3.8159e-01, -1.2134e-01,  4.1675e-01,  2.5220e-01,  8.6365e-03,\n",
       "            2.6074e-01,  9.4666e-02,  1.9922e-01,  1.1084e-01, -7.1533e-02,\n",
       "           -9.8572e-02,  8.2779e-03, -4.3945e-02, -2.5848e-02,  2.1313e-01,\n",
       "           -2.0898e-01, -1.2390e-01,  1.7029e-01, -5.1056e-02,  1.1487e-01,\n",
       "           -3.3179e-01, -4.0479e-01, -9.8755e-02,  1.6650e-01, -3.3173e-02,\n",
       "            2.4365e-01,  3.3960e-01,  2.0959e-01,  5.9448e-02, -2.8882e-01,\n",
       "           -3.6926e-02,  3.7012e-01,  5.0117e+00, -1.5961e-02,  2.3438e-01,\n",
       "           -1.4368e-01,  1.3391e-01,  1.2805e-01, -3.5461e-02,  6.4507e-03,\n",
       "            4.5654e-01,  2.9907e-01, -1.3257e-01, -4.3869e-03,  8.8989e-02,\n",
       "            4.9268e-01,  4.7821e-02,  4.6216e-01,  2.0667e-01, -1.5107e+00,\n",
       "           -6.3965e-02, -2.1179e-01, -2.1106e-01, -1.3391e-01, -9.0149e-02,\n",
       "           -1.8701e-01,  1.4465e-01, -4.0771e-02, -2.4634e-01,  2.5391e-01,\n",
       "           -2.8491e-01, -9.9670e-02,  2.9102e-01, -3.1445e-01, -1.1365e-01,\n",
       "            5.3558e-02,  1.3843e-01, -2.3453e-02, -3.5156e-01,  1.7859e-01,\n",
       "            2.5635e-01, -6.4758e-02,  1.0919e-01,  2.2656e-01,  3.3887e-01,\n",
       "           -4.3921e-01, -3.3374e-01, -1.4441e-01,  1.7627e-01,  1.0669e-01,\n",
       "           -2.1399e-01,  1.0400e-01, -1.0327e-01,  3.6182e-01,  3.0737e-01,\n",
       "            1.8982e-01, -2.2974e-01,  3.1885e-01, -2.6733e-01,  1.3293e-01,\n",
       "           -7.2021e-01, -1.2634e-01, -6.2134e-02,  2.3450e-01,  5.9265e-02,\n",
       "           -1.9788e-01, -3.0518e-01, -7.1533e-02, -2.4500e-01, -3.5132e-01,\n",
       "           -2.1399e-01, -2.5928e-01, -5.0659e-02, -4.6826e-01,  5.3271e-01,\n",
       "            1.6248e-01,  1.2073e-01,  1.7542e-01, -4.3823e-01, -4.6069e-01,\n",
       "           -6.7822e-01, -2.7856e-01, -2.6733e-01, -2.7954e-01, -6.0913e-02,\n",
       "           -2.4048e-02,  4.0375e-02, -3.8940e-01,  2.6016e-02,  4.3579e-01,\n",
       "            5.0830e-01, -1.6882e-01,  1.9958e-01, -3.5522e-02, -3.4741e-01,\n",
       "            7.8247e-02,  9.6313e-02,  1.9849e-01,  1.0425e-01,  9.0332e-02,\n",
       "           -2.4475e-01, -4.8730e-01, -3.5742e-01,  3.3417e-02, -1.3416e-01,\n",
       "           -1.9531e-01, -5.7587e-02, -2.2253e-01,  9.4528e-03, -2.1716e-01,\n",
       "            2.5952e-01, -4.0009e-02,  4.7217e-01, -3.2080e-01, -1.7731e-02,\n",
       "            1.1454e-03, -5.2441e-01,  1.0229e-01,  9.2285e-02,  3.0664e-01,\n",
       "            8.0688e-02, -1.9019e-01, -7.8186e-02,  1.7163e-01,  1.9531e-01,\n",
       "           -3.5449e-01, -6.3232e-02,  3.1006e-02, -9.3628e-02,  1.6589e-01,\n",
       "           -4.8535e-01, -2.0459e-01, -1.2732e-01, -3.9819e-01,  1.7676e-01,\n",
       "           -3.5449e-01, -2.7148e-01, -2.1606e-01,  2.3474e-01,  2.6636e-01,\n",
       "           -2.5223e-02,  1.0614e-01, -1.5503e-01, -5.8252e-01, -1.0931e-01,\n",
       "            2.3560e-01,  1.7914e-02, -1.6199e-01,  1.4551e-01, -4.4141e-01,\n",
       "           -4.4849e-01,  1.3599e-01, -1.3794e-01, -7.2876e-02,  7.7454e-02,\n",
       "            1.2421e-01, -6.4926e-03, -1.3159e-01,  5.0323e-02, -2.7515e-01,\n",
       "           -2.9028e-01,  8.9722e-02, -7.9773e-02,  4.4403e-02, -1.3574e-01,\n",
       "            3.6328e-01,  1.0608e-01, -1.3725e-02, -3.3301e-01,  4.4385e-01,\n",
       "           -1.7480e-01, -2.0618e-01, -2.8662e-01,  2.4243e-01, -1.8323e-01,\n",
       "            1.4734e-01,  7.0862e-02, -1.4539e-01,  4.4067e-02, -3.3545e-01,\n",
       "           -4.3286e-01,  7.6370e-03,  1.7444e-01,  7.5586e-01,  2.9321e-01,\n",
       "            7.5317e-02,  1.1566e-01,  4.1650e-01, -1.9958e-01, -2.7832e-01,\n",
       "            1.7822e-01, -6.9824e-02,  1.6919e-01,  2.2888e-01, -1.2427e-01,\n",
       "           -7.7271e-02,  1.3574e-01, -2.0557e-01,  3.4515e-02,  1.3831e-01,\n",
       "           -2.4963e-01, -2.6880e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.5065e-02, 1.2388e-03, 4.5562e-04, 9.0479e-01, 1.2851e-04, 5.7831e-02,\n",
       "           5.9414e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  569.669250  119.820160  639.883728  345.295502    0.903335     43   \n",
       "  1   79.730812  227.586304  171.381836  423.780518    0.876758     43   \n",
       "  2  357.952301  134.362183  595.500916  349.624207    0.848893     48   \n",
       "  3  171.548096    0.159912  400.826599  119.494812    0.836185     45   \n",
       "  4  187.809662  168.292816  401.718597  410.291504    0.827233     48   \n",
       "  5    0.000000  163.574768  148.030396  320.397766    0.793650     45   \n",
       "  6    0.000000   15.746567  640.000000  422.344788    0.412985     60   \n",
       "  \n",
       "             name  \n",
       "  0         knife  \n",
       "  1         knife  \n",
       "  2      sandwich  \n",
       "  3          bowl  \n",
       "  4      sandwich  \n",
       "  5          bowl  \n",
       "  6  dining table  ,\n",
       "  'caption': ['bottom half of the bowl holding tortilla chips.'],\n",
       "  'bbox_target': [180.93, 40.05, 207.74, 89.03]},\n",
       " 227: {'image_emb': tensor([[-0.0770,  0.3530, -0.3110,  ...,  1.1758,  0.0347,  0.0379],\n",
       "          [-0.2576,  0.4346,  0.1335,  ...,  0.7012,  0.2183,  0.2310],\n",
       "          [-0.1733,  0.3706,  0.0939,  ...,  1.2490,  0.0850, -0.0264],\n",
       "          [-0.1588,  0.2040,  0.1926,  ...,  0.9707, -0.1548, -0.0393],\n",
       "          [-0.1370, -0.2159, -0.2257,  ...,  1.0205,  0.2352, -0.0656],\n",
       "          [ 0.0029,  0.3188,  0.1351,  ...,  0.4062, -0.0078,  0.1412]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1117,  0.0967, -0.1688,  ...,  0.0697,  0.2087, -0.2378],\n",
       "          [-0.3269,  0.1421, -0.0329,  ...,  0.0514,  0.0276,  0.0378]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.4124e-03, 6.2294e-03, 3.6987e-02, 1.3056e-03, 3.8207e-05, 9.5410e-01],\n",
       "          [2.2614e-02, 5.3406e-02, 6.8164e-01, 2.4204e-03, 3.4332e-04, 2.3938e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  315.894928   80.800613  521.605591  219.660095    0.936339     63  laptop\n",
       "  1  367.184906   31.259476  594.606445  219.699219    0.900325      0  person\n",
       "  2   49.588440   89.501205  267.443756  188.420746    0.893880     63  laptop\n",
       "  3    0.000000   94.307800  640.000000  350.982544    0.834895     59     bed\n",
       "  4  368.706726  295.832733  559.502686  355.317719    0.791968     73    book,\n",
       "  'caption': ['The Apple computer on the left.', 'The silver apple computer'],\n",
       "  'bbox_target': [48.13, 92.12, 220.62, 97.08]},\n",
       " 228: {'image_emb': tensor([[ 0.1312,  0.3792, -0.0732,  ...,  0.9644, -0.1614, -0.0811],\n",
       "          [-0.1170,  0.5981, -0.0415,  ...,  0.7983,  0.0323,  0.2979],\n",
       "          [ 0.4534,  0.4438, -0.1322,  ...,  0.8516, -0.2462, -0.1427],\n",
       "          ...,\n",
       "          [-0.1724,  0.3242, -0.3350,  ...,  0.9980, -0.1783, -0.3545],\n",
       "          [-0.1868,  0.6401, -0.0959,  ...,  1.1914, -0.1349,  0.2791],\n",
       "          [ 0.5156,  0.3821, -0.1743,  ...,  0.8506, -0.1449, -0.0098]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1837,  0.2291, -0.0051,  ...,  0.0477, -0.1180,  0.0539],\n",
       "          [-0.0856,  0.0948, -0.2373,  ...,  0.1781, -0.0698,  0.1135]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.6533e-01, 3.4607e-02, 5.9605e-08, 1.9431e-05, 3.0994e-06, 3.7491e-05,\n",
       "           2.7418e-06],\n",
       "          [9.0918e-01, 8.7280e-02, 4.7684e-06, 4.0531e-06, 6.6996e-05, 2.5940e-03,\n",
       "           6.4564e-04]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    97.912331   32.482231  255.320709  160.711365    0.935223     41   \n",
       "  1   419.420837    0.000000  543.975647  206.149841    0.934782     41   \n",
       "  2    57.110397  149.444122  323.069702  358.844635    0.907612     48   \n",
       "  3     0.191978    0.067932  178.246979   92.785950    0.901819      0   \n",
       "  4   298.129333  177.431610  502.460449  336.663483    0.885937     45   \n",
       "  5   566.184143  188.456024  640.000000  329.404449    0.838777     42   \n",
       "  6   529.212341    0.686966  639.187561  128.858780    0.667772      0   \n",
       "  7     2.165070   57.029770  640.000000  468.928284    0.635449     60   \n",
       "  8    13.160004    0.000000  635.964355   91.323532    0.330589      0   \n",
       "  9   585.909729  254.100098  639.945618  405.734375    0.326135     43   \n",
       "  10  295.595917  176.615326  497.587982  340.668243    0.308894     60   \n",
       "  11  581.516602  251.030609  639.598755  406.554047    0.308614     42   \n",
       "  \n",
       "              name  \n",
       "  0            cup  \n",
       "  1            cup  \n",
       "  2       sandwich  \n",
       "  3         person  \n",
       "  4           bowl  \n",
       "  5           fork  \n",
       "  6         person  \n",
       "  7   dining table  \n",
       "  8         person  \n",
       "  9          knife  \n",
       "  10  dining table  \n",
       "  11          fork  ,\n",
       "  'caption': ['A white mug with handle and blue stripes.',\n",
       "   'A white mug with blue stripes set in front of a steak.'],\n",
       "  'bbox_target': [100.74, 31.72, 152.19, 127.54]},\n",
       " 229: {'image_emb': tensor([[-0.4048,  0.5688,  0.0076,  ...,  1.0205,  0.0531,  0.0493],\n",
       "          [-0.2202,  0.5234, -0.2883,  ...,  0.9326,  0.2412, -0.1740],\n",
       "          [-0.3662,  0.5410, -0.1166,  ...,  0.7754, -0.0590,  0.3669],\n",
       "          [-0.5474,  0.3955,  0.0837,  ...,  0.7568,  0.0113,  0.1243]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1705,  0.4128,  0.0249,  ..., -0.1873, -0.4277, -0.2256],\n",
       "          [-0.3262,  0.5117,  0.1693,  ...,  0.1416, -0.2303, -0.0376]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.0903e-02, 8.3447e-07, 9.4531e-01, 3.8033e-03],\n",
       "          [1.5796e-01, 7.0333e-06, 8.4082e-01, 1.3885e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  173.994873  255.809906  413.372986  478.058777    0.903038     45   \n",
       "  1  419.778076  378.608643  638.139526  465.390625    0.849456     44   \n",
       "  2  321.720154    1.936646  639.804321  370.883423    0.718818     45   \n",
       "  3  321.567230    0.000000  639.274536  375.358887    0.623664     47   \n",
       "  4    0.000000    0.000000  290.892151  264.717834    0.587982     41   \n",
       "  5    1.521606    0.263733  637.012695  476.332642    0.529099     60   \n",
       "  \n",
       "             name  \n",
       "  0          bowl  \n",
       "  1         spoon  \n",
       "  2          bowl  \n",
       "  3         apple  \n",
       "  4           cup  \n",
       "  5  dining table  ,\n",
       "  'caption': ['Potatoes in the bowl.',\n",
       "   'A SILVER BOWL WITH CUT UP WHITE POTATOES.'],\n",
       "  'bbox_target': [317.25, 0.0, 322.75, 380.86]},\n",
       " 230: {'image_emb': tensor([[-0.5103,  0.2286,  0.1667,  ...,  0.8218,  0.1377, -0.1570],\n",
       "          [-0.6685,  0.4622,  0.2358,  ...,  0.7690,  0.1449, -0.0979],\n",
       "          [-0.1272,  0.1802, -0.3472,  ...,  0.8604, -0.1970,  0.0750],\n",
       "          [-0.3352,  0.1351, -0.4072,  ...,  1.0127,  0.0676, -0.3965],\n",
       "          [-0.5332,  0.3674,  0.0908,  ...,  0.5444,  0.3274, -0.1210]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0839,  0.0444, -0.5459,  ...,  0.0307, -0.1665,  0.0389],\n",
       "          [ 0.0915,  0.0359,  0.1967,  ...,  0.2402,  0.1283, -0.1321]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0829, 0.3438, 0.0036, 0.1284, 0.4414],\n",
       "          [0.1129, 0.5732, 0.0009, 0.0017, 0.3115]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class    name\n",
       "  0   57.962997   0.247253  320.221191  214.559937    0.935819      0  person\n",
       "  1    1.257233  37.723480  638.727905  420.510132    0.916524     17   horse\n",
       "  2  514.586670   0.171143  580.251465   50.174652    0.845137      0  person\n",
       "  3  245.925995   1.192375  440.696686  114.626007    0.839397      0  person,\n",
       "  'caption': ['Back side portion of two horses',\n",
       "   'the horse that is numbered 6'],\n",
       "  'bbox_target': [353.91, 62.46, 286.09, 198.82]},\n",
       " 231: {'image_emb': tensor([[-0.1160,  0.8599,  0.0757,  ...,  0.7661,  0.1578,  0.3975],\n",
       "          [-0.0609,  0.2773, -0.2040,  ...,  0.9658,  0.1139,  0.1453],\n",
       "          [-0.2006,  0.8452, -0.0217,  ...,  0.8276,  0.5952,  0.1216],\n",
       "          [ 0.1809, -0.0293, -0.2812,  ...,  0.7778, -0.0647,  0.1332],\n",
       "          [-0.1048,  0.0555, -0.1897,  ...,  0.8594, -0.1851, -0.1114],\n",
       "          [ 0.1555,  0.5815,  0.0296,  ...,  0.6406,  0.5151,  0.1350]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0189,  0.2869, -0.1761,  ...,  0.0389,  0.1462,  0.3430],\n",
       "          [-0.2462,  0.0103, -0.2025,  ...,  0.3428,  0.4341,  0.2303]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.4727e-01, 5.0306e-05, 4.5013e-02, 3.4571e-06, 3.2187e-06, 7.6981e-03],\n",
       "          [8.6084e-01, 3.4511e-05, 6.8481e-02, 3.8147e-06, 1.0133e-06, 7.0679e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    43.541561  130.725220  247.897476  324.225983    0.936654     63   \n",
       "  1   273.916687  300.260925  312.441956  340.021423    0.924323     64   \n",
       "  2   248.795227   92.549088  439.820496  252.914047    0.905902     62   \n",
       "  3   467.048279  164.785721  499.726379  255.353455    0.740510     73   \n",
       "  4   451.587433  171.629410  484.176025  256.446716    0.725991     73   \n",
       "  5   431.337799  169.902161  449.866821  257.885254    0.626724     73   \n",
       "  6    46.470623  123.707199   71.304794  253.280502    0.473301     73   \n",
       "  7    79.307449  260.440002  235.674957  293.914459    0.459024     66   \n",
       "  8   441.617859  171.367935  459.365082  257.300415    0.407006     73   \n",
       "  9    11.838765  126.870224   39.481014  257.228882    0.378869     73   \n",
       "  10   26.079777   51.173496   41.267975  111.874481    0.279228     39   \n",
       "  \n",
       "          name  \n",
       "  0     laptop  \n",
       "  1      mouse  \n",
       "  2         tv  \n",
       "  3       book  \n",
       "  4       book  \n",
       "  5       book  \n",
       "  6       book  \n",
       "  7   keyboard  \n",
       "  8       book  \n",
       "  9       book  \n",
       "  10    bottle  ,\n",
       "  'caption': ['a black computer screen',\n",
       "   'Black dell monitor displaying various windows'],\n",
       "  'bbox_target': [85.96, 128.99, 160.95, 124.72]},\n",
       " 232: {'image_emb': tensor([[ 0.1918, -0.1716, -0.1209,  ...,  0.9185,  0.2074, -0.0137],\n",
       "          [ 0.2145, -0.0975, -0.0337,  ...,  0.5630,  0.0137, -0.2856],\n",
       "          [-0.1396, -0.2947, -0.6294,  ..., -0.0590,  0.2898, -0.1556],\n",
       "          ...,\n",
       "          [ 0.1412,  0.0853,  0.1481,  ...,  0.6206,  0.3306, -0.2739],\n",
       "          [-0.3645, -0.0535,  0.0436,  ...,  0.7964, -0.0548, -0.2012],\n",
       "          [ 0.1918, -0.1667, -0.1482,  ..., -0.0240,  0.0164,  0.2458]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3882,  0.2089, -0.0716,  ...,  0.3230, -0.2910,  0.3628],\n",
       "          [ 0.2163,  0.1133, -0.0231,  ...,  0.3599, -0.3748,  0.3774]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0116, 0.0117, 0.1522, 0.3708, 0.0790, 0.0429, 0.3223, 0.0094],\n",
       "          [0.0283, 0.0145, 0.0886, 0.2859, 0.1394, 0.0311, 0.4033, 0.0090]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   187.175995  161.869568  285.237000  406.154907    0.929974      0   \n",
       "  1   200.829376   92.511200  420.271820  346.933075    0.925966      0   \n",
       "  2   457.905090  148.364517  597.912415  414.574829    0.910475      0   \n",
       "  3   599.415833  147.138031  638.885559  301.681488    0.853333      0   \n",
       "  4   170.474792   89.069733  211.792633  105.178925    0.749050     29   \n",
       "  5   565.277954  145.382812  590.206543  175.114197    0.743694      0   \n",
       "  6   565.482239  248.096893  608.853333  309.616241    0.713937     56   \n",
       "  7   125.141151  193.680237  171.063538  234.097595    0.697344      0   \n",
       "  8   572.131531  179.429138  606.153748  268.717834    0.447535      0   \n",
       "  9   539.987549  121.501175  640.000000  162.051056    0.441784      2   \n",
       "  10  330.026154  191.069977  366.600555  266.693634    0.415144      3   \n",
       "  11  259.962158   59.418915  543.395996  139.188293    0.384971     25   \n",
       "  12  382.402313  181.443542  442.893402  270.505371    0.381608      3   \n",
       "  13  428.826630  190.417389  501.923737  265.856842    0.347422      3   \n",
       "  14  399.573425  236.602203  441.242004  283.886200    0.287078     24   \n",
       "  15    0.266239  137.046509   24.104624  147.883331    0.277171      2   \n",
       "  \n",
       "            name  \n",
       "  0       person  \n",
       "  1       person  \n",
       "  2       person  \n",
       "  3       person  \n",
       "  4      frisbee  \n",
       "  5       person  \n",
       "  6        chair  \n",
       "  7       person  \n",
       "  8       person  \n",
       "  9          car  \n",
       "  10  motorcycle  \n",
       "  11    umbrella  \n",
       "  12  motorcycle  \n",
       "  13  motorcycle  \n",
       "  14    backpack  \n",
       "  15         car  ,\n",
       "  'caption': ['A dark blue tent canopy.', 'dark blue tent'],\n",
       "  'bbox_target': [260.91, 62.23, 280.72, 79.51]},\n",
       " 233: {'image_emb': tensor([[ 0.1070,  0.3550, -0.0503,  ...,  0.5493,  0.0967, -0.4165],\n",
       "          [-0.0972,  0.3450, -0.2783,  ...,  0.9736, -0.0666, -0.2085],\n",
       "          [ 0.1425,  0.6821, -0.1089,  ...,  0.6582,  0.0351, -0.0843]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1246,  0.0350, -0.3542,  ..., -0.4868, -0.3201, -0.0540],\n",
       "          [ 0.1837,  0.3831, -0.5581,  ...,  0.0660, -0.0262, -0.1824]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1066, 0.0013, 0.8921],\n",
       "          [0.5239, 0.0061, 0.4697]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   41.944595  124.072403  369.662964  476.084106    0.945705      0   \n",
       "  1  229.497452  341.139069  278.538330  400.740204    0.872915     67   \n",
       "  2  592.625977  172.526733  639.947876  431.487427    0.432544     58   \n",
       "  3  368.910400  400.905853  639.010010  477.551208    0.267259     57   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1    cell phone  \n",
       "  2  potted plant  \n",
       "  3         couch  ,\n",
       "  'caption': ['A woman checking her phone.',\n",
       "   'The woman who is wearing a black shirt and holding her phone.'],\n",
       "  'bbox_target': [42.07, 124.04, 326.83, 350.57]},\n",
       " 234: {'image_emb': tensor([[-0.0934,  0.2522, -0.0226,  ...,  0.8779,  0.5171, -0.4565],\n",
       "          [-0.0367,  0.0629, -0.3066,  ...,  0.9634,  0.1126, -0.5063],\n",
       "          [ 0.3330,  0.6030, -0.0715,  ...,  0.7471, -0.0659,  0.1187],\n",
       "          ...,\n",
       "          [-0.2435, -0.0618,  0.0072,  ...,  1.1250,  0.4397, -0.0865],\n",
       "          [ 0.0390, -0.4910, -0.4529,  ...,  0.6763, -0.0459, -0.2776],\n",
       "          [-0.1440,  0.2275, -0.5098,  ...,  0.8940,  0.3892,  0.0709]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0827,  0.4146, -0.0162,  ..., -0.0687, -0.2242,  0.0477],\n",
       "          [-0.4231, -0.0685, -0.2074,  ..., -0.4663,  0.0667,  0.1407]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.7069e-05, 8.9966e-02, 9.0869e-01, 1.1569e-04, 1.9073e-04, 8.6832e-04,\n",
       "           1.6320e-04],\n",
       "          [3.7098e-03, 5.5029e-01, 3.8428e-01, 3.4294e-03, 2.8275e-02, 1.4221e-02,\n",
       "           1.5617e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     0.993813   57.005692  160.435608  344.967041    0.927667      0   \n",
       "  1   289.189514    6.357605  424.478516  292.205627    0.869671      0   \n",
       "  2   167.674225  357.555359  316.369659  610.269531    0.841078     58   \n",
       "  3   119.555618   76.406952  205.871399  198.469604    0.835133      0   \n",
       "  4    70.648514   67.624870  184.661926  233.276917    0.806834      0   \n",
       "  5   133.770493   62.285519  160.176636   84.524399    0.751936      0   \n",
       "  6   222.057861  242.139420  280.118530  259.184143    0.699826     53   \n",
       "  7   111.604385  238.124710  142.288834  253.026291    0.667972     53   \n",
       "  8   247.407135  160.890594  256.823883  178.088959    0.536926     39   \n",
       "  9   157.633118  228.768585  211.671753  248.226044    0.523682     53   \n",
       "  10  261.775665  157.599396  273.677521  183.718140    0.513013     39   \n",
       "  11  163.631012   79.393463  208.644806  186.510834    0.438204      0   \n",
       "  12  177.779755  285.492645  203.527435  318.172577    0.430279     48   \n",
       "  13  145.397339  291.466370  184.874939  311.464478    0.426823     48   \n",
       "  14  174.724854  210.630173  223.237000  224.741287    0.411611     53   \n",
       "  15  305.454742   12.853790  429.000000  639.414673    0.395298     58   \n",
       "  16  167.631073  279.046692  191.944427  295.107849    0.374633     48   \n",
       "  17  108.354340  187.355499  371.059845  350.295929    0.373370     60   \n",
       "  18  317.748199    1.859100  429.000000  289.902679    0.355034     58   \n",
       "  19  234.426392  162.637024  242.142151  178.726532    0.304510     39   \n",
       "  20  151.360229  268.370026  173.605347  290.915131    0.265046     48   \n",
       "  21  287.906921  160.612030  298.001648  185.368683    0.261314     39   \n",
       "  22  275.923523  160.429901  285.832031  179.487946    0.256584     39   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2   potted plant  \n",
       "  3         person  \n",
       "  4         person  \n",
       "  5         person  \n",
       "  6          pizza  \n",
       "  7          pizza  \n",
       "  8         bottle  \n",
       "  9          pizza  \n",
       "  10        bottle  \n",
       "  11        person  \n",
       "  12      sandwich  \n",
       "  13      sandwich  \n",
       "  14         pizza  \n",
       "  15  potted plant  \n",
       "  16      sandwich  \n",
       "  17  dining table  \n",
       "  18  potted plant  \n",
       "  19        bottle  \n",
       "  20      sandwich  \n",
       "  21        bottle  \n",
       "  22        bottle  ,\n",
       "  'caption': ['Tallest plant',\n",
       "   'The plant behind the ladies back on the right.'],\n",
       "  'bbox_target': [307.09, 8.61, 119.1, 624.22]},\n",
       " 235: {'image_emb': tensor([[-0.0233,  0.1066,  0.0189,  ...,  0.0954,  0.0880,  0.0629],\n",
       "          [-0.0199,  0.2280, -0.0942,  ...,  0.5220,  0.0430,  0.5596],\n",
       "          [ 0.0411,  0.2275, -0.0038,  ...,  0.3794,  0.2874,  0.2615],\n",
       "          ...,\n",
       "          [ 0.2537,  0.0385, -0.1942,  ...,  0.8350,  0.1208, -0.1439],\n",
       "          [-0.0980, -0.0855, -0.0094,  ...,  0.4526,  0.1427,  0.3176],\n",
       "          [ 0.4058,  0.1978, -0.2834,  ...,  0.8911, -0.0897,  0.2642]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1334,  0.0830, -0.0296,  ...,  0.1407, -0.3228,  0.0117],\n",
       "          [-0.2186, -0.0950,  0.0269,  ...,  0.0447,  0.0121,  0.0621]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9707e-01, 1.1027e-04, 1.3506e-04, 1.2887e-04, 1.3828e-05, 1.8358e-03,\n",
       "           5.1260e-05, 2.8610e-04, 4.0591e-05, 3.7885e-04, 1.7619e-04],\n",
       "          [9.9854e-01, 6.3896e-05, 1.6320e-04, 5.5218e-04, 3.4213e-05, 3.7551e-06,\n",
       "           6.7353e-06, 3.6788e-04, 1.1945e-04, 2.5272e-04, 5.9605e-07]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   464.379974   31.078358  588.930664  210.842224    0.939849     56   \n",
       "  1   306.074280  273.922791  406.559387  426.514709    0.917131     41   \n",
       "  2   413.752808  135.017502  476.125763  230.337631    0.904794     41   \n",
       "  3   264.673096   67.440941  308.186096  140.461304    0.896650     41   \n",
       "  4   107.977959  112.078545  166.265274  198.274872    0.865878     41   \n",
       "  5    20.229088   72.020844  587.780029  584.184143    0.854809     60   \n",
       "  6    27.151911  189.725128  179.826385  259.513977    0.802755     45   \n",
       "  7   189.417603   42.647343  208.329712   99.820190    0.797031     39   \n",
       "  8   134.729462   97.493164  245.186508  145.437592    0.792050     55   \n",
       "  9   107.679855  387.932159  319.533875  458.625458    0.744353     43   \n",
       "  10  117.914406   66.586136  158.037369  111.164185    0.570979     75   \n",
       "  11   45.156254  261.151794  209.173172  313.679047    0.569935     42   \n",
       "  12  285.135590  208.962357  466.712708  254.675034    0.568314     44   \n",
       "  13  504.823273  253.569977  587.715149  332.341492    0.557005     42   \n",
       "  14  107.171799  189.027573  218.369415  212.874878    0.419584     44   \n",
       "  15  307.662262  276.914978  471.779083  403.524597    0.418563     42   \n",
       "  16   23.337217  393.876129   89.448486  585.455017    0.259902     56   \n",
       "  \n",
       "              name  \n",
       "  0          chair  \n",
       "  1            cup  \n",
       "  2            cup  \n",
       "  3            cup  \n",
       "  4            cup  \n",
       "  5   dining table  \n",
       "  6           bowl  \n",
       "  7         bottle  \n",
       "  8           cake  \n",
       "  9          knife  \n",
       "  10          vase  \n",
       "  11          fork  \n",
       "  12         spoon  \n",
       "  13          fork  \n",
       "  14         spoon  \n",
       "  15          fork  \n",
       "  16         chair  ,\n",
       "  'caption': ['A brown chair sitting at a table.',\n",
       "   'UPPER PART OF A WOODEN CHAIR'],\n",
       "  'bbox_target': [464.51, 31.01, 126.81, 180.57]},\n",
       " 236: {'image_emb': tensor([[-0.0209,  0.8013, -0.1501,  ...,  1.4180, -0.1610, -0.1418],\n",
       "          [ 0.2472,  0.3970, -0.0179,  ...,  1.2607, -0.1526,  0.0053],\n",
       "          [ 0.3501,  0.5620,  0.1606,  ...,  0.9204,  0.0608, -0.1138]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2949, -0.0691,  0.0400,  ...,  0.2120, -0.0558,  0.0865],\n",
       "          [ 0.1941,  0.0158,  0.3005,  ...,  0.5874, -0.2546, -0.1388]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0144, 0.9077, 0.0781],\n",
       "          [0.0017, 0.2251, 0.7734]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    9.962288   71.176407  207.291016  575.211487    0.947388      0   \n",
       "  1  160.128510  152.784149  258.093781  376.984711    0.767293      2   \n",
       "  2  172.348724  103.536041  424.268616  527.095215    0.691422      7   \n",
       "  3  131.057541  227.446579  152.501007  240.123337    0.482268     67   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1         car  \n",
       "  2       truck  \n",
       "  3  cell phone  ,\n",
       "  'caption': ['a black car',\n",
       "   'rear of minivan, barely seen parked on the other side of the red toyota tacoma truck'],\n",
       "  'bbox_target': [164.32, 157.84, 86.49, 220.54]},\n",
       " 237: {'image_emb': tensor([[-0.1183,  0.4634, -0.1919,  ...,  0.8896,  0.2810, -0.3823],\n",
       "          [-0.2441,  0.1375, -0.3870,  ...,  0.8755,  0.0812, -0.1648],\n",
       "          [-0.2063,  0.3633, -0.2922,  ...,  1.3604,  0.2852, -0.1168],\n",
       "          ...,\n",
       "          [-0.0720,  0.5400,  0.0597,  ...,  1.4053,  0.5542, -0.1742],\n",
       "          [-0.3389,  0.2959, -0.4397,  ...,  1.4561,  0.1875, -0.5776],\n",
       "          [-0.2856,  0.1046, -0.3381,  ...,  1.0020,  0.0897, -0.0548]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1215,  0.4373, -0.1511,  ..., -0.0957,  0.1498,  0.0588],\n",
       "          [-0.1260, -0.0516, -0.1664,  ..., -0.0657, -0.3193,  0.0631]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.5850e-01, 1.0986e-02, 7.5493e-03, 8.3399e-04, 3.4027e-03, 5.6992e-03,\n",
       "           1.3046e-02],\n",
       "          [1.2018e-01, 2.2107e-01, 1.6431e-01, 4.2677e-04, 5.3120e-04, 2.9621e-03,\n",
       "           4.9048e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     0.928406   39.275024  183.100693  303.769409    0.938799      0   \n",
       "  1    60.752884    3.496078  555.273499  392.308167    0.934751      0   \n",
       "  2    84.569496  278.426697  169.044708  424.984924    0.895421     40   \n",
       "  3    29.669472  378.189087   93.689644  423.218079    0.811368     67   \n",
       "  4    92.075020  178.533478  165.409027  262.132660    0.807096     41   \n",
       "  5   393.679626    0.000000  508.952209  175.835754    0.755804      0   \n",
       "  6   483.978668  175.874268  596.525879  350.213196    0.687200     56   \n",
       "  7   173.492050  296.072144  231.789352  344.321472    0.661972     42   \n",
       "  8   114.659836   64.562439  161.374466  144.175262    0.642735      0   \n",
       "  9     0.020922  388.904724   28.760513  425.763733    0.591221     41   \n",
       "  10    0.000000  293.620239  635.184448  423.608093    0.546807     60   \n",
       "  11  428.810638  347.655365  501.573273  397.670380    0.544749     67   \n",
       "  12  164.520538  168.247192  279.500977  263.434692    0.537485     56   \n",
       "  13    0.891247  306.629700   89.474213  348.360352    0.360406     73   \n",
       "  14  237.206650  135.526611  257.267395  147.984711    0.336012     45   \n",
       "  15  511.032867  151.214630  573.748596  176.415863    0.313342     45   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2     wine glass  \n",
       "  3     cell phone  \n",
       "  4            cup  \n",
       "  5         person  \n",
       "  6          chair  \n",
       "  7           fork  \n",
       "  8         person  \n",
       "  9            cup  \n",
       "  10  dining table  \n",
       "  11    cell phone  \n",
       "  12         chair  \n",
       "  13          book  \n",
       "  14          bowl  \n",
       "  15          bowl  ,\n",
       "  'caption': ['man wearing a black shirt',\n",
       "   'A man in a black shirt staring at the plate of food.'],\n",
       "  'bbox_target': [0.0, 37.42, 172.72, 236.05]},\n",
       " 238: {'image_emb': tensor([[ 0.1033,  0.5645, -0.2864,  ...,  0.8989, -0.1477,  0.2026],\n",
       "          [ 0.0722,  0.1799, -0.3247,  ...,  1.3584,  0.0601, -0.0714],\n",
       "          [-0.1710, -0.1453, -0.1963,  ...,  0.7842, -0.1952, -0.0665],\n",
       "          [ 0.4817, -0.0754, -0.2297,  ...,  0.7422, -0.3677,  0.1564]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2374, -0.0945, -0.1820,  ..., -0.0944,  0.0963, -0.3914],\n",
       "          [-0.0340,  0.1895, -0.3611,  ..., -0.5127, -0.1653, -0.2161]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.7803e-01, 7.7486e-07, 1.1921e-07, 2.1942e-02],\n",
       "          [9.9219e-01, 5.3644e-07, 1.1921e-07, 7.5760e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class     name\n",
       "  0  105.840393  83.933334  253.874374  246.301193    0.947952      0   person\n",
       "  1  381.045227  12.392360  500.000000  329.399384    0.945988      0   person\n",
       "  2   35.168411  46.531574   76.843712   65.764183    0.895039     29  frisbee,\n",
       "  'caption': ['A man wearing red shorts playing frisbee.',\n",
       "   'a man wearing red and white trunks jumping into the air'],\n",
       "  'bbox_target': [106.12, 84.16, 148.65, 161.7]},\n",
       " 239: {'image_emb': tensor([[ 0.1450,  0.4998,  0.2507,  ...,  0.8027, -0.0392, -0.5771],\n",
       "          [-0.1115,  0.2194,  0.0019,  ...,  1.4521,  0.1560, -0.0416],\n",
       "          [ 0.2374,  0.5083, -0.0382,  ...,  0.9229, -0.0888, -0.3689],\n",
       "          [ 0.3484,  0.3391,  0.1847,  ...,  0.8223,  0.2949, -0.4993],\n",
       "          [ 0.0213, -0.0338, -0.0426,  ...,  1.1826, -0.3430,  0.3074],\n",
       "          [ 0.0260,  0.1910,  0.2678,  ...,  1.0156,  0.3401, -0.2162]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2683,  0.5684, -0.3655,  ..., -0.2676,  0.2063, -0.2822],\n",
       "          [ 0.2900,  0.5610, -0.2107,  ..., -0.2551,  0.0887, -0.3203]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.4124e-01, 8.1299e-01, 2.0996e-02, 1.6098e-02, 2.0385e-05, 8.4839e-03],\n",
       "          [2.7271e-01, 6.5430e-01, 3.6346e-02, 1.8265e-02, 1.7285e-06, 1.8555e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0   58.048065   10.849876  340.901031  141.116058    0.933305     25  umbrella\n",
       "  1   38.016205  132.666840  338.453583  432.482788    0.908409      0    person\n",
       "  2  302.932129  119.610886  586.732422  236.891479    0.907674     25  umbrella\n",
       "  3  331.829254  256.821411  618.665771  435.009460    0.906788     25  umbrella\n",
       "  4  511.696747  203.548462  573.312561  279.178406    0.727314      0    person\n",
       "  5  451.205109  368.720886  488.061310  426.485168    0.698045     41       cup\n",
       "  6  391.891998  213.564972  535.127625  284.341034    0.581827      0    person\n",
       "  7  330.934814  259.101501  612.524963  437.869202    0.507724      0    person\n",
       "  8  382.464478  289.706146  540.955872  431.860291    0.341897      0    person,\n",
       "  'caption': ['A man wearing red T shirt holding up an umbrella',\n",
       "   'A man wearing red T-shirt with a colorful umbrella.'],\n",
       "  'bbox_target': [40.26, 132.19, 300.52, 305.81]},\n",
       " 240: {'image_emb': tensor([[ 0.0440,  0.2317, -0.0623,  ...,  1.1592,  0.4448,  0.2642],\n",
       "          [ 0.1011,  0.3374, -0.2100,  ...,  1.0508,  0.2382,  0.2075],\n",
       "          [-0.1622,  0.0694, -0.1936,  ...,  1.3057,  0.1406,  0.0254],\n",
       "          ...,\n",
       "          [-0.0416,  0.4124, -0.1018,  ...,  0.4578,  0.4429,  0.5811],\n",
       "          [-0.1265,  0.2429, -0.2798,  ...,  1.2891,  0.0175, -0.1654],\n",
       "          [-0.2080,  0.4167, -0.1409,  ...,  0.7153,  0.2759,  0.3960]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1545, -0.1888,  0.0370,  ...,  0.0908, -0.2852, -0.2479],\n",
       "          [-0.2321,  0.2181, -0.0634,  ...,  0.5854,  0.1403, -0.2808]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.1387e-01, 2.8107e-02, 2.7132e-04, 1.0080e-03, 2.4670e-01, 1.3151e-03,\n",
       "           2.2149e-04, 5.7106e-03, 8.3590e-04, 2.1343e-03],\n",
       "          [5.1807e-01, 5.2414e-03, 2.6822e-06, 7.2479e-05, 4.7192e-01, 3.3200e-05,\n",
       "           2.0862e-06, 3.4924e-03, 1.1623e-05, 1.1158e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   259.836121  185.355621  339.971558  391.891205    0.924389     40   \n",
       "  1   109.413803  172.671371  244.009766  555.803223    0.914474     40   \n",
       "  2     0.000000  115.178185   57.339722  311.142029    0.858030      0   \n",
       "  3   239.220459  153.483749  292.967041  244.022842    0.854527      0   \n",
       "  4   300.966583  300.562012  366.592743  373.740112    0.827073     41   \n",
       "  5   375.995056  220.670502  426.016418  326.094025    0.773920     56   \n",
       "  6    27.351669  209.814240   69.603378  314.236237    0.764347     56   \n",
       "  7     1.514389  310.048706  425.212646  638.331238    0.739805     60   \n",
       "  8    65.101501  208.244461  120.314362  294.789490    0.720360     56   \n",
       "  9   153.560791  145.391663  213.434692  239.192200    0.673343      0   \n",
       "  10  224.878662  239.631546  283.993164  365.139191    0.430110     41   \n",
       "  11  224.144409  238.101929  284.315125  366.140686    0.420424     40   \n",
       "  12   67.744766  292.640717  123.836746  358.732208    0.304217     41   \n",
       "  \n",
       "              name  \n",
       "  0     wine glass  \n",
       "  1     wine glass  \n",
       "  2         person  \n",
       "  3         person  \n",
       "  4            cup  \n",
       "  5          chair  \n",
       "  6          chair  \n",
       "  7   dining table  \n",
       "  8          chair  \n",
       "  9         person  \n",
       "  10           cup  \n",
       "  11    wine glass  \n",
       "  12           cup  ,\n",
       "  'caption': ['A white wine', 'clear wine glass with white wine'],\n",
       "  'bbox_target': [260.55, 187.55, 78.47, 205.68]},\n",
       " 241: {'image_emb': tensor([[ 0.1700,  1.0146, -0.2864,  ...,  0.5649, -0.2313, -0.0856],\n",
       "          [-0.1339,  1.0615, -0.3762,  ...,  0.4766, -0.2871, -0.2861],\n",
       "          [ 0.1433,  1.0342, -0.1182,  ...,  0.9062, -0.2340, -0.0952],\n",
       "          [ 0.0544,  0.7754, -0.2729,  ...,  0.4546, -0.0230, -0.0122],\n",
       "          [ 0.0701,  0.9087, -0.1262,  ...,  0.5430,  0.0807, -0.1997]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1107,  0.2395, -0.7378,  ...,  0.0977, -0.0837, -0.4277],\n",
       "          [-0.0926,  0.2129, -0.6899,  ..., -0.0021, -0.2854, -0.2170]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0013, 0.0068, 0.0011, 0.0038, 0.9868],\n",
       "          [0.0045, 0.0290, 0.0036, 0.0160, 0.9468]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class  \\\n",
       "  0   25.144905  32.871639   85.284843  169.278931    0.932281     67   \n",
       "  1  317.161469  62.278721  379.785797  178.271988    0.930094     67   \n",
       "  2  202.339661  15.740480  288.384308  165.702026    0.927083     67   \n",
       "  3  114.665009  36.331444  170.393005  154.995850    0.919110     67   \n",
       "  \n",
       "           name  \n",
       "  0  cell phone  \n",
       "  1  cell phone  \n",
       "  2  cell phone  \n",
       "  3  cell phone  ,\n",
       "  'caption': ['the second phone on the left. The flip phone',\n",
       "   'The phone that is second from the left is the only flip phone pictured'],\n",
       "  'bbox_target': [113.5, 38.0, 58.0, 118.5]},\n",
       " 242: {'image_emb': tensor([[-1.7004e-01,  2.7271e-01,  3.7109e-01,  ...,  6.5869e-01,\n",
       "           -1.9241e-02,  3.5327e-01],\n",
       "          [-2.3193e-01,  5.4102e-01, -5.7831e-02,  ...,  8.7207e-01,\n",
       "           -7.5012e-02,  3.9429e-02],\n",
       "          [-1.0405e-03,  4.3384e-01, -1.9690e-01,  ...,  7.8320e-01,\n",
       "           -3.6816e-01,  1.9971e-01],\n",
       "          [ 2.7115e-02,  5.1953e-01, -2.9565e-01,  ...,  8.1348e-01,\n",
       "           -3.3276e-01,  2.8296e-01],\n",
       "          [-1.7322e-01, -5.7251e-02, -1.6602e-01,  ...,  1.1113e+00,\n",
       "           -2.5146e-01,  9.4116e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1810,  0.0925, -0.3552,  ...,  0.1072, -0.4258, -0.1462],\n",
       "          [-0.3062, -0.2214, -0.3789,  ...,  0.2866,  0.0585,  0.2778]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7285e-06, 1.4305e-04, 8.4814e-01, 1.4282e-01, 8.7128e-03],\n",
       "          [3.0994e-06, 3.3593e-04, 7.4414e-01, 2.4915e-01, 6.5384e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  449.780029   48.115875  579.695801  262.906097    0.931987      0    person\n",
       "  1  396.519470  124.047913  584.059387  323.294373    0.917053     28  suitcase\n",
       "  2    2.037277  229.625702  475.638611  423.534119    0.898152     28  suitcase\n",
       "  3  232.484787  235.524902  431.426453  402.219727    0.880180      0    person\n",
       "  4   49.970310  338.328522   59.086437  356.242462    0.281736      0    person\n",
       "  5   10.885910  229.047272  468.678284  419.630859    0.258709      0    person,\n",
       "  'caption': ['The suitcase that the child is lying down in',\n",
       "   'A kid in striped pajamas in a suitcase.'],\n",
       "  'bbox_target': [4.79, 228.5, 470.99, 190.51]},\n",
       " 243: {'image_emb': tensor([[-0.2595,  0.2864,  0.1425,  ...,  1.0117, -0.3250, -0.0419],\n",
       "          [-0.3901,  0.5112,  0.1219,  ...,  1.1934, -0.3818, -0.0817],\n",
       "          [ 0.1570, -0.1920, -0.1586,  ...,  1.2490, -0.1289, -0.1129],\n",
       "          [-0.3345,  0.4246,  0.1791,  ...,  0.6899, -0.2109, -0.0063]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3865, -0.0697,  0.1315,  ..., -0.1055, -0.3521, -0.1685],\n",
       "          [-0.4692,  0.0345, -0.0998,  ..., -0.4753, -0.2333, -0.4019]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.1758e-01, 3.2910e-01, 4.8220e-05, 1.5308e-01],\n",
       "          [4.3213e-01, 2.6611e-01, 6.4313e-05, 3.0176e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0   97.649902  210.771667  638.559998  473.923645    0.825415     59    bed\n",
       "  1    0.793900  157.240326  342.037415  408.350861    0.824561     59    bed\n",
       "  2  343.497253  212.044617  385.943176  234.143188    0.780762     74  clock,\n",
       "  'caption': ['bed furthest away from camera.',\n",
       "   'bed in the left most side of the image'],\n",
       "  'bbox_target': [5.38, 156.0, 335.79, 250.76]},\n",
       " 244: {'image_emb': tensor([[ 3.6890e-01,  1.8286e-01, -3.8159e-01,  ...,  3.4985e-01,\n",
       "            4.3433e-01, -2.0020e-01],\n",
       "          [-5.6267e-03,  4.6094e-01, -1.5356e-01,  ...,  6.7773e-01,\n",
       "            5.4053e-01,  1.8262e-01],\n",
       "          [-2.5582e-04,  6.1493e-02, -4.7363e-01,  ...,  1.3066e+00,\n",
       "            1.5564e-01, -8.6260e-04],\n",
       "          ...,\n",
       "          [-1.5088e-01,  3.0225e-01, -2.1985e-01,  ...,  1.0254e+00,\n",
       "           -4.5471e-02, -4.4037e-02],\n",
       "          [ 2.4121e-01,  9.5825e-02, -2.4280e-01,  ...,  8.8623e-01,\n",
       "           -2.0889e-02, -1.1078e-01],\n",
       "          [-4.4861e-02,  2.5366e-01, -3.4570e-01,  ...,  4.4165e-01,\n",
       "            4.8364e-01,  1.4587e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0152,  0.1130, -0.0959,  ..., -0.0611, -0.0255, -0.2686],\n",
       "          [ 0.0278, -0.0911, -0.2922,  ..., -0.1231,  0.2017,  0.3599]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.0928e-01, 1.6057e-04, 1.6093e-06, 1.1921e-06, 7.4959e-03, 4.3373e-03,\n",
       "           1.7881e-07, 0.0000e+00, 2.2054e-06, 4.7852e-01],\n",
       "          [3.8879e-02, 2.6226e-06, 1.6556e-03, 2.7013e-04, 2.6875e-03, 8.1836e-01,\n",
       "           2.2399e-04, 9.8348e-06, 2.4223e-04, 1.3782e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   148.618835   23.522293  470.447327  476.437134    0.962596      0   \n",
       "  1   232.924927  297.477661  304.631836  433.589233    0.917264     41   \n",
       "  2   567.033020   40.956459  639.867859  245.890045    0.881003      0   \n",
       "  3    65.118980  312.582001   94.594460  377.890045    0.861102     40   \n",
       "  4     0.000000  183.445862  133.658722  337.433777    0.858025      0   \n",
       "  5    61.524597  179.645264  229.890594  475.596191    0.854710      0   \n",
       "  6    26.565765  300.957520   59.425743  363.867798    0.819469     40   \n",
       "  7   519.057800  314.229340  546.835388  359.855194    0.737071     43   \n",
       "  8     8.853848  321.708221   43.706734  395.919586    0.724911     40   \n",
       "  9     1.390934  336.166229  119.587402  454.565186    0.679688     60   \n",
       "  10  451.495056  225.450409  493.455322  305.872772    0.621184     41   \n",
       "  11  322.056458  168.475571  372.364929  196.591217    0.529796     54   \n",
       "  12  579.052734  290.771576  639.510986  477.653931    0.528867      0   \n",
       "  13   22.458073  142.748428   63.481899  193.135376    0.524033     73   \n",
       "  14  445.575470  231.301514  639.478394  433.067444    0.491147     60   \n",
       "  15  433.546600  391.708069  610.602661  478.978455    0.461738     56   \n",
       "  16  421.099304  116.015488  436.339600  167.901749    0.400069     39   \n",
       "  17  471.449036   99.164581  482.781494  130.635406    0.348028     39   \n",
       "  18  504.515320   96.765381  515.301208  134.949005    0.327890     39   \n",
       "  19  550.884521  139.649628  562.791748  162.429260    0.318611     41   \n",
       "  20  459.598297  103.247833  471.990936  137.507172    0.302649     39   \n",
       "  21  410.636169  108.526947  424.604248  167.137817    0.302616     39   \n",
       "  22  494.472321   96.845200  505.318634  132.429337    0.298887     39   \n",
       "  23  559.093933  140.656570  572.272400  162.707657    0.297379     41   \n",
       "  24  483.258453   97.512802  495.632843  131.113724    0.286354     39   \n",
       "  25  515.893860  102.274246  527.521545  142.427261    0.285171     39   \n",
       "  26  470.551117  140.942276  485.261322  169.594467    0.276717     41   \n",
       "  27  562.913879  141.686752  580.850403  163.163956    0.262396     41   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1            cup  \n",
       "  2         person  \n",
       "  3     wine glass  \n",
       "  4         person  \n",
       "  5         person  \n",
       "  6     wine glass  \n",
       "  7          knife  \n",
       "  8     wine glass  \n",
       "  9   dining table  \n",
       "  10           cup  \n",
       "  11         donut  \n",
       "  12        person  \n",
       "  13          book  \n",
       "  14  dining table  \n",
       "  15         chair  \n",
       "  16        bottle  \n",
       "  17        bottle  \n",
       "  18        bottle  \n",
       "  19           cup  \n",
       "  20        bottle  \n",
       "  21        bottle  \n",
       "  22        bottle  \n",
       "  23           cup  \n",
       "  24        bottle  \n",
       "  25        bottle  \n",
       "  26           cup  \n",
       "  27           cup  ,\n",
       "  'caption': ['A woman holding a beer eating.',\n",
       "   'The woman with the pink shirt is eating.'],\n",
       "  'bbox_target': [149.93, 22.23, 321.63, 457.77]},\n",
       " 245: {'image_emb': tensor([[ 0.0575,  0.3274, -0.1697,  ...,  0.0744,  0.0049, -0.3936],\n",
       "          [ 0.1007,  0.4329, -0.2102,  ...,  0.9380,  0.2317, -0.1802],\n",
       "          [-0.3655,  0.4565, -0.0233,  ...,  0.4436, -0.0886,  0.1143],\n",
       "          ...,\n",
       "          [-0.0743,  0.2412, -0.1528,  ...,  0.8594, -0.0282, -0.4536],\n",
       "          [-0.3381,  0.4480,  0.0533,  ...,  0.2961, -0.0401,  0.0883],\n",
       "          [-0.3135, -0.0543,  0.0201,  ..., -0.2275, -0.1331, -0.2361]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0170,  0.0974, -0.0675,  ..., -0.1438,  0.0307, -0.2266],\n",
       "          [-0.1504,  0.0724, -0.0927,  ..., -0.3940, -0.1316, -0.0871]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.2139e-01, 1.2106e-04, 2.0035e-02, 1.3506e-04, 1.3936e-04, 1.2939e-02,\n",
       "           4.5166e-02],\n",
       "          [4.8804e-01, 5.6624e-06, 4.7577e-02, 6.1798e-04, 5.2834e-04, 3.9819e-01,\n",
       "           6.5002e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  281.790802  176.766327  614.200684  474.951538    0.929026     13   \n",
       "  1  333.119751  171.886993  390.767212  257.113983    0.904808     14   \n",
       "  2  181.341919  139.995407  314.443542  402.109497    0.894649     13   \n",
       "  3  340.208710    1.008560  418.151764  191.983887    0.868114     58   \n",
       "  4  290.400970    3.461655  334.395172  166.924454    0.840689     58   \n",
       "  5  111.809738  104.079330  216.343979  282.795166    0.797974     13   \n",
       "  6   96.257141    0.167732  224.179779  131.487671    0.422379     58   \n",
       "  7  128.348816  402.820465  277.576721  477.770874    0.405678     73   \n",
       "  8  128.614441  402.714722  278.474152  476.745483    0.297639     13   \n",
       "  \n",
       "             name  \n",
       "  0         bench  \n",
       "  1          bird  \n",
       "  2         bench  \n",
       "  3  potted plant  \n",
       "  4  potted plant  \n",
       "  5         bench  \n",
       "  6  potted plant  \n",
       "  7          book  \n",
       "  8         bench  ,\n",
       "  'caption': ['The bench next to the bench with the bird on it.',\n",
       "   'A chair placed in second of other chairs'],\n",
       "  'bbox_target': [181.76, 140.07, 127.55, 264.39]},\n",
       " 246: {'image_emb': tensor([[-0.2512,  0.4580,  0.1246,  ...,  0.3408,  0.2239, -0.0630],\n",
       "          [-0.3379,  0.7119, -0.1231,  ...,  0.6606,  0.1937, -0.0765],\n",
       "          [-0.5098,  0.0733,  0.2428,  ...,  0.0408,  0.3066,  0.1379]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3228,  0.2401, -0.4114,  ..., -0.5073,  0.0507, -0.4707],\n",
       "          [ 0.2966,  0.2522, -0.3213,  ..., -0.3840,  0.1736, -0.6479]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.9004, 0.0135, 0.0864],\n",
       "          [0.8135, 0.0618, 0.1248]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  217.563400  112.101624  488.691406  514.035889    0.930831      0   \n",
       "  1  273.524109  277.165924  452.584900  593.174316    0.870836      0   \n",
       "  2  339.694214  493.173645  502.425903  535.557800    0.301070     31   \n",
       "  3   48.891357  483.128357  480.047974  537.295715    0.290563     30   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2  snowboard  \n",
       "  3       skis  ,\n",
       "  'caption': ['person in green with another person on their shoulders',\n",
       "   'thep person that has other person on their shoulders'],\n",
       "  'bbox_target': [279.6, 282.56, 178.69, 311.12]},\n",
       " 247: {'image_emb': tensor([[-0.3267, -0.5957, -0.0550,  ...,  0.4780,  0.2988,  0.3269],\n",
       "          [-0.1306, -0.3770, -0.1577,  ...,  0.5991,  0.1018,  0.1760],\n",
       "          [-0.3059, -0.5474, -0.1354,  ...,  0.4451,  0.1780,  0.1298]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 2.5293e-01, -2.1655e-01, -2.5977e-01,  ...,  5.8301e-01,\n",
       "           -2.8564e-01, -3.2739e-01],\n",
       "          [ 4.5776e-05, -1.2311e-01, -3.1079e-01,  ...,  3.5217e-02,\n",
       "           -9.9792e-02, -4.9438e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3352, 0.6460, 0.0189],\n",
       "          [0.1367, 0.4148, 0.4485]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class     name\n",
       "  0  369.995087  76.206299  584.807739  423.375732    0.936259     23  giraffe\n",
       "  1    0.281013  60.708191  193.182648  422.850159    0.931744     23  giraffe,\n",
       "  'caption': ['Giraffe with a darker colored neck.',\n",
       "   'the giraffe head is next to the rock on the right.'],\n",
       "  'bbox_target': [371.35, 79.64, 214.94, 342.56]},\n",
       " 248: {'image_emb': tensor([[-0.1721,  0.2512, -0.3499,  ...,  0.3755,  0.2214, -0.1209],\n",
       "          [-0.1030,  0.0402, -0.1815,  ...,  0.4800,  0.0919,  0.0075],\n",
       "          [ 0.4377, -0.2927, -0.1135,  ...,  0.3083,  0.1322,  0.4717],\n",
       "          [-0.4644, -0.0363, -0.1606,  ...,  0.6270,  0.1577, -0.2130],\n",
       "          [ 0.4819, -0.3491, -0.3237,  ...,  0.3042,  0.1486,  0.3374]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0663,  0.0033, -0.3228,  ...,  0.3174, -0.2559, -0.0369],\n",
       "          [ 0.0590,  0.1785, -0.4221,  ..., -0.1578,  0.0715, -0.2915]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.7812e-01, 1.9983e-01, 1.9983e-01, 1.9293e-03, 2.0096e-02],\n",
       "          [6.1377e-01, 3.7231e-01, 1.2352e-02, 2.6870e-04, 1.1663e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  181.334351  186.559174  327.770203  577.030396    0.919077      0  person\n",
       "  1  224.400391  254.905151  327.901123  405.691223    0.903877     16     dog\n",
       "  2  113.668518   96.452499  478.000000  439.812317    0.895112      7   truck\n",
       "  3  143.051605  385.814972  259.575836  575.361755    0.890919     16     dog\n",
       "  4    0.275146  173.470871   34.466492  276.033600    0.534093      7   truck,\n",
       "  'caption': ['A white dog sitting on the lap of a woman dressed in blue.',\n",
       "   'A white dog being held by a woman in grey'],\n",
       "  'bbox_target': [223.63, 254.98, 107.9, 152.92]},\n",
       " 249: {'image_emb': tensor([[ 0.1847, -0.3025,  0.0553,  ...,  0.1453, -0.1707, -0.2617],\n",
       "          [ 0.2959, -0.1609, -0.2329,  ...,  0.3721, -0.1746,  0.0242],\n",
       "          [ 0.0408, -0.2260, -0.0219,  ...,  0.0236, -0.2712, -0.2788]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3328, -0.1229, -0.2419,  ...,  0.0367, -0.1002, -0.3564],\n",
       "          [-0.1525, -0.2365, -0.3564,  ..., -0.1416, -0.1428, -0.3572]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3606, 0.0721, 0.5674],\n",
       "          [0.2712, 0.1079, 0.6211]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  114.149399   59.268570  427.596680  423.010742    0.943224     23  giraffe\n",
       "  1  204.256683  290.665894  367.264618  450.488281    0.800306     23  giraffe,\n",
       "  'caption': [\"The young giraffe behind it's mom and sibling.\",\n",
       "   'A baby giraffe standing behind an adult giraffe.'],\n",
       "  'bbox_target': [220.13, 276.87, 137.1, 147.42]},\n",
       " 250: {'image_emb': tensor([[-0.3770,  0.5249,  0.1055,  ...,  0.7705,  0.5396,  0.0249],\n",
       "          [ 0.4641,  0.5918, -0.0825,  ...,  0.9189, -0.2566,  0.2321],\n",
       "          [ 0.2010,  0.3616, -0.2002,  ...,  0.9458,  0.0505,  0.1691],\n",
       "          [ 0.6260,  0.2297, -0.0301,  ...,  0.5459,  0.1788,  0.2363],\n",
       "          [-0.2800,  0.4236, -0.2347,  ...,  1.0439, -0.2029, -0.1313],\n",
       "          [ 0.6533,  0.1262,  0.1035,  ...,  0.5938,  0.2032,  0.1810]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1392, -0.1836,  0.3135,  ...,  0.2479, -0.2812, -0.0399],\n",
       "          [ 0.2039,  0.1344, -0.0917,  ..., -0.2159, -0.0754, -0.0677]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7776e-03, 8.7509e-03, 4.6570e-02, 3.6621e-01, 3.6693e-04, 5.7617e-01],\n",
       "          [9.4652e-05, 7.4005e-03, 3.8788e-02, 5.0293e-01, 4.6849e-05, 4.5068e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  435.490173   24.746231  640.000000  316.219055    0.928643     41   \n",
       "  1   39.616631  137.902985  212.232697  314.843597    0.833540     48   \n",
       "  2  192.447601  126.734055  380.657440  288.638000    0.793011     48   \n",
       "  3    7.238373    2.050415  634.232178  471.058228    0.770030     60   \n",
       "  4  347.261566  314.806274  624.165649  471.026794    0.759629     45   \n",
       "  5  353.792542    0.000000  399.812622   24.140289    0.618783     42   \n",
       "  6  583.002869    0.004364  639.890808   61.047897    0.617714     39   \n",
       "  \n",
       "             name  \n",
       "  0           cup  \n",
       "  1      sandwich  \n",
       "  2      sandwich  \n",
       "  3  dining table  \n",
       "  4          bowl  \n",
       "  5          fork  \n",
       "  6        bottle  ,\n",
       "  'caption': ['Half of a sandwhich closest to the beer.',\n",
       "   'Half of a sandwich, nearer to the drink than the other half'],\n",
       "  'bbox_target': [192.32, 127.66, 190.99, 162.77]},\n",
       " 251: {'image_emb': tensor([[ 0.0438, -0.2223, -0.2510,  ...,  0.7822,  0.4194, -0.0034],\n",
       "          [ 0.2015,  0.1168, -0.2949,  ...,  0.7192,  0.1097, -0.0385],\n",
       "          [-0.2651,  0.0300, -0.4072,  ...,  0.3462,  0.1771, -0.1508],\n",
       "          ...,\n",
       "          [ 0.1400,  0.2336, -0.2036,  ...,  0.9434,  0.1617, -0.0480],\n",
       "          [-0.2307,  0.0603,  0.0077,  ...,  0.9883,  0.3142,  0.3049],\n",
       "          [-0.4414,  0.2517,  0.0166,  ..., -0.1842,  0.1660,  0.3982]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2681, -0.2009,  0.1300,  ..., -0.0290,  0.3594, -0.0693],\n",
       "          [-0.0265,  0.0125,  0.0264,  ..., -0.0837, -0.0283, -0.0762]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.5068e-04, 8.9455e-04, 9.5068e-01, 1.7685e-02, 1.1981e-05, 9.0332e-03,\n",
       "           1.0786e-03, 2.0355e-02],\n",
       "          [3.5763e-07, 1.1176e-04, 2.9278e-03, 2.1420e-03, 3.1352e-04, 2.6226e-06,\n",
       "           1.6093e-06, 9.9463e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   195.598526  171.777344  232.310776  208.546265    0.917446     29   \n",
       "  1    90.948402   99.682487  153.809875  355.811249    0.881354      0   \n",
       "  2   148.126770  119.338753  211.659485  347.218506    0.870681      0   \n",
       "  3   206.699020  101.991035  286.158203  351.162720    0.846217      0   \n",
       "  4   270.300232  108.549355  364.461426  353.055511    0.830590      0   \n",
       "  5   282.816711  161.322647  318.956635  190.189224    0.829711     29   \n",
       "  6   150.592682  205.957108  188.150238  232.319901    0.792987     29   \n",
       "  7   316.250244  215.607269  377.876312  246.748734    0.415008     24   \n",
       "  8   268.766632  113.013611  362.282806  221.558426    0.410460      0   \n",
       "  9     3.940177  240.095474  498.760986  369.705627    0.290774     60   \n",
       "  10  278.342072  149.223618  331.970673  200.469116    0.255705      0   \n",
       "  \n",
       "              name  \n",
       "  0        frisbee  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3         person  \n",
       "  4         person  \n",
       "  5        frisbee  \n",
       "  6        frisbee  \n",
       "  7       backpack  \n",
       "  8         person  \n",
       "  9   dining table  \n",
       "  10        person  ,\n",
       "  'caption': ['a man holding a pink frisbee',\n",
       "   'male, second from left, wearing gray sweater and holding red frisbee'],\n",
       "  'bbox_target': [146.96, 119.51, 62.5, 226.35]},\n",
       " 252: {'image_emb': tensor([[-0.0103,  0.5923, -0.1192,  ...,  0.7344, -0.0183,  0.2202],\n",
       "          [-0.3137,  0.5176, -0.1549,  ...,  0.9385,  0.2019,  0.0227],\n",
       "          [ 0.2250,  0.4526, -0.3057,  ...,  0.7715, -0.0856,  0.0903],\n",
       "          [-0.0347,  0.5181, -0.0943,  ...,  0.7896,  0.1218,  0.3306]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0439,  0.3865, -0.0836,  ..., -0.1208,  0.0284, -0.1171],\n",
       "          [-0.2834,  0.3696, -0.3333,  ..., -0.3562,  0.0892, -0.0983]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0140, 0.0032, 0.7417, 0.2408],\n",
       "          [0.1378, 0.1040, 0.6572, 0.1008]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  253.184448  167.824890  561.552979  474.827942    0.925414     47   apple\n",
       "  1  323.605621    0.444572  570.292847  209.716492    0.906602     47   apple\n",
       "  2    0.108826    0.717178  323.743774  469.796265    0.901216     46  banana,\n",
       "  'caption': ['An apple behind another apple to the right of some bananas',\n",
       "   'the apple in the back in the right hand picture'],\n",
       "  'bbox_target': [328.66, 3.39, 243.95, 195.39]},\n",
       " 253: {'image_emb': tensor([[ 9.5886e-02,  2.8418e-01,  1.2482e-01,  ...,  1.0586e+00,\n",
       "           -4.1382e-02,  1.8787e-01],\n",
       "          [ 4.3640e-03,  2.0581e-01, -4.2554e-01,  ...,  1.3867e+00,\n",
       "           -1.2195e-01, -1.2347e-01],\n",
       "          [ 1.3046e-03,  2.4023e-01, -1.0919e-01,  ...,  1.1846e+00,\n",
       "            3.1934e-01, -1.5234e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3167,  0.0581,  0.5527,  ...,  0.3303, -0.0762,  0.0388],\n",
       "          [ 0.0138, -0.0780, -0.2693,  ..., -0.2423,  0.3713, -0.2200]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.9666e-01, 6.5613e-04, 8.0273e-01],\n",
       "          [2.1716e-01, 3.6591e-02, 7.4609e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class  \\\n",
       "  0   80.680481  31.077450  433.706726  327.336060    0.916420     12   \n",
       "  1    0.397947  47.626789   79.630638  326.804779    0.875796     12   \n",
       "  2  375.396454  38.699608  499.391541  321.366608    0.492253      2   \n",
       "  \n",
       "              name  \n",
       "  0  parking meter  \n",
       "  1  parking meter  \n",
       "  2            car  ,\n",
       "  'caption': ['A parking meter reading 01:13.', 'Time meter that reads 1:13'],\n",
       "  'bbox_target': [80.07, 32.05, 353.95, 297.08]},\n",
       " 254: {'image_emb': tensor([[-0.2394,  0.3386, -0.1006,  ...,  0.9438, -0.0163,  0.1907],\n",
       "          [ 0.0152,  0.2468, -0.4128,  ...,  0.7275, -0.0202, -0.0333],\n",
       "          [-0.1318,  0.6255, -0.1833,  ...,  0.7139, -0.3828,  0.2441],\n",
       "          [-0.1682,  0.6929, -0.1857,  ...,  0.6445, -0.3223,  0.3235]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.4917,  0.1295, -0.1722,  ..., -0.4792, -0.1049, -0.2698],\n",
       "          [ 0.0305,  0.0325, -0.1212,  ...,  0.3044, -0.6279, -0.1881]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0702, 0.0189, 0.3677, 0.5435],\n",
       "          [0.4080, 0.2512, 0.2554, 0.0855]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  506.619202  205.473236  599.289368  312.857391    0.820411     50   \n",
       "  1  210.091232   32.870171  349.138733  132.949890    0.763474     50   \n",
       "  2   23.892151    1.210144  629.938354  447.476501    0.761522     45   \n",
       "  3  441.200256  163.123245  556.774109  243.918701    0.680939     50   \n",
       "  4  366.233643  310.171997  497.341492  391.528015    0.629870     50   \n",
       "  5   74.513268  224.042877  177.936554  340.160858    0.601512     50   \n",
       "  6    0.514444  288.160797   60.415672  452.690186    0.495456     43   \n",
       "  7  132.324310  260.106567  204.957153  337.570251    0.373766     50   \n",
       "  8    9.848999    2.280991  633.105774  454.205750    0.350046     60   \n",
       "  9  355.030762  101.678314  451.816956  203.457825    0.320010     50   \n",
       "  \n",
       "             name  \n",
       "  0      broccoli  \n",
       "  1      broccoli  \n",
       "  2          bowl  \n",
       "  3      broccoli  \n",
       "  4      broccoli  \n",
       "  5      broccoli  \n",
       "  6         knife  \n",
       "  7      broccoli  \n",
       "  8  dining table  \n",
       "  9      broccoli  ,\n",
       "  'caption': ['The visible table to the right of the bowl',\n",
       "   'A table with broccoli curry'],\n",
       "  'bbox_target': [386.12, 0.0, 253.88, 448.89]},\n",
       " 255: {'image_emb': tensor([[-0.0592,  0.3799, -0.2449,  ...,  0.6992,  0.1548,  0.1814],\n",
       "          [-0.1523,  0.0735, -0.5181,  ...,  0.7085,  0.0388, -0.1667],\n",
       "          [-0.1115,  0.2837, -0.3811,  ...,  1.0938,  0.1213, -0.1531],\n",
       "          [-0.3691,  0.2715,  0.2133,  ...,  0.5962,  0.2939,  0.0122]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0443,  0.4243, -0.1725,  ..., -0.4583,  0.0894, -0.0450],\n",
       "          [-0.0275,  0.1477, -0.1837,  ...,  0.1957,  0.1522, -0.1660]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.5449e-01, 1.3098e-01, 1.4252e-02, 2.5702e-04],\n",
       "          [1.0370e-01, 8.9551e-01, 5.1117e-04, 2.5296e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  113.234154   40.376953  248.212280  352.081482    0.922829      0    person\n",
       "  1   24.777584   73.220840  146.092560  346.933411    0.915727      0    person\n",
       "  2  158.564117   82.972412  215.424713  155.384338    0.762133     24  backpack\n",
       "  3    0.639282  293.815460  149.798172  359.062347    0.674925     30      skis\n",
       "  4  177.370056  273.937042  303.774902  359.680573    0.593215     30      skis,\n",
       "  'caption': ['woman wearing red jacket is standing near a man',\n",
       "   'A woman dressed in red standing next to a male skier in red on a ski slope.'],\n",
       "  'bbox_target': [23.36, 70.95, 127.24, 283.49]},\n",
       " 256: {'image_emb': tensor([[-0.8027,  0.5239, -0.1089,  ...,  0.4753, -0.1159,  0.0317],\n",
       "          [-0.1094,  0.1360, -0.4270,  ...,  1.1523, -0.0356, -0.3447],\n",
       "          [-0.2251,  0.3425, -0.2024,  ...,  0.6768, -0.0190, -0.0994],\n",
       "          [-0.0078, -0.2069, -0.4585,  ...,  1.1465, -0.1853, -0.2452],\n",
       "          [-0.5166,  0.0613,  0.1260,  ...,  0.1722, -0.0401, -0.0212]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1864,  0.0696, -0.4937,  ..., -0.0355, -0.0507,  0.0044],\n",
       "          [-0.0655,  0.1087, -0.5361,  ...,  0.0967, -0.0096, -0.6191]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.7646e-01, 4.4823e-04, 9.6977e-05, 6.9427e-04, 1.2238e-01],\n",
       "          [5.1562e-01, 3.0351e-04, 4.9531e-05, 1.1206e-05, 4.8413e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   98.599693  251.726822  250.019073  493.114136    0.940666      0   \n",
       "  1   66.337639   34.249474  139.956879  260.108002    0.820760      0   \n",
       "  2  118.186905   80.926224  175.612396  116.107239    0.778578     38   \n",
       "  3  234.886810  350.224579  246.321228  361.833099    0.772759     32   \n",
       "  4   69.009178   39.286224   92.592064   76.358887    0.441295     38   \n",
       "  5   58.489006   38.978870   91.770287  109.558823    0.436589     38   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2  tennis racket  \n",
       "  3    sports ball  \n",
       "  4  tennis racket  \n",
       "  5  tennis racket  ,\n",
       "  'caption': ['Boy in a red t-shirt',\n",
       "   'A BOY IN A BLUE HAT CARRYING A YELLOW TENNIS BALL.'],\n",
       "  'bbox_target': [100.42, 252.45, 149.09, 239.54]},\n",
       " 257: {'image_emb': tensor([[-0.4756,  0.5493, -0.1970,  ...,  0.5981, -0.1737, -0.2300],\n",
       "          [-0.3113,  0.3423, -0.0514,  ...,  1.1914, -0.0133, -0.2639],\n",
       "          [-0.3147,  0.3137, -0.3044,  ...,  1.2549,  0.0513,  0.0283],\n",
       "          [-0.3677,  0.3914, -0.1032,  ...,  1.3506, -0.6309, -0.1686],\n",
       "          [-0.3984,  0.2429,  0.0803,  ...,  0.4795, -0.2595, -0.1519]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2451, -0.2207, -0.1259,  ...,  0.4353,  0.2356,  0.0887],\n",
       "          [-0.2722, -0.1732, -0.1307,  ..., -0.1672,  0.0852,  0.2559]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0597, 0.0158, 0.3328, 0.1224, 0.4692],\n",
       "          [0.1801, 0.0751, 0.0360, 0.1541, 0.5547]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   41.711426   34.529541  409.863586  639.077393    0.951106      0   \n",
       "  1    0.000000   70.632469  205.481750  311.567108    0.851066     62   \n",
       "  2   31.790039  445.855499  148.676361  499.910431    0.844666     67   \n",
       "  3  247.356903  121.733047  333.941925  248.003098    0.799287     67   \n",
       "  4    0.000000  302.467834  120.314194  523.618713    0.698148     62   \n",
       "  5    0.000000  536.665161   51.202133  556.771240    0.482512     65   \n",
       "  6    0.000000  535.705139   50.255676  556.850159    0.384353     73   \n",
       "  7    0.462822  538.455322  150.796967  638.898438    0.268242     60   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1            tv  \n",
       "  2    cell phone  \n",
       "  3    cell phone  \n",
       "  4            tv  \n",
       "  5        remote  \n",
       "  6          book  \n",
       "  7  dining table  ,\n",
       "  'caption': ['Powered off tv monitor with part of a shoulder wearing a striped sweater.',\n",
       "   'A tv monitor behind a man in an office'],\n",
       "  'bbox_target': [4.54, 72.25, 198.2, 226.95]},\n",
       " 258: {'image_emb': tensor([[ 0.1403,  0.5078,  0.1268,  ...,  0.8467,  0.1161, -0.3000],\n",
       "          [-0.1270,  0.1168, -0.2458,  ...,  0.7876,  0.3098, -0.3147],\n",
       "          [-0.0369,  0.3672, -0.1633,  ...,  0.7510,  0.2408, -0.0669],\n",
       "          [-0.3398,  0.0154, -0.4255,  ...,  1.2363,  0.1532, -0.1776],\n",
       "          [-0.0547,  0.1047, -0.1521,  ...,  0.6978,  0.2469, -0.1235]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2905, -0.3530, -0.1176,  ...,  0.0441,  0.1716, -0.4604],\n",
       "          [ 0.1917, -0.2581, -0.1741,  ...,  0.2487,  0.0248, -0.2983]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.2701e-04, 4.2236e-02, 8.8867e-01, 2.4855e-05, 6.8542e-02],\n",
       "          [3.9935e-06, 1.0300e-02, 9.8682e-01, 1.4544e-05, 2.9507e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  211.675461    0.800720  638.922546  299.222961    0.939503      2      car\n",
       "  1  129.999283  101.950150  388.053711  395.405212    0.938156     16      dog\n",
       "  2    0.323563  151.478210  178.126266  422.280151    0.923280     16      dog\n",
       "  3   94.241425  287.478821  238.682800  344.651062    0.896334     29  frisbee,\n",
       "  'caption': ['Block dog carrying a frisbee in its mouth',\n",
       "   'A black dog with a red Frisbee in its mouth.'],\n",
       "  'bbox_target': [0.0, 147.37, 180.44, 275.85]},\n",
       " 259: {'image_emb': tensor([[ 0.2256,  0.2362,  0.0382,  ...,  0.1254, -0.1005,  0.0939],\n",
       "          [-0.0714,  0.3716, -0.0667,  ...,  0.9590,  0.0263, -0.0568],\n",
       "          [ 0.0169,  0.3721, -0.2230,  ...,  0.7832,  0.2515,  0.1262],\n",
       "          [ 0.0731,  0.1588,  0.0518,  ...,  0.2903, -0.1550, -0.0402]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1719,  0.2124, -0.1403,  ..., -0.3674, -0.1348, -0.4246],\n",
       "          [ 0.0633,  0.3433, -0.4062,  ..., -0.1836, -0.1514, -0.1265]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.8839e-02, 9.7021e-01, 1.0735e-04, 8.0538e-04],\n",
       "          [2.0679e-01, 6.3721e-01, 2.7597e-05, 1.5613e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  256.654419   64.277328  579.876160  510.108643    0.953399      0   person\n",
       "  1   43.481606  121.507935  217.415497  525.864197    0.949684      0   person\n",
       "  2  587.753479  162.026337  627.086731  212.729736    0.903403     29  frisbee,\n",
       "  'caption': ['A man wearing a black cap.',\n",
       "   'a man in a white shirt next to a man in a green shirt'],\n",
       "  'bbox_target': [41.75, 122.51, 175.86, 412.45]},\n",
       " 260: {'image_emb': tensor([[-7.9536e-04,  5.5420e-01, -6.8213e-01,  ...,  7.7002e-01,\n",
       "           -3.2568e-01,  2.7075e-01],\n",
       "          [ 6.4880e-02,  7.2217e-01, -6.1084e-01,  ...,  6.4111e-01,\n",
       "           -2.6685e-01,  3.8159e-01],\n",
       "          [-6.8848e-01,  1.7310e-01,  7.5684e-02,  ...,  7.1240e-01,\n",
       "            4.8859e-02,  2.5952e-01],\n",
       "          [-8.5010e-01,  6.1523e-02, -1.9324e-01,  ...,  9.6143e-01,\n",
       "            6.6528e-02,  6.2061e-01],\n",
       "          [-1.0657e-01,  2.7124e-01, -6.4746e-01,  ...,  5.9375e-01,\n",
       "           -9.4727e-02,  3.1555e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3486,  0.0388,  0.0708,  ...,  0.1060, -0.0471,  0.0354],\n",
       "          [-0.3850,  0.0192,  0.0545,  ..., -0.0364, -0.0608, -0.1454]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.1921e-07, 7.7486e-07, 8.7061e-01, 1.2939e-01, 5.3644e-07],\n",
       "          [0.0000e+00, 5.9605e-08, 6.1523e-01, 3.8501e-01, 5.9605e-08]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   94.800140  170.897812  317.918152  335.158325    0.928221     54   \n",
       "  1  350.308533  122.795166  505.881592  232.352234    0.923041     54   \n",
       "  2  517.036987  125.273621  639.349854  248.978638    0.880704     76   \n",
       "  3  552.423950  100.806244  640.000000  199.166046    0.863314     76   \n",
       "  4   19.465027    0.000000  484.691406  152.175644    0.638339     68   \n",
       "  5    2.040588   30.529404  640.000000  470.572815    0.311418     60   \n",
       "  \n",
       "             name  \n",
       "  0         donut  \n",
       "  1         donut  \n",
       "  2      scissors  \n",
       "  3      scissors  \n",
       "  4     microwave  \n",
       "  5  dining table  ,\n",
       "  'caption': ['A pair of orange-handled scissors.',\n",
       "   'A pair of scissors with orange handles.'],\n",
       "  'bbox_target': [517.91, 124.24, 122.09, 126.45]},\n",
       " 261: {'image_emb': tensor([[-0.1233,  0.5054, -0.3623,  ...,  1.0391, -0.1780, -0.2542],\n",
       "          [-0.0182,  0.4297, -0.0859,  ...,  0.7773,  0.0449,  0.0331],\n",
       "          [-0.0804,  0.4438, -0.2874,  ...,  0.8364, -0.0723, -0.0214],\n",
       "          [-0.3135,  0.0165, -0.0443,  ...,  0.6685, -0.1444,  0.3545]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1995,  0.1466, -0.1962,  ...,  0.1570,  0.1768, -0.2532],\n",
       "          [ 0.1875, -0.0826, -0.1360,  ...,  0.1516,  0.0657, -0.3269]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6343, 0.0038, 0.0009, 0.3613],\n",
       "          [0.8950, 0.0175, 0.0015, 0.0859]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  187.091980  172.305527  426.102844  639.418213    0.951856      0  person\n",
       "  1    0.000000  316.565125  193.934601  638.605286    0.948364      0  person\n",
       "  2  141.651321  329.634247  225.539856  476.494293    0.875002     33    kite,\n",
       "  'caption': ['A man wearing black tshirt holding a kite',\n",
       "   'A black T-Shirt boy playing with kite.'],\n",
       "  'bbox_target': [182.32, 168.49, 243.6, 469.74]},\n",
       " 262: {'image_emb': tensor([[-0.0909,  0.3323,  0.1508,  ...,  0.8984,  0.1096, -0.3296],\n",
       "          [-0.3848,  0.5786, -0.0088,  ...,  0.7212, -0.1096, -0.2295],\n",
       "          [-0.1183,  0.2030, -0.1488,  ...,  0.5420,  0.3123, -0.1521],\n",
       "          [-0.3376,  0.3711, -0.1260,  ...,  0.6792,  0.1848, -0.0976],\n",
       "          [-0.0402,  0.2566,  0.0651,  ...,  0.4836,  0.0280, -0.2294]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0555,  0.1371,  0.0660,  ..., -0.0649, -0.1089, -0.0317],\n",
       "          [-0.0183, -0.1749, -0.1490,  ..., -0.0306, -0.0972,  0.0365]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0183, 0.1439, 0.1087, 0.5024, 0.2266],\n",
       "          [0.0314, 0.1752, 0.1924, 0.5396, 0.0615]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  218.356949   11.408035  639.755005  292.604675    0.946159     15   \n",
       "  1  239.837311  213.253479  594.098022  426.186707    0.898550     58   \n",
       "  2    1.303452    0.191193  324.929810  424.313599    0.816319     58   \n",
       "  3  169.513382   77.872742  361.634583  317.271179    0.795820     58   \n",
       "  \n",
       "             name  \n",
       "  0           cat  \n",
       "  1  potted plant  \n",
       "  2  potted plant  \n",
       "  3  potted plant  ,\n",
       "  'caption': ['A flower pot with a bushy plant.', \"plant cat's face is in\"],\n",
       "  'bbox_target': [137.88, 93.93, 223.1, 217.35]},\n",
       " 263: {'image_emb': tensor([[-0.0940,  0.1953, -0.2834,  ...,  0.6338,  0.1719,  0.2347],\n",
       "          [-0.3208, -0.0953, -0.2832,  ...,  0.9932,  0.0597, -0.2437],\n",
       "          [-0.0492,  0.3904, -0.2168,  ...,  0.5811, -0.0980, -0.0028]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 2.3328e-01, -3.2544e-01,  1.2830e-01, -5.4138e-02, -1.5637e-01,\n",
       "            6.3171e-02,  1.3245e-02, -9.6436e-01, -5.1971e-02, -5.6946e-02,\n",
       "           -1.1432e-01,  2.1301e-01,  2.8784e-01,  1.9958e-01,  5.0354e-02,\n",
       "           -2.8516e-01, -7.7209e-02, -1.2549e-01, -1.4783e-01, -3.2593e-02,\n",
       "            3.3740e-01,  4.4116e-01, -3.3521e-01, -9.8938e-02, -2.2034e-01,\n",
       "           -7.4707e-02,  8.8135e-02,  4.1748e-01,  4.0710e-02, -1.0388e-01,\n",
       "            1.0767e-01, -2.2253e-01, -1.2695e-01,  3.2788e-01, -3.7866e-01,\n",
       "           -1.6327e-02, -1.1743e-01, -1.1719e-01, -2.6684e-03,  3.3179e-01,\n",
       "            2.7222e-01,  2.2156e-01, -7.6370e-03,  6.1279e-02,  1.2866e-01,\n",
       "           -8.4290e-02,  3.4937e-01,  2.6392e-01, -1.5503e-01, -3.3154e-01,\n",
       "            4.6783e-02, -5.4004e-01, -1.5759e-01, -3.9966e-01, -8.9966e-02,\n",
       "           -1.3440e-01,  2.5928e-01,  4.8462e-02, -5.1849e-02,  3.0469e-01,\n",
       "            3.6328e-01, -7.7209e-02,  1.7639e-02, -3.0762e-02, -4.4441e-03,\n",
       "            4.6021e-02,  2.2681e-01,  9.7778e-02, -3.0493e-01, -1.0449e-01,\n",
       "           -9.4849e-02, -3.8477e-01,  7.9529e-02,  1.4258e-01,  2.8076e-01,\n",
       "           -1.2769e-01,  2.2827e-01,  2.7783e-01, -1.4624e-01,  1.1975e-01,\n",
       "           -2.2046e-01, -1.3306e-01, -3.8818e-01,  1.8646e-02, -1.4124e-01,\n",
       "            1.8640e-01, -2.1021e-01,  1.5540e-01, -1.1566e-01, -6.3599e-02,\n",
       "           -8.2626e-03,  4.2114e-01, -9.2773e-01,  5.1318e-01, -3.4180e-02,\n",
       "           -4.7708e-04, -1.2646e-01,  1.2781e-01,  2.3621e-01, -2.9858e-01,\n",
       "           -2.9541e-01,  1.8286e-01,  3.1787e-01, -2.3535e-01, -3.3447e-01,\n",
       "            1.2268e-01, -3.3276e-01,  8.4167e-02,  2.7368e-01,  1.4417e-01,\n",
       "           -1.3623e-01, -3.6841e-01, -1.2384e-01,  4.4336e-01,  3.2764e-01,\n",
       "            4.3213e-01,  1.2537e-01, -5.8789e-01, -1.5906e-01, -1.5186e-01,\n",
       "           -2.3950e-01, -5.9570e-01, -1.4929e-01, -9.5215e-02, -2.7908e-02,\n",
       "           -4.0723e-01, -3.3032e-01,  1.2396e-01, -3.3521e-01,  6.6471e-04,\n",
       "            1.0675e-01,  1.0107e-01, -1.0016e-01,  5.2969e+00, -6.1401e-02,\n",
       "           -4.8401e-02, -7.9346e-03,  1.1127e-01,  1.6101e-01, -1.9653e-01,\n",
       "           -1.6508e-03,  2.8955e-01, -2.5122e-01,  7.6111e-02, -1.8311e-01,\n",
       "            1.2988e-01,  3.5919e-02, -3.5083e-01, -5.2344e-01, -2.0044e-01,\n",
       "           -1.5930e-01,  2.2766e-02,  2.3743e-01,  4.4006e-02, -6.9763e-02,\n",
       "            2.7523e-03, -1.7609e-02, -4.4385e-01,  9.9564e-04,  1.8420e-01,\n",
       "            1.5039e-01,  1.3696e-01, -8.5083e-02,  1.6846e-01,  1.4270e-01,\n",
       "            3.5083e-01,  3.7671e-01,  1.0083e-01,  4.0088e-01, -1.2000e-01,\n",
       "            5.9509e-02, -2.2778e-01,  3.8843e-01,  2.8711e-01, -2.7661e-01,\n",
       "           -1.6638e-01, -1.7349e-02, -4.4670e-03, -1.4893e-01, -6.6650e-02,\n",
       "            4.1919e-01,  1.4771e-01, -3.3374e-01,  2.2986e-01, -8.5022e-02,\n",
       "           -9.1370e-02, -2.6440e-01, -1.1765e-02, -2.5928e-01,  3.5620e-01,\n",
       "            3.0542e-01,  2.7756e-02, -9.7656e-02,  2.7740e-02, -1.6541e-01,\n",
       "           -1.4819e-01, -4.3262e-01,  1.7419e-01, -6.5491e-02,  1.5039e-01,\n",
       "           -6.9641e-02,  2.7832e-01,  3.5156e-01, -4.1675e-01,  6.3965e-02,\n",
       "           -2.6337e-02, -2.8336e-02,  7.3486e-02, -1.8591e-01,  2.8503e-02,\n",
       "            3.5962e-01,  1.2433e-01,  1.5527e-01,  4.5703e-01, -9.2285e-02,\n",
       "           -3.2227e-01,  2.4963e-01, -3.7378e-01,  3.5107e-01,  4.7455e-02,\n",
       "            4.5312e-01, -4.5239e-01, -8.9598e-04,  3.7451e-01,  1.3025e-01,\n",
       "           -4.0332e-01,  4.5459e-01, -3.8086e-01, -2.3999e-01, -3.3398e-01,\n",
       "           -7.0117e-01,  3.7964e-01, -7.3792e-02,  2.9370e-01,  1.3806e-01,\n",
       "            1.0858e-01,  1.3947e-02, -1.0034e-01, -1.3863e-02,  2.2803e-01,\n",
       "           -1.9995e-01, -7.8918e-02, -1.3293e-01,  2.3987e-01, -2.3083e-01,\n",
       "           -4.6753e-02,  1.1230e-01,  8.4595e-02,  3.6475e-01,  2.7979e-01,\n",
       "           -3.7134e-01,  5.0830e-01,  2.9785e-01, -7.1716e-02, -2.2415e-02,\n",
       "            2.9327e-02,  4.7455e-02,  1.4502e-01,  5.4150e-01, -9.0454e-02,\n",
       "           -1.6541e-01,  4.3152e-02,  5.7129e-02,  3.7476e-01,  1.2927e-01,\n",
       "            1.2927e-01,  9.5139e-03, -5.4590e-01,  2.6514e-01,  3.5840e-01,\n",
       "           -1.9824e-01,  3.3374e-01, -1.8481e-01,  2.3035e-01,  4.8370e-02,\n",
       "           -6.5979e-02, -6.1310e-02, -3.6914e-01, -6.7871e-02, -1.3770e-01,\n",
       "           -3.0350e-02,  8.4961e-02, -3.8037e-01, -1.6467e-01, -1.4307e-01,\n",
       "            2.7759e-01,  1.1707e-01,  2.0740e-01,  4.6509e-01,  3.6987e-01,\n",
       "            1.4191e-02,  1.1200e-01, -7.4921e-03, -2.9395e-01, -7.2144e-02,\n",
       "            5.3291e-03, -2.8223e-01,  8.2214e-02, -1.1890e-01,  8.6609e-02,\n",
       "           -2.0300e-01, -2.6172e-01,  1.0004e-01, -1.5027e-01,  1.0938e-01,\n",
       "           -4.2896e-01,  2.9709e-02, -4.1992e-01,  1.7725e-01, -9.9792e-02,\n",
       "           -1.7166e-02, -1.3440e-01,  5.3008e+00, -1.5625e-01,  2.1387e-01,\n",
       "           -8.6914e-02,  2.2363e-01, -4.8413e-01,  7.6416e-02,  3.6279e-01,\n",
       "            3.4399e-01, -5.8502e-02,  1.6919e-01, -2.3514e-02, -2.4670e-01,\n",
       "            2.7344e-01, -2.1362e-02, -8.6975e-02,  2.4207e-01, -1.4561e+00,\n",
       "            1.3025e-01, -2.1942e-02,  2.1887e-01,  3.6914e-01,  1.2817e-01,\n",
       "            2.0605e-01, -1.7151e-01,  1.4400e-04,  8.8867e-02,  6.8420e-02,\n",
       "           -7.0703e-01, -2.6810e-02,  2.0422e-01, -3.6743e-01,  2.9468e-01,\n",
       "            1.3770e-01,  4.0991e-01,  3.0347e-01, -1.0095e-01, -5.9418e-02,\n",
       "           -2.5513e-01, -2.7271e-01, -2.5604e-02,  1.9910e-01, -5.6055e-01,\n",
       "           -1.6785e-01, -9.9426e-02,  3.8965e-01,  2.2290e-01, -2.9312e-02,\n",
       "           -2.4731e-01, -1.8845e-02, -2.5684e-01, -2.0129e-01, -8.8928e-02,\n",
       "            3.6987e-01, -1.6220e-02, -2.7008e-02,  2.2058e-01, -3.7476e-01,\n",
       "           -3.6041e-02,  1.7737e-01,  1.7566e-01,  2.0740e-01,  1.1273e-01,\n",
       "            7.2144e-02, -4.8340e-01,  1.7114e-01,  8.9905e-02, -2.2156e-02,\n",
       "            7.0251e-02,  7.8809e-01,  1.8213e-01, -4.1064e-01, -4.6411e-01,\n",
       "            2.0129e-01, -2.5977e-01,  2.6636e-01, -1.2262e-01, -3.8916e-01,\n",
       "           -4.6729e-01, -1.1584e-01, -4.1431e-01, -1.3550e-01,  2.2449e-01,\n",
       "            6.7688e-02,  7.5745e-02,  1.7480e-01,  2.5269e-02,  3.7500e-01,\n",
       "            6.6992e-01,  3.0542e-01, -3.5254e-01, -1.0718e-01, -2.7954e-01,\n",
       "            7.5836e-03,  6.9275e-02,  4.7089e-02,  2.6917e-02,  1.0822e-01,\n",
       "            4.9652e-02,  6.2042e-02, -1.2103e-01, -1.2311e-01, -3.4576e-02,\n",
       "            4.3640e-02,  1.2939e-01, -1.9238e-01, -4.9194e-02, -8.2947e-02,\n",
       "            3.3765e-01, -5.9131e-01,  6.4148e-02, -4.6313e-01, -2.9282e-02,\n",
       "            8.5083e-02,  7.8125e-02, -2.6343e-01,  2.9517e-01,  2.2388e-01,\n",
       "           -4.8340e-01,  1.1505e-01, -5.9424e-01, -2.1655e-01,  4.5532e-02,\n",
       "           -7.6782e-02,  2.9102e-01,  1.9202e-01, -6.5552e-02, -2.5903e-01,\n",
       "           -2.8906e-01,  4.6997e-03,  2.2507e-02, -1.1121e-01, -2.5537e-01,\n",
       "            3.8330e-01,  1.6312e-02,  8.3130e-02, -8.8928e-02, -3.4058e-02,\n",
       "           -2.6733e-01,  1.2708e-01, -1.7700e-01,  1.9897e-01, -3.3386e-02,\n",
       "           -2.0447e-01, -7.9895e-02, -4.7485e-01,  4.2450e-02, -9.1370e-02,\n",
       "           -2.4524e-01, -2.9190e-02, -7.9773e-02,  4.3311e-01, -3.3472e-01,\n",
       "           -1.0059e-01, -1.2805e-01,  7.5439e-02,  1.1902e-01,  1.5466e-01,\n",
       "            1.1768e-01,  7.2754e-02, -2.5879e-01, -3.1592e-01, -8.2214e-02,\n",
       "           -5.1361e-02,  3.4253e-01,  1.3867e-01, -2.6489e-01,  5.3444e-03,\n",
       "           -2.8305e-03, -2.6904e-01,  2.5781e-01, -4.8637e-03,  2.9712e-01,\n",
       "           -2.0776e-01,  8.5999e-02, -9.1064e-02, -1.8225e-01,  2.5415e-01,\n",
       "            8.8440e-02,  1.4514e-01,  1.0474e-01,  1.2627e+00,  2.0557e-01,\n",
       "            1.1200e-01,  1.1725e-01, -2.3181e-01, -1.8570e-02,  6.3660e-02,\n",
       "           -7.3303e-02, -8.5938e-02, -2.2388e-01,  3.3252e-01,  1.9165e-01,\n",
       "           -1.6373e-02, -5.0293e-01, -2.6221e-01,  6.7932e-02,  8.8562e-02,\n",
       "           -1.1368e-02, -1.3513e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6870, 0.0353, 0.2776]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    21.256393  158.101379  427.235840  283.438721    0.879597      4   \n",
       "  1   256.482788  277.578674  298.547607  298.906067    0.735618      7   \n",
       "  2   294.748596  329.497375  461.203003  375.547424    0.689167      7   \n",
       "  3   283.660858  265.279358  349.880768  296.751953    0.653261      7   \n",
       "  4   386.539490  204.511475  639.607361  300.459045    0.614957      4   \n",
       "  5   118.255768  300.454285  157.052429  336.701477    0.582593      7   \n",
       "  6   463.988342  202.226837  635.634338  244.855316    0.573001      4   \n",
       "  7   376.806610  260.104828  528.788086  298.441437    0.550173      5   \n",
       "  8   309.905518  301.857605  638.881042  451.330383    0.547444      4   \n",
       "  9   220.755905  310.123688  252.827042  330.948883    0.343898      7   \n",
       "  10  614.840820  246.339417  639.728760  264.385010    0.267064      7   \n",
       "  \n",
       "          name  \n",
       "  0   airplane  \n",
       "  1      truck  \n",
       "  2      truck  \n",
       "  3      truck  \n",
       "  4   airplane  \n",
       "  5      truck  \n",
       "  6   airplane  \n",
       "  7        bus  \n",
       "  8   airplane  \n",
       "  9      truck  \n",
       "  10     truck  ,\n",
       "  'caption': ['The smaller plane that is to the right of the bus facing away.'],\n",
       "  'bbox_target': [394.44, 208.99, 242.82, 85.09]},\n",
       " 264: {'image_emb': tensor([[-9.2773e-02,  3.0176e-01, -1.9727e-01,  ...,  1.1426e+00,\n",
       "           -1.3379e-01,  1.3013e-01],\n",
       "          [ 2.4700e-03, -7.5806e-02,  5.8655e-02,  ...,  1.1572e+00,\n",
       "            1.4978e-01, -6.7322e-02],\n",
       "          [-7.3914e-02,  2.3621e-01, -3.5431e-02,  ...,  8.8330e-01,\n",
       "            9.7046e-02, -1.2915e-01],\n",
       "          ...,\n",
       "          [-3.0991e-02, -7.0274e-05, -3.9453e-01,  ...,  1.0801e+00,\n",
       "           -1.8646e-02, -9.8694e-02],\n",
       "          [ 2.5299e-02, -2.6367e-01, -4.4019e-01,  ...,  7.5195e-01,\n",
       "           -3.2387e-03, -4.2480e-01],\n",
       "          [-1.7493e-01,  5.9753e-02,  2.3804e-02,  ...,  9.5264e-01,\n",
       "            1.6846e-01,  2.1729e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2625,  0.2883, -0.0229,  ...,  0.0375, -0.2932,  0.0196],\n",
       "          [ 0.0016, -0.2256,  0.0289,  ...,  0.1327, -0.0914, -0.4150]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.5882e-05, 1.5676e-05, 5.6446e-05, 1.5497e-06, 4.9055e-05, 3.5763e-07,\n",
       "           6.2609e-04, 7.3612e-05, 2.3186e-05, 9.9902e-01, 1.2994e-05, 1.1325e-06,\n",
       "           9.6023e-05, 2.3901e-05, 8.1062e-06],\n",
       "          [1.1921e-07, 1.1921e-06, 1.1921e-07, 5.9605e-08, 1.7881e-07, 0.0000e+00,\n",
       "           7.7486e-07, 7.1526e-07, 5.9605e-08, 1.0000e+00, 4.7684e-07, 0.0000e+00,\n",
       "           5.9605e-07, 2.9802e-07, 3.1590e-06]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   492.496063  149.825012  594.011230  361.188416    0.922353      0   \n",
       "  1   131.024414   92.750443  284.902679  421.313477    0.902477      0   \n",
       "  2   479.691315   62.403305  508.956207  152.772980    0.890078      0   \n",
       "  3   149.319885   46.592545  285.135620  235.677856    0.881034      0   \n",
       "  4   429.981659  128.156738  487.252167  307.334290    0.879689      0   \n",
       "  5   362.410095  113.137497  406.726013  221.882904    0.865268      0   \n",
       "  6   537.583740  305.369324  584.637573  393.939148    0.828160     56   \n",
       "  7   340.528473  199.944397  358.858978  240.822449    0.816023     39   \n",
       "  8   529.564575   65.755219  554.543457  152.022980    0.814924      0   \n",
       "  9   257.817749  285.171143  569.283752  420.406250    0.793263     60   \n",
       "  10  111.159164  103.557327  164.914322  170.079102    0.776585      3   \n",
       "  11  249.504379   98.985046  478.553223  226.117676    0.747754      2   \n",
       "  12  547.539856   63.501633  571.579895  154.272156    0.745590      0   \n",
       "  13  259.612366   77.149506  277.237488  101.151886    0.713327      0   \n",
       "  14  338.567261   43.559174  355.113464  100.947937    0.690988      0   \n",
       "  15  241.004730  283.222046  310.470856  305.402527    0.492251     43   \n",
       "  16  566.134705   75.814713  602.906433  132.217697    0.477022      2   \n",
       "  17  438.405029   82.854416  461.743042  125.208023    0.393537      0   \n",
       "  18  353.642212   44.198639  367.190308   64.835175    0.319497      0   \n",
       "  19  261.478333  100.298279  477.041260  228.243591    0.307907      0   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3         person  \n",
       "  4         person  \n",
       "  5         person  \n",
       "  6          chair  \n",
       "  7         bottle  \n",
       "  8         person  \n",
       "  9   dining table  \n",
       "  10    motorcycle  \n",
       "  11           car  \n",
       "  12        person  \n",
       "  13        person  \n",
       "  14        person  \n",
       "  15         knife  \n",
       "  16           car  \n",
       "  17        person  \n",
       "  18        person  \n",
       "  19        person  ,\n",
       "  'caption': ['The end of a table with three plates and a placemat.',\n",
       "   'A wooden table holding foods, plates, and a newspaper'],\n",
       "  'bbox_target': [259.39, 283.33, 313.26, 136.68]},\n",
       " 265: {'image_emb': tensor([[-0.0404,  0.4607, -0.0151,  ...,  1.1602, -0.0277, -0.0053],\n",
       "          [-0.2124,  0.1888, -0.0091,  ...,  1.2188, -0.0020, -0.1766],\n",
       "          [ 0.0060,  0.3931, -0.2200,  ...,  1.0547, -0.1097, -0.0454],\n",
       "          [-0.0221,  0.4290, -0.1914,  ...,  1.1768, -0.1735,  0.0221],\n",
       "          [-0.3716,  0.2455, -0.2434,  ...,  1.1045, -0.1213, -0.0237],\n",
       "          [-0.0936,  0.5537,  0.0964,  ...,  0.8643, -0.1285,  0.1694]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3059,  0.1260, -0.2664,  ...,  0.6279, -0.3369, -0.1615],\n",
       "          [-0.0043, -0.0480, -0.1550,  ...,  0.0371, -0.1835,  0.3508]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.7490e-01, 2.4281e-03, 1.6861e-02, 4.4703e-06, 3.3722e-03, 2.0215e-01],\n",
       "          [4.9316e-01, 4.8370e-03, 7.4463e-02, 5.9903e-05, 5.4779e-03, 4.2188e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  192.458939   94.872650  639.416870  351.615875    0.931399     57   \n",
       "  1  275.319153  314.648163  445.007751  421.934265    0.917857     56   \n",
       "  2    0.365784   83.443634  202.174530  352.530426    0.884561     57   \n",
       "  3  108.946350   97.152100  191.515320  149.877045    0.861451     63   \n",
       "  4  429.254852  212.987701  489.900299  345.848724    0.823126     56   \n",
       "  5  129.991669   94.116928  145.105225  125.044662    0.478092     67   \n",
       "  6  119.953903  236.261566  457.592163  388.330109    0.386613     60   \n",
       "  \n",
       "             name  \n",
       "  0         couch  \n",
       "  1         chair  \n",
       "  2         couch  \n",
       "  3        laptop  \n",
       "  4         chair  \n",
       "  5    cell phone  \n",
       "  6  dining table  ,\n",
       "  'caption': ['Beige sofa with 2 long seat cushions', 'A long pink couch'],\n",
       "  'bbox_target': [198.78, 94.93, 441.22, 258.05]},\n",
       " 266: {'image_emb': tensor([[-0.0238,  0.6235, -0.1151,  ...,  1.3135, -0.3247, -0.0318],\n",
       "          [-0.3093,  0.0030, -0.0181,  ...,  0.8218,  0.2094, -0.3076],\n",
       "          [-0.0261,  0.1394, -0.2367,  ...,  1.1406, -0.1321, -0.1396],\n",
       "          ...,\n",
       "          [-0.0014,  0.6177, -0.1426,  ...,  0.9116, -0.0299, -0.1022],\n",
       "          [-0.2554,  0.5938, -0.0434,  ...,  0.7764, -0.0812, -0.1356],\n",
       "          [-0.3469,  0.3035, -0.0346,  ...,  0.9272, -0.1338, -0.1542]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1545, -0.0500,  0.0504,  ..., -0.0136, -0.1586, -0.1774],\n",
       "          [-0.1096,  0.0511, -0.2539,  ..., -0.1952,  0.0716,  0.0233]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.8583e-04, 2.9802e-07, 1.5631e-03, 0.0000e+00, 9.9219e-01, 2.4986e-03,\n",
       "           3.0613e-03],\n",
       "          [2.4815e-03, 1.1611e-04, 3.0403e-03, 4.7684e-07, 9.8584e-01, 3.0880e-03,\n",
       "           5.5046e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    72.023788   39.656723  142.694489  273.193634    0.898137      0   \n",
       "  1   229.518723  101.045433  475.696747  284.737213    0.891937      1   \n",
       "  2   432.896149   39.632893  464.101501   65.859894    0.870147      2   \n",
       "  3    61.838596  259.683990  108.093689  338.765045    0.868816     28   \n",
       "  4     0.000000   55.343090   98.041664  162.402252    0.834095      2   \n",
       "  5     3.613257   67.060425  396.825348  318.109070    0.804270      2   \n",
       "  6   184.265182   44.639431  196.120407   73.303947    0.677180      0   \n",
       "  7   235.236603   54.968525  247.494553   74.899078    0.656628      0   \n",
       "  8     0.251489   41.097595   10.741662   55.353783    0.460733      2   \n",
       "  9    19.609423   39.709343   48.232548   57.275425    0.409698      2   \n",
       "  10  104.417610  129.781219  191.663834  330.166779    0.379573     28   \n",
       "  11   31.304192   33.188171   69.550537   57.860958    0.353391      2   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1    bicycle  \n",
       "  2        car  \n",
       "  3   suitcase  \n",
       "  4        car  \n",
       "  5        car  \n",
       "  6     person  \n",
       "  7     person  \n",
       "  8        car  \n",
       "  9        car  \n",
       "  10  suitcase  \n",
       "  11       car  ,\n",
       "  'caption': ['A small car with a license plate parked in front of a car with a woman posing',\n",
       "   'The silver car behind the woman.'],\n",
       "  'bbox_target': [0.0, 52.94, 97.06, 114.71]},\n",
       " 267: {'image_emb': tensor([[-0.2986,  0.0632, -0.2559,  ...,  0.9663,  0.1783,  0.0793],\n",
       "          [-0.1986,  0.2947, -0.0858,  ...,  0.9722,  0.0974,  0.1345],\n",
       "          [-0.3037,  0.1304, -0.1022,  ...,  0.8428,  0.1185, -0.1438],\n",
       "          ...,\n",
       "          [ 0.2708, -0.1300, -0.3706,  ...,  0.7427, -0.3557, -0.2556],\n",
       "          [-0.0866,  0.2983, -0.1382,  ...,  1.0996,  0.1398,  0.2319],\n",
       "          [-0.2693,  0.0702, -0.3335,  ...,  1.0918, -0.2070,  0.0753]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0282,  0.2910,  0.0663,  ..., -0.3352, -0.0626, -0.5596],\n",
       "          [ 0.1064,  0.4287, -0.1832,  ...,  0.1098, -0.0289, -0.3213]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.3025e-01, 1.4087e-01, 2.5253e-02, 3.2928e-02, 2.3218e-01, 2.2864e-01,\n",
       "           5.2948e-03, 1.7261e-01, 3.1921e-02],\n",
       "          [2.3206e-01, 1.9543e-01, 1.4374e-02, 9.1887e-04, 4.0955e-02, 4.2017e-01,\n",
       "           1.2517e-06, 7.7698e-02, 1.8463e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   365.562012  317.369507  638.596558  423.313171    0.899860     60   \n",
       "  1   238.635040  329.496521  357.238495  425.294434    0.819533     56   \n",
       "  2     5.852234  170.168030  274.626892  423.415955    0.808263     58   \n",
       "  3   522.706238  292.090179  639.511536  366.407684    0.800291     56   \n",
       "  4   296.559143  262.358887  359.960144  347.473206    0.785219     56   \n",
       "  5   340.567993  282.502991  430.521851  399.013855    0.747616     56   \n",
       "  6   322.514771  183.014801  335.427612  201.644318    0.722355      0   \n",
       "  7   507.833984  259.227631  566.082642  324.405914    0.706447     56   \n",
       "  8   446.704468  230.737701  493.011230  292.334137    0.682598     56   \n",
       "  9   239.186584  152.560333  316.581604  179.982330    0.635925     25   \n",
       "  10  386.197205  234.099548  434.479553  286.771484    0.625645     56   \n",
       "  11  261.904266  189.912903  279.387360  236.408630    0.614023      0   \n",
       "  12  344.704102   55.238998  639.118774  350.067322    0.610531     25   \n",
       "  13  325.228668  200.314178  349.927948  240.400177    0.542817     58   \n",
       "  14  537.397766  256.785370  640.000000  292.458221    0.528180     60   \n",
       "  15  213.048218  276.741608  328.112610  305.879242    0.483313     60   \n",
       "  16  121.383041  141.693008  264.631897  278.010925    0.439935     25   \n",
       "  17  198.371002  215.811859  220.513123  239.976959    0.400132     58   \n",
       "  18  223.357697  204.732056  243.854126  224.987793    0.396810      0   \n",
       "  19  120.608215  143.112457  256.264954  168.949066    0.365952     25   \n",
       "  20  164.004150  248.929657  244.409912  281.603912    0.356265     60   \n",
       "  21  409.663208  235.781281  468.835876  280.543671    0.341741     60   \n",
       "  22  285.864929  212.701721  308.488159  248.194031    0.338042     56   \n",
       "  23  161.537949  278.488068  233.266891  332.463348    0.337508     56   \n",
       "  24   62.399746  187.367279  115.306061  247.753754    0.306553     58   \n",
       "  25  344.186462   55.286148  639.480042  156.403076    0.299167     25   \n",
       "  26  526.270081  243.275085  558.446350  273.244263    0.286445     56   \n",
       "  27  430.238007  211.199493  457.384735  237.891571    0.283410     58   \n",
       "  \n",
       "              name  \n",
       "  0   dining table  \n",
       "  1          chair  \n",
       "  2   potted plant  \n",
       "  3          chair  \n",
       "  4          chair  \n",
       "  5          chair  \n",
       "  6         person  \n",
       "  7          chair  \n",
       "  8          chair  \n",
       "  9       umbrella  \n",
       "  10         chair  \n",
       "  11        person  \n",
       "  12      umbrella  \n",
       "  13  potted plant  \n",
       "  14  dining table  \n",
       "  15  dining table  \n",
       "  16      umbrella  \n",
       "  17  potted plant  \n",
       "  18        person  \n",
       "  19      umbrella  \n",
       "  20  dining table  \n",
       "  21  dining table  \n",
       "  22         chair  \n",
       "  23         chair  \n",
       "  24  potted plant  \n",
       "  25      umbrella  \n",
       "  26         chair  \n",
       "  27  potted plant  ,\n",
       "  'caption': ['An umberlla which is nearest to the camera',\n",
       "   'a brown umbrella that is on the far right of the patio'],\n",
       "  'bbox_target': [345.44, 56.61, 294.56, 82.52]},\n",
       " 268: {'image_emb': tensor([[-4.1357e-01, -2.8076e-01, -7.2559e-01,  ...,  7.9199e-01,\n",
       "            1.4893e-01, -3.4937e-01],\n",
       "          [-6.6833e-02, -6.3721e-02, -6.8164e-01,  ...,  6.9922e-01,\n",
       "            7.5073e-02,  6.7749e-02],\n",
       "          [ 4.0161e-02,  1.8127e-01, -4.0722e-04,  ...,  6.2012e-01,\n",
       "            3.0981e-01,  1.9971e-01],\n",
       "          [ 9.0408e-03,  3.2013e-02, -1.9409e-01,  ...,  1.2451e+00,\n",
       "            1.3817e-02, -2.5684e-01],\n",
       "          [ 2.5452e-02, -1.7529e-01, -3.9307e-01,  ...,  1.0127e+00,\n",
       "           -8.1848e-02,  6.5552e-02],\n",
       "          [-3.6108e-01, -2.3218e-01, -7.6074e-01,  ...,  3.8940e-01,\n",
       "            7.0679e-02, -3.5693e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0111,  0.3416, -0.5420,  ..., -0.0166, -0.4028, -0.6895],\n",
       "          [-0.0822,  0.2473, -0.5146,  ..., -0.1215, -0.2751, -0.3459]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.4492e-01, 9.4271e-04, 7.9274e-06, 4.4107e-05, 2.6035e-03, 4.5166e-01],\n",
       "          [7.4072e-01, 9.9850e-04, 3.6359e-06, 7.1487e-03, 2.8896e-03, 2.4817e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  371.469360   14.892532  638.307617  474.396118    0.939538      0   \n",
       "  1  168.282364   93.261154  431.061096  474.168579    0.926882      0   \n",
       "  2  183.739578  218.950256  213.409164  273.519531    0.895925     41   \n",
       "  3  411.967834  304.856567  460.758118  417.540283    0.869211     27   \n",
       "  4  239.460022  190.775970  261.486023  259.906464    0.734068     40   \n",
       "  5   15.970148  247.062317   51.022591  285.287964    0.675934     41   \n",
       "  6   69.178802   41.890350   85.191437   87.906189    0.318996     39   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2         cup  \n",
       "  3         tie  \n",
       "  4  wine glass  \n",
       "  5         cup  \n",
       "  6      bottle  ,\n",
       "  'caption': ['A man wearing a fedora taking a selfie.',\n",
       "   'man wearing gray vest and black hat'],\n",
       "  'bbox_target': [372.13, 20.49, 267.87, 458.43]},\n",
       " 269: {'image_emb': tensor([[ 0.1658,  0.0482, -0.2778,  ...,  0.5107, -0.0563, -0.0244],\n",
       "          [ 0.1410, -0.2385, -0.2325,  ...,  0.7329, -0.2202, -0.2056],\n",
       "          [ 0.1310, -0.2849, -0.2659,  ...,  0.4978, -0.2380,  0.0462]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0499, -0.1432, -0.4512,  ..., -0.3359, -0.1815, -0.3047],\n",
       "          [ 0.2177, -0.3638, -0.5576,  ..., -0.1700, -0.1313, -0.0256]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.9678, 0.0156, 0.0164],\n",
       "          [0.8413, 0.0564, 0.1021]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin       xmax        ymax  confidence  class     name\n",
       "  0  216.114594   48.334854  351.77951  425.146240    0.901050     23  giraffe\n",
       "  1  231.384354  158.108826  589.58783  425.147522    0.868834     23  giraffe,\n",
       "  'caption': ['a giraffe leaning to the side of the photo',\n",
       "   'A giraffe is behind the another giraffe'],\n",
       "  'bbox_target': [228.01, 160.86, 366.17, 267.14]},\n",
       " 270: {'image_emb': tensor([[-0.1490,  0.3645, -0.1577,  ...,  0.7271, -0.0986,  0.2246],\n",
       "          [-0.1730,  0.4409, -0.0946,  ...,  1.0723, -0.0535,  0.0968],\n",
       "          [ 0.0771, -0.0957, -0.2524,  ...,  0.6006,  0.1383, -0.0253],\n",
       "          [-0.0845,  0.0135, -0.2712,  ...,  1.2236, -0.0843, -0.2537],\n",
       "          [ 0.2057,  0.3843, -0.0604,  ...,  0.7417, -0.0516,  0.3335]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1084,  0.3569, -0.3484,  ..., -0.0955, -0.0568,  0.5342],\n",
       "          [-0.2333,  0.1035, -0.3931,  ..., -0.3352,  0.0972,  0.0715]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9121e-01, 2.4438e-06, 1.4520e-04, 4.0512e-03, 4.7340e-03],\n",
       "          [9.4727e-01, 1.0812e-04, 2.4605e-03, 5.7220e-03, 4.4312e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   63.454819   54.420013  347.568665  257.112640    0.946956     73   \n",
       "  1  339.661285    0.000000  639.901978  394.086060    0.931669     63   \n",
       "  2  125.798035  237.997925  390.334839  477.085754    0.923259     73   \n",
       "  3    0.347992  273.300415  161.735077  475.758545    0.707294     73   \n",
       "  4    0.131260    0.548607   46.905682  224.921631    0.655889     56   \n",
       "  5  498.633575  273.696014  569.001404  382.808929    0.508890     67   \n",
       "  6  498.020081  273.632660  569.035706  381.551666    0.433197     65   \n",
       "  7    2.949829    0.552841  635.997803  472.599731    0.300583     60   \n",
       "  \n",
       "             name  \n",
       "  0          book  \n",
       "  1        laptop  \n",
       "  2          book  \n",
       "  3          book  \n",
       "  4         chair  \n",
       "  5    cell phone  \n",
       "  6        remote  \n",
       "  7  dining table  ,\n",
       "  'caption': ['A math text book open to a page of graphs',\n",
       "   'text book opened with highlighted words on the desk'],\n",
       "  'bbox_target': [63.02, 55.62, 283.9, 204.5]},\n",
       " 271: {'image_emb': tensor([[-0.0410,  0.1766, -0.1697,  ...,  1.5771, -0.1777, -0.0779],\n",
       "          [-0.7803,  0.7690,  0.0831,  ...,  0.9243,  0.3621, -0.2274]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1492,  0.2678, -0.0027,  ..., -0.0918, -0.1716,  0.2793],\n",
       "          [ 0.0750,  0.0981, -0.1072,  ...,  0.0512, -0.2047,  0.5068]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.5654e-01, 4.3701e-02],\n",
       "          [9.9951e-01, 5.3596e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    1.319611    0.388237  224.592957   91.889954    0.826728     56   \n",
       "  1   12.240356   28.813446  639.349731  473.561523    0.670395     60   \n",
       "  2  308.219910  111.840851  468.912292  235.189026    0.557163     44   \n",
       "  3  308.014679    0.000000  397.350372   41.601715    0.425790     56   \n",
       "  \n",
       "             name  \n",
       "  0         chair  \n",
       "  1  dining table  \n",
       "  2         spoon  \n",
       "  3         chair  ,\n",
       "  'caption': ['The outline of a chair at a restaurant',\n",
       "   'The back of a silver metal chair in a restaurant.'],\n",
       "  'bbox_target': [0.58, 0.01, 234.81, 96.25]},\n",
       " 272: {'image_emb': tensor([[-0.2046,  0.5688, -0.0922,  ...,  1.3525,  0.1161, -0.2705],\n",
       "          [-0.4324,  0.3330, -0.0528,  ...,  1.1758,  0.0593, -0.0030],\n",
       "          [-0.2983,  0.2776,  0.1196,  ...,  0.7290,  0.3506, -0.2598]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2019,  0.1284, -0.2319,  ..., -0.0185, -0.2766,  0.1681],\n",
       "          [-0.3440,  0.0423, -0.0778,  ...,  0.4263, -0.3760,  0.1771]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2273, 0.0037, 0.7690],\n",
       "          [0.2466, 0.0170, 0.7363]], dtype=torch.float16),\n",
       "  'df_boxes':        xmin        ymin        xmax        ymax  confidence  class        name\n",
       "  0  0.370169  311.668213  401.920807  431.416229    0.786945     79  toothbrush\n",
       "  1  0.000000  235.978058  275.196686  334.670715    0.754865     79  toothbrush,\n",
       "  'caption': ['A toothbrush facing down with the toothpaste on top of it',\n",
       "   'a white toothbrush with aqua and blue colored bristles'],\n",
       "  'bbox_target': [2.25, 338.2, 398.87, 93.26]},\n",
       " 273: {'image_emb': tensor([[-0.1748,  0.4753, -0.1351,  ...,  0.9780,  0.1200, -0.1163],\n",
       "          [-0.3596,  0.4448, -0.4265,  ...,  1.2256,  0.1931, -0.2844],\n",
       "          [-0.0169,  0.2568, -0.3928,  ...,  0.6069,  0.0772,  0.0242]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2837,  0.3369, -0.3093,  ..., -0.0967,  0.1201,  0.0048],\n",
       "          [ 0.2250,  0.1920, -0.2827,  ..., -0.3149,  0.2549,  0.3655]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0019, 0.8823, 0.1157],\n",
       "          [0.0415, 0.7476, 0.2108]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  306.994690   25.978539  488.009216  169.089294    0.845445     55   \n",
       "  1    3.995193    0.062737  321.372742  113.705353    0.792979      0   \n",
       "  2   67.333725   31.281891  170.831482  232.266418    0.586563     39   \n",
       "  3    0.681145   53.708923  144.730530  219.246063    0.554185      0   \n",
       "  4  262.909210  151.982483  510.457611  219.281494    0.493049     55   \n",
       "  5    9.139374  193.467041  639.654297  388.359497    0.421978     42   \n",
       "  6  293.762177   12.073471  335.772491   85.752762    0.396802      0   \n",
       "  7  192.651779   69.775818  266.692688  225.175720    0.329954     39   \n",
       "  8  261.041504  147.118149  333.870117  216.608948    0.312366     55   \n",
       "  9    0.796265  284.013000  639.791931  470.539124    0.284484     60   \n",
       "  \n",
       "             name  \n",
       "  0          cake  \n",
       "  1        person  \n",
       "  2        bottle  \n",
       "  3        person  \n",
       "  4          cake  \n",
       "  5          fork  \n",
       "  6        person  \n",
       "  7        bottle  \n",
       "  8          cake  \n",
       "  9  dining table  ,\n",
       "  'caption': ['Person in a dark shirt',\n",
       "   \"I believe this is a lady's dress or blouse.\"],\n",
       "  'bbox_target': [1.08, 0.54, 317.84, 116.76]},\n",
       " 274: {'image_emb': tensor([[ 0.1863,  0.6606,  0.0026,  ...,  0.5967, -0.0428, -0.5337],\n",
       "          [ 0.1748,  0.4526, -0.2145,  ...,  0.8115,  0.2103,  0.1445],\n",
       "          [ 0.2124, -0.4111, -0.4404,  ...,  1.3311,  0.0563, -0.0366],\n",
       "          [ 0.0789, -0.2292, -0.3630,  ...,  1.1338,  0.3269,  0.0252],\n",
       "          [ 0.2974,  0.3328,  0.2742,  ...,  0.2284,  0.1775, -0.1035]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1453,  0.3479, -0.1747,  ...,  0.1320,  0.3308, -0.4065],\n",
       "          [-0.1494,  0.4336,  0.1954,  ..., -0.3213, -0.1210, -0.1272]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9072e-01, 1.1134e-04, 3.1888e-05, 6.0558e-05, 8.9798e-03],\n",
       "          [9.6045e-01, 2.1896e-02, 2.6298e-04, 6.2132e-04, 1.6525e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   71.072060  142.914673  222.066010  374.219849    0.929818      0   \n",
       "  1  513.836426  148.980347  563.493652  293.697998    0.882610      0   \n",
       "  2  376.676514  121.498459  394.841003  143.175064    0.793132     11   \n",
       "  3  536.905945  118.210587  558.008606  139.353592    0.757946     11   \n",
       "  4  142.426056  288.159241  171.763977  413.908203    0.665203      1   \n",
       "  5  386.693695  144.357620  397.675934  177.343979    0.403216      0   \n",
       "  6  396.421509  151.981873  405.940979  191.949371    0.286157      0   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2  stop sign  \n",
       "  3  stop sign  \n",
       "  4    bicycle  \n",
       "  5     person  \n",
       "  6     person  ,\n",
       "  'caption': ['A man on a bike has a ref poncho on',\n",
       "   'a person wearking red colour jerkin'],\n",
       "  'bbox_target': [70.61, 140.97, 153.91, 234.87]},\n",
       " 275: {'image_emb': tensor([[-0.2544,  0.7129,  0.3464,  ...,  0.8535,  0.2278, -0.2349],\n",
       "          [-0.2869,  0.9517,  0.3152,  ...,  1.0264,  0.0858, -0.3223],\n",
       "          [-0.0950,  0.7217,  0.1877,  ...,  0.6665,  0.2864, -0.2615]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.4077,  0.2284,  0.3120,  ...,  0.1027, -0.3276, -0.3977],\n",
       "          [-0.3340,  0.4319,  0.2991,  ...,  0.2673, -0.1439, -0.1190]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.9619, 0.0127, 0.0252],\n",
       "          [0.6782, 0.2827, 0.0389]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    0.423660  451.647919  271.056885  639.730713    0.853007     61  toilet\n",
       "  1  183.493073  466.961975  394.716156  640.000000    0.845099     71    sink\n",
       "  2  250.978546  514.773682  389.025543  609.892090    0.352671     71    sink,\n",
       "  'caption': ['The white toilet in the bathroom.',\n",
       "   'A white toilet tank with chrome handle.'],\n",
       "  'bbox_target': [110.74, 451.69, 162.52, 174.02]},\n",
       " 276: {'image_emb': tensor([[-0.1137,  0.3232, -0.1647,  ...,  0.8110,  0.0980, -0.0464],\n",
       "          [-0.2142,  0.0887, -0.0709,  ...,  1.0273,  0.1703, -0.1125],\n",
       "          [-0.2993,  0.2556, -0.0155,  ...,  0.7109,  0.1805, -0.1345]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 4.9988e-02,  9.7717e-02,  3.4717e-01,  ..., -2.2974e-01,\n",
       "           -1.4832e-01, -2.4390e-01],\n",
       "          [-1.5320e-01,  4.9591e-04,  5.7129e-01,  ..., -2.3694e-01,\n",
       "           -1.8152e-01, -3.8501e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6831, 0.0882, 0.2288],\n",
       "          [0.6855, 0.0099, 0.3042]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  114.227249   50.918556  274.756104  488.374054    0.920846     72   \n",
       "  1  258.797150  197.073822  374.627350  458.083160    0.857763     72   \n",
       "  2    1.232195  179.542999  115.401985  328.737183    0.415318     72   \n",
       "  3  321.236511  171.157974  336.997711  204.175385    0.294183     39   \n",
       "  \n",
       "             name  \n",
       "  0  refrigerator  \n",
       "  1  refrigerator  \n",
       "  2  refrigerator  \n",
       "  3        bottle  ,\n",
       "  'caption': ['tall fridge with the letter A', 'A white refridgerator.'],\n",
       "  'bbox_target': [114.86, 50.38, 160.71, 437.16]},\n",
       " 277: {'image_emb': tensor([[ 0.2318,  0.7554,  0.0111,  ...,  0.9038,  0.1576, -0.0199],\n",
       "          [-0.0667,  0.0244, -0.1063,  ...,  1.0859, -0.0337, -0.0211],\n",
       "          [ 0.0041,  0.5630,  0.0997,  ...,  1.0498,  0.0581,  0.1556],\n",
       "          ...,\n",
       "          [ 0.0253,  0.3943, -0.1223,  ...,  0.9570,  0.2654, -0.1664],\n",
       "          [-0.0385,  0.3643, -0.1003,  ...,  1.2891,  0.0431, -0.0272],\n",
       "          [ 0.1183,  0.5908,  0.0032,  ...,  0.7896,  0.1376,  0.3389]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0100,  0.0972,  0.2072,  ...,  0.0214, -0.1293, -0.1595],\n",
       "          [ 0.1671,  0.2091, -0.1410,  ...,  0.6279,  0.2043,  0.0477]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.2769e-01, 1.7214e-04, 4.3872e-01, 2.7881e-01, 7.0496e-02, 2.6913e-03,\n",
       "           8.1177e-02],\n",
       "          [1.5698e-01, 1.4305e-04, 7.1838e-02, 5.3906e-01, 5.0171e-02, 1.0090e-03,\n",
       "           1.8066e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   138.931458  151.078430  363.329987  343.110168    0.888693      3   \n",
       "  1   187.444901   81.539673  275.169312  304.572021    0.884154      0   \n",
       "  2   321.908997   90.646118  406.097778  281.617676    0.862252      0   \n",
       "  3   304.672119  146.719666  474.901123  321.169556    0.838821      3   \n",
       "  4   460.859070  148.775528  639.571472  272.730713    0.833215      3   \n",
       "  5   509.781067  102.347778  592.313904  242.244080    0.721756      0   \n",
       "  6   309.637421  107.247589  341.808746  180.221283    0.648651      0   \n",
       "  7   546.074341  104.430664  622.359741  241.639160    0.644591      0   \n",
       "  8     0.000000   61.377197  307.501953  258.588623    0.642977      2   \n",
       "  9   371.852844  141.502991  441.811523  203.639343    0.511335      3   \n",
       "  10  369.377319  118.471802  401.156555  153.343231    0.327906      0   \n",
       "  11  463.745789  142.829834  638.153137  271.187195    0.261994      0   \n",
       "  \n",
       "            name  \n",
       "  0   motorcycle  \n",
       "  1       person  \n",
       "  2       person  \n",
       "  3   motorcycle  \n",
       "  4   motorcycle  \n",
       "  5       person  \n",
       "  6       person  \n",
       "  7       person  \n",
       "  8          car  \n",
       "  9   motorcycle  \n",
       "  10      person  \n",
       "  11      person  ,\n",
       "  'caption': ['The bike of a lady',\n",
       "   'A black motorcycle with a girl riding on it.'],\n",
       "  'bbox_target': [303.63, 152.49, 169.32, 167.61]},\n",
       " 278: {'image_emb': tensor([[-0.0718,  0.5649, -0.2773,  ...,  0.6938, -0.0626,  0.1793],\n",
       "          [-0.2598,  0.2421, -0.4863,  ...,  0.4641, -0.2078, -0.2991],\n",
       "          [-0.4119,  0.5703, -0.6011,  ...,  0.8311, -0.2615,  0.2881],\n",
       "          [-0.0132,  0.4084, -0.2817,  ...,  0.6841, -0.1111,  0.3005],\n",
       "          [-0.0047,  0.3464, -0.1375,  ...,  0.6201, -0.1665, -0.2566]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-1.1963e-01,  4.4458e-01,  2.1155e-01,  ...,  2.0911e-01,\n",
       "            6.8855e-04, -3.4644e-01],\n",
       "          [-8.5938e-02, -4.0009e-02,  2.0618e-01,  ...,  2.1960e-01,\n",
       "           -3.3691e-01, -7.4023e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.1719e-02, 9.7559e-01, 5.1994e-03, 2.2840e-04, 7.3318e-03],\n",
       "          [5.9009e-06, 9.9609e-01, 2.4681e-03, 5.9605e-08, 1.6441e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   383.194946   79.356140  605.800781  387.177185    0.933716     45   \n",
       "  1    19.206009  207.080933  382.164978  471.675171    0.868801     45   \n",
       "  2   233.074707  314.734375  373.536194  459.908081    0.742591     50   \n",
       "  3   127.440674    0.729759  384.404724  213.204224    0.728653     45   \n",
       "  4   195.636658  285.240295  250.714539  348.468262    0.677890     51   \n",
       "  5     1.802673    0.000000  639.196289  472.934998    0.591839     60   \n",
       "  6   209.953674  340.679535  274.114197  408.035004    0.505970     51   \n",
       "  7   232.432617  232.457397  373.449036  314.611206    0.500919     50   \n",
       "  8    21.797775  216.016754  385.746582  465.706665    0.469254     51   \n",
       "  9   141.940247  324.112793  212.010651  401.002502    0.324643     51   \n",
       "  10  120.860435  234.666626  290.004578  467.405090    0.317319     51   \n",
       "  11  156.773010  392.477142  235.244354  462.580750    0.269641     51   \n",
       "  \n",
       "              name  \n",
       "  0           bowl  \n",
       "  1           bowl  \n",
       "  2       broccoli  \n",
       "  3           bowl  \n",
       "  4         carrot  \n",
       "  5   dining table  \n",
       "  6         carrot  \n",
       "  7       broccoli  \n",
       "  8         carrot  \n",
       "  9         carrot  \n",
       "  10        carrot  \n",
       "  11        carrot  ,\n",
       "  'caption': ['Vegetables in a container.',\n",
       "   'the box of vegetables with carrots, broccoli, and tomatoes'],\n",
       "  'bbox_target': [17.3, 207.57, 363.24, 267.02]},\n",
       " 279: {'image_emb': tensor([[ 0.1078,  0.1406, -0.0681,  ...,  1.2520,  0.1260,  0.0692],\n",
       "          [ 0.1027,  0.4778, -0.2239,  ...,  1.5176,  0.0118, -0.1707],\n",
       "          [-0.1226,  0.6216,  0.1625,  ...,  1.1650, -0.1398, -0.1715],\n",
       "          [ 0.7954,  0.4265,  0.1076,  ...,  0.5298, -0.1884,  0.0540]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0688,  0.0803, -0.1276,  ..., -0.0955, -0.4343, -0.4016],\n",
       "          [ 0.1898,  0.0830, -0.0956,  ..., -0.1791, -0.4929,  0.1227]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.1658e-06, 8.0078e-02, 9.1602e-01, 3.6869e-03],\n",
       "          [3.4690e-05, 1.8539e-02, 2.0557e-01, 7.7588e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  259.802826  213.030060  301.249359  252.179596    0.810248     62   \n",
       "  1  547.217285  234.317047  639.454956  332.416046    0.730035     57   \n",
       "  2  154.974487  226.900146  298.043671  342.239197    0.704297     56   \n",
       "  3    0.261482  231.300629  200.530823  393.807892    0.592587     56   \n",
       "  4  478.605591  181.908600  537.765747  282.558441    0.521404     58   \n",
       "  5    0.404144  231.717529  200.585449  393.855286    0.470183     57   \n",
       "  \n",
       "             name  \n",
       "  0            tv  \n",
       "  1         couch  \n",
       "  2         chair  \n",
       "  3         chair  \n",
       "  4  potted plant  \n",
       "  5         couch  ,\n",
       "  'caption': ['A white couch near the design',\n",
       "   'a single sofa next to the tree'],\n",
       "  'bbox_target': [156.04, 228.68, 143.74, 113.94]},\n",
       " 280: {'image_emb': tensor([[ 0.1208,  0.5435, -0.5151,  ...,  0.6841, -0.0473, -0.4973],\n",
       "          [-0.1326,  0.3035, -0.2659,  ...,  1.1016,  0.1014, -0.4602],\n",
       "          [-0.0441,  0.0578, -0.2961,  ...,  0.9756,  0.0125, -0.2783],\n",
       "          [-0.0899,  0.4177, -0.5981,  ...,  1.0068, -0.0447,  0.0021],\n",
       "          [ 0.1193,  0.3452, -0.4919,  ...,  0.6226,  0.1656, -0.4751]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2379,  0.2876, -0.0898,  ..., -0.0202, -0.2368, -0.0950],\n",
       "          [-0.1190,  0.1870, -0.1436,  ...,  0.0194, -0.4009,  0.0967]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.5322e-01, 1.2939e-01, 8.2703e-03, 7.1411e-02, 2.3792e-01],\n",
       "          [4.4238e-01, 2.2984e-04, 2.8610e-06, 6.7177e-03, 5.5078e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   36.939560   41.063690  247.180115  635.094727    0.935335      0   \n",
       "  1  218.379944    0.000000  362.317871  367.811768    0.933156      0   \n",
       "  2    0.000000    0.509361   36.116211  240.938110    0.785608      0   \n",
       "  3  121.359283  272.789948  252.713898  443.482025    0.759905     26   \n",
       "  4  189.139801   22.767719  247.116791   86.406860    0.629091      0   \n",
       "  5   97.288635  113.023758  112.331848  148.970291    0.541746     67   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2      person  \n",
       "  3     handbag  \n",
       "  4      person  \n",
       "  5  cell phone  ,\n",
       "  'caption': ['A woman in a black and brown fur coat',\n",
       "   'the lady with the red pants and fur hood'],\n",
       "  'bbox_target': [31.64, 40.27, 222.92, 592.54]},\n",
       " 281: {'image_emb': tensor([[-0.1316,  0.1104, -0.5254,  ...,  1.0137,  0.0019, -0.2969],\n",
       "          [ 0.0131,  0.3130, -0.5103,  ...,  0.7896,  0.0771, -0.0676],\n",
       "          [-0.3083, -0.1486, -0.3262,  ...,  1.1133,  0.0817, -0.0691],\n",
       "          [-0.0471,  0.0478, -0.1416,  ...,  0.7710,  0.1797, -0.5059]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0374, -0.1149, -0.0839,  ..., -0.3328, -0.1099, -0.0518],\n",
       "          [-0.0563,  0.0450, -0.0784,  ...,  0.0267, -0.2566,  0.0709]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2625, 0.2280, 0.0115, 0.4980],\n",
       "          [0.3105, 0.6572, 0.0221, 0.0100]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  155.976120  297.176880  494.953796  424.761719    0.855553     13   \n",
       "  1  379.406555  257.541687  588.265991  374.603088    0.816411     13   \n",
       "  2  480.626648  236.768097  613.863464  307.591095    0.739749     13   \n",
       "  3    0.000000    0.884857  618.663696  357.888214    0.657876      6   \n",
       "  4    0.226814  337.853180   70.468002  424.795410    0.528100     56   \n",
       "  5  579.093628  138.964722  621.583008  243.574768    0.518736     58   \n",
       "  6  553.844543  186.237152  595.573181  241.736542    0.507767     58   \n",
       "  7  583.966309  189.329010  617.101562  243.165192    0.311568     58   \n",
       "  \n",
       "             name  \n",
       "  0         bench  \n",
       "  1         bench  \n",
       "  2         bench  \n",
       "  3         train  \n",
       "  4         chair  \n",
       "  5  potted plant  \n",
       "  6  potted plant  \n",
       "  7  potted plant  ,\n",
       "  'caption': ['Middle bench.', 'A full view of a brown plaid bench.'],\n",
       "  'bbox_target': [382.64, 255.09, 204.07, 116.46]},\n",
       " 282: {'image_emb': tensor([[ 0.1442,  0.2319, -0.3667,  ...,  1.1826,  0.3066, -0.1992],\n",
       "          [ 0.1025, -0.0176, -0.0672,  ...,  0.9023,  0.2158, -0.1914],\n",
       "          [-0.0671,  0.2158, -0.3755,  ...,  1.3643,  0.2690,  0.0343],\n",
       "          ...,\n",
       "          [ 0.0440,  0.3240, -0.5127,  ...,  1.2754, -0.1033,  0.0561],\n",
       "          [-0.2627,  0.2289, -0.2177,  ...,  1.3662, -0.0365, -0.0486],\n",
       "          [-0.2800,  0.0282,  0.0892,  ...,  0.6270,  0.0904,  0.2969]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0557, -0.1902, -0.0148,  ...,  0.2578, -0.2041, -0.0189],\n",
       "          [ 0.0766, -0.2671, -0.0072,  ...,  0.2321, -0.3662,  0.0385]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.5167e-02, 1.5676e-05, 1.0222e-04, 1.7881e-06, 7.3486e-02, 3.4571e-06,\n",
       "           6.0081e-05, 1.1623e-05, 6.1035e-05, 1.3542e-04, 6.1035e-05, 1.3466e-03,\n",
       "           9.0967e-01, 2.6226e-05],\n",
       "          [6.0320e-04, 2.3782e-05, 9.5427e-05, 4.0531e-06, 3.2928e-02, 1.5318e-05,\n",
       "           2.2900e-04, 6.7282e-04, 5.3215e-04, 1.5163e-03, 1.2445e-04, 7.8678e-04,\n",
       "           9.6240e-01, 2.4140e-05]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   151.914764   95.512054  367.534576  476.179321    0.947768      0   \n",
       "  1   369.360168  219.772278  600.516663  478.993652    0.919405      0   \n",
       "  2   271.205292  117.447357  363.083649  230.809875    0.880583      0   \n",
       "  3    91.557831  104.408539  225.788666  309.954376    0.852579      0   \n",
       "  4     0.492477  279.220428  171.449997  384.638885    0.823114     28   \n",
       "  5   409.608856    1.006729  638.774902  390.549194    0.816405      5   \n",
       "  6   330.127472  138.608368  437.228607  476.175293    0.798530      0   \n",
       "  7     0.181679  400.249542  101.362000  478.934753    0.795586     24   \n",
       "  8   471.324432  442.257019  507.897369  478.791931    0.793879     39   \n",
       "  9   380.004883   95.720352  399.360352  131.537827    0.781478      0   \n",
       "  10  339.500244  200.075073  423.848999  262.333618    0.774913     24   \n",
       "  11   30.450653  365.720947  173.393463  477.246521    0.720437     28   \n",
       "  12  158.836426  300.647705  228.712616  404.721924    0.704020     28   \n",
       "  13  358.366150   93.992432  371.251709  137.520355    0.681822      0   \n",
       "  14  350.620697   88.707642  360.352142  115.525940    0.671726      0   \n",
       "  15  326.980682   78.888336  350.959686   90.592163    0.636167      2   \n",
       "  16  535.118774  384.236694  639.491943  477.730835    0.625395      0   \n",
       "  17  176.004242  408.729767  240.243225  478.486572    0.607909     28   \n",
       "  18  409.242554   89.786728  419.876404  121.609970    0.606383      0   \n",
       "  19  438.185089  275.296478  633.759155  409.026459    0.592491     24   \n",
       "  20  348.569092  319.214142  402.947571  362.518951    0.570805     26   \n",
       "  21  401.202789  250.907959  413.332611  265.678528    0.521966     67   \n",
       "  22  573.308838   87.277969  633.186279  109.972733    0.461935      2   \n",
       "  23  357.092224   83.824173  379.070312   98.889786    0.402318      2   \n",
       "  24  389.219360  297.512024  546.445251  478.045532    0.385528     26   \n",
       "  25  173.695786   91.005173  183.363113  107.923477    0.377539      0   \n",
       "  26  600.028625  155.642441  640.000000  202.355316    0.375215      0   \n",
       "  27  204.774582   83.187012  239.369186   97.914459    0.341953      2   \n",
       "  28  564.604187  289.453369  634.429749  407.883423    0.321554     24   \n",
       "  29  399.447784   89.479507  408.293304  110.312149    0.273421      0   \n",
       "  \n",
       "            name  \n",
       "  0       person  \n",
       "  1       person  \n",
       "  2       person  \n",
       "  3       person  \n",
       "  4     suitcase  \n",
       "  5          bus  \n",
       "  6       person  \n",
       "  7     backpack  \n",
       "  8       bottle  \n",
       "  9       person  \n",
       "  10    backpack  \n",
       "  11    suitcase  \n",
       "  12    suitcase  \n",
       "  13      person  \n",
       "  14      person  \n",
       "  15         car  \n",
       "  16      person  \n",
       "  17    suitcase  \n",
       "  18      person  \n",
       "  19    backpack  \n",
       "  20     handbag  \n",
       "  21  cell phone  \n",
       "  22         car  \n",
       "  23         car  \n",
       "  24     handbag  \n",
       "  25      person  \n",
       "  26      person  \n",
       "  27         car  \n",
       "  28    backpack  \n",
       "  29      person  ,\n",
       "  'caption': ['a blue suitcase sitting on other items of luggage.',\n",
       "   'blue piece of luggage on top of pile.'],\n",
       "  'bbox_target': [3.24, 275.06, 165.03, 112.18]},\n",
       " 283: {'image_emb': tensor([[ 0.0515,  0.0699, -0.1052,  ...,  0.4285,  0.3582, -0.4126],\n",
       "          [-0.0189,  0.1892, -0.1277,  ...,  1.0254,  0.0666, -0.1385],\n",
       "          [-0.3884, -0.1182,  0.0240,  ...,  0.5903, -0.0698,  0.0043],\n",
       "          ...,\n",
       "          [ 0.1082,  0.1830, -0.0776,  ...,  0.8853,  0.1487,  0.2079],\n",
       "          [ 0.0963,  0.0696, -0.2341,  ...,  1.0537, -0.0764,  0.2314],\n",
       "          [-0.1825,  0.0409, -0.0873,  ...,  0.2485,  0.5786, -0.3418]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2053, -0.3042, -0.0985,  ...,  0.3953, -0.1990, -0.3252],\n",
       "          [ 0.1824, -0.1750, -0.2576,  ...,  0.0596, -0.0006, -0.5332]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.3213e-01, 1.0910e-03, 1.7881e-07, 4.1723e-06, 4.5685e-02, 1.3447e-04,\n",
       "           2.0920e-02],\n",
       "          [6.9873e-01, 1.1230e-01, 4.1723e-07, 2.3246e-06, 4.1321e-02, 1.0347e-03,\n",
       "           1.4648e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  168.991943   37.386963  426.291504  638.086731    0.950831      0  person\n",
       "  1    0.000000   93.189545  229.992371  638.243408    0.910025      0  person\n",
       "  2  325.269775   89.961319  379.514404  178.293198    0.892253      0  person\n",
       "  3  246.283051  377.977051  304.273407  477.710327    0.879764     41     cup\n",
       "  4  281.858002  218.555176  349.451996  279.213898    0.835934     53   pizza\n",
       "  5  149.977020  483.799225  235.831024  546.653931    0.778364     53   pizza,\n",
       "  'caption': ['woman with blonde hair eating pizza',\n",
       "   'A blond woman holding pizza.'],\n",
       "  'bbox_target': [1.44, 96.12, 227.23, 523.51]},\n",
       " 284: {'image_emb': tensor([[ 0.0086,  0.7515,  0.0503,  ...,  0.8335,  0.1978, -0.0279],\n",
       "          [ 0.1868,  0.5645,  0.1963,  ...,  1.2598,  0.1655, -0.1057],\n",
       "          [ 0.2571,  0.1394,  0.1696,  ...,  0.2288, -0.1316, -0.2085]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1407,  0.0047, -0.1298,  ...,  0.0383,  0.0107, -0.1376],\n",
       "          [-0.0526,  0.2098, -0.0306,  ...,  0.1353, -0.0774, -0.1666]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1827, 0.7456, 0.0715],\n",
       "          [0.1552, 0.3284, 0.5166]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  131.136200  276.781738  282.647491  443.098511    0.932809      0   \n",
       "  1  191.572815  445.731689  448.205933  523.756775    0.757899      0   \n",
       "  2   53.958618  430.714081  273.400787  485.543304    0.438398     37   \n",
       "  3  174.547241  496.194366  291.285522  521.080627    0.338522     37   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2  surfboard  \n",
       "  3  surfboard  ,\n",
       "  'caption': ['shirtless male laying on red board over water',\n",
       "   'The man who is laying on his red surfboard.'],\n",
       "  'bbox_target': [203.43, 446.91, 249.33, 77.18]},\n",
       " 285: {'image_emb': tensor([[-0.2205,  0.0965, -0.1055,  ...,  1.3633,  0.3152, -0.2286],\n",
       "          [-0.2961,  0.5420,  0.1576,  ...,  0.9834,  0.2106, -0.0590],\n",
       "          [-0.3418,  0.2773, -0.1199,  ...,  1.3223,  0.1869, -0.3818],\n",
       "          ...,\n",
       "          [ 0.1051, -0.1361, -0.2830,  ...,  0.7812,  0.0561, -0.1735],\n",
       "          [-0.2932,  0.3926, -0.1595,  ...,  0.9033,  0.1305, -0.0870],\n",
       "          [-0.1949,  0.2849, -0.1062,  ...,  0.8506, -0.0234, -0.1083]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1619,  0.0629, -0.2686,  ...,  0.0376,  0.1294, -0.4714],\n",
       "          [ 0.0467, -0.2581, -0.0062,  ..., -0.0043, -0.2566, -0.1508]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.6963e-05, 8.7695e-01, 2.3639e-04, 1.2250e-01, 5.9009e-06, 3.3557e-05,\n",
       "           5.0068e-06],\n",
       "          [5.5027e-04, 7.5562e-02, 1.6689e-03, 9.2041e-01, 5.7650e-04, 1.4277e-03,\n",
       "           6.1750e-05]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  255.201843   34.201080  445.243774  385.063812    0.935493      0  person\n",
       "  1   11.068443  148.615524  107.982811  315.863892    0.934524      0  person\n",
       "  2  116.442352  147.725250  547.954102  447.043823    0.931536     17   horse\n",
       "  3    0.188343  186.336517  100.508224  422.572632    0.874958     17   horse\n",
       "  4    0.000000  319.521942   42.535759  448.014160    0.772419      0  person\n",
       "  5  204.651993  150.343231  303.388428  403.723114    0.767641     17   horse\n",
       "  6  315.068329  154.719681  350.134064  212.130402    0.634434      0  person,\n",
       "  'caption': ['White horse being ridden by a child', 'A white horse.'],\n",
       "  'bbox_target': [0.0, 186.4, 94.85, 239.05]},\n",
       " 286: {'image_emb': tensor([[ 0.0797,  0.3328, -0.1266,  ...,  0.8564,  0.0878, -0.2791],\n",
       "          [-0.2195,  0.3701, -0.0567,  ...,  1.3867,  0.1259, -0.4060],\n",
       "          [-0.0334,  0.2585, -0.1777,  ...,  0.9121, -0.1401, -0.2734],\n",
       "          ...,\n",
       "          [-0.0622,  0.2238, -0.2549,  ...,  1.4053, -0.1736, -0.2766],\n",
       "          [-0.1039,  0.4612, -0.0059,  ...,  1.0811, -0.0856, -0.2576],\n",
       "          [ 0.1637,  0.3289, -0.0448,  ...,  0.4443,  0.1251,  0.0212]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0648,  0.4299, -0.4832,  ...,  0.4578, -0.0314, -0.0431],\n",
       "          [ 0.0123,  0.1666, -0.3062,  ...,  0.1794,  0.0264,  0.2338]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.9174e-05, 0.0000e+00, 1.4061e-02, 0.0000e+00, 9.8584e-01, 2.9802e-07,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 1.1921e-07, 0.0000e+00, 0.0000e+00, 5.3644e-07],\n",
       "          [2.3022e-03, 6.4945e-04, 7.5830e-01, 5.3287e-05, 2.3486e-01, 1.9014e-05,\n",
       "           3.0975e-03, 1.9312e-05, 5.3644e-07, 2.1875e-05, 1.5497e-05, 4.0531e-06,\n",
       "           1.3399e-04, 6.3300e-05, 4.0650e-04, 9.5010e-05, 9.0659e-05]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   303.484131  119.285965  574.280640  423.539612    0.946452      0   \n",
       "  1   209.068817   19.307739  378.770355  116.100098    0.943904      2   \n",
       "  2   239.564667   95.370773  447.900909  287.654297    0.942184      0   \n",
       "  3   380.653442   16.413925  480.462463   65.109604    0.916813      2   \n",
       "  4     0.000000  130.673187  171.514374  424.440552    0.900585      0   \n",
       "  5    95.395493  290.829803  134.714401  341.819000    0.890921     41   \n",
       "  6   556.215637  155.076355  639.487366  422.083557    0.890741      0   \n",
       "  7    56.354649    0.000000  113.306152   43.341904    0.881132      2   \n",
       "  8   187.935699  255.510223  214.226501  331.401520    0.878253     39   \n",
       "  9   430.890076   47.409393  640.000000  130.862061    0.866131      2   \n",
       "  10  334.208008  240.499176  364.779114  283.571869    0.864075     41   \n",
       "  11  158.660187  250.717804  188.510010  329.502228    0.842313     39   \n",
       "  12   56.994816   25.079651  113.245758   94.486572    0.823668      2   \n",
       "  13  373.056793  271.796448  406.159821  313.755432    0.811738     41   \n",
       "  14  602.671326   16.516357  639.875549   75.001190    0.807844      2   \n",
       "  15  343.704376    4.738754  410.214203   53.833572    0.794392      2   \n",
       "  16   58.483887    3.953156  273.519928   94.434326    0.649480      2   \n",
       "  17    3.228455  277.276764  412.227173  422.239624    0.645633     60   \n",
       "  18  171.019424  329.717743  221.449936  343.724640    0.631499     67   \n",
       "  19  267.173889    0.000000  345.334106   24.502106    0.627670      2   \n",
       "  20  209.951752    5.173325  274.098175   48.453506    0.572551      2   \n",
       "  21  607.884399    5.023117  640.000000   30.050110    0.304732      2   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1            car  \n",
       "  2         person  \n",
       "  3            car  \n",
       "  4         person  \n",
       "  5            cup  \n",
       "  6         person  \n",
       "  7            car  \n",
       "  8         bottle  \n",
       "  9            car  \n",
       "  10           cup  \n",
       "  11        bottle  \n",
       "  12           car  \n",
       "  13           cup  \n",
       "  14           car  \n",
       "  15           car  \n",
       "  16           car  \n",
       "  17  dining table  \n",
       "  18    cell phone  \n",
       "  19           car  \n",
       "  20           car  \n",
       "  21           car  ,\n",
       "  'caption': ['A fat old man wearing blue shirt and with a brown cup at a dining table.',\n",
       "   'a fat man in a blue shirt'],\n",
       "  'bbox_target': [234.56, 97.65, 211.59, 193.4]},\n",
       " 287: {'image_emb': tensor([[-0.0865,  0.4187, -0.0676,  ...,  0.7607,  0.0924, -0.2001],\n",
       "          [ 0.0839,  0.5054, -0.1779,  ...,  0.8232,  0.1481, -0.0883],\n",
       "          [-0.1225,  0.2493,  0.0146,  ...,  0.8804,  0.1472, -0.1644],\n",
       "          [-0.0767,  0.4766, -0.0803,  ...,  0.2422,  0.0846, -0.5010]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1763,  0.1664, -0.0582,  ..., -0.0669,  0.1830, -0.3335],\n",
       "          [-0.4150,  0.2126, -0.1395,  ..., -0.1404,  0.0746, -0.6489]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5176, 0.0224, 0.3728, 0.0872],\n",
       "          [0.1548, 0.0606, 0.3770, 0.4077]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  313.127930  261.983582  479.000000  405.974426    0.870062     49  orange\n",
       "  1    0.000000  269.005096  154.884308  407.435699    0.858670     49  orange\n",
       "  2  142.365616  263.461700  322.716492  414.489105    0.847161     49  orange\n",
       "  3  190.683014  145.015915  390.525177  198.940140    0.665492     47   apple\n",
       "  4    3.222305  402.463318  479.000000  625.785461    0.483883     49  orange\n",
       "  5   66.117691  153.720398  167.389206  197.588898    0.345061     47   apple\n",
       "  6    0.000000  411.443512  121.248291  626.400391    0.258925     49  orange,\n",
       "  'caption': ['The container of oranges on the right of the other containers',\n",
       "   'The basket of oranges on the right of the large orange sign.'],\n",
       "  'bbox_target': [314.23, 261.54, 164.77, 145.74]},\n",
       " 288: {'image_emb': tensor([[ 0.0715,  0.1293, -0.1906,  ...,  0.9873,  0.1393,  0.2035],\n",
       "          [-0.0553, -0.3076, -0.0282,  ...,  0.4653,  0.2632, -0.5273],\n",
       "          [-0.1003, -0.0098, -0.2366,  ...,  0.9678, -0.0220, -0.2228],\n",
       "          [-0.0500, -0.1556, -0.2542,  ...,  1.2598,  0.2352,  0.2340],\n",
       "          [-0.0807,  0.3630, -0.3889,  ...,  1.0234,  0.0453, -0.2295],\n",
       "          [-0.0961, -0.3669, -0.1458,  ..., -0.1378,  0.0407,  0.0438]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2037, -0.0974, -0.1056,  ...,  0.0232, -0.1572,  0.2167],\n",
       "          [ 0.0408, -0.1814, -0.1847,  ..., -0.2206,  0.0508,  0.2810]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.4355e-01, 4.9710e-05, 3.1348e-01, 4.2419e-02, 6.2823e-05, 5.7745e-04],\n",
       "          [9.8389e-01, 3.1650e-05, 2.2907e-03, 8.6975e-04, 1.7881e-07, 1.2779e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  387.263611  316.441833  544.643616  477.104980    0.932799      0   person\n",
       "  1  273.128143   89.813477  416.729950  459.618225    0.926629     23  giraffe\n",
       "  2    0.643326  228.128723  103.851112  478.114197    0.899753      0   person\n",
       "  3  539.301514  272.454865  639.666260  476.631836    0.896983      0   person\n",
       "  4  563.243713  155.829407  625.367126  293.547119    0.764978     23  giraffe,\n",
       "  'caption': ['half of the back of a head with medium length blond hair',\n",
       "   'A boy is sitting in the back watching the animals.'],\n",
       "  'bbox_target': [537.17, 292.31, 102.47, 181.22]},\n",
       " 289: {'image_emb': tensor([[-0.2314,  0.5342, -0.1774,  ...,  1.1240, -0.3274, -0.0536],\n",
       "          [ 0.0186,  0.0592, -0.1237,  ...,  1.2539,  0.3755,  0.1038],\n",
       "          [ 0.2251,  0.4695, -0.2473,  ...,  1.0840,  0.4050,  0.2178],\n",
       "          ...,\n",
       "          [-0.0766,  0.3367, -0.1227,  ...,  1.3262, -0.0355, -0.1655],\n",
       "          [ 0.2666,  0.1392, -0.3164,  ...,  0.5737,  0.3708,  0.1849],\n",
       "          [ 0.3689, -0.1276, -0.2798,  ...,  0.7754,  0.3845,  0.1262]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1259,  0.0111, -0.3022,  ...,  0.5845,  0.0422, -0.0365],\n",
       "          [ 0.2002, -0.1393,  0.0075,  ...,  0.0670,  0.2037, -0.3213]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.5977e-04, 8.1421e-02, 1.4294e-01, 7.7246e-01, 1.0371e-05, 1.1325e-06,\n",
       "           2.6822e-06, 3.3283e-04, 9.3317e-04, 1.5383e-03],\n",
       "          [2.1911e-04, 5.9204e-03, 1.2535e-02, 4.1748e-02, 7.5102e-06, 1.4427e-02,\n",
       "           3.5763e-06, 8.8513e-05, 5.5859e-01, 3.6646e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   253.119232  440.209869  478.000000  603.659546    0.937306     42   \n",
       "  1     0.706375    5.316010  300.594971  360.906769    0.920779      0   \n",
       "  2    18.079407  251.606750  139.246918  421.969971    0.920297     41   \n",
       "  3   156.919708  230.560104  258.694916  357.338684    0.907621     41   \n",
       "  4   386.645630  237.577179  454.234863  335.607452    0.897019     41   \n",
       "  5   124.639023  379.619904  331.168701  516.252075    0.893791     53   \n",
       "  6   387.783905  419.294586  477.811584  467.568451    0.818430     43   \n",
       "  7   327.904266  211.561371  478.000000  291.845490    0.812418     56   \n",
       "  8     0.000000  277.375793  475.860840  636.628479    0.724900     60   \n",
       "  9     0.095329  559.498047   80.807892  639.813354    0.596268     56   \n",
       "  10  401.653259    0.466553  477.795593  290.462921    0.592671     58   \n",
       "  11  185.598328  208.383301  256.365234  291.478333    0.355356     56   \n",
       "  12   62.148560  352.989746  409.360352  538.802368    0.311119     60   \n",
       "  \n",
       "              name  \n",
       "  0           fork  \n",
       "  1         person  \n",
       "  2            cup  \n",
       "  3            cup  \n",
       "  4            cup  \n",
       "  5          pizza  \n",
       "  6          knife  \n",
       "  7          chair  \n",
       "  8   dining table  \n",
       "  9          chair  \n",
       "  10  potted plant  \n",
       "  11         chair  \n",
       "  12  dining table  ,\n",
       "  'caption': ['mug half full of beer sitting on the left of table',\n",
       "   'Half full mug of beer to the left of the pizza.'],\n",
       "  'bbox_target': [17.22, 250.83, 121.97, 173.64]},\n",
       " 290: {'image_emb': tensor([[-1.9971e-01,  7.0435e-02, -1.9421e-01,  ...,  9.4482e-01,\n",
       "            3.4766e-01,  6.2561e-03],\n",
       "          [ 2.8839e-02, -1.8677e-02, -3.0518e-01,  ...,  7.3828e-01,\n",
       "            4.4238e-01, -5.4657e-02],\n",
       "          [-9.3140e-02,  2.9346e-01, -1.4404e-01,  ...,  9.9365e-01,\n",
       "            2.5488e-01, -3.3545e-01],\n",
       "          ...,\n",
       "          [-5.4657e-02,  2.0312e-01, -1.8665e-01,  ...,  9.0088e-01,\n",
       "            3.2178e-01, -2.2522e-01],\n",
       "          [-1.6809e-01, -2.5010e-04, -3.0322e-01,  ...,  6.4844e-01,\n",
       "            3.9893e-01, -1.7102e-01],\n",
       "          [-2.7466e-01,  7.3547e-02, -1.1285e-01,  ...,  6.4404e-01,\n",
       "            7.0007e-02, -1.0077e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-1.5717e-02, -1.0483e-02,  3.0908e-01,  1.8640e-01, -1.8066e-01,\n",
       "           -8.5571e-02, -2.3096e-01, -3.8013e-01, -3.2007e-01, -1.0089e-01,\n",
       "            1.6553e-01,  3.1189e-02,  2.0496e-01,  3.4595e-01,  4.5624e-02,\n",
       "           -2.1960e-01, -1.3885e-02,  5.6934e-01, -4.2328e-02,  5.6250e-01,\n",
       "            1.2611e-02,  2.9785e-01,  1.1914e-01,  4.3408e-01,  5.4395e-01,\n",
       "            3.6792e-01, -1.2610e-01, -1.0309e-01,  8.7128e-03,  2.8833e-01,\n",
       "           -1.7871e-01,  2.1912e-02, -4.2163e-01,  3.5278e-01, -2.4207e-01,\n",
       "           -6.1035e-02,  5.9113e-02, -2.6538e-01, -1.8787e-01, -6.9695e-03,\n",
       "            3.4058e-02, -3.4790e-03, -1.3037e-01,  1.6638e-01, -2.9297e-03,\n",
       "            2.6416e-01,  6.1035e-01,  1.0272e-01, -4.1779e-02, -3.1030e-01,\n",
       "           -2.1704e-01, -4.1724e-01, -2.8589e-01, -5.4871e-02, -2.4280e-01,\n",
       "            1.4136e-01,  1.0632e-01, -2.4304e-01, -8.6060e-02,  2.5317e-01,\n",
       "           -5.1758e-02, -1.4148e-01, -1.9763e-01, -3.5742e-01, -5.0146e-01,\n",
       "            1.7395e-01, -4.8608e-01,  7.1777e-01, -1.9516e-02, -7.0984e-02,\n",
       "            3.3252e-01, -2.0630e-01,  3.0151e-01,  1.3818e-01,  2.9321e-01,\n",
       "            9.5642e-02, -2.9297e-01, -1.2720e-01,  6.1401e-02, -4.7729e-02,\n",
       "            2.8763e-02,  4.8340e-01,  1.9263e-01,  2.8589e-01,  8.3984e-02,\n",
       "            2.8638e-01,  1.3684e-01,  2.3242e-01,  1.4502e-01, -4.2229e-03,\n",
       "            1.7347e-03, -2.3303e-01, -9.3359e-01,  4.3921e-01, -5.2246e-01,\n",
       "           -2.0554e-02, -1.3525e-01, -1.7920e-01, -4.4739e-02,  6.4392e-02,\n",
       "            3.8184e-01, -3.3779e-03, -3.5913e-01,  1.3452e-01, -2.5903e-01,\n",
       "            6.7688e-02,  5.1918e-03,  6.3515e-03, -3.2422e-01, -2.1191e-01,\n",
       "           -2.5537e-01,  2.8027e-01,  1.5976e-02,  3.5461e-02, -1.0687e-01,\n",
       "           -1.4380e-01,  2.6050e-01,  4.6234e-02, -1.5833e-01, -2.0950e-02,\n",
       "            1.6516e-01, -5.9912e-01, -1.9812e-01, -3.9380e-01,  2.9221e-02,\n",
       "            2.1436e-01,  1.5869e-01, -1.3440e-01, -2.7734e-01,  2.6978e-01,\n",
       "           -1.7078e-01, -1.9666e-01,  3.8574e-01,  3.7734e+00,  2.6978e-01,\n",
       "            3.4729e-02, -2.2961e-01, -4.0186e-01, -1.9177e-01, -6.2500e-01,\n",
       "            7.3926e-01,  3.7427e-01, -9.4147e-03,  3.4668e-01,  1.3330e-01,\n",
       "           -2.1765e-01, -2.7710e-01,  9.9609e-02,  4.7217e-01,  5.9326e-02,\n",
       "           -7.1680e-01, -1.6956e-01, -6.5552e-02,  1.1987e-01,  3.4155e-01,\n",
       "           -2.3474e-01,  1.9153e-01, -6.7688e-02, -4.4409e-01,  2.1790e-02,\n",
       "           -1.3086e-01, -6.8604e-02,  1.4209e-01,  2.0898e-01, -6.4209e-02,\n",
       "           -4.8804e-01, -1.0925e-01, -2.0911e-01, -1.0400e-01, -8.5297e-03,\n",
       "            3.4326e-01,  1.4514e-01,  2.0203e-01,  1.7548e-02, -2.5586e-01,\n",
       "            9.4238e-01, -1.3586e-01, -1.6333e-01,  5.4639e-01,  1.4294e-01,\n",
       "            2.8320e-02, -9.1171e-03, -3.5938e-01, -2.8052e-01,  3.3936e-01,\n",
       "           -5.3406e-02, -3.6768e-01,  3.2715e-01,  4.0088e-01, -1.1450e-01,\n",
       "            1.5845e-01,  2.0349e-01,  2.1460e-01,  1.8420e-01,  2.3206e-01,\n",
       "            2.2144e-01, -3.7646e-01,  1.4697e-01, -1.8936e-02, -5.1910e-02,\n",
       "            1.4061e-02, -2.9761e-01, -1.3696e-01,  2.8345e-01, -3.3789e-01,\n",
       "           -1.9604e-01,  7.7344e-01,  5.4871e-02, -2.2156e-01,  1.8616e-01,\n",
       "            4.1534e-02, -1.2830e-01,  2.3218e-01,  4.5044e-02, -1.9568e-01,\n",
       "            3.9429e-01, -5.4590e-01, -1.2091e-01, -4.9500e-02, -2.3022e-01,\n",
       "            2.2888e-01,  3.7769e-01, -3.1934e-01,  7.3120e-02,  4.4067e-01,\n",
       "           -1.9165e-01, -4.1284e-01, -4.0479e-01, -8.3435e-02, -1.4343e-01,\n",
       "            1.7859e-01,  1.2976e-01,  4.4653e-01, -3.5461e-02,  4.5728e-01,\n",
       "            1.4124e-01,  7.0117e-01,  4.9976e-01,  1.4331e-01,  2.7295e-01,\n",
       "            1.5356e-01,  4.3042e-01, -4.9463e-01, -6.3110e-02,  1.3940e-01,\n",
       "            5.3284e-02, -2.1509e-01, -1.9739e-01, -2.2290e-01, -4.2188e-01,\n",
       "            2.3895e-02, -2.0166e-01,  6.7566e-02, -2.1960e-01, -1.5149e-01,\n",
       "           -1.9885e-01, -2.3804e-01,  1.3077e-02, -1.2988e-01,  2.7100e-01,\n",
       "           -2.2888e-01,  1.0699e-01,  2.3853e-01, -7.0923e-02, -6.1066e-02,\n",
       "           -1.9055e-01,  4.6997e-01, -1.4844e-01, -4.2676e-01, -2.2827e-01,\n",
       "           -3.5400e-01, -2.0248e-02, -8.6975e-03, -8.1665e-02, -1.4099e-01,\n",
       "            6.0596e-01,  5.9204e-02, -3.6583e-03,  2.2754e-01, -1.7807e-02,\n",
       "            8.2092e-02, -3.3691e-01,  2.0020e-01,  2.3962e-01, -1.2866e-01,\n",
       "           -9.7473e-02,  9.0881e-02,  7.6599e-02,  2.6367e-01, -3.3057e-01,\n",
       "            3.0542e-01, -8.1116e-02,  9.6083e-05,  3.1665e-01,  2.6172e-01,\n",
       "           -1.6504e-01,  2.7695e-02, -1.5588e-01, -5.4590e-01, -2.4939e-01,\n",
       "           -1.8799e-01,  6.9458e-02, -2.7939e-02, -2.7979e-01, -3.8281e-01,\n",
       "            1.7004e-01, -8.5327e-02, -1.7261e-01, -5.8777e-02, -4.6191e-01,\n",
       "           -7.3547e-02,  1.3586e-01,  3.7676e+00, -1.4893e-02,  2.5269e-01,\n",
       "            2.4683e-01,  7.7209e-02, -5.0232e-02,  3.3813e-01,  1.5649e-01,\n",
       "            3.9136e-01,  8.1482e-02,  4.4823e-03, -8.6121e-02, -1.1554e-01,\n",
       "            1.6235e-01,  4.9902e-01,  1.8921e-01,  4.0308e-01, -1.3281e+00,\n",
       "           -2.3438e-01, -3.1494e-01,  3.2043e-02,  7.1594e-02,  2.7197e-01,\n",
       "           -3.0859e-01,  1.5930e-02,  9.5703e-02, -5.8441e-02,  4.1919e-01,\n",
       "           -1.0663e-01,  4.4189e-01, -1.6467e-01,  4.0588e-02,  1.0382e-01,\n",
       "           -9.3506e-02,  5.4004e-01, -3.7012e-01, -4.4873e-01, -3.7085e-01,\n",
       "           -1.0956e-02,  2.3364e-01,  5.2948e-02,  4.9683e-02,  2.3438e-01,\n",
       "           -3.1567e-01, -3.8745e-01, -2.0361e-01,  1.8994e-01,  4.0283e-01,\n",
       "            1.4221e-01, -8.6212e-03, -4.9609e-01,  4.2212e-01,  1.6895e-01,\n",
       "           -2.6733e-02, -1.8542e-01,  2.9272e-01, -1.0315e-01,  7.0129e-02,\n",
       "            1.8958e-01,  1.1731e-01, -1.9775e-02, -5.7678e-03,  3.0624e-02,\n",
       "            9.3994e-02, -6.4659e-03, -3.0398e-04,  4.9225e-02, -3.2129e-01,\n",
       "            1.1330e-02, -5.2393e-01,  1.7395e-01, -1.9727e-01,  6.1737e-02,\n",
       "           -1.9836e-02,  1.2520e-02,  1.8787e-01, -1.4746e-01, -1.9202e-01,\n",
       "           -7.3242e-01,  2.0984e-01,  5.9631e-02,  7.1289e-02,  1.8921e-01,\n",
       "            5.8716e-02,  3.1830e-02,  2.0950e-02, -8.8135e-02,  4.8315e-01,\n",
       "           -3.6285e-02, -3.2886e-01,  4.3091e-01,  2.4670e-01,  1.1371e-01,\n",
       "           -4.2065e-01, -3.5205e-01,  3.8055e-02,  5.0439e-01,  1.0429e-02,\n",
       "           -1.7567e-03,  3.2812e-01,  2.0068e-01,  4.3311e-01, -1.5723e-01,\n",
       "            4.0112e-01, -2.9395e-01,  1.4136e-01, -1.0254e-01, -2.2559e-01,\n",
       "           -1.4191e-02, -5.4980e-01, -4.1138e-01, -2.6953e-01,  2.3560e-01,\n",
       "            9.4788e-02, -1.2756e-01, -7.6416e-02, -4.3610e-02, -1.3562e-01,\n",
       "           -3.8208e-01, -2.0789e-01,  2.0361e-01,  3.8574e-01, -1.5271e-01,\n",
       "           -2.6123e-01, -9.7046e-02,  3.3862e-01, -1.9763e-01,  3.0556e-03,\n",
       "           -2.0618e-01, -1.2537e-01,  1.2805e-01,  9.9731e-02, -1.4673e-01,\n",
       "            6.7261e-02,  3.4180e-01, -5.0751e-02,  1.6003e-01,  5.5878e-02,\n",
       "           -1.8713e-01, -3.1641e-01,  4.6814e-02,  5.8350e-02,  1.5588e-01,\n",
       "            9.3933e-02, -1.2329e-02, -1.3489e-02, -1.8994e-01, -2.1851e-01,\n",
       "            1.0187e-01, -1.4197e-01, -3.4277e-01,  5.4199e-02,  8.6426e-01,\n",
       "           -3.5742e-01, -1.6272e-01,  3.3203e-01, -5.8929e-02,  4.5563e-02,\n",
       "            2.0325e-01,  1.7432e-01,  2.6318e-01,  1.1774e-01,  1.0089e-01,\n",
       "           -6.8420e-02, -1.2018e-01,  5.9180e-01, -2.5366e-01,  3.7256e-01,\n",
       "           -3.1885e-01, -2.7612e-01, -2.6978e-01,  3.7012e-01, -2.8125e-01,\n",
       "           -1.6943e-01,  3.0225e-01, -1.0956e-01,  5.0171e-02, -2.3047e-01,\n",
       "            3.5718e-01, -1.1371e-01, -2.5439e-01,  1.0889e+00,  1.7322e-01,\n",
       "           -1.1584e-01,  3.7402e-01,  3.6963e-01, -2.5223e-02, -9.0332e-02,\n",
       "            4.4922e-01,  6.0059e-02,  1.1670e-01,  3.2275e-01,  2.9099e-02,\n",
       "           -5.4443e-01,  2.7832e-02,  1.1635e-02,  2.9590e-01,  3.7549e-01,\n",
       "           -2.9565e-01, -2.8369e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.6654e-03, 2.8443e-04, 5.5389e-03, 2.1019e-03, 1.2878e-02, 4.8103e-03,\n",
       "           2.2119e-01, 7.4854e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   392.511139   85.165894  516.693115  218.594543    0.906470     41   \n",
       "  1     0.077443   69.302292   94.931541  163.320496    0.884847     41   \n",
       "  2   120.880791   13.191266  251.440186  128.695663    0.862885     41   \n",
       "  3   502.177673  169.101288  600.390808  275.162415    0.826722     41   \n",
       "  4   529.106689  370.707153  611.880310  448.271912    0.756793     46   \n",
       "  5   226.103088   67.998062  308.063660  147.603210    0.739031     41   \n",
       "  6   512.663147  312.124359  592.638428  378.156311    0.716609     46   \n",
       "  7   397.367920  276.392456  514.141846  446.035370    0.657117     46   \n",
       "  8     1.093480  129.740036  324.034973  268.744354    0.599721     53   \n",
       "  9     2.199830   35.024872  611.066284  598.102417    0.571779     60   \n",
       "  10  366.172150   95.221466  412.030884  168.249771    0.571075     41   \n",
       "  11  381.073456    1.422877  610.821472  177.066620    0.564884      0   \n",
       "  12  190.709122   12.563161  345.045135   52.936588    0.480248     53   \n",
       "  \n",
       "              name  \n",
       "  0            cup  \n",
       "  1            cup  \n",
       "  2            cup  \n",
       "  3            cup  \n",
       "  4         banana  \n",
       "  5            cup  \n",
       "  6         banana  \n",
       "  7         banana  \n",
       "  8          pizza  \n",
       "  9   dining table  \n",
       "  10           cup  \n",
       "  11        person  \n",
       "  12         pizza  ,\n",
       "  'caption': ['Cut bananas on top of a stack of pancakes'],\n",
       "  'bbox_target': [395.61, 265.51, 117.87, 178.96]},\n",
       " 291: {'image_emb': tensor([[-0.2373,  0.0837, -0.1917,  ...,  0.5703,  0.3101,  0.0304],\n",
       "          [ 0.1354,  0.4424, -0.3833,  ...,  0.3977,  0.1984, -0.0706],\n",
       "          [-0.2355,  0.1271, -0.4734,  ...,  0.5142,  0.2603, -0.1157],\n",
       "          [-0.0210,  0.4541, -0.3452,  ...,  0.4500,  0.0419,  0.0701],\n",
       "          [-0.1659,  0.1521,  0.0966,  ...,  0.7109,  0.2849, -0.3337]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2656,  0.3130, -0.1088,  ..., -0.1591,  0.1887, -0.2537],\n",
       "          [ 0.2086,  0.5762, -0.4016,  ..., -0.1276,  0.1221, -0.1097]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.3340e-01, 1.9765e-04, 7.6962e-04, 2.6562e-01, 1.4544e-05],\n",
       "          [7.0654e-01, 5.0974e-04, 3.1700e-03, 2.8979e-01, 1.0270e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   511.681793   46.733154  603.469788  295.434204    0.866818     39  bottle\n",
       "  1   278.128906  230.027847  393.113342  301.665863    0.765985     46  banana\n",
       "  2   389.486786  261.296173  455.125824  302.037842    0.755084     49  orange\n",
       "  3   443.036499  109.803558  558.812561  301.588776    0.749126     41     cup\n",
       "  4     8.318321  177.954208  147.378937  299.701447    0.689311     69    oven\n",
       "  5   172.203400    7.943889  301.906616  151.745468    0.658025     39  bottle\n",
       "  6    22.011496  133.770279  123.242607  223.128952    0.586197     45    bowl\n",
       "  7   412.044586   81.846657  501.051636  233.942841    0.565960     41     cup\n",
       "  8   443.442322  110.847969  556.890137  300.958588    0.388545     39  bottle\n",
       "  9   126.088737    8.344552  217.575668  110.930016    0.294589     39  bottle\n",
       "  10    8.371849    8.215128   64.832764  112.840866    0.258727     39  bottle,\n",
       "  'caption': ['PLASTIC CUP WITH RED LID', 'glass with red top'],\n",
       "  'bbox_target': [444.88, 111.0, 113.04, 193.49]},\n",
       " 292: {'image_emb': tensor([[ 0.1345,  0.4785, -0.2744,  ...,  1.0391,  0.0840, -0.3240],\n",
       "          [ 0.1367,  0.3818, -0.3489,  ...,  0.9331,  0.0707, -0.0745],\n",
       "          [ 0.2041,  0.2428, -0.2534,  ...,  0.9160, -0.3650, -0.3481],\n",
       "          ...,\n",
       "          [-0.1931,  0.1289, -0.1069,  ...,  1.1621,  0.2257, -0.0547],\n",
       "          [ 0.2566,  0.4846, -0.3103,  ...,  1.1230, -0.2778, -0.1316],\n",
       "          [-0.1798,  0.1238, -0.3013,  ...,  0.6948, -0.1884, -0.4541]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0389, -0.2368, -0.3496,  ..., -0.0411, -0.4934,  0.1241],\n",
       "          [-0.0782,  0.0247, -0.4534,  ..., -0.1749, -0.2196,  0.0404]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.8369e-01, 1.1415e-03, 2.6550e-03, 0.0000e+00, 1.7681e-03, 5.3644e-06,\n",
       "           2.7299e-05, 2.1094e-01],\n",
       "          [9.2334e-01, 1.2636e-03, 2.9850e-03, 5.9605e-08, 2.2526e-03, 5.9605e-07,\n",
       "           1.5914e-05, 7.0129e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   165.844849    0.000000  363.980164  335.600708    0.948276      0   \n",
       "  1   359.057831    0.000000  482.948090  227.242676    0.925306      0   \n",
       "  2   238.296021  218.721802  416.502136  351.558350    0.899272     55   \n",
       "  3    20.882587   30.574127  144.880569  211.597015    0.861560     58   \n",
       "  4   193.900833  221.778503  639.065369  355.394958    0.844327     60   \n",
       "  5   505.553497  284.158661  585.676392  336.734100    0.800751     45   \n",
       "  6   595.272705  281.256287  640.000000  338.566406    0.778569     45   \n",
       "  7   420.658478  228.916809  452.337616  257.175842    0.598111     42   \n",
       "  8   292.773376  202.940308  311.529602  224.913025    0.441723     43   \n",
       "  9   518.833557  233.309814  548.621887  266.765686    0.359169     42   \n",
       "  10  529.562012  230.328735  566.978027  265.224915    0.346338     42   \n",
       "  11  512.791321  235.911346  535.127625  269.547699    0.337572     42   \n",
       "  12  292.633423  207.969025  349.594543  243.239227    0.305106     43   \n",
       "  13  548.834656  228.482819  591.498962  264.641327    0.287730     42   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2           cake  \n",
       "  3   potted plant  \n",
       "  4   dining table  \n",
       "  5           bowl  \n",
       "  6           bowl  \n",
       "  7           fork  \n",
       "  8          knife  \n",
       "  9           fork  \n",
       "  10          fork  \n",
       "  11          fork  \n",
       "  12         knife  \n",
       "  13          fork  ,\n",
       "  'caption': ['A girl in blue cuts a cake.',\n",
       "   'A woman cutting a cake wearing turqoise.'],\n",
       "  'bbox_target': [166.28, 1.71, 199.37, 334.17]},\n",
       " 293: {'image_emb': tensor([[-0.2605, -0.1230,  0.1974,  ...,  0.0814,  0.0770, -0.0285],\n",
       "          [-0.0650,  0.3743,  0.1978,  ...,  1.1523,  0.2671, -0.0673],\n",
       "          [-0.4089, -0.0303,  0.1098,  ...,  0.1427,  0.3311, -0.1407]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0024, -0.2620, -0.2603,  ..., -0.0760, -0.0038, -0.1752],\n",
       "          [ 0.0847, -0.0401, -0.2048,  ...,  0.1782, -0.1135,  0.1222]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.4341, 0.0815, 0.4844],\n",
       "          [0.0961, 0.0064, 0.8975]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  241.424911  166.903046  501.525513  637.973755    0.929722     17  horse\n",
       "  1  211.432007  219.913330  305.796814  548.234985    0.793419     17  horse\n",
       "  2    1.045311  250.024551   34.455246  268.349762    0.307706      2    car,\n",
       "  'caption': ['White horse standing behind the black horse',\n",
       "   'Horse standing behind another, along a fence.'],\n",
       "  'bbox_target': [212.85, 215.73, 87.73, 330.79]},\n",
       " 294: {'image_emb': tensor([[-0.0920, -0.3425, -0.0908,  ...,  0.4531,  0.3987, -0.1236],\n",
       "          [-0.3020,  0.1328, -0.2637,  ...,  0.9541,  0.2476,  0.1792],\n",
       "          [-0.2581,  0.0616,  0.1683,  ...,  1.0254,  0.1952,  0.0519],\n",
       "          [ 0.0342, -0.1675, -0.0111,  ...,  0.4043,  0.0544, -0.0541]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-3.1055e-01, -4.9072e-01, -4.4281e-02, -9.6191e-02,  2.1240e-01,\n",
       "            2.3279e-01, -2.0703e-01, -6.0840e-01, -4.9365e-01,  3.0737e-01,\n",
       "           -1.0614e-01,  2.2070e-01,  3.2349e-01, -3.3105e-01, -7.8125e-03,\n",
       "            3.7994e-02, -1.6602e-01,  1.6809e-01, -7.6965e-02, -2.5543e-02,\n",
       "            4.7656e-01,  5.5029e-01, -5.6201e-01,  1.0730e-01, -1.5369e-01,\n",
       "           -3.0444e-01, -1.9116e-01,  1.0883e-01,  7.1167e-02, -3.6987e-01,\n",
       "            2.0166e-01, -7.9529e-02, -2.0935e-01, -4.0845e-01,  2.9358e-02,\n",
       "           -3.5083e-01, -3.1830e-02,  1.4075e-01, -2.9175e-01,  9.9243e-02,\n",
       "            5.8014e-02,  4.0625e-01, -3.1421e-01,  3.5278e-02, -1.4978e-01,\n",
       "            1.1963e-01, -3.4473e-01, -7.9041e-02, -1.1932e-01, -5.2002e-01,\n",
       "            1.5076e-01,  4.3457e-01,  9.0759e-02,  2.6050e-01, -2.0325e-01,\n",
       "           -5.4443e-01,  4.5258e-02,  5.5267e-02, -1.8188e-01,  5.3516e-01,\n",
       "            5.0830e-01, -1.9263e-01,  1.7395e-01, -1.7114e-01, -7.3242e-02,\n",
       "           -2.3035e-01,  2.1423e-01,  8.7402e-01, -2.6514e-01, -1.9934e-01,\n",
       "            8.0200e-02,  1.1072e-01, -1.2317e-01,  3.7109e-01, -8.8074e-02,\n",
       "            5.6061e-02, -2.1155e-01,  2.5171e-01,  1.0718e-01, -2.7344e-01,\n",
       "            3.3569e-01,  6.1218e-02, -2.4219e-01,  1.4441e-01, -8.9355e-02,\n",
       "           -1.1035e-01, -4.5361e-01,  5.7495e-02, -3.3789e-01, -1.8079e-01,\n",
       "            3.4943e-02, -1.2421e-01, -1.1924e+00,  3.4644e-01, -2.5894e-02,\n",
       "           -9.2529e-02, -4.2944e-01,  3.5461e-02,  4.6417e-02, -3.3984e-01,\n",
       "            2.8223e-01,  9.6069e-02,  6.0742e-01,  2.6147e-01, -2.8152e-02,\n",
       "           -3.2202e-01, -2.8369e-01, -2.5952e-01,  1.1955e-02,  2.8394e-01,\n",
       "            4.2572e-02, -5.0354e-02,  3.3173e-02, -2.0581e-01,  5.1514e-01,\n",
       "            1.6431e-01,  7.8613e-02, -1.5198e-01,  5.2452e-03, -1.4990e-01,\n",
       "           -1.6968e-01, -1.3550e-01,  1.8774e-01, -5.3955e-01, -2.9565e-01,\n",
       "           -1.8835e-01,  5.4413e-02,  1.8140e-01, -4.4385e-01,  3.2275e-01,\n",
       "            3.8745e-01, -3.9502e-01,  2.7832e-01,  4.1211e+00, -2.0520e-01,\n",
       "           -1.5991e-01, -2.1094e-01, -6.2500e-01,  1.5369e-01, -1.2573e-01,\n",
       "            5.9448e-02,  7.9285e-02, -1.9495e-01,  4.6680e-01, -1.3599e-01,\n",
       "           -3.5669e-01, -4.7760e-02, -2.6199e-02, -2.1509e-01,  9.9670e-02,\n",
       "           -1.8774e-01,  1.1877e-01,  5.2930e-01,  1.9153e-01,  3.2715e-02,\n",
       "           -1.5527e-01, -2.6230e-02,  1.8604e-01,  6.0211e-02, -9.7961e-02,\n",
       "           -2.7759e-01, -4.1772e-01, -4.5990e-02, -1.8167e-03, -4.4751e-01,\n",
       "            1.6418e-01,  1.8286e-01, -9.0515e-02, -3.6182e-01,  1.4185e-01,\n",
       "           -1.4929e-01, -6.6345e-02,  4.0253e-02,  1.2537e-01, -1.3332e-03,\n",
       "            8.4595e-02, -1.7798e-01,  7.6538e-02,  7.8064e-02, -1.0608e-01,\n",
       "           -1.3110e-01, -2.4185e-02, -3.8989e-01, -3.5962e-01, -2.8198e-02,\n",
       "           -5.0830e-01,  1.0425e-01, -1.2488e-01,  2.8296e-01,  1.5063e-01,\n",
       "            3.0615e-01, -3.4943e-02,  3.8867e-01, -1.9409e-02,  9.1064e-02,\n",
       "            3.1445e-01, -2.8857e-01,  8.0200e-02,  2.9346e-01,  5.9998e-02,\n",
       "            2.4353e-02, -1.9727e-01,  9.2224e-02, -1.2781e-01, -1.9989e-03,\n",
       "           -2.8320e-01,  7.0923e-02, -1.3147e-01, -1.5884e-02,  1.4868e-01,\n",
       "           -1.4380e-01,  2.7026e-01, -4.0680e-02,  1.5161e-01,  3.1030e-01,\n",
       "           -2.3895e-02,  2.8491e-01,  5.6396e-02,  3.6450e-01,  1.1597e-02,\n",
       "            7.9117e-03, -1.6113e-01,  1.8921e-02, -7.6904e-02, -2.2058e-01,\n",
       "           -1.1147e-02,  1.9028e-02, -4.7989e-03, -3.4058e-02, -1.0901e-01,\n",
       "           -6.7017e-02, -4.6295e-02, -1.7688e-01, -1.4429e-01,  6.1279e-02,\n",
       "           -2.2668e-01,  2.5049e-01,  9.6191e-02, -2.3181e-01,  6.5186e-02,\n",
       "           -7.7332e-02,  4.5166e-01,  4.0497e-02,  4.0698e-01, -3.4033e-01,\n",
       "           -5.6055e-01,  1.9177e-01,  5.9277e-01, -7.2632e-02, -1.2903e-01,\n",
       "            1.4977e-02, -3.1567e-01, -1.1444e-01, -1.7670e-02, -3.1860e-01,\n",
       "           -2.1045e-01, -2.1545e-01, -1.2549e-01,  1.9421e-01,  3.7598e-01,\n",
       "            2.4634e-01, -1.8799e-01, -1.9562e-02, -1.0889e-01,  7.0984e-02,\n",
       "           -3.5083e-01,  1.2671e-01, -1.7676e-01, -4.0552e-01,  6.1218e-02,\n",
       "            5.1575e-02, -8.9294e-02,  4.2633e-02, -2.9639e-01, -5.5518e-01,\n",
       "           -2.3972e-02,  5.0879e-01,  1.0114e-01, -1.7349e-02, -2.9099e-02,\n",
       "           -1.4661e-01,  5.4102e-01,  3.8770e-01, -2.6904e-01, -1.5945e-02,\n",
       "            1.1493e-01,  1.8555e-01, -2.2400e-02, -6.0333e-02, -1.2610e-01,\n",
       "           -1.4050e-01, -3.4082e-01,  8.4167e-02,  6.2793e-01,  3.0151e-01,\n",
       "            1.5625e-01,  3.0444e-01, -1.2335e-01, -2.4277e-02, -5.4901e-02,\n",
       "           -1.6455e-01,  1.2794e-02,  1.1304e-01,  1.3452e-01,  1.5961e-02,\n",
       "           -8.0762e-01,  5.7031e-01, -7.2746e-03,  5.4639e-01, -2.1191e-01,\n",
       "            2.5116e-02,  2.2607e-01,  4.1172e+00,  1.9995e-01,  3.1348e-01,\n",
       "            4.1577e-01,  3.0249e-01, -4.6045e-01, -1.0211e-01,  9.2590e-02,\n",
       "            1.7651e-01, -6.0272e-02, -6.2927e-02, -2.5620e-02,  3.3051e-02,\n",
       "            2.3816e-01,  3.3813e-02, -2.9587e-02, -3.2861e-01, -1.5771e+00,\n",
       "            1.0620e-01,  2.2827e-02,  8.4045e-02, -1.1407e-01,  2.5854e-01,\n",
       "           -1.3196e-01, -4.7058e-02,  2.7539e-01,  1.5344e-01, -2.9224e-01,\n",
       "           -3.0005e-01, -3.4082e-01,  1.4648e-01,  1.2732e-01,  3.0371e-01,\n",
       "           -3.1860e-01,  3.9478e-01,  5.9845e-02, -1.8201e-01,  2.6123e-01,\n",
       "            7.5439e-02,  8.3923e-02,  2.3645e-01,  3.7183e-01, -8.0261e-02,\n",
       "            1.7334e-01,  4.9463e-01, -5.5481e-02,  1.2793e-01, -7.4646e-02,\n",
       "            1.4307e-01, -7.6904e-02, -1.6571e-02,  3.2715e-01,  1.4722e-01,\n",
       "            2.8418e-01, -6.6147e-03, -5.1392e-02,  4.8737e-02, -8.9233e-02,\n",
       "           -5.4199e-01, -2.7417e-01,  1.7664e-01,  2.2400e-02,  1.3904e-01,\n",
       "           -1.9849e-01,  1.9397e-01, -2.1509e-01, -2.2498e-01, -2.9663e-01,\n",
       "            1.2952e-01,  1.6309e-01, -3.1934e-01, -4.8071e-01,  1.1163e-01,\n",
       "           -8.8440e-02, -3.3398e-01,  1.7480e-01, -4.3018e-01, -2.2217e-01,\n",
       "           -5.5176e-01,  1.5015e-01, -2.6489e-01, -2.6709e-01,  9.7839e-02,\n",
       "           -2.0898e-01, -1.9189e-01, -1.6632e-03,  7.6416e-02, -1.1597e-01,\n",
       "           -5.5313e-03, -4.4067e-01,  3.5767e-02,  1.4526e-01, -3.2520e-01,\n",
       "           -4.1748e-02, -8.7280e-02, -5.3662e-01,  1.1865e-01,  4.3701e-02,\n",
       "           -5.1147e-02, -1.8274e-01, -7.5378e-02, -2.0447e-01, -8.3618e-02,\n",
       "           -2.4341e-01, -1.2305e-01,  2.7539e-01,  2.4612e-02,  1.7181e-02,\n",
       "            2.6587e-01, -2.7051e-01,  3.8647e-01, -6.0352e-01,  1.9019e-01,\n",
       "           -3.7476e-01, -8.6304e-02, -4.0991e-01,  4.1724e-01,  4.5459e-01,\n",
       "            4.0259e-01, -4.9902e-01, -3.5736e-02,  1.0413e-01,  2.1631e-01,\n",
       "           -1.4575e-01, -2.2144e-01,  1.2585e-01, -6.3721e-02,  9.2468e-02,\n",
       "           -4.5801e-01, -6.0449e-01, -4.2310e-01, -2.3743e-01, -2.2751e-02,\n",
       "            2.0117e-01, -1.2463e-01,  1.2598e-01,  3.0200e-01,  1.7786e-01,\n",
       "           -9.9182e-02, -1.1444e-01,  5.8746e-02,  1.3757e-01, -6.0883e-02,\n",
       "            3.7354e-01, -2.0599e-02, -1.4465e-01,  4.8218e-01, -1.5784e-01,\n",
       "            8.7830e-02, -1.3953e-01,  4.0771e-01,  6.0791e-01,  3.8867e-01,\n",
       "           -1.6052e-01,  5.8937e-03,  1.5247e-01, -1.4307e-01, -1.3953e-01,\n",
       "            4.1479e-01,  3.2867e-02,  7.8674e-02,  9.5520e-02,  5.5713e-01,\n",
       "            1.5735e-01, -1.3208e-01,  1.2128e-01,  1.4026e-01,  3.0396e-01,\n",
       "            1.8274e-01, -7.0166e-01,  1.8738e-01, -3.0103e-01,  2.1835e-02,\n",
       "            3.1470e-01,  6.7810e-02, -8.7433e-03, -1.5576e-01, -2.0679e-01,\n",
       "           -5.0195e-01,  4.4897e-01, -4.3060e-02,  7.0264e-01,  1.5881e-01,\n",
       "           -2.6099e-01,  1.6357e-01,  1.9318e-02, -5.6445e-01,  8.2581e-02,\n",
       "           -9.0271e-02, -8.3984e-02,  2.8809e-01,  3.4644e-01,  2.6840e-02,\n",
       "           -9.3506e-02, -2.6062e-02,  3.6475e-01, -5.5322e-01, -2.1777e-01,\n",
       "            2.1460e-01, -2.9297e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2671, 0.0193, 0.0319, 0.6816]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  158.881363   73.466003  567.562195  322.339966    0.933252      0   person\n",
       "  1  469.176636    0.000000  569.505981  133.350403    0.929942      0   person\n",
       "  2  134.064209  247.977539  233.764893  313.769592    0.920376     29  frisbee\n",
       "  3  589.529968    0.000000  637.861511   17.183258    0.273007      0   person,\n",
       "  'caption': ['a man in the background of a game of frisbee'],\n",
       "  'bbox_target': [460.26, 1.06, 111.49, 132.46]},\n",
       " 295: {'image_emb': tensor([[ 0.0074, -0.0635, -0.2922,  ..., -0.0428, -0.1497,  0.0409],\n",
       "          [ 0.1049,  0.6685, -0.1926,  ...,  0.4976, -0.3887,  0.0251],\n",
       "          [-0.0830, -0.3679, -0.2537,  ...,  0.2375, -0.1500,  0.1685]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.4587, -0.5264,  0.1378,  ...,  0.0869,  0.3713,  0.5220],\n",
       "          [-0.2808, -0.2646,  0.0560,  ...,  0.3235,  0.1076,  0.5444]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6943, 0.0804, 0.2253],\n",
       "          [0.3503, 0.5776, 0.0723]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class   name\n",
       "  0  257.909546  14.026627  519.027222  420.571045    0.945807     22  zebra\n",
       "  1   36.300301  50.158310  255.325073  374.952698    0.896190     22  zebra,\n",
       "  'caption': ['Small zebra turning around to look.',\n",
       "   'a zebra with its head turned around so its face is visible'],\n",
       "  'bbox_target': [257.63, 12.36, 261.44, 405.94]},\n",
       " 296: {'image_emb': tensor([[-0.3831, -0.0709, -0.0280,  ...,  0.3857, -0.0558, -0.0161],\n",
       "          [ 0.1109, -0.0034, -0.2922,  ...,  0.5703, -0.0986, -0.0914],\n",
       "          [-0.2898,  0.1714,  0.2443,  ...,  0.3552,  0.0267, -0.3765],\n",
       "          [-0.2053,  0.1058, -0.0576,  ...,  0.3955, -0.0581, -0.2167]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-1.2524e-01, -1.4868e-01,  4.1040e-01,  3.4332e-02,  7.2754e-02,\n",
       "            3.8232e-01, -1.1041e-01, -4.0674e-01,  6.7139e-02,  1.6830e-02,\n",
       "            3.8867e-01,  2.4933e-02,  2.7148e-01,  1.6577e-01, -1.8152e-01,\n",
       "           -2.2644e-01,  1.2225e-01,  9.1858e-02,  1.8262e-01,  1.6589e-01,\n",
       "            3.9307e-02,  5.5511e-02, -3.4741e-01,  4.2664e-02,  1.7487e-02,\n",
       "            2.2229e-01, -6.5063e-02,  5.3174e-01,  7.4158e-02,  1.5649e-01,\n",
       "           -1.8457e-01, -1.6882e-01, -1.8884e-01, -3.0914e-02, -5.2783e-01,\n",
       "            7.7576e-02, -1.2415e-01,  1.2012e-01,  8.0627e-02,  6.4026e-02,\n",
       "            1.1206e-01,  3.7036e-01,  4.7827e-01, -1.0645e-01,  3.3203e-01,\n",
       "            2.9761e-01, -6.3171e-02, -2.0190e-01,  8.1726e-02,  1.2054e-02,\n",
       "            2.1826e-01, -6.4307e-01,  6.3904e-02,  2.3392e-02, -6.5796e-02,\n",
       "            1.6321e-01, -1.1096e-01,  3.9551e-02, -2.3767e-01, -8.4229e-02,\n",
       "           -2.2217e-01,  1.3939e-02,  2.3340e-01, -3.0176e-01, -2.4084e-01,\n",
       "           -7.4951e-02,  3.2910e-01,  1.4197e-01, -3.0176e-01, -1.8323e-01,\n",
       "           -2.1216e-01, -7.2815e-02, -8.0566e-02, -2.1680e-01, -2.2640e-03,\n",
       "           -4.1466e-03, -9.7733e-03, -1.1353e-01, -1.8677e-01, -9.3140e-02,\n",
       "            1.2122e-01,  1.5112e-01,  3.4814e-01, -2.3010e-01,  1.3513e-01,\n",
       "            2.2873e-02,  4.3604e-01,  3.4766e-01, -7.3438e-01,  7.2693e-02,\n",
       "            1.5161e-01,  3.1055e-01, -1.1357e+00,  6.2164e-02, -3.6523e-01,\n",
       "            9.8114e-03,  1.9507e-01,  3.5205e-01,  3.0981e-01,  5.8411e-02,\n",
       "            2.5659e-01,  3.1885e-01,  1.9299e-01, -3.0762e-01, -3.3496e-01,\n",
       "           -1.7822e-02, -9.7595e-02,  9.8267e-02,  1.2067e-01, -4.3091e-02,\n",
       "           -1.4197e-01,  3.9612e-02,  1.5149e-01, -1.9312e-01,  2.9712e-01,\n",
       "           -1.5039e-01,  6.2622e-02, -1.2335e-01, -3.1274e-01,  1.1963e-01,\n",
       "           -5.9143e-02, -4.8511e-01,  5.2686e-01, -9.0454e-02,  2.4963e-01,\n",
       "           -8.8867e-02,  1.5454e-01,  1.5271e-01,  4.0015e-01, -1.5526e-02,\n",
       "            1.9287e-01, -2.4377e-01,  4.0308e-01,  4.1953e+00, -2.7100e-01,\n",
       "            1.2952e-01, -2.9541e-01, -3.1787e-01, -3.4937e-01, -3.9258e-01,\n",
       "           -1.1902e-01, -1.4862e-02, -6.5234e-01,  5.6982e-01, -2.2192e-01,\n",
       "           -5.2185e-02, -8.1299e-02,  1.2939e-01,  4.9390e-01,  4.0436e-02,\n",
       "           -1.0052e-01, -3.0615e-01, -1.0144e-01,  9.6436e-02,  4.7974e-02,\n",
       "           -1.1772e-02,  1.5942e-01, -3.2788e-01,  1.6772e-01,  6.2500e-02,\n",
       "           -2.2003e-02,  1.9958e-01, -3.4277e-01,  5.1758e-02, -3.2886e-01,\n",
       "            4.2633e-02, -4.9934e-03, -1.9482e-01,  1.7969e-01, -1.1847e-01,\n",
       "            7.1960e-02, -3.4229e-01, -1.1609e-01, -8.2886e-02, -1.7297e-01,\n",
       "           -2.5903e-01,  1.4214e-02, -1.3342e-01,  1.9861e-01,  1.1139e-02,\n",
       "           -4.0552e-01, -3.5132e-01, -6.2891e-01,  4.0063e-01, -3.7183e-01,\n",
       "           -6.5137e-01, -3.0200e-01, -1.4697e-01, -7.4768e-02,  1.0944e-01,\n",
       "            2.4280e-01, -2.6855e-01, -7.6465e-01, -1.7126e-01,  6.9618e-03,\n",
       "            3.8647e-01,  4.1260e-02,  1.1774e-01, -1.1719e-01,  1.7334e-01,\n",
       "            2.3950e-01, -3.8672e-01,  9.2468e-02, -6.2439e-02, -1.5515e-01,\n",
       "           -2.0581e-01,  2.3450e-01,  4.2915e-04,  3.2104e-01,  1.1182e-01,\n",
       "            3.6621e-01,  4.4995e-01,  2.7881e-01, -1.4136e-01, -5.1941e-02,\n",
       "           -2.4426e-01,  1.4636e-01, -6.5796e-02,  6.1157e-02,  4.4952e-02,\n",
       "           -1.4319e-01, -1.8542e-01, -1.5576e-01, -5.7910e-01,  4.9530e-02,\n",
       "            4.3579e-02,  1.9751e-01, -2.0276e-01,  5.1562e-01,  1.4819e-01,\n",
       "           -3.2410e-02,  2.9077e-01,  5.7709e-02,  1.4490e-01,  1.0602e-01,\n",
       "            5.3809e-01, -2.4695e-01,  4.8981e-02,  4.5166e-01, -2.1008e-01,\n",
       "           -1.9189e-01, -2.8931e-02,  2.3950e-01,  1.9495e-01, -5.8228e-02,\n",
       "           -1.2378e-01,  8.7280e-02,  1.4136e-01,  1.6797e-01,  2.2571e-01,\n",
       "           -2.3511e-01,  1.8530e-01, -8.6853e-02,  1.7419e-01, -3.7720e-01,\n",
       "           -3.9868e-01,  1.9226e-01,  2.2961e-01, -1.5942e-01,  1.4417e-01,\n",
       "           -4.2944e-01, -1.2000e-01, -3.3051e-02,  6.9641e-02,  2.7856e-01,\n",
       "            1.6711e-01, -4.3732e-02, -7.1350e-02,  1.7932e-01,  4.9390e-01,\n",
       "           -2.7368e-01, -4.8242e-01, -2.7344e-01,  3.2910e-01, -9.0576e-02,\n",
       "           -1.3562e-01, -1.1212e-01, -3.7903e-02,  3.4692e-01,  3.0957e-01,\n",
       "            6.0205e-01,  8.9783e-02,  1.6064e-01,  2.0129e-01, -1.3977e-01,\n",
       "            1.5784e-01, -2.0386e-01, -5.6458e-02,  4.5258e-02,  3.8635e-02,\n",
       "            1.5202e-03,  4.4922e-01,  4.2383e-01, -3.5767e-01, -3.2812e-01,\n",
       "            1.5295e-01, -4.9023e-01,  3.8672e-01, -4.0649e-02, -9.9548e-02,\n",
       "           -3.6133e-01,  1.6101e-01,  2.5073e-01, -1.8936e-02,  4.9487e-01,\n",
       "            7.3792e-02, -1.0590e-02,  3.3057e-01,  2.1375e-01, -6.6223e-02,\n",
       "           -7.0374e-02,  5.2002e-01,  4.1992e+00,  4.7272e-02,  3.9703e-02,\n",
       "            5.9619e-01,  8.2703e-02, -4.2065e-01,  5.5859e-01, -2.3425e-01,\n",
       "            4.9878e-01,  3.2544e-01,  2.9297e-01,  5.1025e-02, -3.8843e-01,\n",
       "            1.1224e-01,  1.6980e-01, -5.0293e-01, -5.8716e-02, -1.8262e+00,\n",
       "            2.4211e-04,  1.2103e-01,  7.3975e-02,  1.0394e-01,  3.6353e-01,\n",
       "           -1.9592e-02, -6.4514e-02, -3.9478e-01,  3.4912e-01, -1.0626e-01,\n",
       "            2.1313e-01, -4.9255e-02,  2.1594e-01, -1.9092e-01,  2.0703e-01,\n",
       "            4.6539e-02,  1.9336e-01, -1.0834e-01,  1.5637e-01, -7.5745e-02,\n",
       "            4.3652e-01,  3.9917e-02, -4.6411e-01, -1.8701e-01, -2.5171e-01,\n",
       "           -6.3232e-02,  7.4615e-03,  1.4624e-01,  9.3918e-03, -5.2100e-01,\n",
       "           -1.6663e-01, -4.4409e-01, -3.0054e-01, -7.9285e-02,  8.2031e-02,\n",
       "           -1.2109e-01,  3.2379e-02,  2.5830e-01, -3.0078e-01, -1.1273e-01,\n",
       "            1.4246e-01, -1.8286e-01,  3.6426e-01,  2.9590e-01,  3.5797e-02,\n",
       "           -2.6782e-01, -8.9233e-02,  9.7046e-03, -6.0693e-01, -1.5796e-01,\n",
       "           -2.1887e-01,  1.4050e-01, -1.8921e-01, -3.5889e-01,  1.1938e-01,\n",
       "           -1.0834e-03, -1.7249e-01,  4.8218e-02, -4.9878e-01, -1.3843e-01,\n",
       "           -3.6060e-01,  4.8389e-01,  1.9238e-01,  1.9788e-01,  1.8628e-01,\n",
       "           -3.7769e-01,  2.2107e-01,  1.1377e-01, -1.5576e-01,  5.2051e-01,\n",
       "            4.9133e-02,  6.4880e-02,  1.8555e-02,  4.5166e-02, -1.8201e-01,\n",
       "           -5.9375e-01, -9.5459e-02, -9.8877e-02, -1.7303e-02, -1.9958e-01,\n",
       "            2.6782e-01, -3.9160e-01,  4.3604e-01,  1.6357e-01,  3.1958e-01,\n",
       "            1.4734e-01,  5.9143e-02, -1.6687e-01, -1.6809e-01, -2.6587e-01,\n",
       "            2.4988e-01,  2.7148e-01,  2.2278e-01, -6.1426e-01,  1.9092e-01,\n",
       "            4.4800e-01, -2.5528e-02,  1.2048e-01, -4.5685e-02,  5.3125e-01,\n",
       "            2.9892e-02, -1.8433e-01, -1.6858e-01,  7.6709e-01, -2.3285e-02,\n",
       "            1.2323e-01, -2.7490e-01,  2.1277e-01,  1.0669e-01,  2.5732e-01,\n",
       "            1.5027e-01, -3.5107e-01, -4.0845e-01, -4.7192e-01, -4.3823e-01,\n",
       "           -1.1829e-01, -3.6841e-01,  2.0084e-03, -3.0469e-01, -4.2297e-02,\n",
       "            3.1860e-01, -2.1179e-02, -1.9873e-01,  1.6016e-01, -4.1687e-02,\n",
       "           -1.5063e-01, -1.8005e-02, -7.6111e-02, -7.7637e-02, -2.8613e-01,\n",
       "           -2.2876e-01,  5.8990e-02,  1.4612e-01,  2.6270e-01,  1.0394e-01,\n",
       "           -7.0618e-02,  2.4643e-02,  1.2054e-01,  1.0605e-02,  2.5391e-01,\n",
       "            5.1074e-01, -4.6973e-01,  1.8604e-01, -2.8858e-03,  6.8115e-02,\n",
       "            3.3472e-01, -1.4368e-01,  3.5327e-01, -1.1719e-02, -6.3293e-02,\n",
       "            5.3131e-02, -2.2485e-01, -6.5308e-02,  1.8713e-01, -4.2578e-01,\n",
       "            3.4985e-01, -2.4353e-02, -3.7207e-01, -2.9834e-01, -1.3123e-01,\n",
       "            2.6245e-01,  1.8982e-01, -2.2141e-02,  8.0762e-01,  4.7046e-01,\n",
       "            1.5930e-01, -2.9617e-02,  3.9307e-01,  2.1591e-02,  3.8116e-02,\n",
       "           -5.1453e-02,  9.4421e-02,  1.1816e-01,  1.9824e-01,  8.6133e-01,\n",
       "           -1.7175e-01, -6.0242e-02, -4.3799e-01, -1.5955e-01,  2.2400e-01,\n",
       "            2.0776e-01, -2.2290e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0551e-02, 4.4918e-04, 6.9482e-01, 2.9419e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   95.148270   99.563095  402.548645  598.812012    0.937393      0   \n",
       "  1  118.444626  417.067688  168.298828  567.322632    0.891113     15   \n",
       "  2    5.434982    2.730194  326.943176  614.708496    0.871188     72   \n",
       "  3  207.185181  376.306946  239.627563  428.351257    0.652606     39   \n",
       "  4  382.649994  106.739609  426.494263  419.240540    0.398672     56   \n",
       "  5  228.652313  379.171387  248.914093  429.552917    0.328384     39   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1           cat  \n",
       "  2  refrigerator  \n",
       "  3        bottle  \n",
       "  4         chair  \n",
       "  5        bottle  ,\n",
       "  'caption': ['the white freezer door of a refrigerator'],\n",
       "  'bbox_target': [10.02, 4.43, 148.91, 538.35]},\n",
       " 297: {'image_emb': tensor([[-0.0781,  0.0714, -0.7358,  ...,  0.4158,  0.2825,  0.3411],\n",
       "          [-0.0359,  0.0049, -0.6812,  ...,  0.2852,  0.3540,  0.2881],\n",
       "          [ 0.2869, -0.0304, -0.3108,  ...,  0.4714, -0.0318, -0.0480],\n",
       "          [ 0.0576,  0.2737, -0.5195,  ...,  0.0983,  0.0335,  0.1094]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1857,  0.0686, -0.4531,  ...,  0.1493,  0.3809, -0.0587],\n",
       "          [ 0.1709,  0.0036, -0.2910,  ...,  0.3904,  0.2517,  0.0555]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.7305e-01, 2.2986e-01, 1.1921e-06, 3.9722e-01],\n",
       "          [4.4824e-01, 3.2275e-01, 9.0003e-06, 2.2888e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    0.236740  142.419586  309.528870  359.886017    0.924733      6   train\n",
       "  1  345.303619  136.521194  639.057739  362.735657    0.909291      6   train\n",
       "  2  329.091431  219.836945  343.954041  259.184357    0.807434      0  person\n",
       "  3  481.004028  213.823578  493.785095  251.774994    0.375184      0  person,\n",
       "  'caption': ['A yellow train on the right side of the platform.',\n",
       "   'Yellow train parked on platform four.'],\n",
       "  'bbox_target': [344.32, 136.58, 292.39, 215.44]},\n",
       " 298: {'image_emb': tensor([[-0.5337,  0.4385, -0.1497,  ...,  0.5854,  0.0572,  0.2111],\n",
       "          [-0.0173,  0.2026, -0.3850,  ...,  1.0654,  0.0388, -0.0023],\n",
       "          [-0.4260,  0.5151, -0.0446,  ...,  1.0205, -0.0559,  0.2216],\n",
       "          ...,\n",
       "          [ 0.1121, -0.0534, -0.5005,  ...,  0.5322, -0.0455, -0.2549],\n",
       "          [ 0.0497, -0.2482, -0.2947,  ...,  1.0713, -0.2539,  0.2529],\n",
       "          [-0.2859,  0.4092, -0.1906,  ...,  0.8901,  0.2874,  0.3633]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1147,  0.1560, -0.1444,  ...,  0.1813,  0.2239,  0.1646],\n",
       "          [ 0.0631,  0.1812,  0.0145,  ...,  0.2051,  0.0627,  0.1631]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.6992e-03, 1.0943e-04, 9.8926e-01, 6.5994e-04, 8.8787e-04, 9.5069e-05,\n",
       "           3.5114e-03],\n",
       "          [5.5962e-03, 4.2081e-05, 9.8633e-01, 5.3692e-04, 3.3092e-04, 6.8307e-05,\n",
       "           6.9656e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0   28.660187  169.376495  490.695648  326.611115    0.927477      4  airplane\n",
       "  1  564.961365  376.372131  600.896912  451.799255    0.908540      0    person\n",
       "  2  512.599304  186.588806  639.327332  318.158813    0.851652      4  airplane\n",
       "  3  445.083405  313.943634  462.431854  342.151398    0.802600      0    person\n",
       "  4  486.205261  318.548218  501.349304  345.487488    0.735405      0    person\n",
       "  5  495.036041  306.749054  571.163086  346.213470    0.719576      7     truck\n",
       "  6  564.167847  319.608948  580.645264  349.774902    0.670987      0    person,\n",
       "  'caption': ['The tail end of the airplane that is only partially visible in the right hand picture.',\n",
       "   'The white tail of a plane.'],\n",
       "  'bbox_target': [513.1, 186.0, 126.81, 136.24]},\n",
       " 299: {'image_emb': tensor([[-0.3381,  0.4268, -0.1826,  ...,  0.4802, -0.0406,  0.0805],\n",
       "          [-0.5874,  0.5020, -0.2664,  ...,  0.7041,  0.1019,  0.0983],\n",
       "          [-0.6348,  0.3755, -0.0984,  ...,  0.5098, -0.0892,  0.1608]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0748,  0.2440, -0.2349,  ..., -0.2883,  0.1843, -0.1373],\n",
       "          [-0.2339,  0.3501, -0.2642,  ...,  0.1888,  0.1779, -0.4951]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.1511e-03, 9.9854e-01, 9.4533e-05],\n",
       "          [6.3667e-03, 9.9023e-01, 3.5706e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  126.220108   66.153351  296.434082  327.566437    0.933692      0  person\n",
       "  1  363.816833   97.025513  527.874023  345.288757    0.933452      0  person\n",
       "  2   17.934189  311.822144  377.731018  339.339294    0.609471     30    skis,\n",
       "  'caption': ['A person who is wearing yellow and red color dress',\n",
       "   'A man on snow slidding with yellow jerkin and red phant'],\n",
       "  'bbox_target': [355.87, 98.17, 175.04, 241.41]},\n",
       " 300: {'image_emb': tensor([[ 0.1743,  0.2539, -0.1187,  ...,  0.8550, -0.1346, -0.1893],\n",
       "          [-0.3406,  0.1410, -0.1034,  ...,  1.4775, -0.2737, -0.0735],\n",
       "          [-0.0635,  0.6069, -0.2341,  ...,  0.9141, -0.0599,  0.0215],\n",
       "          [ 0.2170,  0.2499, -0.0994,  ...,  0.7832, -0.1283, -0.1014]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2128, -0.3674,  0.0406,  ...,  0.2261,  0.0717, -0.2424],\n",
       "          [ 0.1199,  0.0161,  0.0874,  ..., -0.3923,  0.0672, -0.2566]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.8228e-02, 8.6594e-03, 9.1113e-01, 2.2110e-02],\n",
       "          [3.5645e-01, 5.9795e-04, 2.4512e-01, 3.9771e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.328857   53.267868  636.560608  409.574707    0.796136     53   \n",
       "  1    1.176079    0.811630  201.270325   85.003342    0.786798      0   \n",
       "  2  450.888794  213.959534  639.221924  287.290222    0.783658     43   \n",
       "  3    0.000000   24.751160  637.928955  411.449829    0.316795     60   \n",
       "  4  437.427704   19.012894  552.468933   63.554642    0.293798     43   \n",
       "  \n",
       "             name  \n",
       "  0         pizza  \n",
       "  1        person  \n",
       "  2         knife  \n",
       "  3  dining table  \n",
       "  4         knife  ,\n",
       "  'caption': ['The slice of pizza on the spatula.',\n",
       "   'A slice of pizza being pulled.'],\n",
       "  'bbox_target': [447.06, 175.19, 192.94, 89.98]},\n",
       " 301: {'image_emb': tensor([[ 0.1001,  0.1279,  0.0562,  ...,  0.6270,  0.0361,  0.6479],\n",
       "          [ 0.1184,  0.1702,  0.0499,  ...,  0.9863, -0.1070,  0.1831],\n",
       "          [ 0.2144,  0.1718,  0.1156,  ...,  0.4700,  0.0530,  0.4734]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2578, -0.2664,  0.0528,  ...,  0.0529, -0.0130,  0.0296],\n",
       "          [ 0.1599, -0.3406,  0.1422,  ...,  0.4539, -0.0340,  0.0429]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.8486e-01, 1.0556e-04, 1.5190e-02],\n",
       "          [2.2247e-02, 1.3000e-01, 8.4766e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class name\n",
       "  0  174.173492   23.595169  638.691162  344.004761    0.956584      5  bus\n",
       "  1   49.480282  112.392944  176.685913  227.287476    0.896552      5  bus\n",
       "  2    0.681080  131.567871   49.537380  199.131226    0.634429      5  bus,\n",
       "  'caption': ['a bus named gunther',\n",
       "   'A large white bus is first in line at a curb with two other buses behind it.'],\n",
       "  'bbox_target': [174.43, 23.18, 465.57, 315.97]},\n",
       " 302: {'image_emb': tensor([[ 0.3936,  0.3254, -0.1434,  ...,  0.3975,  0.4375, -0.0292],\n",
       "          [-0.0442,  0.3645, -0.2367,  ...,  1.0078,  0.0688,  0.1904],\n",
       "          [ 0.3274,  0.7568,  0.0743,  ...,  0.6733, -0.0605,  0.0095],\n",
       "          ...,\n",
       "          [ 0.2971, -0.0647, -0.4822,  ...,  0.9551,  0.2384,  0.0950],\n",
       "          [ 0.2419,  0.4390, -0.2247,  ...,  0.4709,  0.1628,  0.1823],\n",
       "          [ 0.2323,  0.6333, -0.1151,  ..., -0.1669,  0.0994, -0.3118]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0106,  0.0316,  0.0476,  ...,  0.0869,  0.2988, -0.2871],\n",
       "          [ 0.1876,  0.1203, -0.1475,  ...,  0.0780,  0.1243, -0.4785]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.3193e-01, 2.3186e-05, 1.7762e-05, 2.5848e-02, 1.1325e-06, 5.5361e-04,\n",
       "           2.4146e-01],\n",
       "          [6.9580e-01, 3.8147e-06, 3.0398e-06, 6.3721e-02, 5.3644e-07, 7.0095e-05,\n",
       "           2.4048e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  130.894501  177.973328  282.264679  367.851562    0.905819      0   \n",
       "  1  367.043579  235.258606  451.693726  357.462830    0.878892      2   \n",
       "  2  408.534241  228.880920  624.517395  360.298889    0.851577      2   \n",
       "  3  239.962708  151.427017  311.940491  226.147034    0.846535     11   \n",
       "  4  312.567169  197.809937  355.223602  353.911499    0.825831      0   \n",
       "  5  214.566589  253.376648  373.695312  350.557678    0.728282      2   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1        car  \n",
       "  2        car  \n",
       "  3  stop sign  \n",
       "  4     person  \n",
       "  5        car  ,\n",
       "  'caption': ['A man holding a stop sign and directing traffic.',\n",
       "   'A man in a hat holding a stop sign.'],\n",
       "  'bbox_target': [135.4, 174.08, 129.28, 192.41]},\n",
       " 303: {'image_emb': tensor([[-0.0352,  0.2102, -0.1925,  ...,  0.9634,  0.3032, -0.3838],\n",
       "          [ 0.0422,  0.3538, -0.0883,  ...,  0.8994,  0.0503, -0.2161],\n",
       "          [ 0.1277,  0.2756, -0.1165,  ...,  0.9375,  0.0540,  0.1024],\n",
       "          [-0.0766,  0.6934, -0.0061,  ...,  0.4856,  0.0749, -0.3877]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1744,  0.1288, -0.3696,  ...,  0.3181, -0.0220, -0.5947],\n",
       "          [-0.1522, -0.0605, -0.2847,  ...,  0.1873,  0.0376, -0.8442]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.9170e-01, 5.3263e-04, 5.6696e-04, 5.0732e-01],\n",
       "          [9.2697e-03, 2.1124e-04, 1.3530e-05, 9.9072e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  125.878082   72.314713  294.840820  469.565247    0.932185      0   \n",
       "  1   38.118561  156.546829  547.677612  315.543396    0.904007     37   \n",
       "  2  334.169800   55.790100  434.392395  477.786743    0.894781      0   \n",
       "  3   87.904449  319.689697  152.178101  378.539185    0.337251     56   \n",
       "  4    0.000000  311.697266   56.010239  406.876099    0.286353     56   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1  surfboard  \n",
       "  2     person  \n",
       "  3      chair  \n",
       "  4      chair  ,\n",
       "  'caption': ['A shirtless man with santa hat and floral shorts carrying a surfboard.',\n",
       "   'man in santa hat holds the surfboard in the snow'],\n",
       "  'bbox_target': [127.79, 73.12, 167.51, 395.17]},\n",
       " 304: {'image_emb': tensor([[-1.6342e-02,  2.1729e-01, -3.7109e-02,  ...,  3.5645e-01,\n",
       "            1.3878e-02, -1.2488e-01],\n",
       "          [-3.7085e-01,  1.8884e-01,  5.0079e-02,  ...,  7.7686e-01,\n",
       "           -1.8356e-02, -2.6709e-01],\n",
       "          [-3.0444e-01,  4.0918e-01, -6.4331e-02,  ...,  6.0352e-01,\n",
       "           -1.4008e-02, -1.3647e-01],\n",
       "          [-1.3580e-03,  2.7539e-01, -2.6131e-04,  ...,  3.5498e-01,\n",
       "           -7.5134e-02, -1.4246e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-2.7686e-01,  2.4927e-01, -4.6417e-02,  1.6699e-01,  2.7603e-02,\n",
       "           -4.1162e-01, -5.3040e-02, -1.0400e+00, -1.5491e-01,  2.2644e-01,\n",
       "            8.6731e-02,  2.6050e-01, -2.0349e-01, -2.8784e-01, -1.1938e-01,\n",
       "           -8.7585e-02, -2.3108e-01,  2.0264e-02, -4.2572e-02,  4.3549e-02,\n",
       "            1.5564e-01,  1.3635e-01, -4.8560e-01,  4.4897e-01,  2.5317e-01,\n",
       "            7.0862e-02,  1.2384e-01, -5.2216e-02,  2.5708e-01, -1.4331e-01,\n",
       "           -2.3108e-01,  1.9073e-02, -1.7871e-01, -6.4758e-02,  7.0152e-03,\n",
       "           -7.2449e-02,  3.1104e-01,  8.1482e-02,  2.3071e-01,  6.2744e-02,\n",
       "           -3.0347e-01, -2.3403e-03, -1.2634e-01, -3.8281e-01,  4.6814e-02,\n",
       "            1.9745e-02, -4.8492e-02,  2.0813e-01, -1.7981e-01,  3.5620e-01,\n",
       "           -5.7159e-02, -3.9771e-01, -2.1484e-01,  2.2400e-01, -4.5264e-01,\n",
       "            5.2551e-02,  3.5858e-02, -4.4495e-02,  2.8589e-01,  4.1455e-01,\n",
       "            2.9224e-01, -2.6660e-01,  9.3018e-02,  3.3783e-02,  2.1753e-01,\n",
       "           -6.1768e-01, -3.6963e-01,  1.1859e-01,  8.9417e-02,  1.9275e-01,\n",
       "           -7.3975e-01, -2.1057e-01,  3.4204e-01, -2.1899e-01,  1.1841e-02,\n",
       "           -3.9380e-01,  1.6375e-03, -5.1514e-02,  1.7297e-01, -1.6040e-01,\n",
       "            3.2886e-01, -2.1179e-01,  5.1270e-02,  3.4981e-03,  6.7139e-02,\n",
       "           -4.7803e-01,  4.0112e-01,  2.6782e-01, -6.2134e-02, -2.7271e-01,\n",
       "            4.5166e-01,  3.3398e-01, -1.4912e+00,  3.7427e-01, -2.0752e-01,\n",
       "            2.8052e-01,  5.3076e-01, -6.1230e-01, -2.2476e-02, -2.5421e-02,\n",
       "           -6.9214e-02,  1.6159e-02,  3.9673e-01, -5.8014e-02, -1.1154e-02,\n",
       "            1.8604e-01,  1.1572e-01,  1.6907e-01,  3.6914e-01,  1.2329e-01,\n",
       "           -4.3213e-02,  1.3672e-01, -6.3354e-02, -5.8708e-03, -3.5059e-01,\n",
       "           -5.9418e-02, -1.3892e-01, -1.2769e-01,  1.4246e-01, -3.2349e-01,\n",
       "           -2.4524e-01, -2.3267e-01,  1.8054e-01, -5.0537e-01, -1.3086e-01,\n",
       "           -4.1577e-01, -9.2407e-02,  7.3730e-02,  4.7266e-01,  2.9150e-01,\n",
       "            5.4626e-03,  1.5002e-01, -2.1530e-02,  4.8125e+00, -1.9730e-02,\n",
       "           -4.0039e-01, -2.6196e-01,  1.2152e-01, -3.0249e-01,  2.1210e-03,\n",
       "           -1.5039e-01,  1.0516e-01, -4.7681e-01, -1.6357e-01, -2.3047e-01,\n",
       "           -1.8774e-01,  1.3635e-01,  1.8225e-01, -7.3242e-02,  2.8931e-01,\n",
       "           -4.9896e-03,  1.4636e-01,  4.1382e-01, -6.3477e-02,  1.0181e-01,\n",
       "            1.9470e-01,  4.4067e-01, -2.4207e-01, -1.2366e-01, -5.9296e-02,\n",
       "            3.8757e-02, -7.4463e-02, -4.2877e-02,  1.7688e-01,  6.8665e-02,\n",
       "            1.5320e-01,  3.3301e-01, -2.1594e-01,  2.0874e-02,  1.2535e-02,\n",
       "           -2.2900e-01, -5.6519e-02,  2.6147e-01,  1.1157e-01, -4.8145e-01,\n",
       "           -9.7351e-02, -4.3506e-01, -9.6008e-02, -8.9233e-02,  3.7689e-02,\n",
       "           -9.4543e-02,  2.7173e-01,  2.0850e-01, -2.0740e-01, -3.4058e-01,\n",
       "           -4.8633e-01,  2.7115e-02, -4.5459e-01,  4.7339e-01, -2.2876e-01,\n",
       "            1.7090e-02,  4.3848e-01,  1.4758e-01, -7.8186e-02,  5.2930e-01,\n",
       "            9.9304e-02, -2.5757e-01, -1.3159e-01, -5.4207e-03,  1.5881e-01,\n",
       "            3.5229e-01, -8.5632e-02, -1.4111e-01, -9.7900e-02,  4.2090e-01,\n",
       "           -3.8849e-02,  2.0544e-01, -4.1431e-01, -1.2457e-01, -3.4790e-01,\n",
       "           -5.1544e-02,  2.1802e-01, -1.9287e-01, -2.2266e-01,  2.8296e-01,\n",
       "            6.0254e-01, -2.2369e-02, -2.3596e-01,  4.1455e-01, -1.6943e-01,\n",
       "           -3.7048e-02,  9.4543e-02, -3.2153e-01, -5.9776e-03, -2.4829e-01,\n",
       "            3.1860e-01,  1.3379e-01, -1.9617e-01, -2.5391e-01,  1.5906e-01,\n",
       "            8.4717e-02, -1.1523e-01,  1.7346e-01, -7.0190e-02,  6.6772e-02,\n",
       "            9.0820e-02,  2.9956e-01, -2.1741e-01, -3.9746e-01, -2.1558e-01,\n",
       "            1.9922e-01,  2.3279e-01, -1.2561e-01, -2.3401e-01, -4.2334e-01,\n",
       "            9.8145e-02,  1.9989e-02, -4.4861e-03,  1.1047e-01,  3.8306e-01,\n",
       "           -2.4551e-02,  7.7087e-02,  1.6235e-01,  1.4099e-02, -9.0576e-02,\n",
       "           -4.1772e-01,  4.6600e-02, -1.9946e-01,  1.7444e-01, -1.2054e-01,\n",
       "           -8.7463e-02,  5.1956e-03,  2.4011e-01,  1.6235e-01, -4.7028e-02,\n",
       "           -1.2793e-01, -8.6670e-02,  2.8271e-01, -4.3823e-01,  1.6235e-01,\n",
       "           -1.4124e-01, -1.9019e-01, -3.4888e-01, -1.8127e-01, -3.3521e-01,\n",
       "           -3.0737e-01,  1.4453e-01,  3.6621e-01, -2.7637e-01,  2.3230e-01,\n",
       "            2.0337e-01,  3.4106e-01,  3.7524e-01,  4.6387e-01, -5.7526e-02,\n",
       "            4.0430e-01,  3.1323e-01,  3.9038e-01,  1.9958e-01, -6.9031e-02,\n",
       "            4.5280e-03,  4.1504e-01, -1.0022e-01, -1.0846e-01,  2.4780e-02,\n",
       "            1.3208e-01, -3.3862e-01, -2.1741e-01, -1.8115e-01,  1.4087e-01,\n",
       "           -2.1729e-01,  2.5195e-01, -6.2134e-02,  1.1255e-01, -1.6736e-01,\n",
       "           -1.5759e-01,  4.6021e-01, -3.1836e-01, -3.1338e-03, -3.7573e-01,\n",
       "           -2.8418e-01, -6.2012e-02,  4.8086e+00, -1.2830e-01,  6.4819e-02,\n",
       "            2.1960e-01,  3.3862e-01,  3.6938e-01,  3.4497e-01,  5.6934e-01,\n",
       "           -3.4210e-02,  6.8115e-02, -1.2891e-01, -1.6565e-01, -1.8604e-01,\n",
       "            3.1403e-02,  4.7729e-01, -2.8320e-01, -1.6272e-01, -2.1562e+00,\n",
       "            1.5002e-01, -3.3594e-01,  6.1523e-01,  1.1450e-01, -2.5299e-02,\n",
       "           -2.9053e-01,  1.5747e-01, -3.3716e-01, -1.5271e-01,  3.0908e-01,\n",
       "           -4.6826e-01,  2.1042e-02,  1.5454e-01, -6.9763e-02, -1.9373e-01,\n",
       "            1.6565e-01,  2.3132e-01, -1.0419e-01, -3.3569e-02, -7.2815e-02,\n",
       "            3.2013e-02,  4.6997e-02,  4.1162e-01,  3.9868e-01, -6.0883e-02,\n",
       "           -2.2717e-01, -8.0750e-02, -8.4412e-02, -9.1125e-02,  2.8931e-01,\n",
       "           -3.2056e-01,  2.4734e-02,  3.6499e-02,  1.9324e-01,  1.0156e-01,\n",
       "            6.1523e-02,  1.2469e-01,  2.4878e-01,  1.2805e-01,  1.0522e-01,\n",
       "           -2.0679e-01, -3.6743e-01, -1.3989e-01, -1.0156e-01,  2.0599e-02,\n",
       "            8.7830e-02, -1.4636e-01,  1.0132e-01,  3.6224e-02, -1.0321e-01,\n",
       "            1.1804e-01, -9.7752e-04,  1.9360e-01,  3.1006e-01,  2.2049e-02,\n",
       "            3.5034e-01, -6.2103e-02,  3.7720e-01,  1.9495e-01, -3.1836e-01,\n",
       "           -8.7158e-01,  6.3667e-03,  6.1676e-02,  4.8926e-01, -1.3452e-01,\n",
       "           -1.4258e-01, -2.4841e-02, -2.3483e-02, -3.6597e-01,  2.0166e-01,\n",
       "            8.3496e-02, -1.0101e-01, -1.2396e-01,  4.6967e-02,  3.0054e-01,\n",
       "           -4.4434e-02,  1.4404e-01, -9.0820e-02, -2.1558e-01, -3.7549e-01,\n",
       "            7.7332e-02,  1.4587e-01,  1.0077e-01, -2.2620e-01, -2.0789e-01,\n",
       "           -1.4917e-01, -2.0218e-02,  3.3350e-01,  1.0199e-01, -1.0742e-01,\n",
       "           -2.8687e-02, -2.7686e-01, -1.4258e-01, -2.1545e-01, -7.9346e-03,\n",
       "            6.6956e-02, -2.3914e-01,  9.9365e-02,  3.2959e-01,  2.5659e-01,\n",
       "           -1.5503e-01,  3.3081e-01, -1.8875e-02,  8.8623e-02,  3.4668e-01,\n",
       "           -5.1697e-02, -3.0121e-02,  3.5352e-01,  1.0919e-01,  2.4915e-01,\n",
       "           -1.7566e-01, -4.5190e-01, -4.1199e-03,  1.6821e-01, -2.0215e-01,\n",
       "           -1.3306e-01, -1.9189e-01, -1.8640e-01,  5.0629e-02, -3.5669e-01,\n",
       "            4.4800e-01, -5.1117e-02, -2.1899e-01, -7.9199e-01,  2.6001e-02,\n",
       "            2.5586e-01,  1.7993e-01, -1.9580e-01,  3.2471e-01, -6.3721e-01,\n",
       "            1.8204e-02,  2.4933e-02, -4.6265e-02, -7.8064e-02,  1.2903e-01,\n",
       "            3.4229e-01, -9.5093e-02, -2.7979e-01,  1.7590e-01, -3.7744e-01,\n",
       "            5.5908e-01, -4.8291e-01,  1.7432e-01, -9.7351e-02,  5.0110e-02,\n",
       "            3.3913e-03,  1.1688e-01, -1.2164e-01, -3.0664e-01,  5.8301e-01,\n",
       "            3.0005e-01, -2.1436e-01, -2.8955e-01, -9.3811e-02, -3.7939e-01,\n",
       "           -5.8545e-01,  1.1682e-01,  6.3232e-02,  3.3496e-01, -9.3262e-02,\n",
       "            1.2067e-01, -9.3201e-02, -1.8701e-01,  9.4873e-01,  3.4204e-01,\n",
       "           -6.4697e-02,  4.5020e-01,  2.2168e-01, -2.1133e-02,  4.8779e-01,\n",
       "            2.9224e-01, -4.9121e-01,  2.4072e-01,  1.0089e-01, -3.2074e-02,\n",
       "           -1.3330e-01, -3.5620e-01,  7.1449e-03,  9.6619e-02, -4.7412e-01,\n",
       "           -1.0547e-01,  1.1528e-02]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0853, 0.0664, 0.8218, 0.0268]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  418.062439   49.755531  551.445557  186.821716    0.926503     49   \n",
       "  1  281.124817   94.663681  421.643280  231.262085    0.921378     49   \n",
       "  2  211.100693  199.861023  388.744476  371.646545    0.917207     49   \n",
       "  3   11.655865    7.333533  608.698914  431.940155    0.549821     60   \n",
       "  \n",
       "             name  \n",
       "  0        orange  \n",
       "  1        orange  \n",
       "  2        orange  \n",
       "  3  dining table  ,\n",
       "  'caption': ['the furthest back orange'],\n",
       "  'bbox_target': [422.28, 53.04, 126.89, 135.22]},\n",
       " 305: {'image_emb': tensor([[ 0.4043,  0.2917, -0.1323,  ...,  0.5791, -0.5049,  0.0119],\n",
       "          [ 0.1196,  0.6938, -0.0743,  ...,  0.2942, -0.2844, -0.0598],\n",
       "          [-0.0443,  0.5688, -0.0906,  ...,  0.2328, -0.2766,  0.2856],\n",
       "          [ 0.0497,  0.0556, -0.2844,  ...,  0.5400, -0.1819,  0.4146],\n",
       "          [ 0.0579,  0.2446,  0.0670,  ..., -0.0382, -0.2405, -0.2153]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0830,  0.0316, -0.1921,  ...,  0.4189, -0.6982,  0.1578],\n",
       "          [ 0.1919, -0.0559, -0.2402,  ...,  0.6641, -0.6084,  0.2722]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0051, 0.8662, 0.0274, 0.0142, 0.0871],\n",
       "          [0.0013, 0.0120, 0.0847, 0.0735, 0.8286]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   46.548218  233.639465  275.440857  331.376770    0.928559      2   \n",
       "  1  226.530304  225.331055  453.969513  319.183655    0.895880      2   \n",
       "  2  484.314392  225.595703  609.373840  322.552246    0.895057      3   \n",
       "  3  448.716248  227.096405  512.476868  320.679413    0.700851      3   \n",
       "  4  111.621719  189.849792  341.915955  261.278870    0.495684      7   \n",
       "  \n",
       "           name  \n",
       "  0         car  \n",
       "  1         car  \n",
       "  2  motorcycle  \n",
       "  3  motorcycle  \n",
       "  4       truck  ,\n",
       "  'caption': ['A blue and green checkered car.',\n",
       "   'A silver, green, and blue checkered car next to some motorcycles.'],\n",
       "  'bbox_target': [231.91, 223.88, 226.35, 89.31]},\n",
       " 306: {'image_emb': tensor([[-0.1044,  0.4688,  0.1595,  ...,  1.0273, -0.3098, -0.1129],\n",
       "          [-0.1946,  0.2350,  0.5181,  ...,  0.9956,  0.2593, -0.1917],\n",
       "          [ 0.3816,  0.3916,  0.1875,  ...,  0.8447,  0.0189, -0.2759],\n",
       "          [ 0.1260,  0.2505,  0.0768,  ...,  0.9248,  0.1857, -0.2224],\n",
       "          [-0.1134, -0.2018, -0.4065,  ...,  1.3154,  0.0761, -0.1218],\n",
       "          [ 0.1288,  0.3003,  0.1339,  ...,  1.2148, -0.2932, -0.2883]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1061,  0.0561, -0.1025,  ...,  0.4355, -0.3723, -0.3035],\n",
       "          [ 0.0797,  0.0468,  0.0887,  ...,  0.1864, -0.2418, -0.3044]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9609e-01, 5.3048e-06, 2.2054e-06, 7.1108e-05, 1.0848e-04, 3.8242e-03],\n",
       "          [9.6582e-01, 1.7619e-04, 6.0511e-04, 6.6109e-03, 6.4421e-04, 2.6138e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    44.449150  226.116760  226.562592  405.026367    0.929933     56   \n",
       "  1   207.323090  197.773926  515.735718  339.242676    0.899603     57   \n",
       "  2   449.366364   32.053070  640.000000  208.443207    0.864284     58   \n",
       "  3   156.269745   83.501984  250.601135  279.185760    0.813395     58   \n",
       "  4   281.751740  253.633240  321.876129  301.737610    0.748371     75   \n",
       "  5    17.619720    0.099350  131.929291  161.129272    0.631276     58   \n",
       "  6   486.157959  200.347656  640.000000  415.781433    0.596777     56   \n",
       "  7   571.970276  156.090790  639.721741  250.352570    0.384245     58   \n",
       "  8    53.415947   69.703873  114.014000  201.678345    0.330764     74   \n",
       "  9   319.496094  255.905884  362.466187  307.418091    0.283374     73   \n",
       "  10  384.004333  255.844116  402.177063  272.798767    0.281166      0   \n",
       "  \n",
       "              name  \n",
       "  0          chair  \n",
       "  1          couch  \n",
       "  2   potted plant  \n",
       "  3   potted plant  \n",
       "  4           vase  \n",
       "  5   potted plant  \n",
       "  6          chair  \n",
       "  7   potted plant  \n",
       "  8          clock  \n",
       "  9           book  \n",
       "  10        person  ,\n",
       "  'caption': ['A tan sitting chair with a patterned pillow in it to the right of the coffee table',\n",
       "   'stuffed beige armchair'],\n",
       "  'bbox_target': [486.41, 201.37, 153.59, 213.51]},\n",
       " 307: {'image_emb': tensor([[-0.2859,  0.4163, -0.3108,  ...,  1.2744,  0.2170, -0.5239],\n",
       "          [-0.0074,  0.4336, -0.2329,  ...,  1.0273,  0.3672,  0.1920],\n",
       "          [-0.3840,  0.0629, -0.3049,  ...,  0.8335,  0.2190, -0.4080]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1675,  0.2047, -0.5059,  ..., -0.0139,  0.2507,  0.1842],\n",
       "          [ 0.0591,  0.2163, -0.3423,  ...,  0.0452,  0.2477,  0.1077]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9707e-01, 1.7881e-07, 2.8458e-03],\n",
       "          [9.9756e-01, 0.0000e+00, 2.3594e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  369.510651  160.798523  469.464325  423.566040    0.944740      0   \n",
       "  1  264.827850   52.827209  325.921844  118.519714    0.868220     74   \n",
       "  2  613.847534  196.014954  633.353271  227.877991    0.634737      0   \n",
       "  3  459.943176  353.214905  521.168030  411.446594    0.410054     24   \n",
       "  4  246.864059  287.451050  285.212189  328.760254    0.315769     58   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1         clock  \n",
       "  2        person  \n",
       "  3      backpack  \n",
       "  4  potted plant  ,\n",
       "  'caption': ['A man in a yellow shirt.',\n",
       "   'Mature man standing by flowers with a yellow shirt on.'],\n",
       "  'bbox_target': [365.45, 173.59, 99.06, 248.12]},\n",
       " 308: {'image_emb': tensor([[-0.3198, -0.3274,  0.0897,  ...,  0.8618, -0.1350,  0.0363],\n",
       "          [-0.4636,  0.0183,  0.1126,  ...,  1.1221, -0.1949, -0.0418],\n",
       "          [-0.2913, -0.3567, -0.0153,  ...,  0.8613, -0.1665,  0.1155]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3501, -0.0549, -0.3945,  ...,  0.3228, -0.3130,  0.4429],\n",
       "          [ 0.0670, -0.0435, -0.3113,  ..., -0.0396, -0.1050,  0.2445]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.7134, 0.0403, 0.2465],\n",
       "          [0.6841, 0.0562, 0.2598]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0   48.043030   35.056808  538.414978  419.848694    0.939205     17  horse\n",
       "  1  372.956299  185.711578  638.564087  420.482483    0.860605     17  horse,\n",
       "  'caption': [\"a dark brown and black horse, poking it's nose through the bars of the fence\",\n",
       "   'An animal sticking its face through the fence.'],\n",
       "  'bbox_target': [56.19, 37.19, 480.17, 383.11]},\n",
       " 309: {'image_emb': tensor([[ 0.1309,  0.5752, -0.2479,  ...,  0.9458, -0.0724,  0.1929],\n",
       "          [ 0.0511,  0.6016, -0.2886,  ...,  0.8906,  0.0728,  0.0486],\n",
       "          [-0.1248,  0.4026, -0.3528,  ...,  0.8887,  0.1848, -0.3984],\n",
       "          ...,\n",
       "          [-0.0481,  0.1846, -0.1797,  ...,  0.7368, -0.2316, -0.3701],\n",
       "          [-0.2629,  0.4980, -0.2686,  ...,  0.8271,  0.1519, -0.3936],\n",
       "          [-0.1525,  0.1580,  0.1287,  ...,  0.6865, -0.0556, -0.2676]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3164,  0.3909, -0.3867,  ...,  0.4565,  0.0654, -0.2240],\n",
       "          [-0.0434, -0.1710, -0.4963,  ..., -0.3745,  0.2078,  0.1893]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.3641e-04, 2.7294e-03, 4.8184e-04, 4.2953e-03, 2.8172e-03, 3.3319e-05,\n",
       "           1.9360e-03, 9.8730e-01, 1.4019e-04],\n",
       "          [3.1201e-01, 1.9333e-02, 8.0139e-02, 3.9453e-01, 8.1062e-04, 1.4595e-02,\n",
       "           1.3635e-01, 4.2496e-03, 3.7872e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     0.257713  104.983810   90.776894  419.792480    0.934290      0   \n",
       "  1   322.737549   99.341980  469.936218  458.519409    0.895722      0   \n",
       "  2   157.708435  101.027740  304.181030  427.245453    0.879266      0   \n",
       "  3    44.515755  150.527695  137.148590  399.120178    0.871474     67   \n",
       "  4   447.799438   65.498535  581.145386  336.961975    0.865797     67   \n",
       "  5   310.969421   89.239578  399.455200  373.120850    0.864370      0   \n",
       "  6   242.550217  119.744278  319.010864  397.318237    0.822797      0   \n",
       "  7   449.880219  161.808853  576.497742  449.641418    0.732457      0   \n",
       "  8   232.678070  149.663864  286.954742  296.389526    0.695825     26   \n",
       "  9   398.210266  158.225586  480.216736  329.620300    0.606372     26   \n",
       "  10    0.120134   88.135727   20.154398  154.848740    0.322069      0   \n",
       "  \n",
       "            name  \n",
       "  0       person  \n",
       "  1       person  \n",
       "  2       person  \n",
       "  3   cell phone  \n",
       "  4   cell phone  \n",
       "  5       person  \n",
       "  6       person  \n",
       "  7       person  \n",
       "  8      handbag  \n",
       "  9      handbag  \n",
       "  10      person  ,\n",
       "  'caption': ['A cell phone costume being held on the sidewalk by a female.',\n",
       "   'gray camera-looking thing behind the lady in blue'],\n",
       "  'bbox_target': [43.9, 153.84, 92.29, 244.8]},\n",
       " 310: {'image_emb': tensor([[-0.1406,  0.1930, -0.2026,  ...,  0.6885,  0.0523,  0.2267],\n",
       "          [-0.1875,  0.1439, -0.1094,  ...,  0.5830,  0.0135, -0.2505],\n",
       "          [-0.0324,  0.3450, -0.6162,  ...,  0.7822, -0.1992,  0.2070],\n",
       "          [ 0.1963,  0.4595, -0.1367,  ...,  1.0537,  0.3267,  0.4170],\n",
       "          [-0.0185,  0.0828, -0.3738,  ...,  0.7661,  0.1418,  0.0158]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1094,  0.1532, -0.7446,  ..., -0.4412,  0.1606, -0.0883],\n",
       "          [-0.2272, -0.1719, -0.2590,  ...,  0.1444,  0.0326, -0.0411]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.0665e-03, 9.9512e-01, 1.8156e-04, 3.3927e-04, 8.1837e-05],\n",
       "          [2.2232e-02, 9.7559e-01, 5.0688e-04, 9.0003e-06, 1.7147e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   97.993988   36.857208  417.205414  634.978760    0.929155      0   \n",
       "  1    0.120438  147.893478  178.398315  638.294067    0.898399      0   \n",
       "  2  226.297180  231.558258  480.000000  638.096069    0.889204     60   \n",
       "  3  365.013611   66.531380  404.270508  105.459038    0.809506     74   \n",
       "  4  347.460510  353.497498  477.819946  413.837341    0.611469     45   \n",
       "  5  384.179138  596.486389  480.000000  639.858704    0.496751     45   \n",
       "  6  331.043243  353.623016  395.968475  402.100739    0.490791     43   \n",
       "  7  389.980286  354.856262  409.102295  398.182312    0.472478     42   \n",
       "  8  438.639709  489.447937  479.680359  598.056824    0.254364     43   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2  dining table  \n",
       "  3         clock  \n",
       "  4          bowl  \n",
       "  5          bowl  \n",
       "  6         knife  \n",
       "  7          fork  \n",
       "  8         knife  ,\n",
       "  'caption': ['girl with closed eyes and red top in the front side of the image',\n",
       "   'child in pink smiling sitting next to woman in white sweater'],\n",
       "  'bbox_target': [0.0, 151.01, 230.11, 488.99]},\n",
       " 311: {'image_emb': tensor([[-0.1214, -0.3789, -0.0918,  ...,  0.7598,  0.0266,  0.5229],\n",
       "          [ 0.0182, -0.2179, -0.1266,  ...,  0.6802,  0.0779,  0.5166],\n",
       "          [-0.0240, -0.0047, -0.0428,  ...,  1.4902,  0.0326, -0.1964],\n",
       "          [ 0.0859, -0.2859,  0.0990,  ...,  0.5327,  0.0226,  0.5796]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1395,  0.2988,  0.0521,  ...,  0.1733, -0.5166, -0.2983],\n",
       "          [-0.1089,  0.2142, -0.3145,  ..., -0.2900, -0.1581,  0.1400]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3472, 0.0194, 0.6191, 0.0142],\n",
       "          [0.1805, 0.4685, 0.3025, 0.0486]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  267.189880    2.665070  538.684570  422.188721    0.954120     20  elephant\n",
       "  1   56.962662  125.044342  330.696899  422.367432    0.891210      0    person\n",
       "  2  165.180542  208.532166  266.672302  422.401794    0.826078      0    person\n",
       "  3    0.599136  138.509689  170.830826  332.391479    0.640103      2       car,\n",
       "  'caption': ['car parked near the house',\n",
       "   'The gray car in the background behind the people'],\n",
       "  'bbox_target': [0.96, 137.22, 142.97, 175.59]},\n",
       " 312: {'image_emb': tensor([[-0.0974, -0.3218, -0.1561,  ...,  0.4856,  0.1477,  0.0233],\n",
       "          [-0.1965, -0.3003, -0.1167,  ...,  0.3076,  0.1372, -0.0053],\n",
       "          [-0.2961, -0.4099, -0.1283,  ..., -0.0292, -0.1667,  0.0083]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3589, -0.0889, -0.1008,  ...,  0.1049,  0.1379,  0.0839],\n",
       "          [-0.4126, -0.1764,  0.1353,  ...,  0.3579,  0.2534,  0.5854]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1225, 0.2554, 0.6221],\n",
       "          [0.0670, 0.5352, 0.3977]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   33.055084   48.154617  624.346558  462.683655    0.946152     22   zebra\n",
       "  1  235.012955  150.027969  582.404724  480.370911    0.942753     22   zebra\n",
       "  2  411.780212    3.451614  438.318787   49.933060    0.384421      0  person,\n",
       "  'caption': ['A baby zebra standing next to an adult zebra.', 'baby zebra'],\n",
       "  'bbox_target': [240.46, 157.49, 340.55, 317.71]},\n",
       " 313: {'image_emb': tensor([[ 0.0246,  0.3276,  0.5552,  ...,  0.4165, -0.0651, -0.0611],\n",
       "          [-0.4985,  0.2000,  0.0831,  ...,  0.2123, -0.6318, -0.2959],\n",
       "          [-0.3130,  0.3938, -0.0295,  ...,  0.5171, -0.2151, -0.2064],\n",
       "          [-0.5913,  0.1869, -0.0233,  ...,  1.0811, -0.3999,  0.0160],\n",
       "          [-0.0383,  0.1799,  0.5928,  ...,  0.3572, -0.0969, -0.2300]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0341, -0.0573, -0.2062,  ...,  0.0914, -0.1497,  0.0286],\n",
       "          [-0.0490, -0.1821, -0.2030,  ...,  1.0713, -0.5308,  0.0426]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0154, 0.6870, 0.2412, 0.0453, 0.0111],\n",
       "          [0.0059, 0.9248, 0.0364, 0.0316, 0.0014]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    3.286484   30.851227  378.163818  419.424805    0.956572      0   \n",
       "  1  322.551544  128.029465  528.653137  420.175110    0.943095      0   \n",
       "  2  448.054260   32.144135  639.959778  419.439209    0.930588      0   \n",
       "  3  534.115601  218.365540  586.283203  413.836548    0.795731      0   \n",
       "  4    0.256635  346.296173   31.853138  422.987549    0.518452     34   \n",
       "  5  448.717194   48.156921  484.190460  124.428070    0.404437     35   \n",
       "  \n",
       "               name  \n",
       "  0          person  \n",
       "  1          person  \n",
       "  2          person  \n",
       "  3          person  \n",
       "  4    baseball bat  \n",
       "  5  baseball glove  ,\n",
       "  'caption': ['man sitting touching arm',\n",
       "   'Baseball player sitting on bench behind fence'],\n",
       "  'bbox_target': [323.5, 128.73, 211.52, 295.27]},\n",
       " 314: {'image_emb': tensor([[ 0.0315,  0.3269, -0.1805,  ...,  0.9805, -0.2615, -0.0277],\n",
       "          [-0.3289,  0.6392, -0.2629,  ...,  0.9448,  0.4250, -0.3831],\n",
       "          [-0.2874,  0.5767, -0.4651,  ...,  0.7827,  0.0560,  0.0565],\n",
       "          ...,\n",
       "          [-0.3369,  0.4348, -0.0276,  ...,  1.0469, -0.0341,  0.2754],\n",
       "          [ 0.0119,  0.0237, -0.1559,  ...,  1.2715,  0.2581,  0.2058],\n",
       "          [ 0.0252,  0.2695, -0.2825,  ...,  0.7026, -0.2646,  0.0238]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0640, -0.0725, -0.0425,  ..., -0.0531, -0.2411,  0.1547],\n",
       "          [ 0.4141,  0.4719, -0.2484,  ..., -0.1526, -0.2737,  0.0845]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.5000e-01, 3.9637e-05, 3.9966e-01, 1.0881e-03, 3.5524e-05, 4.2200e-05,\n",
       "           7.5698e-06, 1.2260e-02, 5.5552e-04, 1.4410e-03, 2.0444e-04, 1.1162e-02,\n",
       "           3.7789e-05, 1.4126e-05, 7.9584e-04, 1.2451e-02, 2.0981e-03, 1.1578e-03,\n",
       "           3.0640e-01, 3.9411e-04, 5.0902e-05],\n",
       "          [1.0693e-01, 4.4136e-03, 1.9775e-02, 7.5073e-03, 1.3959e-04, 2.5391e-02,\n",
       "           3.9995e-05, 1.9470e-02, 9.9003e-05, 3.6583e-03, 2.4128e-04, 3.6957e-02,\n",
       "           4.7722e-03, 9.2506e-04, 1.6647e-02, 6.8359e-03, 1.3596e-02, 1.5640e-02,\n",
       "           6.8652e-01, 3.0624e-02, 1.1623e-05]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     0.468384    0.526596  148.374054  270.184021    0.932626      0   \n",
       "  1   532.542847  127.047501  636.181519  252.502625    0.915937     39   \n",
       "  2   416.485596    0.185173  639.676758  138.685944    0.912076      0   \n",
       "  3   141.259003   52.571472  189.009460  109.591400    0.908461     41   \n",
       "  4     0.226593  295.835968  165.078995  471.697021    0.906984     53   \n",
       "  5   463.660217  129.336761  512.844299  189.246002    0.906967     41   \n",
       "  6   239.185303   47.895599  397.424927  185.287750    0.904375     53   \n",
       "  7   289.330322  341.848785  369.143738  431.443268    0.892920     43   \n",
       "  8     0.885956   29.388702  639.272461  470.409058    0.888968     60   \n",
       "  9   338.989655  286.556519  398.056427  367.119690    0.881021     53   \n",
       "  10  472.913635  290.862030  639.718323  391.841217    0.877587     53   \n",
       "  11  433.458466  194.265961  497.064484  276.477325    0.860720     53   \n",
       "  12  156.140457  235.783783  202.770493  287.586884    0.856022     41   \n",
       "  13  404.038940  294.156464  450.272095  348.282074    0.855706     41   \n",
       "  14  363.970978  194.146973  424.624176  275.661682    0.810364     53   \n",
       "  15   90.393524  113.750381  148.501343  177.629700    0.802653     53   \n",
       "  16  388.008453  202.617096  451.661285  280.467194    0.792658     53   \n",
       "  17  289.048309  301.702362  339.046295  330.245880    0.744894     53   \n",
       "  18  216.274597  309.540009  296.095093  394.255585    0.742710     42   \n",
       "  19  158.113831  147.866348  202.971466  199.234985    0.720502     53   \n",
       "  20  473.271942   70.462036  542.615967   97.562531    0.600152     43   \n",
       "  21   77.267601  215.556183  167.127533  251.718292    0.504165     42   \n",
       "  22  318.382843  339.404297  352.616180  366.413696    0.494390     53   \n",
       "  23  215.638962  311.290619  294.974976  393.834808    0.473713     43   \n",
       "  24    0.285374  294.027710   65.971054  322.168457    0.437846     43   \n",
       "  25  455.469025  373.095856  516.389832  420.430023    0.347992     53   \n",
       "  26  478.594696  422.024353  540.073364  460.750671    0.287044     53   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         bottle  \n",
       "  2         person  \n",
       "  3            cup  \n",
       "  4          pizza  \n",
       "  5            cup  \n",
       "  6          pizza  \n",
       "  7          knife  \n",
       "  8   dining table  \n",
       "  9          pizza  \n",
       "  10         pizza  \n",
       "  11         pizza  \n",
       "  12           cup  \n",
       "  13           cup  \n",
       "  14         pizza  \n",
       "  15         pizza  \n",
       "  16         pizza  \n",
       "  17         pizza  \n",
       "  18          fork  \n",
       "  19         pizza  \n",
       "  20         knife  \n",
       "  21          fork  \n",
       "  22         pizza  \n",
       "  23         knife  \n",
       "  24         knife  \n",
       "  25         pizza  \n",
       "  26         pizza  ,\n",
       "  'caption': ['A person wearing a black sweater finishing their meal',\n",
       "   'A persons arms with black sleeves.'],\n",
       "  'bbox_target': [415.0, 1.88, 225.0, 132.5]},\n",
       " 315: {'image_emb': tensor([[-0.1591, -0.0820,  0.2549,  ...,  0.7114, -0.1260, -0.1223],\n",
       "          [-0.4351,  0.1522, -0.0561,  ...,  1.2441, -0.0247, -0.1323],\n",
       "          [-0.2081,  0.6260,  0.0328,  ...,  1.1729,  0.0402, -0.1589],\n",
       "          [-0.3684,  0.4612, -0.2024,  ...,  1.2363, -0.1365,  0.1163],\n",
       "          [-0.4690, -0.2095,  0.1527,  ...,  1.1416,  0.0097, -0.0521],\n",
       "          [-0.2998, -0.3337,  0.1207,  ...,  0.8638,  0.0198, -0.0088]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2167,  0.1223,  0.0154,  ..., -0.0687, -0.0976,  0.0506],\n",
       "          [-0.3069, -0.1404, -0.0337,  ...,  0.2338, -0.3271, -0.0371]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5747, 0.1205, 0.0698, 0.0397, 0.0226, 0.1726],\n",
       "          [0.4443, 0.0726, 0.0433, 0.1273, 0.0992, 0.2133]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  name\n",
       "  0   26.508942   63.122757  639.457520  395.855835    0.944582     15   cat\n",
       "  1  188.275894  257.290955  388.629761  374.566711    0.931794     45  bowl\n",
       "  2   59.443382  323.133728  276.651184  424.368164    0.929852     45  bowl\n",
       "  3    0.315205  254.812592   80.373474  418.786987    0.928456     45  bowl\n",
       "  4   13.897232  174.940063  202.760452  317.476440    0.893507     45  bowl,\n",
       "  'caption': ['A bowl that the cat is currently eating out of.',\n",
       "   'the food bowl that the cat is eating from'],\n",
       "  'bbox_target': [12.47, 180.4, 195.75, 135.29]},\n",
       " 316: {'image_emb': tensor([[ 0.0977, -0.3855,  0.1447,  ...,  0.4106, -0.0471, -0.2727],\n",
       "          [ 0.2310, -0.1361,  0.0229,  ...,  0.7666, -0.0457, -0.2382],\n",
       "          [-0.2480, -0.4060,  0.0576,  ...,  0.5366, -0.1212, -0.1704]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0728, -0.1743, -0.0641,  ...,  0.0759, -0.5742, -0.1873],\n",
       "          [ 0.1026, -0.2771, -0.2281,  ..., -0.1312, -0.2380, -0.2888]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5547, 0.1665, 0.2788],\n",
       "          [0.3303, 0.2455, 0.4241]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  213.895187   80.907700  513.736023  378.570435    0.955939     23  giraffe\n",
       "  1  469.761597  158.284073  501.494507  224.557922    0.872615     23  giraffe,\n",
       "  'caption': ['A brown and white giraffe bending down to eat.',\n",
       "   'A giraffee grazing'],\n",
       "  'bbox_target': [215.62, 86.41, 296.89, 292.55]},\n",
       " 317: {'image_emb': tensor([[-0.1454,  0.5845, -0.2600,  ...,  1.0693,  0.1306,  0.0677],\n",
       "          [ 0.0853,  0.7627, -0.1274,  ...,  0.4224, -0.1697,  0.0898],\n",
       "          [-0.4036,  0.5679, -0.2043,  ...,  1.3047,  0.2725,  0.0304],\n",
       "          [-0.1035,  0.4553, -0.1566,  ...,  0.7080, -0.3027, -0.5522]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1748,  0.1192, -0.3679,  ...,  0.5747,  0.2314, -0.2291],\n",
       "          [-0.2568, -0.0091,  0.1364,  ..., -0.0133, -0.2964, -0.1304]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1785, 0.7168, 0.0121, 0.0926],\n",
       "          [0.0837, 0.7227, 0.0108, 0.1827]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  395.989075  305.560303  527.763306  414.372925    0.795009     47   \n",
       "  1  294.209930    7.153229  618.147583  372.230988    0.766331     50   \n",
       "  2  397.701141  186.663025  478.937042  252.422363    0.740371     51   \n",
       "  3    8.501862    6.519684  634.427612  486.000000    0.619361     45   \n",
       "  4  192.418182  340.503052  484.360931  430.577942    0.584200     50   \n",
       "  5  323.732117  204.309143  365.816223  249.081604    0.482586     50   \n",
       "  6    4.376556    5.245056  635.630249  486.000000    0.408247     60   \n",
       "  \n",
       "             name  \n",
       "  0         apple  \n",
       "  1      broccoli  \n",
       "  2        carrot  \n",
       "  3          bowl  \n",
       "  4      broccoli  \n",
       "  5      broccoli  \n",
       "  6  dining table  ,\n",
       "  'caption': ['A bit of broccoli sits beside some radishes at the front of the container',\n",
       "   'A small amount of parts of broccoli flowerettes in the extreme bottom of the picture.'],\n",
       "  'bbox_target': [197.15, 342.6, 284.67, 85.6]},\n",
       " 318: {'image_emb': tensor([[ 0.2502, -0.0593, -0.4351,  ...,  0.0923,  0.0573, -0.0696],\n",
       "          [ 0.0222,  0.3789, -0.1954,  ...,  1.1426,  0.0569, -0.0220],\n",
       "          [ 0.1219, -0.0522, -0.1725,  ...,  0.9731, -0.0560, -0.2517],\n",
       "          [ 0.1643, -0.1971, -0.2722,  ...,  1.2529,  0.0084, -0.3320],\n",
       "          [ 0.3369, -0.1209, -0.3298,  ...,  0.0587, -0.0692, -0.1141]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2771,  0.0721, -0.1458,  ..., -0.1218,  0.0278, -0.1792],\n",
       "          [ 0.0271,  0.0112, -0.0149,  ...,  0.2449, -0.2815, -0.0569]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7519e-03, 8.7939e-01, 1.0796e-03, 5.1785e-04, 1.1719e-01],\n",
       "          [4.3716e-03, 9.0088e-01, 4.2381e-03, 1.4420e-03, 8.9172e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  136.986755  102.468613  515.363586  292.458252    0.938815      6   train\n",
       "  1   16.044918  223.414185   99.363953  378.802612    0.862846     13   bench\n",
       "  2  609.590637  174.690582  630.846008  248.944305    0.852265      0  person\n",
       "  3  550.049316  174.533173  568.139771  248.030487    0.717038      0  person\n",
       "  4  521.636719  180.696472  539.643188  247.539368    0.694570      0  person\n",
       "  5  588.772766  172.404846  610.931213  248.370056    0.689116      0  person\n",
       "  6  537.251038  175.414032  554.275208  247.348969    0.632997      0  person\n",
       "  7  514.694580  176.707489  530.358887  244.803619    0.573884      0  person\n",
       "  8   49.352867  219.559387  101.297539  264.316467    0.490169     13   bench\n",
       "  9  227.229645  164.762329  240.075653  212.537231    0.300167      0  person,\n",
       "  'caption': ['empty bench on platform, to the left',\n",
       "   'A wood bench sitting at the train station.'],\n",
       "  'bbox_target': [14.16, 223.69, 93.44, 151.95]},\n",
       " 319: {'image_emb': tensor([[ 0.0727,  1.0928,  0.1914,  ...,  0.8174, -0.3193,  0.1582],\n",
       "          [-0.0291,  0.8110,  0.0784,  ...,  0.4583, -0.4644,  0.1279],\n",
       "          [ 0.0518,  0.6270, -0.3989,  ...,  0.5210, -0.0392, -0.3328],\n",
       "          [ 0.0629,  0.4688, -0.2184,  ...,  0.5278,  0.0782, -0.1259],\n",
       "          [-0.0981,  0.8213,  0.2688,  ...,  0.5854, -0.1005, -0.0919]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0621,  0.0636, -0.1589,  ...,  0.3230,  0.2615,  0.1893],\n",
       "          [-0.1065,  0.1821, -0.2739,  ..., -0.5049, -0.0450, -0.3506]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2209, 0.2927, 0.0663, 0.2427, 0.1775],\n",
       "          [0.1060, 0.1566, 0.0838, 0.0533, 0.6001]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    5.188406   11.533934  218.354797  168.693466    0.942763     63  laptop\n",
       "  1  271.740875  190.645370  471.770813  358.782898    0.942485     63  laptop\n",
       "  2  246.864136    7.622642  500.000000  174.180664    0.934064     63  laptop\n",
       "  3   50.040733  207.793671  217.264374  356.186432    0.895587     63  laptop,\n",
       "  'caption': ['The fully-opened black laptop.', 'the laptop at the top left'],\n",
       "  'bbox_target': [3.33, 12.47, 214.51, 162.97]},\n",
       " 320: {'image_emb': tensor([[-0.1493,  0.1393, -0.2852,  ...,  1.0410,  0.2032,  0.0826],\n",
       "          [-0.4155,  0.3730, -0.2600,  ...,  1.1611,  0.1317,  0.2460],\n",
       "          [-0.1158, -0.0830, -0.1187,  ...,  0.3044,  0.2798,  0.4102]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0869, -0.0481, -0.0735,  ..., -0.4661, -0.0969, -0.1588],\n",
       "          [-0.0041,  0.0527, -0.2076,  ..., -0.3250,  0.1434, -0.1742]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0168, 0.9629, 0.0203],\n",
       "          [0.0693, 0.8315, 0.0993]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  216.546967  137.564072  353.638184  411.205444    0.877061      0  person\n",
       "  1  120.169701  160.613983  228.633148  402.487854    0.826323      0  person\n",
       "  2  258.193146  212.281616  278.497772  295.317352    0.625017     27     tie,\n",
       "  'caption': ['Woman in the picture', 'the woman in the right hand picture'],\n",
       "  'bbox_target': [112.98, 160.06, 111.85, 246.09]},\n",
       " 321: {'image_emb': tensor([[-0.3916, -0.0829, -0.0896,  ...,  0.7920, -0.0323, -0.2355],\n",
       "          [-0.1566,  0.0930, -0.4280,  ...,  0.9048, -0.0296,  0.1357],\n",
       "          [-0.1631,  0.1171, -0.0261,  ...,  1.0850, -0.1725,  0.0935],\n",
       "          ...,\n",
       "          [-0.1155, -0.1382, -0.0377,  ...,  0.8188, -0.2430, -0.1755],\n",
       "          [-0.3132,  0.3279,  0.0745,  ...,  1.1592, -0.0590, -0.3391],\n",
       "          [-0.3931,  0.0476,  0.0660,  ...,  0.4636,  0.0535, -0.2378]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 2.2156e-02,  3.1342e-02, -3.9990e-01, -3.6548e-01, -2.2766e-01,\n",
       "            5.4321e-02, -1.2585e-01, -3.4399e-01, -1.3403e-01, -2.2974e-01,\n",
       "            1.4197e-01, -6.1432e-02, -6.9824e-02, -1.5234e-01,  6.8481e-02,\n",
       "            2.5610e-01,  2.5610e-01,  7.0923e-02, -6.0400e-01,  3.6646e-01,\n",
       "            2.5073e-01,  8.3069e-02, -4.5410e-02,  5.3857e-01,  4.0576e-01,\n",
       "           -4.4159e-02, -3.5645e-01,  1.3281e-01, -7.1655e-02, -2.2485e-01,\n",
       "            1.6516e-01, -7.9651e-02, -9.2712e-02, -2.2571e-01, -1.3945e+00,\n",
       "           -5.5023e-02,  1.9421e-01, -1.2201e-01,  1.4343e-01,  6.9153e-02,\n",
       "           -3.4210e-02,  1.5076e-01,  3.1982e-01, -5.1270e-01,  1.0809e-01,\n",
       "           -5.3711e-02,  4.5654e-01,  1.1481e-01, -3.6523e-01,  2.0850e-01,\n",
       "           -1.6602e-01, -6.8555e-01,  6.1328e-01, -3.3203e-01, -4.3994e-01,\n",
       "            3.6768e-01, -7.2174e-03,  5.6488e-02, -2.9346e-01,  2.8442e-01,\n",
       "           -4.2896e-01,  2.2485e-01,  3.8055e-02, -6.7810e-02, -2.5708e-01,\n",
       "           -2.6416e-01,  4.6802e-01,  3.1226e-01, -4.2651e-01, -9.0332e-02,\n",
       "            3.0914e-02, -2.3096e-01,  1.1407e-01, -5.5634e-02,  2.2302e-01,\n",
       "           -4.9774e-02,  3.1021e-02, -9.3994e-02, -9.6802e-02,  6.5063e-02,\n",
       "           -6.4404e-01,  3.6377e-01, -2.2766e-02,  9.2529e-02, -1.3562e-01,\n",
       "           -1.7993e-01,  2.1313e-01,  6.7383e-02, -9.3445e-02,  1.8173e-02,\n",
       "            7.0435e-02, -1.6968e-01, -7.5439e-01,  1.4417e-01, -4.0381e-01,\n",
       "            2.7881e-01,  1.1505e-01, -1.0602e-01,  6.9385e-01, -7.5500e-02,\n",
       "           -3.0249e-01, -2.7527e-02, -4.8218e-02, -4.0576e-01,  6.6895e-02,\n",
       "            1.6467e-01,  8.6792e-02,  3.1714e-01,  4.7882e-02,  2.8290e-02,\n",
       "           -1.2146e-01,  6.2805e-02,  6.8726e-02,  4.9414e-01, -3.7500e-01,\n",
       "            7.9773e-02,  2.4963e-01,  5.7275e-01, -1.3412e-02,  4.4946e-01,\n",
       "            2.6703e-02, -5.2881e-01, -1.6266e-02,  4.0967e-01,  2.6685e-01,\n",
       "           -3.7793e-01,  1.1298e-01, -2.7563e-01,  8.2947e-02,  3.2397e-01,\n",
       "            1.1353e-01, -2.3682e-01, -1.4526e-01,  3.0410e+00, -6.2500e-01,\n",
       "            3.5767e-01,  2.2620e-01, -2.2070e-01,  6.5308e-02, -2.1924e-01,\n",
       "            4.9171e-03,  4.2334e-01, -5.9033e-01,  2.1545e-01,  1.0156e-01,\n",
       "           -3.9844e-01, -1.5222e-01, -7.6221e-01, -1.4624e-01,  1.9958e-01,\n",
       "           -1.4143e-03,  4.4769e-02,  3.1152e-01,  9.9854e-02, -2.8540e-01,\n",
       "           -1.4636e-01, -1.7310e-01,  9.4557e-04,  7.1973e-01,  2.2412e-01,\n",
       "            1.8372e-01, -1.9226e-01, -1.5515e-01, -3.7384e-02, -1.3397e-02,\n",
       "            1.9580e-01,  2.9541e-01, -1.4343e-01,  1.0199e-01, -7.2327e-02,\n",
       "           -2.8418e-01, -1.4709e-01,  1.2901e-02,  1.7358e-01, -9.0820e-02,\n",
       "           -5.9784e-02,  2.1753e-01, -3.4229e-01,  6.8408e-01, -2.7661e-01,\n",
       "           -2.4670e-01,  3.6865e-02, -4.7437e-01, -1.8921e-01, -2.0789e-01,\n",
       "           -3.4668e-01,  2.0004e-02, -1.6003e-01,  1.4778e-02,  2.2842e-02,\n",
       "            2.1216e-01, -1.4435e-02, -1.4648e-01, -1.2683e-01,  1.1536e-01,\n",
       "           -8.1055e-02, -2.3962e-01,  3.6890e-01, -1.1597e-01, -2.3547e-01,\n",
       "            2.8296e-01,  3.4180e-01, -8.4900e-02,  1.6663e-01, -4.9927e-02,\n",
       "            1.0913e-01,  3.5620e-01, -4.2511e-02,  7.9041e-02, -2.2693e-01,\n",
       "            4.2627e-01,  1.8848e-01,  2.2180e-01,  8.8684e-02,  1.4331e-01,\n",
       "           -2.5415e-01, -8.9111e-02, -1.4490e-01,  5.8301e-01,  2.9419e-01,\n",
       "            4.3604e-01,  6.4355e-01, -1.5137e-01, -1.9922e-01, -3.1641e-01,\n",
       "            1.1871e-01,  1.5625e-01, -1.8054e-01, -2.5977e-01,  6.0638e-02,\n",
       "           -4.6533e-01,  1.0760e-01,  8.2764e-02, -1.5222e-01,  5.2673e-02,\n",
       "            3.7415e-02,  3.3844e-02, -3.2997e-03, -2.2021e-01, -1.6647e-02,\n",
       "           -3.8501e-01,  5.4688e-02,  2.4463e-01,  2.0422e-01, -4.2407e-01,\n",
       "            1.8530e-01, -1.7151e-02,  1.1615e-01,  1.3416e-01, -6.6805e-04,\n",
       "            1.2128e-01,  3.9551e-02,  5.2441e-01, -1.6968e-01, -1.7908e-01,\n",
       "            2.3633e-01, -1.1131e-02,  3.4576e-02, -6.2103e-02, -1.0602e-01,\n",
       "           -4.6460e-01,  5.1367e-01, -1.4868e-01,  2.2339e-02, -1.2537e-01,\n",
       "            2.5659e-01,  3.8770e-01,  5.1666e-02, -5.2539e-01,  2.0093e-01,\n",
       "           -1.5430e-01,  1.5076e-01, -6.6338e-03,  1.4111e-01, -8.3191e-02,\n",
       "           -5.4688e-01, -1.3135e-01,  2.9272e-01, -1.2109e-01,  2.1204e-01,\n",
       "            3.0981e-01, -7.7858e-03,  4.4336e-01,  3.6133e-01,  1.4392e-01,\n",
       "            7.9041e-02, -7.3792e-02,  1.6248e-01,  2.9517e-01,  4.6289e-01,\n",
       "           -4.5752e-01, -3.4698e-02,  7.2449e-02,  1.2787e-02,  5.3864e-02,\n",
       "            1.8445e-01, -3.2031e-01,  1.5869e-02, -4.2114e-01,  2.4353e-01,\n",
       "           -5.3174e-01, -2.6782e-01, -3.7933e-02,  8.0688e-02,  2.9346e-01,\n",
       "           -7.1167e-02,  1.6235e-01, -1.6211e-01,  9.3262e-02, -3.7903e-02,\n",
       "            6.8726e-02,  7.2510e-01,  3.0352e+00, -2.1741e-01, -7.1602e-03,\n",
       "            6.1328e-01,  3.4863e-01, -1.3940e-01,  1.7480e-01,  2.7710e-01,\n",
       "           -1.8689e-01,  1.6272e-01, -5.2979e-02, -3.1885e-01, -4.5215e-01,\n",
       "            1.2537e-01, -2.9802e-04, -2.8638e-01, -3.7598e-02, -8.1104e-01,\n",
       "            4.1064e-01, -1.9360e-01,  2.4036e-01, -3.0078e-01, -1.5552e-01,\n",
       "            6.9214e-02, -2.7856e-01, -4.9042e-02,  4.7339e-01,  5.2094e-02,\n",
       "           -4.8535e-01, -4.1260e-01,  2.3346e-02, -3.4863e-01,  1.3196e-01,\n",
       "            5.6229e-03,  9.0088e-02, -1.2805e-01, -1.1499e-01,  3.9398e-02,\n",
       "            1.4001e-01, -2.9541e-02,  3.1815e-03,  5.4883e-01, -3.4863e-01,\n",
       "           -4.0625e-01, -1.1438e-01,  6.0455e-02, -6.9458e-02, -2.9883e-01,\n",
       "           -5.5811e-01, -1.5405e-01,  1.9495e-01,  8.9990e-01,  7.1716e-02,\n",
       "           -1.2268e-01,  4.4141e-01,  2.8271e-01, -3.6938e-01,  2.6459e-02,\n",
       "            2.7100e-02, -4.7559e-01, -9.7351e-02, -3.6304e-01, -3.8208e-01,\n",
       "            3.4204e-01,  2.4796e-02, -9.7930e-05,  4.9341e-01,  9.7778e-02,\n",
       "            1.1627e-01, -2.3889e-01,  5.3925e-02,  1.3171e-01,  4.7180e-02,\n",
       "            8.4717e-02,  1.3214e-02,  9.3506e-02,  2.9022e-02,  4.2915e-03,\n",
       "           -3.8721e-01,  4.2407e-01,  1.4392e-01,  4.8279e-02,  1.7798e-01,\n",
       "           -6.1798e-02,  4.5654e-01,  1.8140e-01, -5.4718e-02, -1.9336e-01,\n",
       "            6.4514e-02, -3.5913e-01,  5.4590e-01, -2.3352e-01, -3.9307e-01,\n",
       "           -3.9558e-03,  5.1758e-02,  1.5454e-01, -2.0032e-01, -3.1470e-01,\n",
       "            3.2837e-01,  1.2177e-01,  5.9631e-02,  3.9771e-01,  1.7432e-01,\n",
       "           -1.7792e-02, -4.8364e-01, -2.5610e-01, -7.0251e-02,  8.8318e-02,\n",
       "            1.5173e-01,  1.0681e-01,  2.2888e-01,  1.9141e-01,  1.9128e-01,\n",
       "            2.5586e-01, -2.5830e-01,  3.4839e-01,  1.4087e-01, -1.5332e-01,\n",
       "            7.8186e-02, -6.5247e-02, -2.5635e-01,  4.2450e-02, -4.2944e-01,\n",
       "            4.0375e-02, -2.1210e-02,  1.1407e-01, -1.2952e-01,  3.7842e-01,\n",
       "            8.1024e-03, -2.1838e-01, -4.9255e-02, -1.7932e-01, -1.5698e-01,\n",
       "           -2.3230e-01, -4.5215e-01, -2.9639e-01, -1.7407e-01,  3.3911e-01,\n",
       "            3.6230e-01,  2.2141e-02,  1.0498e-01, -2.0630e-01,  1.0901e-01,\n",
       "           -3.4058e-01,  5.6738e-01,  5.4053e-01,  2.4084e-01,  3.4302e-01,\n",
       "            1.3672e-01, -2.1088e-02,  4.5654e-01,  1.4038e-01,  3.9136e-01,\n",
       "           -1.4380e-01,  2.6520e-02, -2.7173e-01,  2.6718e-02,  4.9469e-02,\n",
       "            3.2031e-01, -3.6450e-01,  1.0101e-01,  2.2690e-02, -3.8013e-01,\n",
       "           -1.7407e-01, -7.0923e-02,  2.9724e-02, -5.4230e-02,  3.9917e-01,\n",
       "            2.7441e-01, -2.5806e-01,  1.8079e-01,  1.9934e-01, -2.5635e-01,\n",
       "            2.7637e-01, -1.3953e-01, -1.8689e-01,  6.5552e-02, -1.7273e-01,\n",
       "            6.9946e-02, -2.5684e-01,  5.1117e-03,  1.3965e+00,  3.2690e-01,\n",
       "           -2.6758e-01,  2.3047e-01, -5.5762e-01, -2.0715e-01, -1.2091e-01,\n",
       "           -1.8726e-01,  8.4351e-02, -1.1548e-01,  2.0959e-01,  3.0664e-01,\n",
       "           -1.9409e-01, -1.0229e-01, -1.3281e-01,  6.8115e-02,  1.0651e-01,\n",
       "           -3.4106e-01,  1.5552e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.2507e-02, 5.3662e-01, 1.1078e-01, 9.7778e-02, 2.2729e-01, 9.1970e-05,\n",
       "           2.9526e-03, 1.8473e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   438.428711  202.402344  537.106140  269.905579    0.914529     62   \n",
       "  1   451.964478  288.947906  639.679565  422.748779    0.902351     57   \n",
       "  2     0.016724  285.167480  355.872467  423.036560    0.852682     57   \n",
       "  3   131.384811  217.397003  176.661026  305.535126    0.829740     56   \n",
       "  4    12.146568  218.693268   77.070465  266.829437    0.821484     56   \n",
       "  5   192.382996   97.839951  215.277771  122.299515    0.783762     74   \n",
       "  6   112.585175  196.606079  171.458405  263.109070    0.769296     72   \n",
       "  7     0.000000  249.224152  160.325592  346.266022    0.423141     60   \n",
       "  8     5.344509  247.957062  117.825806  345.386932    0.415526     56   \n",
       "  9   174.933411  201.170013  220.681763  212.324371    0.414930     71   \n",
       "  10  250.547150  185.219299  287.595062  204.803772    0.376234     68   \n",
       "  11    0.000000  220.076141   16.610716  269.214691    0.325751     56   \n",
       "  \n",
       "              name  \n",
       "  0             tv  \n",
       "  1          couch  \n",
       "  2          couch  \n",
       "  3          chair  \n",
       "  4          chair  \n",
       "  5          clock  \n",
       "  6   refrigerator  \n",
       "  7   dining table  \n",
       "  8          chair  \n",
       "  9           sink  \n",
       "  10     microwave  \n",
       "  11         chair  ,\n",
       "  'caption': ['Red couch facing the bar of the kitchen'],\n",
       "  'bbox_target': [455.34, 289.74, 181.57, 137.26]},\n",
       " 322: {'image_emb': tensor([[ 0.0485,  0.0570, -0.1522,  ...,  1.1201,  0.1587,  0.0911],\n",
       "          [-0.0440,  0.1920, -0.1250,  ...,  1.2012,  0.1876,  0.1294],\n",
       "          [-0.0260,  0.0422,  0.2656,  ...,  0.7739,  0.2006,  0.1180]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2499, -0.4705, -0.2712,  ...,  0.4106, -0.4365, -0.3110],\n",
       "          [ 0.1263, -0.3865, -0.3323,  ...,  0.0808, -0.1765, -0.2778]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2561, 0.1482, 0.5957],\n",
       "          [0.0121, 0.0298, 0.9580]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class name\n",
       "  0   56.313332  52.507477  123.747398  179.813858    0.908292     15  cat\n",
       "  1  114.577370  34.495529  187.854172  155.223755    0.903371     15  cat\n",
       "  2    8.308316   0.497595  169.031082   45.551674    0.682155     62   tv,\n",
       "  'caption': ['A white cat sitting next to a cat with black spots.',\n",
       "   'White cat on the right'],\n",
       "  'bbox_target': [114.41, 42.21, 73.65, 112.2]},\n",
       " 323: {'image_emb': tensor([[ 0.1802,  0.5469,  0.2007,  ...,  0.5396,  0.1179,  0.2333],\n",
       "          [ 0.3513,  0.5371,  0.0372,  ...,  0.7256,  0.3540, -0.1587],\n",
       "          [ 0.0925,  0.5117,  0.1799,  ...,  0.3533,  0.0654,  0.3188]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.4133,  0.0359,  0.1752,  ...,  0.6943,  0.4587, -0.1721],\n",
       "          [ 0.0254,  0.0103, -0.0077,  ...,  0.0981,  0.2546, -0.2810]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.7427, 0.1683, 0.0887],\n",
       "          [0.1345, 0.8369, 0.0286]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   72.124573  166.939087  272.190460  351.806335    0.947010      0   \n",
       "  1  159.161469  295.212708  369.439484  381.651611    0.717309     37   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1  surfboard  ,\n",
       "  'caption': ['The surfer with the black wetsuit', 'a man surfing'],\n",
       "  'bbox_target': [75.68, 166.93, 196.61, 183.38]},\n",
       " 324: {'image_emb': tensor([[-0.1725,  0.1376,  0.0770,  ...,  0.6313, -0.2751,  0.0317],\n",
       "          [-0.0992,  0.3098, -0.2566,  ...,  1.1602, -0.0798, -0.1553],\n",
       "          [-0.5605, -0.0290, -0.1741,  ...,  0.5796, -0.2356, -0.0969]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1577, -0.1832, -0.4314,  ..., -0.3315,  0.1004,  0.1781],\n",
       "          [-0.1101,  0.1558,  0.1936,  ...,  0.0241,  0.1567,  0.3933]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2384, 0.6792, 0.0824],\n",
       "          [0.2448, 0.5869, 0.1682]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0    0.647690  233.353989  272.148132  546.675720    0.919014     17  horse\n",
       "  1  278.380371  262.796204  427.205383  634.889465    0.863742     17  horse,\n",
       "  'caption': ['horse in the background', 'Out of focus horse in weeds.'],\n",
       "  'bbox_target': [274.08, 261.09, 153.92, 314.26]},\n",
       " 325: {'image_emb': tensor([[-0.0555,  0.3503, -0.0560,  ...,  0.8013,  0.4241, -0.5591],\n",
       "          [-0.0294,  0.5684, -0.2522,  ...,  0.6851,  0.2878, -0.0847],\n",
       "          [ 0.0298,  0.4690, -0.2216,  ...,  0.6724,  0.4258, -0.6079],\n",
       "          ...,\n",
       "          [ 0.0327,  0.3918, -0.2734,  ...,  0.6133, -0.0412, -0.2952],\n",
       "          [ 0.0941,  0.1152, -0.0405,  ...,  1.3643,  0.0525,  0.1459],\n",
       "          [ 0.6343,  0.4534,  0.5464,  ...,  0.0452,  0.7700,  0.0434]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0714,  0.4521, -0.1862,  ..., -0.2356,  0.1589, -0.7573],\n",
       "          [ 0.2869,  0.3826, -0.3352,  ..., -0.2101,  0.0488, -0.3750]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.5605e-01, 2.9964e-03, 3.8269e-02, 2.7013e-04, 8.4496e-04, 3.7730e-05,\n",
       "           1.3094e-03],\n",
       "          [6.9434e-01, 1.9503e-03, 3.0322e-01, 1.2082e-04, 1.3685e-04, 2.6941e-05,\n",
       "           2.4796e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  355.191254  167.600677  473.253571  293.527435    0.927258     25  umbrella\n",
       "  1  178.710266  210.419403  256.711487  463.431274    0.914316      0    person\n",
       "  2  128.107208  179.378387  267.075714  260.934662    0.909875     25  umbrella\n",
       "  3  283.752319  196.260620  377.251587  458.728027    0.900497      0    person\n",
       "  4  324.316559  306.450989  395.258514  479.666687    0.853516      0    person\n",
       "  5  273.607819  240.884277  318.185150  322.042542    0.757311     26   handbag\n",
       "  6  348.747467  348.776733  394.389191  401.011414    0.542532     24  backpack\n",
       "  7  348.017426  221.563538  377.817902  273.341858    0.527594      0    person\n",
       "  8  380.454681  240.201202  393.528839  278.498749    0.401896     39    bottle\n",
       "  9  257.931763  236.853088  284.190247  279.344604    0.268308      0    person,\n",
       "  'caption': ['A person holding a white umbrella.',\n",
       "   'A woman in blue holding an umbrella.'],\n",
       "  'bbox_target': [282.42, 195.54, 102.01, 262.01]},\n",
       " 326: {'image_emb': tensor([[-0.3633,  0.2661, -0.1025,  ...,  1.3818,  0.2052, -0.1398],\n",
       "          [-0.1009,  0.2015, -0.2396,  ...,  0.4082,  0.2502,  0.5942],\n",
       "          [-0.3428,  0.2954,  0.0799,  ...,  0.9355,  0.0927,  0.0704],\n",
       "          [-0.2812,  0.5645, -0.2610,  ...,  0.9238,  0.5312, -0.1261],\n",
       "          [-0.5234,  0.3533, -0.0296,  ...,  0.7090,  0.0673, -0.1458]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3130,  0.1980,  0.0632,  ..., -0.2903, -0.0848,  0.0393],\n",
       "          [-0.3745,  0.3184,  0.0715,  ...,  0.0334, -0.1254, -0.1493]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.6162e-01, 1.9568e-01, 1.0309e-01, 2.0950e-02, 1.8478e-02],\n",
       "          [4.7505e-05, 9.9902e-01, 1.2219e-05, 1.1921e-06, 9.1076e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    0.000000   23.991806   76.275078  127.028488    0.884353     45    bowl\n",
       "  1  353.686829  265.648132  640.000000  477.529724    0.837576     45    bowl\n",
       "  2  390.814392  104.660355  601.781616  230.834595    0.835659     43   knife\n",
       "  3  298.567505    2.869873  387.210449  118.382812    0.794009     41     cup\n",
       "  4  383.271606  326.503784  639.320496  478.302551    0.683668     51  carrot\n",
       "  5  144.804901  110.949509  175.011719  134.995895    0.328580     51  carrot\n",
       "  6  233.726181   84.912750  267.085571  114.001953    0.316139     51  carrot\n",
       "  7  208.020081   91.763519  232.491455  113.602600    0.314483     51  carrot\n",
       "  8  128.190186  129.584946  151.508484  152.539200    0.287510     51  carrot,\n",
       "  'caption': ['bowl', 'A blue bowl containing shredded carrots.'],\n",
       "  'bbox_target': [354.1, 266.29, 285.9, 213.65]},\n",
       " 327: {'image_emb': tensor([[-0.1628,  0.4880, -0.0373,  ...,  1.0596,  0.5195,  0.1074],\n",
       "          [-0.3962,  0.5903, -0.3450,  ...,  0.8491,  0.2561,  0.3015],\n",
       "          [-0.4109,  0.1552, -0.0186,  ...,  1.0850, -0.1464, -0.0346],\n",
       "          [-0.1808,  0.2747, -0.1471,  ...,  1.1680,  0.2046, -0.2341],\n",
       "          [ 0.1449,  0.3240, -0.1637,  ...,  0.7861,  0.1340,  0.0919],\n",
       "          [-0.0958,  0.0387, -0.2069,  ...,  0.7646,  0.2603,  0.1887]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2129, -0.1229, -0.2355,  ...,  0.1842,  0.0202, -0.1842],\n",
       "          [-0.0778,  0.1860,  0.1356,  ..., -0.3665, -0.0035, -0.3777]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.8278e-03, 9.0430e-01, 3.4733e-03, 4.6730e-03, 7.9041e-02, 4.7731e-04],\n",
       "          [5.2783e-01, 3.9844e-01, 2.5070e-02, 2.1790e-02, 2.6688e-02, 2.6178e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   157.151535   29.401703  336.872437  432.560547    0.957630      0   \n",
       "  1   368.740448   22.030579  529.838440  443.091370    0.941147      0   \n",
       "  2     0.351730  147.841522  121.157768  405.845428    0.918967     56   \n",
       "  3   533.485718  245.575928  639.373901  469.030396    0.841784     57   \n",
       "  4   570.653992    0.146545  639.157288  119.987244    0.770190     58   \n",
       "  5   221.321884   47.211021  251.558884   72.252396    0.680388     65   \n",
       "  6   324.916992  177.443970  413.835022  373.864929    0.661715     56   \n",
       "  7   368.655426  149.524857  388.647125  167.869217    0.595682     65   \n",
       "  8   218.234009  222.875031  307.886597  331.596100    0.548710     56   \n",
       "  9   448.372192   79.747208  465.884827   94.023605    0.501949     65   \n",
       "  10  274.718079  132.400345  288.835876  150.409378    0.447841     65   \n",
       "  11  126.815552  110.176834  143.201202  132.642044    0.401085     74   \n",
       "  12  268.762939  132.199692  289.362244  165.844620    0.354453     65   \n",
       "  13  276.310760  213.092682  303.933685  261.616974    0.258885     56   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2          chair  \n",
       "  3          couch  \n",
       "  4   potted plant  \n",
       "  5         remote  \n",
       "  6          chair  \n",
       "  7         remote  \n",
       "  8          chair  \n",
       "  9         remote  \n",
       "  10        remote  \n",
       "  11         clock  \n",
       "  12        remote  \n",
       "  13         chair  ,\n",
       "  'caption': ['black dress in a boy',\n",
       "   'A boy wearing white socks and a black shirt.'],\n",
       "  'bbox_target': [369.98, 22.65, 162.87, 416.36]},\n",
       " 328: {'image_emb': tensor([[ 0.0226, -0.1203, -0.0240,  ...,  0.3381, -0.4917, -0.2427],\n",
       "          [ 0.0612,  0.0851, -0.4446,  ...,  0.8950, -0.3894,  0.0623],\n",
       "          [ 0.0646,  0.0120,  0.0423,  ...,  0.4321, -0.3486, -0.0578],\n",
       "          [ 0.2223, -0.3208,  0.1148,  ..., -0.0046, -0.4182, -0.0402]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 7.9712e-02,  5.2307e-02,  8.4167e-02,  ...,  2.2302e-01,\n",
       "            1.5210e-01,  1.8417e-02],\n",
       "          [ 1.3318e-01,  1.0687e-01, -1.1063e-04,  ...,  1.0199e-01,\n",
       "            1.6821e-01, -1.0443e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2056, 0.0060, 0.0818, 0.7065],\n",
       "          [0.0738, 0.0051, 0.4246, 0.4966]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  281.975769  188.144043  635.526062  348.270874    0.917923      7   \n",
       "  1    0.201635  270.166321   58.435112  395.978149    0.880205      2   \n",
       "  2   23.641800  172.064423  294.949036  372.443451    0.864312      7   \n",
       "  3  558.994690  149.162460  584.718933  191.602509    0.686873      9   \n",
       "  4  245.706604  150.104614  268.770874  184.060669    0.672754      9   \n",
       "  5    0.000000  144.821198  217.884521  240.024933    0.588726      5   \n",
       "  6  194.280533  152.439148  208.753647  178.036652    0.538899      9   \n",
       "  7    0.000000  236.105133   26.152355  290.040680    0.501158      2   \n",
       "  8  238.556976  150.737366  250.907318  183.027924    0.419566      9   \n",
       "  9  592.929382  225.203918  609.053162  241.074158    0.388842      0   \n",
       "  \n",
       "              name  \n",
       "  0          truck  \n",
       "  1            car  \n",
       "  2          truck  \n",
       "  3  traffic light  \n",
       "  4  traffic light  \n",
       "  5            bus  \n",
       "  6  traffic light  \n",
       "  7            car  \n",
       "  8  traffic light  \n",
       "  9         person  ,\n",
       "  'caption': ['an emergency response truck is following a second truck.',\n",
       "   'A truck with red and yellow stripes driving behind a similar truck.'],\n",
       "  'bbox_target': [25.01, 167.07, 291.14, 206.58]},\n",
       " 329: {'image_emb': tensor([[ 0.1329,  0.2808, -0.1304,  ...,  0.9922,  0.2749,  0.0813],\n",
       "          [-0.1592,  0.0905, -0.1241,  ...,  0.9131,  0.4600,  0.0983],\n",
       "          [ 0.0277,  0.1207, -0.4241,  ...,  1.0352,  0.2969,  0.0634],\n",
       "          ...,\n",
       "          [ 0.1946, -0.0886, -0.2102,  ...,  0.8472,  0.0562, -0.0777],\n",
       "          [ 0.0602, -0.0135, -0.2473,  ...,  0.9644,  0.1272, -0.1818],\n",
       "          [ 0.0099,  0.2505, -0.1538,  ...,  0.8667,  0.3967, -0.0803]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0140,  0.0355, -0.2156,  ..., -0.0638, -0.1301, -0.3618],\n",
       "          [-0.0814,  0.0627, -0.0550,  ...,  0.1527, -0.0840, -0.2708]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.4661e-01, 8.1787e-01, 1.9531e-02, 2.8753e-04, 2.4071e-03, 2.3103e-04,\n",
       "           1.3016e-02],\n",
       "          [3.5132e-01, 3.9819e-01, 1.7944e-01, 2.7256e-03, 5.6488e-02, 6.7444e-03,\n",
       "           4.9362e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   33.375488   23.926746  217.983002  371.502686    0.947948      0   \n",
       "  1  277.247925   68.214241  493.235596  361.468872    0.945945      0   \n",
       "  2  264.267639  172.447723  294.906342  280.065094    0.880465     39   \n",
       "  3  230.079437  237.910751  255.107040  335.529480    0.868977     39   \n",
       "  4  186.058090  220.207031  215.404938  272.722565    0.741251     40   \n",
       "  5   18.334352  229.335358   39.263580  373.462921    0.718155     56   \n",
       "  6  192.467621  295.836792  462.528137  373.737000    0.666307     60   \n",
       "  7  459.615662  280.886322  499.488892  373.807129    0.402216     56   \n",
       "  8  165.583115  197.315903  212.574478  219.852493    0.400187     60   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2        bottle  \n",
       "  3        bottle  \n",
       "  4    wine glass  \n",
       "  5         chair  \n",
       "  6  dining table  \n",
       "  7         chair  \n",
       "  8  dining table  ,\n",
       "  'caption': ['A table with a bottle of wine on it.',\n",
       "   'a brown table that has wine on it'],\n",
       "  'bbox_target': [187.11, 295.75, 285.55, 76.26]},\n",
       " 330: {'image_emb': tensor([[ 0.1908,  0.2300,  0.1727,  ...,  0.3303,  0.0424,  0.2200],\n",
       "          [-0.5444,  0.6948, -0.1827,  ...,  0.5566, -0.0657, -0.0233],\n",
       "          [ 0.0900,  0.2998,  0.1880,  ...,  0.7974,  0.1587, -0.0777],\n",
       "          [-0.0403,  0.1520, -0.2223,  ...,  0.7578, -0.3308,  0.1108],\n",
       "          [-0.0670,  0.4529,  0.0673,  ...,  0.8657,  0.1730, -0.1639]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3833,  0.3647,  0.3591,  ..., -0.1866, -0.1965, -0.4146],\n",
       "          [-0.2620,  0.3943,  0.3118,  ..., -0.1783, -0.2534, -0.4092]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0009, 0.0255, 0.3865, 0.0068, 0.5801],\n",
       "          [0.0451, 0.0444, 0.4282, 0.0121, 0.4702]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   188.713913  167.416672  437.302094  401.783752    0.921395     45   \n",
       "  1   361.948029   51.994976  558.886597  238.813385    0.794540     51   \n",
       "  2    11.129509   21.708637  607.208313  607.203735    0.734473     45   \n",
       "  3   378.386322  376.245789  480.766510  461.359558    0.723929     50   \n",
       "  4   445.101990  454.451050  527.859741  541.335510    0.624402     50   \n",
       "  5   309.667267  465.626953  437.007080  602.981750    0.528854     50   \n",
       "  6    84.974632  210.400772  310.749237  331.031464    0.518530     44   \n",
       "  7    84.595505  212.884628  306.692352  330.635681    0.432980     42   \n",
       "  8   500.792267  162.279755  558.636047  214.704651    0.416018     51   \n",
       "  9   455.513336  105.314423  489.131439  183.327881    0.401747     51   \n",
       "  10  308.712830  404.275665  377.462860  467.170593    0.399352     50   \n",
       "  \n",
       "          name  \n",
       "  0       bowl  \n",
       "  1     carrot  \n",
       "  2       bowl  \n",
       "  3   broccoli  \n",
       "  4   broccoli  \n",
       "  5   broccoli  \n",
       "  6      spoon  \n",
       "  7       fork  \n",
       "  8     carrot  \n",
       "  9     carrot  \n",
       "  10  broccoli  ,\n",
       "  'caption': ['The bowl of veggies.', 'A bowl filled with vegetables.'],\n",
       "  'bbox_target': [12.38, 16.6, 599.62, 585.86]},\n",
       " 331: {'image_emb': tensor([[-5.8984e-01,  3.8086e-01, -4.2871e-01,  ...,  7.2852e-01,\n",
       "            2.1759e-02, -5.1709e-01],\n",
       "          [-7.1973e-01,  3.9648e-01, -3.4912e-01,  ...,  6.2793e-01,\n",
       "            3.3951e-04, -3.8501e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.4600,  0.0703, -0.2847,  ..., -0.0436, -0.0264, -0.1036],\n",
       "          [-0.3352, -0.0960, -0.4165,  ...,  0.4678, -0.1996, -0.6885]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.4844, 0.5156],\n",
       "          [0.6074, 0.3923]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   32.161057   13.642573  249.063019  496.702393    0.834947      0   \n",
       "  1   97.356369  116.946411  240.323822  443.751862    0.675217     38   \n",
       "  2  201.137360  154.879929  296.682373  453.941162    0.647095     24   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1  tennis racket  \n",
       "  2       backpack  ,\n",
       "  'caption': ['the only person in the photo holding a tennis racket',\n",
       "   'A young girl wearing a white hat, white shirt, and white shorts carrying a tennis racket.'],\n",
       "  'bbox_target': [31.86, 13.29, 221.81, 486.71]},\n",
       " 332: {'image_emb': tensor([[-0.1709,  0.3018,  0.5024,  ...,  0.6255, -0.5278, -0.1637],\n",
       "          [-0.1510,  0.3152, -0.1580,  ...,  0.3992, -0.3262, -0.1788],\n",
       "          [ 0.1191,  0.4512,  0.2389,  ...,  0.5654, -0.2722, -0.1302],\n",
       "          [-0.0379,  0.1932, -0.3833,  ...,  1.1006,  0.1168, -0.2117],\n",
       "          [-0.1201,  0.4739,  0.4990,  ...,  0.7656, -0.3813, -0.0569]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1323, -0.0125,  0.1177,  ...,  0.1647, -0.2001, -0.0643],\n",
       "          [-0.0499, -0.0482, -0.2646,  ...,  0.2312, -0.3169,  0.0440]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5474, 0.3882, 0.0286, 0.0202, 0.0158],\n",
       "          [0.2487, 0.0217, 0.0262, 0.0169, 0.6865]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   48.697556  145.815063  251.612900  420.813538    0.933591      0   \n",
       "  1  202.816589  190.778839  341.154480  400.792755    0.925590      0   \n",
       "  2  243.738800   69.005325  391.462799  345.090210    0.918809      0   \n",
       "  3  297.800873  260.764282  344.432037  299.875122    0.834643     35   \n",
       "  4  293.462189    6.749557  384.562347  120.176071    0.689060     34   \n",
       "  \n",
       "               name  \n",
       "  0          person  \n",
       "  1          person  \n",
       "  2          person  \n",
       "  3  baseball glove  \n",
       "  4    baseball bat  ,\n",
       "  'caption': ['umpire',\n",
       "   \"The man in the blue shirt who has his hand on the catcher's back.\"],\n",
       "  'bbox_target': [50.86, 146.81, 202.46, 274.43]},\n",
       " 333: {'image_emb': tensor([[ 0.2593,  0.3545, -0.5386,  ...,  0.9258,  0.0514, -0.0307],\n",
       "          [-0.1122,  0.5420, -0.4451,  ...,  0.9443,  0.0883,  0.0908],\n",
       "          [-0.3577,  0.2078, -0.2249,  ...,  0.9551,  0.0992, -0.0129],\n",
       "          [ 0.0986,  0.4924, -0.4907,  ...,  0.6714,  0.1750, -0.0177]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1099, -0.0795,  0.0808,  ..., -0.0906, -0.2139, -0.1658],\n",
       "          [-0.1218, -0.0063, -0.1086,  ..., -0.2903, -0.2156,  0.0980]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.4116e-02, 6.0196e-03, 3.6646e-01, 5.3320e-01],\n",
       "          [9.6387e-01, 6.6221e-05, 2.3758e-02, 1.2321e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   183.620041    0.000000  349.453400  150.177307    0.893874      0  person\n",
       "  1    88.580193  223.428268  415.583710  373.554504    0.833761     45    bowl\n",
       "  2   453.994843   58.454681  499.647156  269.036041    0.701924      0  person\n",
       "  3   209.351532  295.576324  240.739731  325.791229    0.688070     54   donut\n",
       "  4   162.566864  106.171944  336.579376  138.926697    0.603477     45    bowl\n",
       "  5   258.400299  306.754181  294.465698  327.277771    0.582521     54   donut\n",
       "  6   201.985001  281.803833  236.097687  301.877502    0.557514     54   donut\n",
       "  7   297.321228  297.171967  332.285980  322.649384    0.556765     54   donut\n",
       "  8   175.574905  282.157806  203.461426  307.958069    0.552422     54   donut\n",
       "  9   183.239487  256.373749  211.758209  282.883240    0.551453     54   donut\n",
       "  10  239.073181  294.326599  268.815765  318.834259    0.550257     54   donut\n",
       "  11  206.055450  261.141296  236.005356  283.534149    0.504745     54   donut\n",
       "  12  309.360168  263.654785  338.270721  283.566345    0.454889     54   donut\n",
       "  13  249.610489  250.167496  284.217224  276.995453    0.446769     54   donut\n",
       "  14  189.504700  235.727905  214.227600  257.666901    0.431462     54   donut\n",
       "  15  137.574142  193.745285  345.617432  241.041428    0.367713     45    bowl\n",
       "  16  297.426636  276.935730  327.822937  300.168732    0.341930     54   donut\n",
       "  17  238.484909  277.532104  268.293823  298.697998    0.341878     54   donut\n",
       "  18  272.628784  137.251602  304.014008  164.178894    0.293049     54   donut,\n",
       "  'caption': ['The person behind the plates one above another.',\n",
       "   'a man wearing grey sweater'],\n",
       "  'bbox_target': [181.18, 1.69, 169.38, 138.2]},\n",
       " 334: {'image_emb': tensor([[-0.2172, -0.0299,  0.0790,  ...,  0.6621, -0.0253, -0.1685],\n",
       "          [-0.1661,  0.0658, -0.0848,  ...,  1.3633, -0.2113, -0.0454],\n",
       "          [-0.1952, -0.1111,  0.0175,  ...,  0.5596,  0.0220, -0.1558]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0157,  0.2170, -0.4041,  ...,  0.0540, -0.0025,  0.0521],\n",
       "          [-0.1549, -0.1687, -0.0275,  ..., -0.0730,  0.0334, -0.0188]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.5752e-01, 2.2423e-04, 4.2084e-02],\n",
       "          [6.9238e-01, 3.9506e-04, 3.0713e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   42.614990   91.929443  428.523804  477.246338    0.944552      0   \n",
       "  1  340.605530  119.343613  427.274658  303.401062    0.837682      0   \n",
       "  2    6.609039  411.495239   41.471313  482.847656    0.462910     41   \n",
       "  3    5.492249  535.426758  430.739380  636.570801    0.352896     60   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2           cup  \n",
       "  3  dining table  ,\n",
       "  'caption': ['A woman in a checkered shirt.', 'Woman workingslicing.'],\n",
       "  'bbox_target': [44.58, 96.12, 390.42, 365.3]},\n",
       " 335: {'image_emb': tensor([[ 0.6406,  0.3103,  0.3708,  ...,  0.6768,  0.5181, -0.5508],\n",
       "          [ 0.3870,  0.3196,  0.3052,  ...,  0.6929,  0.6060, -0.3745],\n",
       "          [ 0.3618,  0.2061,  0.1218,  ...,  0.4470,  0.5303, -0.4048],\n",
       "          [ 0.3481,  0.1475,  0.0121,  ...,  0.4490,  0.2874, -0.5278]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2493,  0.2212, -0.0318,  ..., -0.2996, -0.2974, -0.4839],\n",
       "          [-0.0185, -0.2505, -0.0039,  ...,  0.1920,  0.1027, -0.3088]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2238, 0.5537, 0.0890, 0.1335],\n",
       "          [0.2722, 0.5762, 0.0082, 0.1434]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  name\n",
       "  0  350.140594  140.201416  579.357544  289.805725    0.917944     14  bird\n",
       "  1  489.702271  100.097198  624.353882  269.229340    0.763556     14  bird\n",
       "  2  119.567764  158.313110  300.611023  361.850830    0.760805     14  bird\n",
       "  3  278.093597  159.860687  378.914642  289.882599    0.618778     14  bird,\n",
       "  'caption': ['A shiny bird preening its feathers to the right of three other birds.',\n",
       "   'A young black duck on the right side of the picture with its back to the camera.'],\n",
       "  'bbox_target': [500.46, 101.01, 123.97, 166.21]},\n",
       " 336: {'image_emb': tensor([[ 0.2141,  0.7305, -0.2291,  ...,  0.5913,  0.0481, -0.3003],\n",
       "          [ 0.3025,  0.5742, -0.3105,  ...,  0.7051,  0.1566, -0.0562],\n",
       "          [ 0.1782,  0.4336,  0.0784,  ...,  0.5088,  0.1724,  0.1089],\n",
       "          [ 0.2859,  0.3975, -0.2109,  ...,  0.8628, -0.2054,  0.0041],\n",
       "          [ 0.2241,  0.1449, -0.0145,  ...,  0.4380,  0.1914,  0.0026]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2642,  0.0848, -0.6426,  ...,  0.0806,  0.3733,  0.2125],\n",
       "          [-0.1448,  0.2141, -0.3438,  ...,  0.1776, -0.1719, -0.4421]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.3328e-01, 6.0498e-01, 6.5796e-02, 3.0637e-05, 9.5764e-02],\n",
       "          [1.9702e-01, 1.9702e-01, 2.3767e-01, 5.9605e-08, 3.6816e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  208.817078   25.811279  401.255432  423.579956    0.942913      0   person\n",
       "  1  104.097466   37.626190  262.166840  391.719208    0.940521      0   person\n",
       "  2  166.888382    0.000000  446.130798  253.593536    0.937931     29  frisbee\n",
       "  3  501.521637  395.031128  580.819824  427.804688    0.802111     33     kite,\n",
       "  'caption': ['Boy in yellow shirt.',\n",
       "   'A boy standing in the sand next to another boy holding a giant frisbee.'],\n",
       "  'bbox_target': [103.87, 33.66, 155.82, 352.02]},\n",
       " 337: {'image_emb': tensor([[-0.0213,  0.3757, -0.2069,  ...,  1.3496, -0.2500, -0.2157],\n",
       "          [-0.1591, -0.0787, -0.0504,  ...,  0.8862,  0.1148, -0.2783],\n",
       "          [-0.0349,  0.3413, -0.1780,  ...,  1.2930, -0.4133, -0.0908],\n",
       "          ...,\n",
       "          [ 0.3792,  0.2690, -0.3416,  ...,  1.0195, -0.3928, -0.2610],\n",
       "          [-0.3545,  0.5503, -0.4788,  ...,  1.1533, -0.0747,  0.0025],\n",
       "          [ 0.1898,  0.3213, -0.4175,  ...,  1.3301, -0.4988, -0.0316]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0873, -0.1835, -0.4717,  ...,  0.2791,  0.0452, -0.1863],\n",
       "          [ 0.1663, -0.1025, -0.5371,  ...,  0.1879, -0.1482, -0.1890]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.1910e-03, 9.8348e-06, 8.1482e-03, 2.1805e-02, 5.6314e-04, 9.4141e-01,\n",
       "           2.4704e-02],\n",
       "          [1.1993e-02, 1.6391e-05, 2.4854e-01, 3.7537e-02, 4.6492e-04, 6.6504e-01,\n",
       "           3.6377e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  231.153992  173.671738  425.122253  464.930481    0.934824     28  suitcase\n",
       "  1  511.278473    0.580246  606.357239  154.574539    0.931602      0    person\n",
       "  2   31.235100  248.275909  235.535339  477.406616    0.887968     24  backpack\n",
       "  3  389.676147  230.902161  637.501221  477.353027    0.854924     24  backpack\n",
       "  4   76.020576    0.834702  243.987549  359.934814    0.808569     28  suitcase\n",
       "  5  239.918869    4.798515  435.312805  240.729950    0.794912     24  backpack\n",
       "  6  389.018188  171.906769  487.581970  377.440765    0.473478     28  suitcase,\n",
       "  'caption': ['The black and yellow backpack sitting on top of a suitcase.',\n",
       "   'A yellow and black back pack sitting on top of a blue suitcase'],\n",
       "  'bbox_target': [243.76, 6.55, 194.36, 230.87]},\n",
       " 338: {'image_emb': tensor([[ 0.0246,  0.3801, -0.1368,  ...,  0.8179,  0.0651,  0.1581],\n",
       "          [ 0.0531, -0.1259, -0.2588,  ...,  0.9434, -0.0295,  0.0181],\n",
       "          [ 0.0835,  0.2074, -0.4824,  ...,  0.7964,  0.3040,  0.1882],\n",
       "          [ 0.1401, -0.2581, -0.6836,  ...,  0.9956, -0.1473, -0.0765],\n",
       "          [-0.0482,  0.4834, -0.2815,  ...,  0.7925, -0.0208,  0.4753]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1450,  0.0565, -0.3835,  ..., -0.0536, -0.0930,  0.2639],\n",
       "          [ 0.0210,  0.1349, -0.5112,  ...,  0.0323, -0.3062,  0.3071]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.5596e-01, 0.0000e+00, 1.3123e-01, 5.9605e-08, 1.2993e-02],\n",
       "          [9.3604e-01, 0.0000e+00, 3.7445e-02, 0.0000e+00, 2.6550e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   21.545071   65.278496  190.602707  323.692413    0.941611      0   \n",
       "  1   63.262772  335.908020  109.944466  373.957001    0.894485     65   \n",
       "  2  160.394470   67.232826  333.163788  358.083527    0.872517      0   \n",
       "  3  219.860489  219.640274  246.148468  231.842072    0.818786     65   \n",
       "  4    0.000000   89.291954  496.115967  374.402222    0.643307     57   \n",
       "  5  157.572556   65.922821  334.852417  366.412262    0.502321     57   \n",
       "  6  104.968140  195.576340  134.342789  206.842987    0.492183     65   \n",
       "  7    0.000000  321.868317   13.199078  345.376923    0.350703     65   \n",
       "  8    4.275966  286.149902  285.958435  372.590637    0.263030     60   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        remote  \n",
       "  2        person  \n",
       "  3        remote  \n",
       "  4         couch  \n",
       "  5         couch  \n",
       "  6        remote  \n",
       "  7        remote  \n",
       "  8  dining table  ,\n",
       "  'caption': ['Man playing wii wearing a red University of Georgia shirt.',\n",
       "   'A man sitting on the couch playing wii in a red University of Georgia shirt'],\n",
       "  'bbox_target': [19.38, 65.47, 172.75, 263.77]},\n",
       " 339: {'image_emb': tensor([[-0.3093,  0.1572, -0.2480,  ...,  1.4893, -0.0782, -0.1703],\n",
       "          [-0.1597,  0.2546, -0.2106,  ...,  0.6475, -0.4453, -0.0782],\n",
       "          [-0.4189,  0.2537, -0.1664,  ...,  1.3633, -0.1708,  0.0765],\n",
       "          [-0.1548,  0.3833, -0.2008,  ...,  1.1113, -0.0956,  0.0325],\n",
       "          [-0.0210,  0.0297, -0.4727,  ...,  0.8994, -0.2494,  0.0801]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2181, -0.1270, -0.2681,  ...,  0.0563, -0.0591,  0.0331],\n",
       "          [ 0.0485,  0.1470, -0.0370,  ..., -0.1204, -0.3618, -0.1427]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.3583e-04, 5.0659e-03, 1.3351e-02, 5.9814e-02, 9.2139e-01],\n",
       "          [2.4963e-02, 5.1208e-02, 6.3379e-01, 1.5527e-01, 1.3489e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  117.245796   70.150604  225.565613  290.390106    0.894673     28  suitcase\n",
       "  1  242.401581  213.658020  639.766724  420.713684    0.866208     28  suitcase\n",
       "  2  171.081116   99.238174  250.726562  294.095886    0.795844     28  suitcase\n",
       "  3  250.276138   34.514343  637.984924  236.851898    0.734404     57     couch\n",
       "  4    1.203362   44.194580  109.119820  262.503845    0.574908     56     chair\n",
       "  5    0.446308   39.956940  109.535751  263.606476    0.491185     13     bench\n",
       "  6  313.112854   83.945480  491.285095  228.355835    0.488717     26   handbag\n",
       "  7  166.310013   22.434464  189.386398   45.181564    0.434941      0    person\n",
       "  8  211.649048  310.502075  247.504272  423.341309    0.381518     28  suitcase,\n",
       "  'caption': ['An empty chair with luggage to the right.',\n",
       "   'A blue seat with a white arm rest.'],\n",
       "  'bbox_target': [0.0, 37.62, 112.86, 220.19]},\n",
       " 340: {'image_emb': tensor([[ 2.0508e-01,  5.2686e-01,  1.4282e-01,  ...,  1.0303e+00,\n",
       "            1.0730e-01,  2.3645e-01],\n",
       "          [ 2.7026e-01, -8.0948e-03, -5.6689e-01,  ...,  1.4221e-01,\n",
       "           -2.0337e-01,  1.0278e-01],\n",
       "          [-4.7607e-02,  5.8643e-01, -6.0974e-02,  ...,  2.8320e-01,\n",
       "            3.9368e-02, -2.9321e-01],\n",
       "          ...,\n",
       "          [-1.7700e-01,  2.8955e-01,  2.5803e-02,  ...,  1.3291e+00,\n",
       "           -1.6907e-02, -1.8225e-01],\n",
       "          [ 1.3745e-01,  8.3557e-02, -1.7044e-02,  ...,  1.2207e+00,\n",
       "           -7.9041e-02, -2.7295e-01],\n",
       "          [ 1.4624e-01,  1.2767e-04, -1.7578e-01,  ..., -1.9556e-01,\n",
       "           -1.2955e-02, -1.0480e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 1.2671e-01, -2.4719e-01, -3.0078e-01, -4.2511e-02,  1.1127e-01,\n",
       "           -1.4124e-01, -2.5903e-01, -4.1333e-01, -5.4150e-01, -2.9028e-01,\n",
       "            4.3457e-01, -3.6938e-01,  2.5146e-02,  9.3262e-02, -3.2251e-01,\n",
       "            8.3008e-02,  6.9336e-02, -8.0139e-02, -2.7197e-01,  3.5938e-01,\n",
       "            3.8599e-01,  2.6318e-01, -9.1887e-04, -2.5464e-01, -5.6000e-02,\n",
       "            3.7085e-01,  1.8631e-02,  1.4233e-01, -1.4966e-01,  6.7139e-02,\n",
       "           -2.1277e-01, -5.7129e-01, -3.0792e-02,  2.3743e-01, -2.4890e-01,\n",
       "           -9.6191e-02,  5.1178e-02,  5.4840e-02,  1.2347e-01,  2.7344e-01,\n",
       "            1.2177e-01,  2.9736e-01, -1.2805e-01, -3.1769e-02, -1.3260e-02,\n",
       "            2.9373e-02, -3.4692e-01,  7.0557e-02, -1.5198e-01, -3.5010e-01,\n",
       "           -1.8323e-01, -7.5684e-01, -5.3223e-02, -2.7734e-01, -8.6914e-02,\n",
       "           -1.9653e-01,  3.3325e-01, -3.3203e-01, -2.6550e-02, -2.9370e-01,\n",
       "           -3.4393e-02, -4.2651e-01, -4.2505e-01,  1.3062e-01,  2.0935e-01,\n",
       "            4.2944e-01,  4.3945e-01,  1.8945e-01,  2.6978e-01,  2.1240e-02,\n",
       "            3.9795e-02, -4.0381e-01,  8.3374e-02,  1.9562e-02, -1.1572e-01,\n",
       "            3.0151e-02, -4.7424e-02,  4.0070e-02, -6.2683e-02, -1.3965e-01,\n",
       "           -1.5247e-01,  4.3359e-01, -3.1030e-01,  1.4453e-01,  9.3018e-02,\n",
       "            1.5967e-01,  1.9617e-01,  4.2023e-02, -1.4880e-01, -5.2765e-02,\n",
       "            2.2995e-02, -7.1533e-02, -7.4316e-01,  2.2400e-01,  2.3242e-01,\n",
       "           -3.9764e-02, -7.9163e-02,  1.6748e-01, -4.5972e-01, -2.4487e-01,\n",
       "            1.6577e-01,  3.7769e-01,  1.8567e-01, -4.2383e-01, -6.8555e-01,\n",
       "           -2.3389e-01, -1.2378e-01,  1.5930e-01,  1.1986e-02,  2.0044e-01,\n",
       "           -2.9028e-01, -7.3242e-02, -4.9463e-01,  1.5479e-01, -1.8237e-01,\n",
       "           -3.0762e-02,  2.6318e-01, -1.1963e-01, -1.1182e-01,  1.7271e-03,\n",
       "           -1.6138e-01, -3.9355e-01, -5.9692e-02,  2.9694e-02, -3.6401e-01,\n",
       "           -2.2363e-01, -3.5815e-01,  4.3511e-04,  1.2769e-01,  3.6914e-01,\n",
       "            1.7773e-01, -5.8044e-02, -1.1316e-01,  3.2949e+00,  2.4207e-01,\n",
       "           -4.3848e-01, -3.2007e-01, -3.9038e-01, -2.9160e-02, -3.3594e-01,\n",
       "            4.3701e-02, -2.1289e-01, -3.1860e-01,  4.1504e-02,  2.4628e-02,\n",
       "           -1.7078e-01,  1.0138e-01, -8.2764e-02, -3.8086e-01,  1.9238e-01,\n",
       "           -1.0468e-02,  3.1885e-01,  3.4131e-01,  3.4058e-01, -1.5405e-01,\n",
       "           -3.4180e-01, -2.9956e-01, -4.9316e-01, -6.2317e-02,  5.9509e-02,\n",
       "            1.1694e-01,  4.5685e-02, -1.3281e-01,  4.3848e-01, -1.6748e-01,\n",
       "            1.6108e-03,  4.4238e-01, -8.8867e-02,  2.5586e-01, -8.1848e-02,\n",
       "           -2.3291e-01,  1.0663e-01,  2.5732e-01,  1.5491e-01, -3.5498e-01,\n",
       "            8.0948e-03,  1.2585e-01,  3.2837e-02, -1.8274e-01,  1.2733e-02,\n",
       "            2.4927e-01,  2.9053e-02, -3.6084e-01,  6.1798e-02, -5.0781e-01,\n",
       "           -3.4180e-01, -4.7290e-01,  4.4250e-03, -6.2317e-02,  2.0581e-01,\n",
       "            4.6875e-01, -1.4175e-02,  3.8033e-03, -4.5972e-01, -1.2537e-01,\n",
       "           -3.3105e-01, -4.4238e-01,  3.5864e-01, -1.3391e-01,  2.8717e-02,\n",
       "           -4.3396e-02, -4.8340e-01,  4.2725e-01, -7.1228e-02,  5.0690e-02,\n",
       "           -2.0554e-02,  3.1299e-01, -7.8735e-02,  5.3802e-02,  2.1924e-01,\n",
       "            4.4385e-01,  6.9763e-02,  1.8585e-02,  3.5645e-01,  1.3025e-01,\n",
       "           -1.9910e-01,  5.2460e-02, -8.4961e-02,  3.7476e-01,  1.5076e-01,\n",
       "            4.2236e-02, -3.1299e-01, -4.7192e-01,  5.9448e-02, -1.0876e-01,\n",
       "           -7.1289e-02,  3.7695e-01, -7.3730e-01, -2.2278e-01, -2.6807e-01,\n",
       "           -9.8389e-01,  1.0089e-01, -1.8396e-01,  1.0887e-02,  1.1279e-01,\n",
       "            4.3726e-01, -4.1931e-02, -1.1554e-01,  1.9958e-01,  2.8027e-01,\n",
       "           -2.6440e-01,  2.3828e-01,  2.5635e-01,  3.9331e-01,  1.5594e-02,\n",
       "            2.4060e-01,  4.3579e-02, -2.4597e-02,  3.0688e-01,  2.2546e-01,\n",
       "           -4.9268e-01,  3.9673e-01,  4.3506e-01,  2.7930e-01, -2.6392e-01,\n",
       "            4.6692e-02,  3.5864e-01,  2.3840e-01,  3.0786e-01, -3.0420e-01,\n",
       "           -9.3262e-02, -7.1716e-02,  4.9561e-02,  2.7466e-01,  5.9766e-01,\n",
       "           -1.1481e-01,  9.9792e-02, -4.7485e-01,  2.0520e-01,  2.0557e-01,\n",
       "           -7.4524e-02,  1.5405e-01, -1.4191e-02, -7.5012e-02, -2.6147e-01,\n",
       "            1.9150e-02, -1.1060e-01, -2.7539e-01,  2.5439e-01, -1.3025e-01,\n",
       "           -2.4890e-01,  9.3445e-02, -3.1934e-01,  8.8501e-03,  4.8798e-02,\n",
       "            2.0828e-02, -3.5461e-02,  3.6963e-01,  5.5518e-01,  1.4282e-01,\n",
       "           -1.9080e-01, -1.5884e-02, -4.4983e-02, -1.2805e-01, -4.0314e-02,\n",
       "            2.9037e-02,  5.5664e-02, -2.2656e-01, -1.2354e-01,  2.7130e-02,\n",
       "            4.0039e-02, -3.4399e-01,  8.4717e-02,  1.2878e-01,  3.3447e-01,\n",
       "           -2.4109e-01,  2.5098e-01,  3.1543e-01,  4.6387e-01,  1.0944e-01,\n",
       "            2.1570e-01,  1.9699e-02,  3.2969e+00,  2.4109e-01,  2.6880e-01,\n",
       "            6.2103e-02,  2.4353e-01, -1.8079e-01,  1.5869e-01,  2.9004e-01,\n",
       "           -6.9580e-02,  2.8015e-02, -9.2224e-02, -2.1149e-02, -5.6934e-01,\n",
       "            2.7954e-01,  9.5459e-02, -2.8882e-01, -2.0984e-01, -8.8867e-01,\n",
       "            3.7231e-01, -2.9590e-01,  2.2388e-01,  5.4932e-01,  2.8735e-01,\n",
       "           -1.4429e-01, -9.9182e-02,  2.8229e-03, -1.7981e-01, -1.2006e-01,\n",
       "           -6.4404e-01, -3.6426e-01,  3.3154e-01, -5.2295e-01, -2.7294e-03,\n",
       "           -8.3191e-02,  5.4346e-01,  2.3389e-01, -1.4905e-01, -1.8677e-01,\n",
       "           -4.1406e-01, -6.6528e-02,  1.8994e-01,  1.6772e-01, -6.0742e-01,\n",
       "            4.5624e-02, -5.7764e-01,  3.9893e-01, -3.2104e-02, -2.4377e-01,\n",
       "            2.7100e-02, -2.4097e-01,  1.0907e-01, -6.6162e-01, -3.8544e-02,\n",
       "            2.7173e-01, -2.3328e-01,  1.7407e-01,  1.6113e-01, -7.5073e-02,\n",
       "            1.6406e-01,  1.5039e-01, -1.7261e-01,  1.0246e-02,  4.4312e-01,\n",
       "            1.4526e-01, -2.4402e-01,  3.6377e-01,  2.7069e-02,  4.2090e-01,\n",
       "            4.1431e-01,  6.1816e-01,  2.2168e-01,  1.2891e-01, -1.3098e-01,\n",
       "            1.1765e-02, -1.3428e-01,  2.1667e-01, -3.2153e-01, -3.1396e-01,\n",
       "           -6.2354e-01, -1.9263e-01, -4.2676e-01,  4.3365e-02,  1.9482e-01,\n",
       "            3.6035e-01, -1.1536e-01,  1.7554e-01,  3.4332e-02,  4.1943e-01,\n",
       "           -1.2123e-02,  2.3486e-01, -3.8770e-01,  1.7651e-01, -2.7759e-01,\n",
       "           -2.4194e-01,  8.8989e-02, -1.0034e-01,  8.6670e-02, -2.7267e-02,\n",
       "           -2.7173e-01, -8.7646e-02, -2.6953e-01,  9.5886e-02, -1.1200e-01,\n",
       "           -1.8237e-01,  9.5032e-02, -1.2549e-01,  7.2693e-02, -7.0007e-02,\n",
       "           -2.2107e-01, -5.3027e-01,  4.9072e-01, -7.5635e-01,  3.9368e-02,\n",
       "           -7.2632e-02, -1.4880e-01, -5.1270e-01,  2.3291e-01,  1.1121e-01,\n",
       "           -2.3999e-01, -3.7537e-02, -4.7729e-01,  7.8308e-02, -2.3083e-01,\n",
       "            3.4351e-01, -4.4861e-03,  4.2529e-01,  2.7368e-01, -9.3323e-02,\n",
       "           -1.3696e-01, -3.6469e-02,  1.8433e-02, -5.8929e-02, -5.1611e-01,\n",
       "            9.7473e-02,  2.0337e-01, -1.2207e-01, -2.8076e-01,  2.6294e-01,\n",
       "           -8.9539e-02,  2.1777e-01, -1.0071e-01,  1.8457e-01, -2.9007e-02,\n",
       "           -3.1421e-01,  7.1899e-02, -1.4587e-01,  1.2854e-01, -6.4636e-02,\n",
       "            1.0138e-01, -3.6987e-02, -4.8584e-01,  7.2998e-01,  1.5015e-01,\n",
       "            1.4148e-01,  3.2007e-01, -4.7485e-01,  1.2833e-02,  3.5327e-01,\n",
       "            1.4722e-01, -3.1567e-01, -2.3572e-01, -1.1584e-01,  8.1116e-02,\n",
       "           -1.8274e-01,  1.9495e-01, -9.3201e-02,  5.8044e-02,  2.0416e-02,\n",
       "            9.9243e-02, -1.7065e-01,  1.5884e-02,  3.1470e-01,  2.2119e-01,\n",
       "            5.8685e-02, -6.9153e-02,  2.6978e-01, -4.0918e-01,  2.4121e-01,\n",
       "            1.6321e-01, -2.6636e-01, -6.0986e-01,  1.2344e+00,  3.5449e-01,\n",
       "            8.4045e-02, -3.7140e-02, -5.1025e-01,  3.3862e-01,  1.7078e-01,\n",
       "           -2.9932e-01, -6.1859e-02,  2.6062e-02,  3.5010e-01,  8.5645e-01,\n",
       "           -1.7371e-01, -3.8452e-01, -3.3301e-01,  3.1836e-01,  3.7744e-01,\n",
       "           -3.6401e-01, -2.4368e-02]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.1299e-02, 7.5586e-01, 8.8043e-03, 4.1175e-04, 2.8908e-05, 1.3411e-05,\n",
       "           2.2351e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   447.766968  302.275177  638.996460  423.547791    0.945698      2     car\n",
       "  1   172.100388   68.070999  397.168396  413.491699    0.926973      5     bus\n",
       "  2   370.616608  283.925201  493.390411  388.315887    0.916641      2     car\n",
       "  3     0.793892  302.369354  221.787598  422.760437    0.902177      2     car\n",
       "  4   234.620911  227.875641  280.668579  277.589142    0.850343      0  person\n",
       "  5   150.827408  283.159454  170.254715  328.836395    0.748921      0  person\n",
       "  6   567.218384  261.797150  585.082520  276.077606    0.679256      0  person\n",
       "  7    31.427975  281.442780  142.682602  310.185883    0.659218      2     car\n",
       "  8   251.048584  121.887726  267.296265  137.163116    0.643492      0  person\n",
       "  9   234.994583  119.671112  250.584671  137.743561    0.642330      0  person\n",
       "  10  139.124069  290.038940  151.947098  309.266602    0.612313      0  person\n",
       "  11  604.024597  262.340057  618.227844  275.290497    0.601804      0  person\n",
       "  12   11.514995  277.120056   34.529285  312.182434    0.497824      0  person\n",
       "  13  312.421967  121.625473  334.065460  139.116562    0.437127      0  person\n",
       "  14  330.727997  259.895599  344.771576  272.164093    0.381601      0  person\n",
       "  15  167.192566  291.819031  175.567444  331.328064    0.291447      0  person\n",
       "  16  422.131958  253.680206  455.638000  272.555206    0.290822      2     car\n",
       "  17  318.720184  258.479706  332.588837  270.343964    0.289849      0  person\n",
       "  18  198.277145  142.284882  320.573853  360.809174    0.280567      5     bus\n",
       "  19   32.170322  284.564423   57.089314  295.788971    0.269433      2     car,\n",
       "  'caption': ['A light gray car parked next to the big green bus.'],\n",
       "  'bbox_target': [0.0, 303.67, 224.42, 120.66]},\n",
       " 341: {'image_emb': tensor([[-0.2499,  0.3132, -0.3835,  ...,  1.4717,  0.0658,  0.0829],\n",
       "          [-0.2283,  0.5127,  0.0174,  ...,  1.1777,  0.1061,  0.0456],\n",
       "          [-0.6123,  0.3948, -0.2174,  ...,  1.2734, -0.0023, -0.2295],\n",
       "          ...,\n",
       "          [-0.4934,  0.3835, -0.1733,  ...,  1.2344,  0.3352, -0.1349],\n",
       "          [-0.5757,  0.4236, -0.2805,  ...,  1.3994,  0.1041,  0.0802],\n",
       "          [-0.2883,  0.7275, -0.0929,  ...,  0.7373, -0.5151, -0.0912]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2898,  0.0948, -0.0285,  ..., -0.0680, -0.4207, -0.0131],\n",
       "          [-0.2437, -0.0445, -0.1781,  ...,  0.1840, -0.0916,  0.2345]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.0092e-03, 6.6471e-04, 2.3303e-01, 1.2741e-02, 6.5332e-01, 8.2245e-03,\n",
       "           8.1444e-04, 8.7097e-02],\n",
       "          [5.2399e-02, 5.0068e-05, 6.8359e-02, 1.2642e-02, 8.5938e-01, 1.5676e-04,\n",
       "           3.8567e-03, 3.2978e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    0.804337  304.889801  348.581360  434.320068    0.922882     42    fork\n",
       "  1  168.350250    0.000000  338.514709  201.637329    0.922349     41     cup\n",
       "  2  141.354370  101.548828  321.286102  227.955353    0.890229     44   spoon\n",
       "  3  415.248993  222.107544  639.474976  296.397156    0.863263     42    fork\n",
       "  4  332.934631    5.353714  470.084595  326.470276    0.851149     44   spoon\n",
       "  5    3.250519    0.537682  638.238647  167.940918    0.832007      0  person\n",
       "  6   53.902267  186.377899  252.968887  283.138092    0.755063     44   spoon\n",
       "  7  190.744904  182.789551  615.147705  476.368347    0.659593     45    bowl\n",
       "  8    0.542793    5.520874  133.808716  353.816589    0.281505      0  person,\n",
       "  'caption': ['a fork in front of a bowl of soup',\n",
       "   'A shiny silver fork with reflection in it.'],\n",
       "  'bbox_target': [1.85, 305.34, 336.99, 133.5]},\n",
       " 342: {'image_emb': tensor([[ 0.1099,  0.0630,  0.4175,  ...,  0.5542,  0.3643, -0.3958],\n",
       "          [ 0.3125,  0.3936,  0.5322,  ...,  0.9253,  0.2262, -0.5586],\n",
       "          [ 0.2480,  0.0539,  0.4802,  ...,  0.7896,  0.2490, -0.3728],\n",
       "          [ 0.0055, -0.0691,  0.4963,  ...,  0.4763,  0.2842, -0.4438]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3225,  0.3105,  0.0936,  ...,  0.7437, -0.2051, -0.1260],\n",
       "          [-0.2311,  0.0357,  0.0322,  ..., -0.1689,  0.1136, -0.2067]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3066, 0.3115, 0.1542, 0.2278],\n",
       "          [0.5522, 0.2451, 0.1312, 0.0714]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class name\n",
       "  0   81.524185  18.863922  341.717896  356.959625    0.952180     19  cow\n",
       "  1  254.079926  46.068207  476.415985  190.795380    0.923214     19  cow\n",
       "  2  362.906738  44.696762  531.649353  159.961853    0.738596     19  cow\n",
       "  3  542.544861  88.466660  568.551941  103.627884    0.638289     19  cow,\n",
       "  'caption': ['A large brown steer standing in the middle of a dirt road with two other steers behind it.',\n",
       "   'goat closest to camera'],\n",
       "  'bbox_target': [82.52, 13.96, 259.68, 343.01]},\n",
       " 343: {'image_emb': tensor([[-0.3228,  0.2976, -0.1018,  ...,  1.3066, -0.0603, -0.2327],\n",
       "          [-0.4758,  0.2382,  0.3152,  ...,  0.7646, -0.0791, -0.0712],\n",
       "          [-0.3320,  0.0831,  0.1858,  ...,  0.9785, -0.4189, -0.2330],\n",
       "          [-0.4482,  0.1594,  0.1445,  ...,  1.5186,  0.0862, -0.1499],\n",
       "          [-0.2756,  0.2358,  0.4780,  ...,  0.6235, -0.2998, -0.0302]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1708, -0.1201, -0.1266,  ..., -0.2041, -0.2085, -0.1780],\n",
       "          [-0.1311,  0.0878, -0.4551,  ..., -0.3757, -0.1996, -0.2578]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.4136e-03, 1.2064e-03, 8.9551e-01, 1.8215e-04, 9.8877e-02],\n",
       "          [4.3259e-03, 4.7241e-02, 8.2422e-01, 1.7748e-03, 1.2250e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class  \\\n",
       "  0  308.403564   0.000000  455.962463   79.972321    0.887546     73   \n",
       "  1    1.671677  92.993256  439.869995  391.513092    0.880161     77   \n",
       "  2  352.929993  23.128387  639.692139  187.362213    0.857651      0   \n",
       "  3  137.864426  63.068420  256.817139  135.455566    0.795613     73   \n",
       "  4    3.812012  16.595901  636.230774  417.676819    0.650378     59   \n",
       "  \n",
       "           name  \n",
       "  0        book  \n",
       "  1  teddy bear  \n",
       "  2      person  \n",
       "  3        book  \n",
       "  4         bed  ,\n",
       "  'caption': ['A mother reading a book to child laying in bed.',\n",
       "   'The woman laying by the child.'],\n",
       "  'bbox_target': [364.34, 37.99, 274.5, 144.51]},\n",
       " 344: {'image_emb': tensor([[-0.0682, -0.0898,  0.0589,  ...,  1.7344,  0.2324,  0.1539],\n",
       "          [-0.0921,  0.1117, -0.2421,  ...,  1.1367,  0.0565, -0.1164],\n",
       "          [-0.1785,  0.2455, -0.0099,  ...,  1.0693,  0.1926, -0.2101],\n",
       "          [ 0.1515,  0.0566, -0.0166,  ...,  0.2444,  0.1823, -0.3684]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1168,  0.1176, -0.4089,  ..., -0.2499, -0.1042, -0.4224],\n",
       "          [ 0.1635,  0.0574, -0.2380,  ..., -0.3201,  0.0959, -0.2188]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0123, 0.0238, 0.1389, 0.8252],\n",
       "          [0.0119, 0.0605, 0.2467, 0.6812]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  138.149902  167.100296  183.645081  208.186890    0.873093     33    kite\n",
       "  1  118.119576  343.359222  178.320831  498.927124    0.844054      0  person\n",
       "  2  173.655945  334.407104  274.204590  499.157532    0.824139      0  person,\n",
       "  'caption': ['The male to the right.', 'person on right'],\n",
       "  'bbox_target': [148.31, 335.96, 124.72, 161.79]},\n",
       " 345: {'image_emb': tensor([[-0.3376,  0.3025, -0.1876,  ...,  0.7856,  0.1858,  0.2218],\n",
       "          [-0.2646,  0.0038,  0.0777,  ...,  0.9009, -0.0132,  0.1837],\n",
       "          [-0.2554,  0.5186, -0.0469,  ...,  0.9702, -0.0217,  0.1302],\n",
       "          [-0.1898, -0.0656, -0.3848,  ...,  0.9326,  0.0662, -0.1196],\n",
       "          [-0.2385, -0.2910, -0.3809,  ...,  1.0000,  0.0354, -0.0464],\n",
       "          [-0.2477, -0.0660,  0.2603,  ...,  0.8594, -0.0374,  0.1281]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.4067, -0.1043, -0.0427,  ..., -0.1824, -0.0785, -0.0035],\n",
       "          [-0.2118, -0.4932,  0.0832,  ...,  0.0796, -0.3699,  0.0536]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.5283e-01, 9.8572e-02, 1.7297e-01, 2.2812e-03, 4.0436e-02, 3.3020e-02],\n",
       "          [7.3096e-01, 1.2311e-01, 8.5938e-02, 2.0444e-05, 6.8069e-05, 5.9998e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   173.498260  192.666656  314.953033  411.424103    0.933213     56   \n",
       "  1   234.878815  189.727570  619.160767  462.325806    0.913776     60   \n",
       "  2   302.191132  254.409454  475.547943  462.775513    0.913319     56   \n",
       "  3   336.688934  192.138123  413.183014  227.411194    0.859823     45   \n",
       "  4    52.659996  169.059052  163.138397  233.331573    0.808767     68   \n",
       "  5    81.698868  137.292023  151.589890  176.083496    0.605249     68   \n",
       "  6   296.331757  125.666977  319.430878  175.606689    0.393901     73   \n",
       "  7   276.291107  127.428925  293.435822  174.477966    0.326635     73   \n",
       "  8   529.121887  185.544586  594.237854  219.688202    0.325230     45   \n",
       "  9   252.439911  119.192474  347.158295  177.122650    0.305493     73   \n",
       "  10  325.150146  134.044022  337.014893  176.605286    0.293719     73   \n",
       "  11  309.985718  134.852463  323.866089  175.566193    0.273661     73   \n",
       "  12  334.283295  132.407120  343.238739  176.615723    0.262857     73   \n",
       "  \n",
       "              name  \n",
       "  0          chair  \n",
       "  1   dining table  \n",
       "  2          chair  \n",
       "  3           bowl  \n",
       "  4      microwave  \n",
       "  5      microwave  \n",
       "  6           book  \n",
       "  7           book  \n",
       "  8           bowl  \n",
       "  9           book  \n",
       "  10          book  \n",
       "  11          book  \n",
       "  12          book  ,\n",
       "  'caption': ['The chair next to the trashcan',\n",
       "   'The wooden chair with the blue cushion on it that is in front of the trash can and the rug space directly under the chair.'],\n",
       "  'bbox_target': [175.0, 191.97, 141.79, 214.9]},\n",
       " 346: {'image_emb': tensor([[-0.2186,  0.1832, -0.1801,  ...,  0.7266,  0.2673, -0.0385],\n",
       "          [-0.2605, -0.1029, -0.0714,  ...,  0.6606,  0.0697,  0.1009],\n",
       "          [-0.4595, -0.0185, -0.1952,  ...,  0.5605, -0.1028,  0.0636]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2520, -0.1960, -0.1281,  ...,  0.1394,  0.1893,  0.6099],\n",
       "          [-0.2145, -0.4836, -0.0070,  ...,  0.3079, -0.1301,  0.4517]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.8687, 0.0601, 0.0713],\n",
       "          [0.0412, 0.1310, 0.8276]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  361.124146  146.546448  466.828674  341.241699    0.886446     22  zebra\n",
       "  1   11.690826  175.975311  410.626373  472.444336    0.833752     22  zebra,\n",
       "  'caption': ['Zebra in the front of a picture', '3 ZEBRA  EATING THE GRASS'],\n",
       "  'bbox_target': [9.75, 162.26, 445.33, 310.97]},\n",
       " 347: {'image_emb': tensor([[ 0.0629,  0.0859, -0.4485,  ...,  1.5566,  0.3564,  0.0154],\n",
       "          [ 0.1508,  0.4021, -0.6562,  ...,  1.1064,  0.4456, -0.2686],\n",
       "          [-0.4216, -0.1015, -0.1510,  ...,  1.1777,  0.5542, -0.4031],\n",
       "          [-0.0219,  0.6660, -0.2729,  ...,  0.9746,  0.0419, -0.2825],\n",
       "          [-0.0024,  0.0813, -0.4827,  ...,  1.0391,  0.2546,  0.0710],\n",
       "          [-0.0638, -0.1293, -0.3213,  ...,  0.3833,  0.0910, -0.1240]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2583,  0.5498, -0.3987,  ..., -0.2754,  0.1819,  0.0883],\n",
       "          [ 0.1931,  0.2303, -0.6016,  ..., -0.1714,  0.2769, -0.1355]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.0059e-02, 9.3945e-01, 5.9605e-08, 3.6860e-04, 9.3579e-06, 2.3842e-06],\n",
       "          [5.2738e-04, 9.9902e-01, 3.9792e-04, 5.9605e-06, 9.3579e-06, 1.1086e-05]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   81.309998  128.858368  185.657715  397.731598    0.935393      0   \n",
       "  1  152.657242  136.802734  264.970337  422.769226    0.920288      0   \n",
       "  2  230.402435  102.355911  315.098846  379.744934    0.854850      0   \n",
       "  3  501.111481   14.020302  639.336304  121.461151    0.845706     25   \n",
       "  4  605.933411  166.894165  639.671082  249.841553    0.733123      0   \n",
       "  5  557.760132  241.818146  639.598877  412.314087    0.620339     60   \n",
       "  6  462.873230    0.031342  638.103455   39.053009    0.478824     25   \n",
       "  7  226.019104  241.332001  246.724274  269.624908    0.454628     40   \n",
       "  8   64.911469  235.333313   99.491074  282.122253    0.442362     26   \n",
       "  9    0.000000  201.630920    9.869450  261.536011    0.263677      0   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2        person  \n",
       "  3      umbrella  \n",
       "  4        person  \n",
       "  5  dining table  \n",
       "  6      umbrella  \n",
       "  7    wine glass  \n",
       "  8       handbag  \n",
       "  9        person  ,\n",
       "  'caption': ['A woman wearing a yellow shirt and red pants rolled up.',\n",
       "   'A woman in a yellow top holding a tray of glasses.'],\n",
       "  'bbox_target': [150.56, 136.27, 116.26, 283.97]},\n",
       " 348: {'image_emb': tensor([[ 0.2445, -0.0931, -0.1230,  ..., -0.0496,  0.1580, -0.3223],\n",
       "          [ 0.1588,  0.2405, -0.1450,  ...,  0.5747, -0.1632, -0.2076],\n",
       "          [ 0.2097, -0.0822,  0.0662,  ..., -0.2007,  0.1726, -0.3115]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3411,  0.0131, -0.2393,  ..., -0.2871, -0.4119, -0.2788],\n",
       "          [-0.0546,  0.1755, -0.2998,  ..., -0.2012, -0.1327, -0.2498]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1169, 0.0056, 0.8774],\n",
       "          [0.7578, 0.0285, 0.2137]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   86.937592  127.984222  389.365326  629.267090    0.939696      0   \n",
       "  1   33.356125   11.497166  109.532150   86.936478    0.936453     32   \n",
       "  2  327.810791  251.507935  379.500793  328.969543    0.611795      0   \n",
       "  3   32.937958  278.547607   97.340210  341.730042    0.320169      0   \n",
       "  \n",
       "            name  \n",
       "  0       person  \n",
       "  1  sports ball  \n",
       "  2       person  \n",
       "  3       person  ,\n",
       "  'caption': ['Two men playing a ball game.',\n",
       "   'A man wearing black t-shirt hitting the ball'],\n",
       "  'bbox_target': [86.29, 129.2, 304.9, 496.18]},\n",
       " 349: {'image_emb': tensor([[-0.1531, -0.3330, -0.0304,  ...,  0.5288,  0.0273,  0.3257],\n",
       "          [ 0.0120,  0.4392, -0.1976,  ...,  0.9771, -0.0622,  0.0890],\n",
       "          [ 0.0199,  0.4570, -0.2483,  ...,  1.2734, -0.1084, -0.1766],\n",
       "          [-0.0997, -0.0207,  0.1622,  ...,  1.0332,  0.0346, -0.0090]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1263, -0.0792, -0.3071,  ...,  0.0327,  0.2251, -0.0887],\n",
       "          [ 0.3782, -0.0501, -0.5029,  ...,  0.2101, -0.0870,  0.1382]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.3579e-06, 2.8149e-01, 7.1875e-01, 3.4750e-05],\n",
       "          [7.6890e-06, 1.9189e-01, 8.0811e-01, 1.1325e-06]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0   47.136833   65.001663  268.366211  305.940857    0.936867     20  elephant\n",
       "  1  476.580719   77.199402  631.110596  418.420532    0.919321      0    person\n",
       "  2  445.811707  150.026764  576.182373  415.716675    0.897964     26   handbag\n",
       "  3  295.817627  172.277527  325.035095  230.254211    0.682182      0    person\n",
       "  4  398.729614  181.065155  417.647949  217.015961    0.644684      0    person\n",
       "  5  248.734238  187.165283  293.568848  257.424377    0.390595      0    person,\n",
       "  'caption': ['A black bag on a mans shoulder.', 'a black shoulder bag.'],\n",
       "  'bbox_target': [446.92, 154.23, 127.01, 264.52]},\n",
       " 350: {'image_emb': tensor([[-0.2220,  0.1764, -0.1251,  ...,  0.9888,  0.3230, -0.0633],\n",
       "          [ 0.1691,  0.4207, -0.4092,  ...,  1.1748,  0.2076, -0.1118],\n",
       "          [-0.4656,  0.0252, -0.1309,  ...,  0.7812,  0.5410,  0.2172],\n",
       "          [-0.0271,  0.0772, -0.0475,  ...,  1.2637,  0.1440, -0.1247],\n",
       "          [-0.0756,  0.1049, -0.2893,  ...,  1.3584,  0.0104, -0.1758],\n",
       "          [-0.1493,  0.1067,  0.4224,  ...,  0.3228,  0.4985,  0.2462]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0602, -0.1880, -0.1804,  ...,  0.1324,  0.3198, -0.1155],\n",
       "          [-0.2542,  0.2747, -0.2947,  ..., -0.3413,  0.1064, -0.3101]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.5979e-01, 1.4715e-03, 3.0375e-04, 8.6546e-04, 1.0496e-04, 8.3740e-01],\n",
       "          [8.6719e-01, 1.6647e-02, 1.4324e-03, 9.5367e-04, 2.2650e-06, 1.1377e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  103.417572   21.831879  453.643463  635.791870    0.952421      0  person\n",
       "  1    0.418098  258.662292  149.696503  639.087158    0.922871      0  person\n",
       "  2   71.380814  233.429047  243.992371  616.601685    0.902171     56   chair\n",
       "  3  276.841370  143.026031  307.521790  329.945068    0.866805     27     tie\n",
       "  4  537.221252  221.142441  561.025330  253.651932    0.820430     41     cup\n",
       "  5    0.000000  110.355026   31.889015  480.268921    0.259979      0  person,\n",
       "  'caption': ['The gentleman in a three piece suit with a walking stick.',\n",
       "   'A man in a grey suite and red tie holding a cane.'],\n",
       "  'bbox_target': [105.85, 24.18, 346.51, 615.82]},\n",
       " 351: {'image_emb': tensor([[-0.1395,  0.1456, -0.1063,  ...,  0.4121, -0.0388,  0.2197],\n",
       "          [-0.1929,  0.1302, -0.1191,  ...,  0.6436, -0.2439,  0.3376],\n",
       "          [ 0.0197,  0.1758, -0.0205,  ...,  1.0205,  0.2725, -0.2646],\n",
       "          [-0.1237, -0.0630, -0.2408,  ...,  0.8711, -0.0381, -0.0403],\n",
       "          [-0.3813,  0.4788, -0.1465,  ...,  1.1650, -0.0550, -0.0117],\n",
       "          [-0.4097, -0.0461, -0.1421,  ...,  0.2109,  0.0710,  0.2583]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1019, -0.2891,  0.1292,  ...,  0.5215, -0.0156, -0.3257],\n",
       "          [ 0.0381, -0.1233,  0.1366,  ...,  0.3230, -0.2469, -0.0357]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.4131e-01, 1.6081e-04, 1.5808e-01, 2.5034e-05, 6.5565e-07, 1.9097e-04],\n",
       "          [6.3086e-01, 1.9653e-02, 3.3765e-01, 1.4019e-03, 1.0806e-04, 1.0353e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   59.096039  180.923584  297.592102  475.911316    0.946840      0  person\n",
       "  1  477.814453   96.213425  624.348389  270.262390    0.935981      0  person\n",
       "  2  273.588226   99.689926  416.573700  473.127747    0.925299      0  person\n",
       "  3  527.379761  167.695724  550.577026  234.971985    0.846981     27     tie\n",
       "  4   41.952091  351.004944  101.010040  387.003540    0.842953     45    bowl\n",
       "  5  420.096558  136.515274  437.249146  168.145737    0.537638     41     cup\n",
       "  6  335.410217  316.320740  595.657043  476.651672    0.515226     69    oven\n",
       "  7   22.885704  348.654877   64.235481  368.328339    0.319617     44   spoon,\n",
       "  'caption': ['An elderly man with a blue apron.', 'white man in blue apron'],\n",
       "  'bbox_target': [60.4, 180.88, 236.23, 292.32]},\n",
       " 352: {'image_emb': tensor([[ 0.2319,  0.4226, -0.0137,  ...,  0.7026, -0.0602, -0.3625],\n",
       "          [-0.0295,  0.1149,  0.0178,  ...,  1.1641,  0.0434, -0.2094],\n",
       "          [ 0.1210,  0.1951, -0.1157,  ...,  1.0078,  0.1454, -0.3552],\n",
       "          [ 0.2383, -0.0494, -0.3311,  ...,  0.7720,  0.1340, -0.1816],\n",
       "          [ 0.1614,  0.2688, -0.0767,  ...,  0.4089, -0.0247, -0.4238]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0514,  0.3879, -0.1216,  ..., -0.2488, -0.0919,  0.1711],\n",
       "          [ 0.0304,  0.1858, -0.2040,  ..., -0.3115,  0.1196, -0.0654]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.4199e-03, 6.4230e-04, 9.9365e-01, 6.5279e-04, 1.7462e-03],\n",
       "          [1.2732e-03, 1.3132e-03, 7.4707e-01, 2.4634e-01, 3.8605e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  285.643311   78.147110  596.114380  423.916016    0.943686      0    person\n",
       "  1  163.464737   46.430939  463.057800  133.466705    0.940290     76  scissors\n",
       "  2    0.071388  286.968903  141.701050  426.177551    0.895970      0    person\n",
       "  3  146.853943  350.744354  265.351013  427.305725    0.717019      0    person,\n",
       "  'caption': ['A smiling man that is wearing glasses.',\n",
       "   'man wearing glasses standing to the left of woman'],\n",
       "  'bbox_target': [0.96, 284.69, 137.54, 137.54]},\n",
       " 353: {'image_emb': tensor([[-1.3049e-01,  2.9199e-01, -6.9458e-02,  ...,  5.6885e-01,\n",
       "           -5.7739e-02,  1.7871e-01],\n",
       "          [-3.3203e-01,  2.1777e-01, -3.9673e-01,  ...,  8.5498e-01,\n",
       "            8.3313e-02,  1.6235e-01],\n",
       "          [-3.6914e-01,  2.2961e-01, -3.9771e-01,  ...,  1.0049e+00,\n",
       "           -8.5449e-04, -1.6418e-01],\n",
       "          [-4.2578e-01,  4.1626e-01, -1.4575e-01,  ...,  1.2100e+00,\n",
       "            1.6638e-01, -1.1517e-01],\n",
       "          [-2.6685e-01,  2.2131e-01,  1.8845e-03,  ...,  6.6455e-01,\n",
       "            9.8114e-03,  1.9324e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3572, -0.1669, -0.5913,  ..., -0.6670,  0.1611,  0.0920],\n",
       "          [-0.3699, -0.0259, -0.6162,  ..., -0.5464,  0.0316, -0.2554]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0986, 0.4561, 0.0857, 0.3184, 0.0411],\n",
       "          [0.0129, 0.2332, 0.0445, 0.7070, 0.0026]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  142.374817    0.747436  474.435913  351.401917    0.947753      0   \n",
       "  1  416.040131  189.526871  499.501312  354.415436    0.885757      0   \n",
       "  2   79.313835  220.269363  209.376099  354.912079    0.859205      0   \n",
       "  3    0.000000  106.227089  158.288116  353.119354    0.848830      0   \n",
       "  4    0.398803   77.036354  121.265472  198.454315    0.300535      0   \n",
       "  5  241.477066   45.655983  338.467560  131.003616    0.289850     34   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2        person  \n",
       "  3        person  \n",
       "  4        person  \n",
       "  5  baseball bat  ,\n",
       "  'caption': ['Child in the far left of the background',\n",
       "   'smiling child in the left side of the image'],\n",
       "  'bbox_target': [0.81, 104.56, 161.17, 248.21]},\n",
       " 354: {'image_emb': tensor([[-0.1099,  0.1493,  0.1481,  ...,  0.7871,  0.0149,  0.1831],\n",
       "          [ 0.0250,  0.2078, -0.1749,  ...,  1.0850,  0.0278,  0.2693],\n",
       "          [ 0.1182,  0.0911, -0.0707,  ...,  1.0234, -0.1371, -0.3025],\n",
       "          [-0.0694, -0.1718,  0.0124,  ...,  1.1582, -0.0525,  0.1471],\n",
       "          [-0.0704,  0.1587,  0.0705,  ...,  0.7554,  0.1003,  0.1437],\n",
       "          [-0.0761,  0.1221,  0.0896,  ...,  0.7236,  0.0056,  0.1455]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1256,  0.0734, -0.3826,  ...,  0.0854,  0.2341,  0.0436],\n",
       "          [-0.0581,  0.0522, -0.2832,  ..., -0.5693,  0.0848, -0.1367]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1350, 0.0095, 0.0007, 0.0037, 0.4290, 0.4224],\n",
       "          [0.1924, 0.0188, 0.0153, 0.0309, 0.4836, 0.2588]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  346.253540   87.544724  639.692749  477.161621    0.932805      0    person\n",
       "  1  262.044189  230.056091  432.200073  478.756165    0.917035      0    person\n",
       "  2    0.461304    0.638321  172.443726  264.313049    0.894984      0    person\n",
       "  3   90.531754  267.034851  347.729370  479.263794    0.794919      0    person\n",
       "  4    2.248566    1.803406  639.562866  476.789673    0.778982     25  umbrella,\n",
       "  'caption': ['Little boy in front of umbrella near girl with missing tooth',\n",
       "   'smallest boy on left'],\n",
       "  'bbox_target': [112.75, 264.26, 195.44, 210.47]},\n",
       " 355: {'image_emb': tensor([[ 0.4531,  0.1124, -0.2076,  ...,  0.5938,  0.0028,  0.1583],\n",
       "          [ 0.2542,  0.4897, -0.3630,  ...,  1.3516,  0.1235,  0.1929],\n",
       "          [-0.0040,  0.1202, -0.3977,  ...,  1.2910,  0.0961,  0.0932],\n",
       "          [ 0.0708,  0.3938, -0.2664,  ...,  0.9468,  0.0801,  0.5039],\n",
       "          [ 0.1934, -0.1149, -0.3162,  ...,  0.9888, -0.0049, -0.0751],\n",
       "          [ 0.3616,  0.1777, -0.3164,  ...,  0.8564, -0.0346,  0.5259]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1147,  0.1407, -0.0648,  ..., -0.2311, -0.0814,  0.0413],\n",
       "          [ 0.0516, -0.2837, -0.3105,  ...,  0.3059, -0.0004, -0.0252]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.2764e-01, 1.3098e-01, 1.3168e-02, 7.5035e-03, 2.5520e-03, 1.8280e-02],\n",
       "          [7.2119e-01, 2.3448e-04, 1.7524e-05, 1.7126e-01, 1.1325e-06, 1.0718e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  135.458588   93.989090  373.049438  498.823059    0.953777      0  person\n",
       "  1    0.000000  194.303726  152.240662  426.823517    0.878879      0  person\n",
       "  2  135.210663  146.241974  229.917282  322.411621    0.822962      0  person\n",
       "  3    0.382268  225.865036  283.117676  499.484406    0.810284     57   couch\n",
       "  4  138.477036  378.519714  157.843826  400.572693    0.803207     65  remote\n",
       "  5  338.496033  188.721283  375.000000  393.934448    0.650048     56   chair,\n",
       "  'caption': ['a lady wearing glasses',\n",
       "   'A woman in a white sweater playing wii.'],\n",
       "  'bbox_target': [134.83, 91.01, 240.17, 408.99]},\n",
       " 356: {'image_emb': tensor([[ 0.0699,  0.3696, -0.0826,  ...,  0.8140,  0.3057, -0.2474],\n",
       "          [ 0.3621, -0.0148, -0.2524,  ...,  0.7983,  0.1317,  0.0095]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0205,  0.2861,  0.1888,  ...,  0.5576,  0.1260, -0.2825],\n",
       "          [ 0.1477, -0.1236,  0.1884,  ..., -0.2756, -0.2494, -0.3674]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2942, 0.7056],\n",
       "          [0.0244, 0.9756]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class   name\n",
       "  0  312.184723  34.316704  590.959473  559.204773    0.726871     53  pizza\n",
       "  1   21.017742  37.515594  572.971741  584.135254    0.465245     53  pizza,\n",
       "  'caption': ['Half of a steak, onions, and cheese pizza', 'A pizza'],\n",
       "  'bbox_target': [17.95, 39.17, 386.79, 540.19]},\n",
       " 357: {'image_emb': tensor([[-0.4402,  0.5869,  0.0482,  ...,  0.8164, -0.1519,  0.0703],\n",
       "          [-0.3564,  0.6025, -0.2800,  ...,  0.4382, -0.2206,  0.6514],\n",
       "          [-0.4744,  0.7612, -0.0695,  ...,  0.4021, -0.2922,  0.2084]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2006, -0.0057,  0.0380,  ...,  0.1206, -0.1177, -0.0938],\n",
       "          [-0.2291,  0.2671, -0.2908,  ...,  0.6812, -0.3210,  0.0311]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1281, 0.0373, 0.8350],\n",
       "          [0.8516, 0.0116, 0.1368]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  244.705536    0.000000  625.371826  230.657928    0.910489     45   bowl\n",
       "  1  390.412201  202.228271  539.678589  367.433655    0.712732     45   bowl\n",
       "  2   57.183121   98.342300  353.035278  378.139038    0.641222     45   bowl\n",
       "  3  346.194489  210.350677  418.148407  286.603973    0.521799     43  knife\n",
       "  4  346.989471  217.358459  439.041351  309.352722    0.414009     42   fork,\n",
       "  'caption': ['There is many food items on the white plate',\n",
       "   'A white plate with food that is smothered in gravy and parsley.'],\n",
       "  'bbox_target': [56.55, 97.76, 296.46, 281.04]},\n",
       " 358: {'image_emb': tensor([[-0.0139,  0.2864, -0.4478,  ...,  0.2129, -0.0782,  0.0795],\n",
       "          [ 0.3994,  0.4958, -0.2747,  ...,  0.8242, -0.0865,  0.1038],\n",
       "          [-0.0468,  0.1940, -0.3071,  ...,  0.5947, -0.0010, -0.2478],\n",
       "          [ 0.2646,  0.1285, -0.3906,  ...,  0.1256,  0.0631,  0.1158]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1711, -0.2299,  0.0089,  ...,  0.5078, -0.0986,  0.2383],\n",
       "          [ 0.4165, -0.1798,  0.1915,  ...,  0.3669, -0.1198, -0.0791]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0222, 0.2832, 0.5996, 0.0949],\n",
       "          [0.0056, 0.6470, 0.3411, 0.0063]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0    6.983734   85.982224  471.406036  451.054077    0.942811      7  truck\n",
       "  1  453.490570  164.522583  553.466431  375.313660    0.892101      7  truck\n",
       "  2  529.989441  192.225372  615.393616  349.597870    0.764879      7  truck\n",
       "  3  588.393066  222.391602  638.988037  315.729858    0.559543      7  truck,\n",
       "  'caption': ['The second green truck in a row of green trucks.',\n",
       "   'truck with license plate e9fgb'],\n",
       "  'bbox_target': [453.82, 167.06, 99.74, 210.71]},\n",
       " 359: {'image_emb': tensor([[-0.1158,  0.0723, -0.2058,  ...,  0.1372, -0.0334, -0.4170],\n",
       "          [-0.2191,  0.2742, -0.3911,  ...,  1.1914,  0.0063, -0.5366],\n",
       "          [ 0.0266, -0.2495, -0.2986,  ..., -0.0305,  0.3418, -0.3057],\n",
       "          [-0.0555, -0.0703, -0.2031,  ...,  0.8867,  0.3840, -0.1005],\n",
       "          [ 0.2698,  0.0060, -0.3623,  ...,  0.2139,  0.0349, -0.4399],\n",
       "          [ 0.1714, -0.1947, -0.3167,  ..., -0.0248,  0.0770, -0.2450]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3706,  0.4624, -0.1343,  ..., -0.4465,  0.1301,  0.0402],\n",
       "          [-0.0720,  0.1254, -0.0345,  ..., -0.3738, -0.0534, -0.2155]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.7451e-01, 1.0608e-01, 9.8944e-06, 1.5318e-05, 1.9012e-02, 5.6553e-04],\n",
       "          [8.0061e-04, 9.9512e-01, 5.6624e-06, 1.8430e-04, 9.7156e-05, 3.9406e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0    7.568527   98.724976  307.775818  578.712524    0.929213      0    person\n",
       "  1   18.170624  476.029846  121.171356  623.153870    0.924139     28  suitcase\n",
       "  2  233.973114  361.090881  449.187500  639.984192    0.891811      0    person\n",
       "  3  157.717743  188.079712  197.877228  306.866180    0.878041     27       tie\n",
       "  4  314.762451  101.746521  479.720032  317.128571    0.822153      0    person,\n",
       "  'caption': ['A man holding open his coat.',\n",
       "   'A MAN STANDING WITH HER LADY AND PUT HIS RIGHT LEG ABOVE HIS SUIT CASE'],\n",
       "  'bbox_target': [1.32, 98.29, 307.03, 481.72]},\n",
       " 360: {'image_emb': tensor([[ 0.3054,  0.4873, -0.1530,  ...,  0.9155,  0.0018,  0.0367],\n",
       "          [ 0.2534,  0.0537, -0.3501,  ...,  1.2529, -0.2832, -0.0195],\n",
       "          [ 0.0345, -0.0231, -0.3567,  ...,  1.0723, -0.1011,  0.0567],\n",
       "          ...,\n",
       "          [ 0.1028, -0.0577, -0.0745,  ...,  0.8647,  0.0175, -0.1445],\n",
       "          [-0.0623,  0.3618, -0.2585,  ...,  1.0869, -0.3928, -0.1997],\n",
       "          [ 0.1104,  0.0701,  0.1276,  ...,  0.9619, -0.2253,  0.0754]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0887,  0.2634,  0.5127,  ..., -0.0293, -0.0108,  0.0867],\n",
       "          [-0.1667,  0.0087,  0.0537,  ...,  0.0571,  0.1637, -0.0809]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.4843e-04, 5.2094e-05, 4.1842e-05, 4.8096e-02, 1.8597e-05, 8.5235e-06,\n",
       "           2.8125e-01, 3.5400e-03, 6.6406e-01, 2.6302e-03],\n",
       "          [2.0790e-03, 1.1482e-03, 7.5293e-04, 1.2268e-01, 8.1396e-04, 2.2602e-04,\n",
       "           3.7207e-01, 2.7542e-03, 4.7778e-01, 1.9714e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   374.894867   48.866547  508.468170  279.519897    0.929320      0   \n",
       "  1   166.485596   77.749619  271.913086  303.612244    0.921786      0   \n",
       "  2   286.646484  276.208008  368.455200  436.832275    0.898910     28   \n",
       "  3   535.002625  149.828903  639.267761  291.221252    0.856609      0   \n",
       "  4   415.199799  166.303070  484.269379  274.898773    0.855265     28   \n",
       "  5   283.340271  256.123138  371.003113  294.134552    0.797109     73   \n",
       "  6     0.000000  203.785004  247.605774  474.289185    0.796677     13   \n",
       "  7   546.668335  132.566010  604.247681  208.782288    0.791221     26   \n",
       "  8    45.455353  156.508850  225.753510  299.011902    0.729297     13   \n",
       "  9   408.758057  133.678833  465.387756  160.662567    0.689875     63   \n",
       "  10  234.712830  356.351990  325.791504  478.616089    0.671090     24   \n",
       "  11  260.159607  135.146194  338.111450  285.151489    0.617892     28   \n",
       "  12    0.000000  189.621246   67.083664  221.126495    0.490632     73   \n",
       "  13  231.136505   21.116013  258.270447   52.641953    0.474106      0   \n",
       "  14  259.006348  142.145691  326.797363  205.419250    0.393558     28   \n",
       "  15  258.919861  140.310745  327.110962  206.812378    0.386625     24   \n",
       "  16  232.609375  116.742081  383.537598  228.309265    0.351017     13   \n",
       "  17  218.227280  164.296783  265.910645  204.129974    0.337365     26   \n",
       "  18  348.342499  118.138336  394.473663  189.455963    0.334065     13   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2   suitcase  \n",
       "  3     person  \n",
       "  4   suitcase  \n",
       "  5       book  \n",
       "  6      bench  \n",
       "  7    handbag  \n",
       "  8      bench  \n",
       "  9     laptop  \n",
       "  10  backpack  \n",
       "  11  suitcase  \n",
       "  12      book  \n",
       "  13    person  \n",
       "  14  suitcase  \n",
       "  15  backpack  \n",
       "  16     bench  \n",
       "  17   handbag  \n",
       "  18     bench  ,\n",
       "  'caption': ['A purple and orange section of a bench.',\n",
       "   'an orange and pink empty seats.'],\n",
       "  'bbox_target': [0.0, 204.24, 250.91, 253.49]},\n",
       " 361: {'image_emb': tensor([[ 0.2345,  0.1199, -0.2159,  ...,  1.2500,  0.1362, -0.3982],\n",
       "          [ 0.2920,  1.0176,  0.3237,  ...,  1.2129,  0.2379, -0.5166],\n",
       "          [ 0.0777,  0.2615, -0.2749,  ...,  0.7432, -0.0815, -0.1259],\n",
       "          ...,\n",
       "          [ 0.2466, -0.2500, -0.3875,  ...,  0.7021, -0.1120, -0.2094],\n",
       "          [ 0.0481, -0.2256, -0.3208,  ...,  0.9473, -0.2045, -0.2766],\n",
       "          [ 0.0835, -0.2151,  0.3328,  ...,  0.5283,  0.0480,  0.0318]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 2.3792e-01,  1.8213e-01,  4.5929e-02, -4.2297e-02, -1.7139e-01,\n",
       "            2.4072e-01,  5.7936e-04,  1.1772e-02, -4.8981e-02, -1.7517e-01,\n",
       "            2.5049e-01, -4.5433e-03,  2.1082e-01, -1.5063e-01, -2.3157e-01,\n",
       "            1.6382e-01,  3.9062e-01,  6.2866e-02,  1.9424e-02,  2.0798e-02,\n",
       "            2.9395e-01,  2.3224e-02, -1.3062e-01, -2.9443e-01, -4.2084e-02,\n",
       "            4.4946e-01, -1.9653e-01,  3.2495e-01,  3.9307e-01, -1.2805e-01,\n",
       "           -9.6802e-02, -1.4893e-01, -1.1957e-01,  1.2207e-01, -5.5322e-01,\n",
       "            5.4749e-02, -3.6230e-01,  1.5320e-01, -1.2451e-02,  3.6230e-01,\n",
       "            2.3291e-01,  2.7930e-01, -2.3645e-01,  2.4744e-01, -7.4577e-03,\n",
       "            2.2302e-01,  7.2607e-01, -1.8994e-01, -1.4099e-01, -3.8770e-01,\n",
       "           -4.9438e-02, -2.9346e-01, -1.6516e-01, -3.4888e-01, -1.5625e-01,\n",
       "           -3.4033e-01,  2.1387e-01,  9.3811e-02,  2.9614e-01, -3.5254e-01,\n",
       "           -6.3904e-02, -3.2324e-01,  2.8491e-01,  6.0059e-01,  1.9714e-01,\n",
       "            2.4094e-02, -7.4890e-02,  1.5759e-01, -6.6872e-03, -5.9845e-02,\n",
       "           -9.6359e-03, -1.0223e-01,  1.3611e-02,  2.3975e-01,  1.1151e-01,\n",
       "           -8.4351e-02,  1.9128e-01,  2.4475e-01,  7.0190e-02, -8.5266e-02,\n",
       "           -1.1914e-01,  2.3279e-01, -1.9983e-01, -9.7717e-02, -1.0205e-01,\n",
       "            2.6443e-02, -8.9966e-02, -2.6489e-01,  3.1104e-01, -1.5869e-01,\n",
       "            6.5857e-02, -2.5415e-01, -6.5674e-01,  7.2852e-01, -2.1021e-01,\n",
       "            9.1553e-02, -2.2925e-01, -2.2559e-01,  4.7211e-02, -3.0298e-01,\n",
       "            1.2988e-01,  1.4062e-01,  5.8057e-01, -1.4732e-02, -5.2246e-01,\n",
       "           -3.2806e-02, -2.2205e-01,  4.7577e-02,  3.5669e-01,  8.2581e-02,\n",
       "           -1.7065e-01, -2.9565e-01, -9.4849e-02,  4.2627e-01,  3.7866e-01,\n",
       "            7.6904e-02,  4.6729e-01, -1.7676e-01,  4.4212e-03, -5.6305e-02,\n",
       "           -3.9551e-02, -7.8076e-01, -3.1714e-01, -2.1326e-01, -1.9928e-02,\n",
       "           -1.4233e-01, -2.7002e-01, -3.6841e-01, -1.0956e-02,  3.6304e-01,\n",
       "            1.8860e-01, -2.7197e-01,  3.1799e-02,  4.4414e+00,  3.1592e-01,\n",
       "           -1.9153e-01, -2.1411e-01, -3.7866e-01, -1.6956e-01, -3.7427e-01,\n",
       "            2.6855e-01, -2.5684e-01, -3.9648e-01,  2.1680e-01,  5.0812e-02,\n",
       "           -1.0498e-01, -1.3245e-01,  1.7029e-01, -1.0712e-01, -1.3391e-01,\n",
       "           -3.0273e-01, -1.5210e-01,  1.6064e-01,  6.2158e-01, -1.2732e-01,\n",
       "            1.0834e-01,  2.5879e-01, -6.4648e-01, -1.1969e-01,  1.7285e-01,\n",
       "           -1.5381e-01,  1.1212e-01, -1.6382e-01, -5.6641e-02, -5.4291e-02,\n",
       "           -1.4209e-01,  4.0771e-01, -4.1113e-01, -8.2703e-02,  3.2202e-01,\n",
       "            5.0842e-02, -1.6272e-01,  3.6896e-02,  1.7542e-01, -2.7661e-01,\n",
       "           -2.8748e-02,  1.8164e-01, -2.1960e-01, -5.4993e-02, -2.1680e-01,\n",
       "            2.0374e-01, -6.6589e-02, -5.2002e-01,  5.9631e-02, -3.8300e-02,\n",
       "           -2.9663e-01, -1.5942e-01, -2.8101e-01, -4.1382e-01,  2.0544e-01,\n",
       "            1.2646e-01,  2.2046e-01, -1.0876e-01, -3.1738e-01, -4.6240e-01,\n",
       "            1.1133e-01, -4.5288e-02,  1.6675e-01, -1.4368e-01, -2.9297e-01,\n",
       "           -1.0498e-01, -3.3905e-02,  1.0632e-01,  2.3743e-01,  2.9883e-01,\n",
       "           -3.0786e-01,  3.4363e-02, -3.7231e-01, -1.4343e-01,  1.2610e-01,\n",
       "            8.2764e-01, -4.7302e-03, -1.4001e-01,  3.2349e-01,  1.8335e-01,\n",
       "           -3.6084e-01,  2.6392e-01,  2.3474e-01, -1.2927e-01,  9.4177e-02,\n",
       "           -1.0059e-01, -8.9294e-02, -2.2949e-01,  1.4610e-02, -2.2598e-02,\n",
       "           -4.2114e-01,  7.8125e-02, -4.8535e-01, -2.3206e-01, -5.4199e-02,\n",
       "           -5.9326e-01, -1.2123e-02, -5.4504e-02,  2.6099e-01,  2.3766e-03,\n",
       "            2.2583e-01,  2.3779e-01, -2.7417e-01, -3.1445e-01, -7.2144e-02,\n",
       "           -3.0566e-01,  1.4221e-01,  1.4905e-01,  3.1592e-01, -4.1968e-01,\n",
       "            2.3328e-01,  2.3242e-01, -1.9989e-02,  3.3862e-01, -2.1866e-02,\n",
       "           -3.9087e-01,  1.7651e-01,  4.3359e-01,  2.5464e-01, -1.9214e-01,\n",
       "            1.1993e-01, -1.9714e-02, -8.1543e-02,  3.5547e-01, -2.0227e-01,\n",
       "           -3.1787e-01,  2.5803e-02, -1.9678e-01,  9.9915e-02,  4.4897e-01,\n",
       "            4.0955e-02,  4.2822e-01, -1.3037e-01,  5.0049e-02,  5.7220e-02,\n",
       "           -1.9727e-01, -1.2032e-02, -2.2205e-01, -1.9568e-01, -2.1191e-01,\n",
       "           -7.2754e-02, -2.6352e-02,  1.0056e-02,  4.7192e-01, -3.1030e-01,\n",
       "            5.8197e-02, -1.4355e-01, -3.9526e-01,  2.2791e-01, -1.5845e-01,\n",
       "           -2.3743e-01, -1.2549e-01,  1.7761e-01,  6.4795e-01,  2.0959e-01,\n",
       "            3.8940e-01,  2.2571e-01,  2.1973e-01,  1.8250e-01, -8.2458e-02,\n",
       "            1.2402e-01, -4.0869e-01, -4.0649e-01, -1.0950e-01, -1.8945e-01,\n",
       "            1.1890e-01,  9.4604e-02,  8.5022e-02, -5.9448e-02,  1.5320e-01,\n",
       "           -4.5068e-01,  3.4698e-02,  1.9104e-01,  2.9932e-01, -6.8359e-02,\n",
       "            4.7363e-02,  2.4878e-01,  4.4492e+00,  3.3594e-01,  2.5439e-01,\n",
       "            3.7427e-01,  1.6443e-01, -2.4890e-01,  2.2632e-01, -2.8934e-03,\n",
       "            5.5957e-01,  3.0396e-01, -1.0028e-01,  7.0801e-02, -2.2876e-01,\n",
       "            7.4402e-02,  1.6321e-01, -2.4341e-01, -1.3733e-01, -4.2358e-01,\n",
       "            3.3179e-01,  4.2786e-02, -3.3252e-01,  2.6660e-01, -7.8964e-03,\n",
       "           -2.7319e-01, -1.1725e-01, -3.6060e-01,  3.4766e-01,  1.4001e-01,\n",
       "           -3.0615e-01, -3.2251e-01,  2.1191e-01, -4.1650e-01, -8.5693e-02,\n",
       "            2.0520e-01,  4.5972e-01,  3.3355e-04, -1.4697e-01, -2.4207e-01,\n",
       "           -3.0737e-01, -3.8013e-01,  2.7881e-01,  6.8054e-03, -7.7881e-01,\n",
       "            5.8380e-02, -5.6396e-01,  3.0225e-01,  1.0730e-01, -2.1545e-01,\n",
       "            2.0935e-02, -6.9922e-01, -3.2056e-01,  1.6235e-02,  4.5502e-02,\n",
       "            2.1741e-01, -1.3916e-01,  3.1982e-01, -7.3914e-02,  8.2214e-02,\n",
       "           -3.9734e-02,  1.0718e-01, -2.7808e-01,  2.0850e-01,  1.0504e-01,\n",
       "            1.5332e-01, -1.1279e-01, -1.5430e-01, -4.0137e-01,  1.5344e-01,\n",
       "            2.5073e-01,  7.5244e-01,  2.4377e-01, -4.1064e-01,  2.9984e-02,\n",
       "            1.8591e-01, -2.7661e-01,  9.4177e-02,  1.5472e-02, -4.1797e-01,\n",
       "           -1.3684e-01, -7.0129e-02, -1.4771e-01,  2.3572e-01,  2.4683e-01,\n",
       "            2.5195e-01, -9.8816e-02,  1.4209e-01, -8.6243e-02,  2.5317e-01,\n",
       "            2.4561e-01,  3.5083e-01, -3.6914e-01,  4.9683e-01, -4.9878e-01,\n",
       "           -3.9380e-01,  2.4338e-02,  2.1167e-01,  8.9111e-03,  1.8677e-01,\n",
       "           -3.2684e-02, -9.5520e-02,  1.0590e-01, -2.4948e-02,  1.7627e-01,\n",
       "            2.1228e-01,  7.0923e-02, -3.6792e-01,  7.1350e-02,  2.4036e-01,\n",
       "            1.7273e-01,  1.1467e-02,  9.6178e-04, -4.9390e-01, -4.3188e-01,\n",
       "           -3.7628e-02, -5.9937e-02,  3.9844e-01, -1.6602e-02,  2.3108e-01,\n",
       "            1.2439e-01,  2.3389e-01, -5.2295e-01, -8.3557e-02, -1.2915e-01,\n",
       "           -4.5868e-02, -2.8174e-01,  2.4451e-01, -2.9321e-01,  1.8115e-01,\n",
       "           -3.0859e-01,  1.6187e-01, -3.6133e-02, -2.9980e-01, -1.4990e-01,\n",
       "            2.2290e-01, -2.0459e-01, -3.1152e-01, -8.6975e-02, -2.3666e-02,\n",
       "           -3.5522e-02,  7.3303e-02, -1.2585e-01,  5.6396e-02, -4.5135e-02,\n",
       "           -2.5040e-02, -5.5603e-02,  4.9934e-03, -8.6731e-02,  6.8604e-02,\n",
       "           -2.5464e-01,  6.9092e-02, -1.5283e-01,  2.9199e-01,  4.6216e-01,\n",
       "            2.1411e-01,  6.6284e-02,  5.2582e-02, -3.7622e-01,  3.6084e-01,\n",
       "            3.9368e-02, -6.7749e-02, -4.5746e-02,  1.9128e-01, -1.4404e-01,\n",
       "           -7.5378e-02,  1.0541e-01,  1.8933e-01, -1.1438e-01,  1.0059e+00,\n",
       "            3.4332e-02, -2.7344e-01,  4.2389e-02,  3.1201e-01,  7.5195e-02,\n",
       "            3.4839e-01,  4.7333e-02, -1.7529e-01, -2.1777e-01, -3.5425e-01,\n",
       "            5.7487e-03, -4.6844e-02, -2.9614e-01,  1.7100e+00,  1.7456e-01,\n",
       "           -1.8555e-01,  2.0422e-01, -4.0253e-02,  2.7979e-01,  1.8396e-01,\n",
       "           -5.3406e-02,  1.9580e-01,  1.4954e-01,  4.6460e-01,  3.2178e-01,\n",
       "            2.7368e-01, -2.5024e-01, -3.0054e-01, -3.2422e-01, -1.0950e-01,\n",
       "           -2.1033e-01, -5.9326e-02]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.1899e-01, 7.8506e-03, 4.4584e-05, 7.5244e-01, 3.1424e-04, 3.9721e-04,\n",
       "           4.0340e-04, 3.9935e-05, 4.5955e-05, 1.9440e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     0.659164   11.818039  251.702515  455.123901    0.884547      7   \n",
       "  1   344.351807  236.273926  610.860474  404.923462    0.875749      2   \n",
       "  2   604.764038  236.562042  639.966431  305.744659    0.875737      0   \n",
       "  3   573.053284  280.513153  639.787537  435.117218    0.865127      2   \n",
       "  4   391.911194  222.867004  472.106262  403.838013    0.864149      0   \n",
       "  5   481.741852  169.574280  500.049103  208.399689    0.844567      9   \n",
       "  6   392.492462  314.194885  472.920013  441.097900    0.843186      1   \n",
       "  7   333.136902  182.018539  346.163025  215.471420    0.761097      9   \n",
       "  8   347.885223  178.875107  362.026215  212.402084    0.758556      9   \n",
       "  9   245.117371  233.044739  378.872864  380.864441    0.652533      7   \n",
       "  10  374.360168  132.247192  391.392334  176.078766    0.502653      9   \n",
       "  \n",
       "               name  \n",
       "  0           truck  \n",
       "  1             car  \n",
       "  2          person  \n",
       "  3             car  \n",
       "  4          person  \n",
       "  5   traffic light  \n",
       "  6         bicycle  \n",
       "  7   traffic light  \n",
       "  8   traffic light  \n",
       "  9           truck  \n",
       "  10  traffic light  ,\n",
       "  'caption': ['a truck parked behind an suv'],\n",
       "  'bbox_target': [246.58, 231.23, 133.57, 153.7]},\n",
       " 362: {'image_emb': tensor([[-0.3279,  0.4116, -0.1489,  ...,  0.6963, -0.0792, -0.2883],\n",
       "          [-0.1501,  0.3735, -0.0160,  ...,  0.9204,  0.1348,  0.1592],\n",
       "          [-0.2976,  0.2397, -0.0706,  ...,  0.8496,  0.3853, -0.0818],\n",
       "          ...,\n",
       "          [ 0.0162,  0.4229, -0.0706,  ...,  1.0996,  0.0680, -0.2556],\n",
       "          [ 0.2386,  0.1809, -0.1343,  ...,  1.1992,  0.1100, -0.1226],\n",
       "          [-0.2871,  0.0143, -0.1274,  ...,  1.0303,  0.2289, -0.0060]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.4243,  0.0045, -0.2463,  ...,  0.2100,  0.2610,  0.4136],\n",
       "          [ 0.0657, -0.2808, -0.3745,  ...,  0.2277,  0.0470,  0.1774]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.1829e-05, 6.5327e-05, 5.0812e-02, 1.1053e-03, 3.0398e-05, 1.4026e-01,\n",
       "           8.0466e-06, 8.0713e-01, 8.7023e-06, 2.6250e-04, 1.0443e-04],\n",
       "          [1.0097e-04, 1.0914e-04, 2.4704e-02, 1.2362e-04, 9.7119e-01, 4.1842e-04,\n",
       "           3.3379e-06, 2.9964e-03, 3.8922e-05, 2.0206e-05, 1.4460e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   116.790848  264.164490  220.043045  417.960388    0.931359     56   \n",
       "  1   336.012482  188.796844  433.116669  346.060577    0.879523      0   \n",
       "  2   506.851166  258.440948  639.503113  385.764435    0.867488     63   \n",
       "  3   334.207642  295.615326  414.317566  477.299377    0.866348     56   \n",
       "  4   460.647736  356.285248  531.891357  455.000305    0.865226     41   \n",
       "  5    42.641487  200.182709  107.946770  257.568329    0.845718     62   \n",
       "  6   467.165283  205.721802  504.370300  269.642334    0.795621     62   \n",
       "  7   503.133118  167.033737  639.671692  312.474670    0.767921     62   \n",
       "  8   550.363098  379.852325  623.638611  407.736420    0.738081     64   \n",
       "  9   515.530273  421.116302  591.437012  451.520935    0.707342     67   \n",
       "  10   54.915237  255.877380  115.751678  267.954102    0.697584     66   \n",
       "  11  289.464539  269.007172  350.069336  403.371246    0.406544     56   \n",
       "  12  329.947449  130.946136  370.201477  163.503113    0.297600     62   \n",
       "  13  538.943970  340.552734  636.485596  370.559692    0.271799     66   \n",
       "  \n",
       "            name  \n",
       "  0        chair  \n",
       "  1       person  \n",
       "  2       laptop  \n",
       "  3        chair  \n",
       "  4          cup  \n",
       "  5           tv  \n",
       "  6           tv  \n",
       "  7           tv  \n",
       "  8        mouse  \n",
       "  9   cell phone  \n",
       "  10    keyboard  \n",
       "  11       chair  \n",
       "  12          tv  \n",
       "  13    keyboard  ,\n",
       "  'caption': ['A computer monitor with two green lines on it.',\n",
       "   'the monitor closest to the dunkin donuts cup'],\n",
       "  'bbox_target': [495.84, 167.26, 142.51, 157.36]},\n",
       " 363: {'image_emb': tensor([[ 0.0986,  0.4529, -0.1448,  ...,  0.8496,  0.0746,  0.0790],\n",
       "          [ 0.3462,  0.7026,  0.1449,  ...,  0.9111,  0.0091,  0.2352],\n",
       "          [ 0.2603,  0.1958, -0.2566,  ...,  0.7769,  0.4197,  0.0728],\n",
       "          ...,\n",
       "          [-0.2352,  0.2947, -0.1736,  ...,  1.0586,  0.2234,  0.1281],\n",
       "          [ 0.0062, -0.0072, -0.1973,  ...,  0.6699, -0.0252, -0.0633],\n",
       "          [ 0.2008,  0.3635, -0.2876,  ...,  0.6704,  0.3628,  0.1888]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1478,  0.1504, -0.2244,  ..., -0.0541,  0.0939,  0.0226],\n",
       "          [-0.2190, -0.1926,  0.0079,  ...,  0.1626,  0.1622, -0.3813]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.0955e-02, 2.6294e-01, 5.3215e-04, 4.2700e-01, 1.8864e-03, 2.1038e-03,\n",
       "           3.2410e-02, 1.0689e-02, 2.2144e-01],\n",
       "          [1.8445e-01, 1.0345e-01, 5.0697e-03, 6.6406e-01, 2.8717e-02, 5.7793e-04,\n",
       "           1.8082e-03, 7.1487e-03, 4.7646e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   235.595840  290.783844  284.342285  374.387024    0.886931     41   \n",
       "  1   281.184631  200.898605  412.237030  336.587769    0.881687     63   \n",
       "  2   103.425003   51.370407  302.993317  320.932129    0.863219      0   \n",
       "  3   373.723602  240.705353  499.516113  371.770203    0.854522     63   \n",
       "  4   468.070496  157.314438  499.768555  243.378876    0.846750     63   \n",
       "  5    52.408947   63.829018  120.394989  205.892685    0.807136      0   \n",
       "  6   202.142410   63.520836  266.449799  153.106003    0.737600      0   \n",
       "  7   398.847748   68.552185  428.450195  109.899162    0.714170      0   \n",
       "  8   201.133270  117.660378  242.909073  167.930420    0.467117     56   \n",
       "  9   399.599945  108.157280  436.974945  194.034027    0.463685     56   \n",
       "  10  432.676117   68.358040  498.498169  157.870667    0.461295      0   \n",
       "  11   51.903526   75.120255   77.000732  122.783875    0.439420      0   \n",
       "  12  298.993042   69.600845  327.840973  107.613350    0.385693      0   \n",
       "  13    0.757867  115.110588   54.471344  157.111313    0.375475     60   \n",
       "  14    0.199243  106.280479   18.561277  126.200523    0.371877      0   \n",
       "  15    0.660500  161.179535   57.734253  199.612244    0.361960     60   \n",
       "  16  318.377502   73.125717  359.285919  114.203979    0.312659      0   \n",
       "  17  428.529297   65.783623  468.054047  112.690666    0.303755      0   \n",
       "  18  345.272003   81.866287  403.922363  172.429077    0.296496     57   \n",
       "  19  288.020020   79.086281  302.869812  125.390579    0.269579     39   \n",
       "  20  290.680847   77.919479  302.375885  100.952484    0.263042     67   \n",
       "  \n",
       "              name  \n",
       "  0            cup  \n",
       "  1         laptop  \n",
       "  2         person  \n",
       "  3         laptop  \n",
       "  4         laptop  \n",
       "  5         person  \n",
       "  6         person  \n",
       "  7         person  \n",
       "  8          chair  \n",
       "  9          chair  \n",
       "  10        person  \n",
       "  11        person  \n",
       "  12        person  \n",
       "  13  dining table  \n",
       "  14        person  \n",
       "  15  dining table  \n",
       "  16        person  \n",
       "  17        person  \n",
       "  18         couch  \n",
       "  19        bottle  \n",
       "  20    cell phone  ,\n",
       "  'caption': [\"The table the women's laptop is sitting on.\",\n",
       "   'Glass of milk on office desk next to a plate and clipboard.'],\n",
       "  'bbox_target': [138.2, 296.63, 268.82, 72.47]},\n",
       " 364: {'image_emb': tensor([[-0.5166,  0.1361, -0.2327,  ...,  0.7646,  0.1581,  0.1188],\n",
       "          [-0.4316, -0.1642, -0.0025,  ...,  0.3877,  0.2136, -0.0254],\n",
       "          [ 0.2120,  0.1608, -0.2379,  ...,  0.9248,  0.0443, -0.0918],\n",
       "          [ 0.3076, -0.3789, -0.4019,  ...,  0.9692, -0.0674, -0.0370],\n",
       "          [-0.7749,  0.2810, -0.0419,  ...,  0.1013,  0.3186,  0.1925]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0665,  0.1292, -0.3079,  ..., -0.8418, -0.1013, -0.2700],\n",
       "          [ 0.0030, -0.1118,  0.0518,  ...,  0.5854,  0.2031, -0.1688]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.9541e-01, 4.8790e-03, 2.1194e-02, 3.9215e-03, 1.7468e-01],\n",
       "          [9.4287e-01, 4.9194e-02, 3.9935e-06, 3.5763e-07, 7.9041e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  136.855072  164.345520  241.664978  405.368103    0.938875      0  person\n",
       "  1   83.188393  368.116699  280.278961  434.463257    0.886921     30    skis\n",
       "  2  102.237175  250.114166  116.526573  276.971161    0.823017      0  person\n",
       "  3  288.834991  206.813629  301.163483  237.077667    0.808171      0  person,\n",
       "  'caption': ['The woman posing for the picture.',\n",
       "   'A lady with a red parka on skis getting ready to go down the mountain'],\n",
       "  'bbox_target': [138.07, 163.63, 105.71, 248.09]},\n",
       " 365: {'image_emb': tensor([[ 0.2886, -0.0845, -0.4531,  ...,  0.9985,  0.1569,  0.1493],\n",
       "          [-0.3254,  0.1084, -0.3147,  ...,  0.7407, -0.0829,  0.0396],\n",
       "          [-0.1472,  0.2634, -0.4910,  ...,  1.0059, -0.0410,  0.1241],\n",
       "          [-0.3350,  0.0126, -0.2625,  ...,  1.0537,  0.0654,  0.0277],\n",
       "          [ 0.1222, -0.1440, -0.4849,  ...,  0.6060, -0.0704,  0.2500]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.4326,  0.0046, -0.0074,  ...,  0.1855,  0.0364, -0.3274],\n",
       "          [ 0.3689,  0.2277,  0.0833,  ...,  0.3059,  0.2340, -0.2097]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3875, 0.0117, 0.0030, 0.0428, 0.5552],\n",
       "          [0.1522, 0.0486, 0.0091, 0.2671, 0.5229]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0   50.464249  258.402649  392.176819  409.184082    0.914227      7     truck\n",
       "  1    6.660858   35.973663  518.178223  254.257721    0.853150      4  airplane\n",
       "  2    0.339184  103.399094   49.620991  131.944626    0.719316      4  airplane\n",
       "  3  233.649872  201.872864  408.308441  275.215637    0.716058      7     truck\n",
       "  4  113.488815  121.702744  180.688736  150.038757    0.340619      7     truck\n",
       "  5  113.574486  180.214478  169.109253  212.932068    0.339768      7     truck\n",
       "  6  618.438782  197.499878  637.382629  211.293762    0.261903      2       car\n",
       "  7  600.896484  198.899780  620.715088  216.679199    0.252919      2       car,\n",
       "  'caption': ['An airplane with 2 trucks lifting load.',\n",
       "   'An airplane having things loaded onto it from large containers'],\n",
       "  'bbox_target': [3.84, 39.81, 513.2, 222.21]},\n",
       " 366: {'image_emb': tensor([[-0.2457,  0.1681,  0.2737,  ...,  0.0355,  0.2891,  0.3572],\n",
       "          [ 0.1990,  0.4170, -0.1398,  ...,  0.4285, -0.1953, -0.0043],\n",
       "          [ 0.1187,  0.2761, -0.2932,  ...,  0.6704,  0.4141,  0.4636],\n",
       "          [ 0.0302, -0.1007, -0.2776,  ...,  0.7310, -0.1378,  0.0670],\n",
       "          [-0.1109,  0.2959,  0.0403,  ...,  0.3367, -0.0277,  0.3311]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1436, -0.0621, -0.3567,  ...,  0.0180, -0.1091,  0.2839],\n",
       "          [-0.2489, -0.1360, -0.2791,  ..., -0.0965, -0.1287,  0.0792]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.7852e-01, 6.3276e-04, 2.8610e-06, 7.7486e-04, 2.0309e-02],\n",
       "          [2.1301e-01, 7.5537e-01, 3.6659e-03, 1.3405e-02, 1.4725e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   321.162415  108.479599  590.729614  412.766541    0.897112      0   \n",
       "  1    37.359772   60.183517  276.152344  416.257874    0.853920      0   \n",
       "  2   129.617859  121.495605  195.395386  173.516571    0.797439     67   \n",
       "  3     0.011396  180.596832   27.231943  271.755463    0.787406      0   \n",
       "  4   531.702393  106.192825  639.329834  417.635681    0.615507      0   \n",
       "  5   319.307770  275.356720  556.361877  421.633484    0.534303     56   \n",
       "  6    15.982269  252.359863  305.954010  422.737183    0.466064     13   \n",
       "  7     0.752548  255.007690  160.071625  420.033813    0.435440     24   \n",
       "  8     5.688770  157.397980   84.062424  213.328278    0.415098      2   \n",
       "  9   319.511353  276.817200  553.134338  420.804688    0.398316     13   \n",
       "  10  377.177979  246.489960  385.199585  259.451447    0.388059      0   \n",
       "  11    0.401672  253.227478  160.099289  419.369812    0.365576     26   \n",
       "  \n",
       "            name  \n",
       "  0       person  \n",
       "  1       person  \n",
       "  2   cell phone  \n",
       "  3       person  \n",
       "  4       person  \n",
       "  5        chair  \n",
       "  6        bench  \n",
       "  7     backpack  \n",
       "  8          car  \n",
       "  9        bench  \n",
       "  10      person  \n",
       "  11     handbag  ,\n",
       "  'caption': ['The back of a chair next to a long haired man.',\n",
       "   'An empty chair in an office.'],\n",
       "  'bbox_target': [316.95, 273.05, 159.46, 150.37]},\n",
       " 367: {'image_emb': tensor([[ 0.1348,  0.0679, -0.0793,  ...,  0.9238,  0.1295,  0.3809],\n",
       "          [-0.0253,  0.0710, -0.1071,  ...,  0.7588,  0.1919,  0.2971],\n",
       "          [ 0.0062, -0.0200, -0.2979,  ...,  0.6821,  0.5332,  0.1743],\n",
       "          [ 0.0478, -0.0527,  0.0890,  ...,  0.3887, -0.0145, -0.0100]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 2.1399e-01, -8.8440e-02, -4.8608e-01, -1.9751e-01,  1.5833e-01,\n",
       "            7.8064e-02, -1.1115e-01, -3.4253e-01, -6.5735e-02, -1.4600e-01,\n",
       "            1.4954e-01,  1.5784e-01,  8.2214e-02, -1.4758e-01,  1.2262e-01,\n",
       "            9.8999e-02, -4.7437e-01, -1.3135e-01, -4.5581e-01,  4.9316e-01,\n",
       "            6.3721e-02, -1.3330e-01,  9.1934e-03, -2.3914e-01,  2.0264e-01,\n",
       "            1.5088e-01, -3.4961e-01,  2.1179e-01, -2.2815e-01, -1.6785e-01,\n",
       "            1.4941e-01,  1.9470e-01, -3.0981e-01, -9.8633e-02, -3.7964e-01,\n",
       "           -2.0642e-01,  3.5303e-01,  3.9526e-01, -2.7771e-03, -4.4434e-02,\n",
       "           -7.0435e-02,  6.5247e-02, -2.6382e-02, -3.4009e-01,  2.8613e-01,\n",
       "            1.3879e-01, -1.3196e-01, -2.3926e-02,  1.0406e-02, -1.0040e-02,\n",
       "           -3.1812e-01, -2.3730e-01,  2.4243e-01, -4.3408e-01, -6.3281e-01,\n",
       "            1.8933e-01, -4.6387e-02,  1.1084e-01,  1.0992e-01,  3.5645e-01,\n",
       "           -2.9492e-01, -2.2571e-01, -2.7100e-01, -8.4961e-02,  9.8267e-02,\n",
       "           -1.7432e-01,  3.7329e-01,  1.8726e-01,  4.5258e-02, -1.9214e-01,\n",
       "           -3.6938e-01, -8.1238e-02,  3.1982e-01,  3.1396e-01,  3.7988e-01,\n",
       "            1.6614e-01,  3.5327e-01, -4.9500e-02,  7.9041e-03, -9.8572e-02,\n",
       "           -3.1738e-01,  1.0083e-01,  1.2541e-04, -3.2715e-02,  4.7180e-02,\n",
       "            4.3799e-01,  7.0984e-02,  2.7496e-02, -4.6265e-01,  2.7847e-02,\n",
       "           -1.2219e-01, -3.6133e-01, -9.5654e-01, -1.2927e-01,  2.4707e-01,\n",
       "            4.2511e-02,  3.4888e-01,  1.8384e-01, -2.5806e-01, -1.1707e-01,\n",
       "            5.9319e-03,  1.4270e-01, -3.2886e-01, -3.3203e-01, -2.4658e-01,\n",
       "            1.0266e-01, -5.9509e-02,  3.6719e-01,  1.8506e-01, -3.4448e-01,\n",
       "           -5.2539e-01,  9.7961e-02, -1.8347e-01,  3.0029e-01, -3.3716e-01,\n",
       "            1.9348e-01,  3.6792e-01,  3.9038e-01, -1.5051e-01,  1.8250e-01,\n",
       "            1.6199e-01, -8.6621e-01, -3.5449e-01,  2.0557e-01, -2.9190e-02,\n",
       "           -1.8030e-01,  5.8350e-01, -3.1348e-01,  1.4172e-01,  5.8496e-01,\n",
       "            5.8838e-01, -2.7734e-01, -1.0620e-01,  2.1504e+00, -5.5298e-02,\n",
       "            2.4060e-01, -2.8247e-01, -2.5366e-01, -2.1313e-01,  5.9113e-02,\n",
       "            7.3242e-03, -5.3978e-03,  2.1805e-02,  4.5312e-01,  2.5439e-01,\n",
       "           -9.4543e-02, -3.2532e-02, -4.4360e-01, -1.3916e-01, -9.6497e-02,\n",
       "           -4.2871e-01,  2.7161e-02,  3.1376e-03,  3.4351e-01,  2.7756e-02,\n",
       "           -3.2178e-01, -3.0542e-01, -4.3555e-01,  8.5571e-02,  6.3721e-01,\n",
       "            1.5088e-01, -2.9449e-03, -1.4417e-01, -4.4525e-02, -1.8774e-01,\n",
       "            3.2080e-01,  2.1606e-02, -4.9927e-01,  7.1960e-02, -5.8057e-01,\n",
       "           -1.2781e-01, -2.7026e-01,  3.0615e-01,  2.9712e-01, -3.2227e-01,\n",
       "            6.3599e-02,  8.3252e-02, -1.2962e-02, -3.5181e-01, -4.6484e-01,\n",
       "           -4.1748e-02,  7.3395e-03, -2.0056e-01, -1.3037e-01, -2.4170e-01,\n",
       "           -2.1997e-01, -1.4595e-02, -3.4668e-01,  1.3745e-01,  1.3867e-01,\n",
       "            2.8052e-01,  4.1724e-01,  1.8689e-01, -3.1348e-01, -8.2214e-02,\n",
       "            5.9319e-03, -7.8964e-03,  5.0049e-01, -1.3953e-01, -2.8003e-01,\n",
       "            1.1115e-01,  3.1274e-01, -4.7607e-02,  2.2229e-01, -1.5771e-01,\n",
       "            1.4355e-01,  1.5747e-01,  1.3147e-01, -2.6611e-01, -3.5132e-01,\n",
       "            4.1211e-01,  6.4941e-01, -2.2119e-01,  1.2769e-01,  1.0785e-01,\n",
       "           -1.7700e-01,  1.1206e-01,  2.5732e-01,  8.6963e-01,  1.3159e-01,\n",
       "            3.4790e-01, -8.9417e-02, -4.7754e-01, -2.8247e-01, -2.0337e-01,\n",
       "            1.5588e-01, -5.9753e-02, -5.2734e-01, -5.0830e-01,  1.1517e-01,\n",
       "           -9.1162e-01,  4.1455e-01, -3.0737e-01, -5.4443e-02, -2.9327e-02,\n",
       "            1.4465e-01,  4.5380e-02,  8.6426e-02, -8.7585e-02,  5.2094e-02,\n",
       "            1.3440e-01,  1.9653e-01,  6.7432e-01,  1.3098e-01,  2.5391e-02,\n",
       "            1.3623e-01,  8.1238e-02,  3.3936e-01,  5.2399e-02, -1.4319e-01,\n",
       "           -3.3472e-01, -3.8892e-01,  4.2456e-01, -2.6131e-03, -2.3169e-01,\n",
       "            6.6406e-02,  3.6597e-01,  1.5320e-01, -2.7142e-03, -7.3547e-02,\n",
       "            8.6548e-02,  3.9429e-01,  2.0361e-01,  1.5979e-01, -1.2292e-01,\n",
       "            7.4829e-02, -2.5415e-01, -6.9678e-01, -1.6248e-01, -1.6129e-02,\n",
       "            5.1636e-02,  1.0657e-01,  3.7402e-01, -3.3105e-01,  1.9592e-02,\n",
       "            1.3953e-01, -1.0461e-01,  4.4678e-01,  1.0541e-01, -2.8516e-01,\n",
       "           -1.7004e-01, -1.2024e-01,  1.8478e-02,  6.0944e-02,  3.1299e-01,\n",
       "            3.7994e-02, -3.7256e-01, -3.8135e-01,  4.2139e-01, -4.3854e-02,\n",
       "           -1.1346e-01, -9.3079e-02, -2.1338e-01, -1.5222e-01, -2.3108e-01,\n",
       "            9.4604e-02,  3.1641e-01, -3.5913e-01, -5.1660e-01,  1.7004e-01,\n",
       "            2.4429e-02, -5.6787e-01, -2.6709e-01,  5.9845e-02,  1.7932e-01,\n",
       "            4.8706e-02,  1.5088e-01, -4.9780e-01,  8.9722e-02,  2.0142e-02,\n",
       "            1.1401e-01,  2.9297e-01,  2.1484e+00,  1.9287e-01, -5.7861e-01,\n",
       "            5.1953e-01, -2.5635e-02,  1.8982e-02,  1.9983e-01,  2.8027e-01,\n",
       "           -5.1953e-01,  1.6736e-01,  3.0365e-02, -2.5171e-01, -4.1968e-01,\n",
       "            9.9792e-03,  2.7808e-01, -3.4497e-01, -3.4668e-02, -9.9072e-01,\n",
       "            1.7529e-01,  3.5425e-01,  6.1816e-01, -4.3921e-01, -1.2146e-02,\n",
       "            2.3145e-01,  2.2058e-01,  5.2490e-01,  2.6953e-01,  2.8223e-01,\n",
       "           -5.9961e-01, -4.3726e-01,  4.1821e-01,  3.7956e-03, -3.5126e-02,\n",
       "           -9.8816e-02,  2.7252e-02,  1.5735e-01,  2.2980e-02, -1.7624e-02,\n",
       "            6.8909e-02, -1.1597e-01,  3.3252e-01,  2.3462e-01, -7.1899e-02,\n",
       "           -1.7319e-02,  8.6060e-03,  1.0657e-01, -1.0406e-01, -9.9365e-02,\n",
       "            2.5787e-02, -3.5547e-01,  3.9795e-01, -2.3376e-01, -5.2783e-01,\n",
       "            3.8745e-01, -1.9177e-01,  8.7219e-02,  1.8478e-02, -1.1475e-01,\n",
       "           -1.5518e-02, -1.8079e-01, -8.5876e-02, -2.3743e-01, -3.5083e-01,\n",
       "            1.3208e-01,  3.8037e-01,  1.9470e-02,  1.7822e-02, -3.0167e-02,\n",
       "           -2.4670e-01, -8.0322e-02,  3.5645e-01,  5.8044e-02,  2.6489e-01,\n",
       "           -1.9910e-01,  8.4167e-02,  3.5229e-01,  8.1787e-02,  2.7100e-02,\n",
       "           -6.1133e-01,  3.2715e-02, -8.0200e-02, -1.5833e-01,  1.6769e-02,\n",
       "            4.3481e-01,  3.4204e-01, -4.7302e-02,  2.3178e-02,  8.7662e-03,\n",
       "            8.0322e-02, -1.7554e-01, -3.0823e-03, -2.7313e-02, -2.1765e-01,\n",
       "           -1.9165e-01, -1.8661e-02, -7.4585e-02, -6.1768e-01, -2.6025e-01,\n",
       "           -4.0802e-02,  2.7881e-01, -2.8564e-01,  1.4148e-01, -4.1107e-02,\n",
       "           -4.5532e-01, -4.1528e-01, -2.3303e-01,  2.1509e-01, -3.1030e-01,\n",
       "           -3.4027e-02,  6.0883e-02,  3.2739e-01, -1.2732e-01,  1.3977e-01,\n",
       "           -2.0081e-01,  9.9487e-02,  8.0261e-02,  1.2311e-01,  1.4954e-01,\n",
       "            4.1473e-02, -2.2278e-01,  2.0117e-01, -1.6895e-01, -4.4238e-01,\n",
       "            2.6025e-01, -5.4150e-01,  1.1707e-01, -3.7695e-01,  1.6980e-01,\n",
       "           -1.0065e-01,  2.1460e-01, -5.7404e-02, -3.5010e-01,  1.0669e-01,\n",
       "           -3.0005e-01, -3.3472e-01, -2.6779e-02,  4.7272e-02,  5.1025e-01,\n",
       "           -1.4824e-02,  1.2561e-01,  1.9543e-01, -1.0391e-02,  1.9189e-01,\n",
       "           -1.4502e-01,  6.7773e-01,  3.5327e-01, -1.6516e-01,  2.3828e-01,\n",
       "            2.6440e-01,  5.5811e-01,  3.0127e-01,  4.5868e-02,  3.4570e-01,\n",
       "           -2.3584e-01,  2.2668e-01, -3.6426e-01,  1.2463e-01,  2.2876e-01,\n",
       "            4.6924e-01, -5.2246e-01, -1.9653e-01, -5.1117e-02,  3.8929e-03,\n",
       "           -1.5295e-01, -3.3203e-02, -2.5098e-01,  1.2781e-01,  3.4253e-01,\n",
       "            2.9102e-01, -2.1082e-01,  5.2295e-01,  2.3560e-01, -1.5222e-01,\n",
       "           -1.1444e-01,  8.8654e-03,  3.7012e-01, -3.1934e-01, -6.5088e-01,\n",
       "           -5.8868e-02, -1.6980e-01, -1.5027e-01,  8.4277e-01, -1.2659e-01,\n",
       "           -1.7957e-01,  1.6382e-01, -4.9365e-01,  2.5063e-03,  2.7612e-01,\n",
       "            1.2952e-01, -2.1143e-01,  7.7820e-02, -1.4526e-01,  5.6250e-01,\n",
       "           -5.7275e-01, -4.0259e-01, -7.8308e-02, -2.7191e-02,  2.0911e-01,\n",
       "           -3.4009e-01,  9.7656e-02]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.2939e-02, 1.6016e-01, 8.2617e-01, 7.7724e-04]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   403.528442  188.130615  487.908691  304.737915    0.895454      0   \n",
       "  1   484.559753  186.728027  581.939880  336.264099    0.880058      0   \n",
       "  2   377.510986  200.715668  639.321350  391.558136    0.853360     57   \n",
       "  3   287.979156   96.268539  298.780853  116.562057    0.660569     75   \n",
       "  4   528.616150  218.868958  620.589417  305.481140    0.650830      0   \n",
       "  5     0.626610  341.457001  142.228943  424.267334    0.498958     57   \n",
       "  6    14.409107  212.945374   29.413280  256.231445    0.409656     73   \n",
       "  7    14.162631  265.539886   42.910320  307.944733    0.360340     73   \n",
       "  8   316.992004  299.683167  390.984558  322.782166    0.345522     73   \n",
       "  9   335.052887  143.550812  429.788361  179.553375    0.339802     73   \n",
       "  10  576.607971  295.274872  639.729553  315.249664    0.320508     73   \n",
       "  11    0.000000  267.732849   27.252777  310.789429    0.286421     73   \n",
       "  12   27.864540  209.902191   41.903122  254.221954    0.285132     73   \n",
       "  13    0.337585  167.470154   17.985485  203.670654    0.274290     73   \n",
       "  14  280.955688   64.916733  302.905151  116.643112    0.271754     58   \n",
       "  15    2.229956  160.608307   53.428513  204.566437    0.268154     73   \n",
       "  16  576.595520  293.487457  639.528381  315.257904    0.264114     63   \n",
       "  17  330.559143  181.449463  425.891846  220.248230    0.262989     73   \n",
       "  18  344.208862  297.035278  389.898926  315.945923    0.253723     73   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2          couch  \n",
       "  3           vase  \n",
       "  4         person  \n",
       "  5          couch  \n",
       "  6           book  \n",
       "  7           book  \n",
       "  8           book  \n",
       "  9           book  \n",
       "  10          book  \n",
       "  11          book  \n",
       "  12          book  \n",
       "  13          book  \n",
       "  14  potted plant  \n",
       "  15          book  \n",
       "  16        laptop  \n",
       "  17          book  \n",
       "  18          book  ,\n",
       "  'caption': ['A greenish couch being sat on by a woman and two children.'],\n",
       "  'bbox_target': [381.55, 218.78, 258.45, 174.4]},\n",
       " 368: {'image_emb': tensor([[-0.1490, -0.2725,  0.1483,  ...,  0.8486,  0.2449,  0.0305],\n",
       "          [-0.1130, -0.0530,  0.0930,  ...,  1.0479,  0.3708,  0.0313],\n",
       "          [-0.2747, -0.2260,  0.0028,  ...,  0.5737, -0.0477,  0.2983]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0731,  0.1642, -0.1567,  ..., -0.1813,  0.1530,  0.0441],\n",
       "          [-0.0590,  0.0168, -0.3132,  ...,  0.0266,  0.0946, -0.1501]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6797, 0.2207, 0.0995],\n",
       "          [0.5576, 0.0472, 0.3953]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin       xmax        ymax  confidence  class      name\n",
       "  0   60.665169  184.616119  311.22345  383.308441    0.946046     20  elephant\n",
       "  1  490.232391  282.954346  612.16333  365.325317    0.922353     20  elephant,\n",
       "  'caption': ['the big elephant', 'Big elephant, on the left.'],\n",
       "  'bbox_target': [45.78, 185.74, 257.02, 199.78]},\n",
       " 369: {'image_emb': tensor([[-0.1660,  0.3979,  0.3215,  ...,  0.4075, -0.0759,  0.2457],\n",
       "          [-0.2395,  0.0513,  0.5718,  ...,  0.5635,  0.1334,  0.1145],\n",
       "          [-0.4238,  0.3618,  0.1963,  ...,  0.0991, -0.0937,  0.1570],\n",
       "          ...,\n",
       "          [-0.0692,  0.1449, -0.2148,  ...,  1.5381, -0.0371,  0.1006],\n",
       "          [-0.3157,  0.2927,  0.1223,  ...,  0.3267,  0.1565,  0.2573],\n",
       "          [-0.1294,  0.2913,  0.2666,  ...,  0.3821, -0.0498,  0.2317]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0622, -0.1626,  0.2198,  ...,  0.3074, -0.0058, -0.2708],\n",
       "          [ 0.0400, -0.2317, -0.0510,  ...,  0.2847,  0.0776, -0.5049]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2520, 0.0466, 0.2769, 0.0005, 0.0082, 0.1349, 0.2810],\n",
       "          [0.1547, 0.0269, 0.1499, 0.0008, 0.0034, 0.3711, 0.2935]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  257.347046    0.000000  638.106506  385.958923    0.947747      0   \n",
       "  1   83.681763    0.000000  283.774414  175.210724    0.938436      0   \n",
       "  2  224.619202  164.461288  639.385498  446.036011    0.927315     36   \n",
       "  3    0.000326  119.124191   32.475525  159.339981    0.810819     36   \n",
       "  4    0.193474    0.311821  117.975136  121.605011    0.785308      0   \n",
       "  5   42.487106   99.919586  301.659302  212.658295    0.755508     36   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2  skateboard  \n",
       "  3  skateboard  \n",
       "  4      person  \n",
       "  5  skateboard  ,\n",
       "  'caption': ['A wide stance on a skateboard.', 'Skateboard on the right.'],\n",
       "  'bbox_target': [218.97, 165.78, 421.03, 282.61]},\n",
       " 370: {'image_emb': tensor([[-0.4814,  0.4285,  0.1143,  ...,  1.0713,  0.3254,  0.2598],\n",
       "          [-0.1545,  0.5249,  0.0477,  ...,  1.1855,  0.1392,  0.2032],\n",
       "          [-0.3193,  0.5889, -0.0630,  ...,  0.8711, -0.2089, -0.1072],\n",
       "          ...,\n",
       "          [-0.0829, -0.0580, -0.4839,  ...,  0.9302, -0.0304, -0.2993],\n",
       "          [ 0.0594, -0.3633, -0.5171,  ...,  1.0586, -0.2120, -0.1515],\n",
       "          [-0.4763,  0.3994,  0.3833,  ...,  0.2771,  0.1572,  0.1951]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1279,  0.3630, -0.1721,  ..., -0.1293, -0.2332, -0.5688],\n",
       "          [-0.0915,  0.0533, -0.1554,  ..., -0.1261, -0.2463, -0.4109]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.6641e-04, 4.4250e-04, 9.9561e-01, 1.2517e-06, 3.5763e-07, 7.1526e-07,\n",
       "           3.1681e-03],\n",
       "          [6.2466e-04, 1.8835e-05, 9.9658e-01, 4.7684e-07, 1.1921e-07, 1.5497e-06,\n",
       "           2.9793e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   356.479584  118.417618  465.818787  215.935776    0.936540      0   \n",
       "  1   203.599075  111.086891  301.479401  193.461746    0.932694      0   \n",
       "  2     0.373006  185.567947  217.704147  277.210052    0.917512      0   \n",
       "  3   320.106873  210.353851  499.867340  244.973083    0.898189     31   \n",
       "  4   159.931961  191.349915  355.393616  219.420319    0.888839     31   \n",
       "  5   327.068085   76.257637  332.419067   86.127998    0.713129      0   \n",
       "  6   117.004570  298.676605  187.654251  353.174164    0.696410     31   \n",
       "  7   295.941650   74.919907  301.634155   82.929436    0.639846      0   \n",
       "  8   187.289642   70.284737  193.254517   78.945099    0.628825      0   \n",
       "  9   222.762756   72.412476  227.844833   81.161156    0.438235      0   \n",
       "  10  115.785507  196.000076  491.916565  363.974823    0.392242     31   \n",
       "  11  158.259323   67.240021  163.065155   73.811386    0.259907      0   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2      person  \n",
       "  3   snowboard  \n",
       "  4   snowboard  \n",
       "  5      person  \n",
       "  6   snowboard  \n",
       "  7      person  \n",
       "  8      person  \n",
       "  9      person  \n",
       "  10  snowboard  \n",
       "  11     person  ,\n",
       "  'caption': ['A WOMAN LAYING ON SNOW',\n",
       "   'The person wearing red dress and take a rest in snow side'],\n",
       "  'bbox_target': [0.45, 186.07, 216.52, 89.52]},\n",
       " 371: {'image_emb': tensor([[ 0.1536,  0.1562, -0.1609,  ...,  0.6060, -0.0124, -0.3130],\n",
       "          [ 0.2939, -0.0836, -0.0461,  ...,  0.9600,  0.0523, -0.0923],\n",
       "          [-0.1989,  0.2578, -0.2389,  ...,  0.6504, -0.3394, -0.4128]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-5.2795e-02,  4.9951e-01, -1.0522e-01,  2.8394e-01, -6.8176e-02,\n",
       "           -8.5022e-02, -4.8315e-01, -4.5654e-01,  2.4536e-01, -4.1962e-03,\n",
       "            5.2148e-01, -1.1438e-01, -3.5181e-01,  1.2451e-01,  3.0249e-01,\n",
       "            2.8345e-01, -1.9543e-01, -8.6548e-02,  1.1337e-02,  2.1826e-01,\n",
       "           -1.2006e-01,  2.8394e-01,  3.4277e-01, -5.1514e-01, -2.2559e-01,\n",
       "            5.2214e-04,  2.6465e-01,  3.8330e-02, -1.1554e-01,  7.8613e-02,\n",
       "            2.2571e-01,  9.6207e-03, -4.2334e-01, -1.9690e-01,  2.7808e-01,\n",
       "           -3.7598e-01,  2.3535e-01,  3.6426e-01,  2.0801e-01, -1.5613e-01,\n",
       "           -2.1460e-01,  1.5894e-01,  6.3293e-02,  7.9041e-02,  1.1572e-01,\n",
       "           -2.3608e-01, -3.1226e-01,  2.4329e-01,  1.7236e-01,  4.7112e-03,\n",
       "           -2.8784e-01, -5.2002e-01, -5.4932e-01,  2.7710e-01, -4.2920e-01,\n",
       "            3.9355e-01, -1.5808e-01, -2.9541e-01, -8.4900e-02,  3.4302e-02,\n",
       "           -1.6724e-01, -4.8462e-02,  1.7346e-01,  1.2262e-01, -1.1774e-01,\n",
       "           -2.4609e-01, -2.3047e-01,  9.6741e-02, -1.6101e-01, -1.6333e-01,\n",
       "            2.3840e-01,  1.8784e-02,  1.2347e-01,  1.5710e-01, -1.8677e-01,\n",
       "            3.3423e-01,  5.5615e-01,  4.8920e-02, -3.1836e-01,  4.9683e-01,\n",
       "            7.8064e-02,  2.6025e-01,  1.0669e-01,  2.4036e-01, -2.5781e-01,\n",
       "           -5.5542e-02, -2.0251e-01, -1.2671e-01,  1.4124e-01,  5.1575e-02,\n",
       "            1.3586e-01,  2.9248e-01, -8.9062e-01,  2.8613e-01, -4.8242e-01,\n",
       "            2.0251e-01,  5.8990e-02, -2.0227e-01, -4.0918e-01,  4.0918e-01,\n",
       "            4.0576e-01,  3.5962e-01,  6.5857e-02, -2.3462e-01, -4.1406e-01,\n",
       "            5.4932e-01, -4.9622e-02, -2.0920e-02,  1.1823e-01, -1.8823e-01,\n",
       "           -6.9922e-01,  1.2488e-01, -5.4260e-02,  2.9419e-01, -1.5930e-01,\n",
       "           -1.1389e-01,  2.3291e-01,  3.5889e-01, -1.9788e-01, -1.3660e-01,\n",
       "           -5.0659e-02, -6.1523e-01, -1.6968e-01, -5.2002e-01, -1.5063e-01,\n",
       "           -9.5886e-02,  6.5283e-01,  1.7065e-01, -1.1285e-01,  4.7827e-01,\n",
       "            6.4990e-01,  1.1945e-01, -1.2299e-01,  4.0352e+00,  2.3901e-01,\n",
       "            2.4109e-01, -4.5752e-01, -4.6191e-01,  2.1484e-01,  9.9854e-02,\n",
       "           -4.1595e-02,  6.8066e-01,  1.6968e-01, -2.1387e-01,  2.1667e-03,\n",
       "            9.0210e-02, -4.8706e-01, -3.1299e-01,  1.7029e-01, -2.5903e-01,\n",
       "            1.0291e-01, -5.1788e-02,  2.3730e-01,  1.7590e-01,  5.2393e-01,\n",
       "            2.1118e-01,  8.8074e-02,  9.5947e-02,  3.2446e-01,  1.3977e-01,\n",
       "           -1.6333e-01, -2.2998e-01, -5.2295e-01, -4.5441e-02,  5.0098e-01,\n",
       "            1.4168e-02,  4.6295e-02,  4.1699e-01, -2.0435e-01, -5.9700e-03,\n",
       "            6.5796e-02,  1.5320e-01,  1.2006e-01,  6.8970e-02, -4.2023e-02,\n",
       "            1.7776e-02, -4.6704e-01,  2.4719e-01,  6.4209e-02,  1.3928e-01,\n",
       "            6.0486e-02, -3.7280e-01,  2.0618e-01, -1.2433e-01, -1.2634e-01,\n",
       "           -1.1871e-01,  3.6719e-01, -1.0419e-01, -3.2202e-01, -1.2756e-01,\n",
       "           -4.4067e-02, -1.9641e-01,  4.5703e-01,  1.4087e-01, -1.5442e-01,\n",
       "           -1.3745e-01, -1.4465e-01,  1.0199e-01,  5.3174e-01, -5.4492e-01,\n",
       "            9.3933e-02, -2.2754e-01,  2.4707e-01, -6.7993e-02, -2.1378e-02,\n",
       "           -2.0557e-01, -3.8452e-02, -1.8481e-01, -7.2607e-01, -3.0322e-01,\n",
       "            1.9397e-01,  5.7007e-02,  7.7942e-02, -5.0195e-01,  5.0000e-01,\n",
       "            5.1910e-02, -5.3027e-01,  6.2988e-02,  3.4644e-01,  1.2238e-01,\n",
       "            3.3887e-01,  1.5210e-01, -4.1577e-01,  6.3965e-02, -1.3806e-01,\n",
       "           -2.5659e-01, -2.5146e-01, -2.9590e-01,  1.3611e-01, -4.7876e-01,\n",
       "           -3.5986e-01, -8.6975e-02,  1.7615e-01, -1.0138e-01, -1.2854e-01,\n",
       "            4.3555e-01, -2.6978e-01,  1.2427e-01,  3.4912e-02,  2.3155e-03,\n",
       "           -2.0874e-01,  1.7014e-03,  4.9487e-01, -2.2351e-01,  9.3231e-03,\n",
       "            1.1713e-01, -5.4626e-03, -1.7627e-01, -9.5032e-02,  1.8778e-03,\n",
       "           -3.1348e-01, -2.3474e-01, -1.1047e-01,  3.5327e-01, -2.5879e-01,\n",
       "           -1.1212e-01, -2.3230e-01,  1.9629e-01,  9.8572e-02,  2.7954e-02,\n",
       "            5.5225e-01,  1.4246e-01,  1.2573e-01,  2.6703e-02,  6.7368e-03,\n",
       "            4.2200e-05,  4.9756e-01, -1.5991e-01, -2.5439e-01,  3.0396e-01,\n",
       "            4.8242e-01, -9.7961e-03, -3.0713e-01,  4.8004e-02,  7.2510e-02,\n",
       "            2.6978e-01,  1.2781e-01, -4.4873e-01, -5.0262e-02,  3.2373e-01,\n",
       "            5.0964e-02,  3.2104e-01,  3.4985e-01,  2.4756e-01,  2.4902e-01,\n",
       "           -2.1106e-01,  3.8055e-02, -3.1787e-01,  1.3672e-01, -2.0581e-01,\n",
       "           -1.0645e-01,  2.6611e-01,  4.8438e-01, -3.0371e-01, -6.9397e-02,\n",
       "           -9.2773e-02,  1.9348e-02,  9.5032e-02, -1.6504e-01, -1.9360e-01,\n",
       "           -2.2754e-01,  2.3950e-01, -1.9678e-01,  1.1505e-01, -2.6978e-01,\n",
       "           -2.4426e-01, -2.2583e-01,  1.0864e-01,  2.4048e-01, -3.8989e-01,\n",
       "            3.3276e-01, -3.6255e-02,  4.0312e+00,  2.2546e-01,  7.7576e-02,\n",
       "            1.9678e-01,  2.5317e-01,  3.2886e-01,  2.7759e-01,  1.6077e-01,\n",
       "            1.6101e-01,  3.5229e-01,  2.0215e-01, -2.9858e-01, -4.2664e-02,\n",
       "           -5.6396e-02, -2.9370e-01, -2.3834e-02, -4.1748e-01, -7.9980e-01,\n",
       "            1.2659e-01, -1.4673e-01, -1.1002e-02, -1.3208e-01,  2.1777e-01,\n",
       "           -4.9805e-01,  2.8247e-01,  3.6206e-01, -2.4072e-01,  4.4385e-01,\n",
       "           -5.5811e-01,  2.6514e-01,  6.1737e-02, -6.0596e-01,  9.0637e-02,\n",
       "           -1.6418e-01, -4.3091e-01, -4.1107e-02, -1.2439e-01, -2.3621e-02,\n",
       "            3.8770e-01,  1.3074e-01,  4.6631e-01,  2.8003e-01, -8.7585e-02,\n",
       "           -3.5278e-01, -3.8525e-01, -1.5613e-01, -3.8159e-01,  7.9773e-02,\n",
       "           -3.7695e-01, -3.0664e-01,  5.1904e-01, -2.9517e-01, -1.0498e-01,\n",
       "            5.0928e-01, -4.8071e-01,  4.6704e-01, -1.6321e-01,  2.1802e-01,\n",
       "           -7.0129e-02,  1.4746e-01,  9.8633e-02,  4.0894e-01, -8.2947e-02,\n",
       "            3.5645e-01, -5.7275e-01, -7.9102e-02, -4.9774e-02,  3.2959e-02,\n",
       "           -3.5962e-01, -7.0312e-01,  5.3613e-01, -3.2446e-01, -1.9116e-01,\n",
       "            3.1738e-02,  1.1584e-01,  4.6997e-02, -6.8481e-02, -2.6855e-01,\n",
       "           -7.8955e-01,  2.1484e-01, -2.5830e-01, -2.5830e-01, -7.3608e-02,\n",
       "            6.7566e-02,  2.2620e-01,  3.2788e-01,  4.5776e-02,  5.6738e-01,\n",
       "           -1.5503e-01,  2.3889e-01, -1.1848e-02, -8.5571e-02, -3.9154e-02,\n",
       "           -2.6657e-02, -5.0098e-01,  1.9531e-03,  2.8198e-01, -2.6978e-01,\n",
       "            1.1395e-01, -1.1542e-01, -7.8247e-02, -4.6802e-01,  3.4644e-01,\n",
       "            1.8665e-01,  5.2681e-03, -4.5825e-01, -1.3611e-01, -5.3027e-01,\n",
       "            6.2256e-03,  1.0779e-01,  1.0529e-01, -5.5957e-01, -2.5537e-01,\n",
       "            1.6956e-01, -5.0873e-02, -5.1611e-01,  5.5237e-02,  4.6420e-04,\n",
       "           -4.0436e-02, -7.8918e-02,  2.5195e-01, -6.3300e-05,  1.4172e-01,\n",
       "            3.9673e-01, -3.3716e-01, -4.5624e-02,  1.3977e-01,  3.3130e-01,\n",
       "            1.8445e-01, -2.9248e-01, -7.0618e-02, -5.2691e-04, -4.2432e-01,\n",
       "           -1.1707e-01, -6.6162e-02,  1.3647e-01, -1.5833e-01,  1.2878e-01,\n",
       "           -2.7368e-01, -1.6516e-01, -3.6279e-01,  2.7881e-01, -2.8030e-02,\n",
       "           -1.1078e-01,  4.7119e-02, -2.8857e-01, -1.6006e-02,  1.8909e-01,\n",
       "           -1.1041e-01,  2.9053e-01, -4.4342e-02,  4.1333e-01, -1.0468e-01,\n",
       "           -3.6060e-01,  1.1487e-01, -4.6069e-01, -1.6113e-01, -1.0272e-01,\n",
       "            1.1542e-01,  6.6650e-02, -9.8022e-02,  2.8735e-01,  1.1182e-01,\n",
       "           -7.5500e-02, -2.4939e-01,  4.8676e-02,  1.3550e-01,  1.7834e-01,\n",
       "            1.8768e-02,  3.2623e-02,  1.7908e-01, -1.6138e-01, -5.1849e-02,\n",
       "           -1.1743e-01,  3.3936e-01,  3.7384e-04,  4.4823e-03,  8.0688e-02,\n",
       "            8.0261e-03, -3.6938e-01, -1.0040e-01,  9.7119e-01,  1.7773e-01,\n",
       "            1.9440e-02, -1.1238e-02,  3.1525e-02,  1.6638e-01, -2.2778e-01,\n",
       "            1.3416e-01, -4.5728e-01,  2.2461e-01, -2.4683e-01,  2.1692e-01,\n",
       "           -2.8735e-01,  1.4575e-01,  6.9458e-02, -4.9774e-02,  4.5605e-01,\n",
       "           -2.8711e-01, -1.3403e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.3335e-02, 8.7166e-04, 9.5557e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  189.794708  204.795929  315.016022  340.723175    0.870112     50   \n",
       "  1  304.953430  243.388519  427.323669  357.376068    0.834332     50   \n",
       "  2  378.052612  117.582977  634.399414  353.778839    0.607600     50   \n",
       "  3  339.468109  140.180450  413.752899  228.567596    0.336693     50   \n",
       "  4    1.762238    0.725327  637.997559  117.018082    0.255246     60   \n",
       "  \n",
       "             name  \n",
       "  0      broccoli  \n",
       "  1      broccoli  \n",
       "  2      broccoli  \n",
       "  3      broccoli  \n",
       "  4  dining table  ,\n",
       "  'caption': ['A bunch of kale grouped with other vegetables.'],\n",
       "  'bbox_target': [328.73, 124.51, 163.74, 106.38]},\n",
       " 372: {'image_emb': tensor([[-0.6128,  0.3479, -0.0068,  ...,  1.4775, -0.0717,  0.1342],\n",
       "          [-0.1970,  0.0814, -0.2112,  ...,  1.3467, -0.1984, -0.0714],\n",
       "          [ 0.3000,  0.3650, -0.0643,  ...,  0.8633,  0.0813, -0.0547],\n",
       "          [-0.2152,  0.3271,  0.0546,  ...,  0.4644,  0.0707,  0.0220]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0640, -0.0289, -0.0215,  ..., -0.2316,  0.0891, -0.3679],\n",
       "          [ 0.1294,  0.0870, -0.0244,  ..., -0.2054,  0.1393, -0.3809]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.1161e-04, 3.0518e-03, 3.8721e-01, 6.0938e-01],\n",
       "          [6.6757e-04, 5.1727e-03, 4.2383e-01, 5.7031e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.000000  182.880310  161.130493  280.633667    0.860339     42   \n",
       "  1  232.682953  277.136108  323.495636  421.837097    0.745194     43   \n",
       "  2  233.147430  148.961670  397.531219  267.084839    0.732433     53   \n",
       "  3    0.150482  112.800018  636.639404  417.373535    0.357841     60   \n",
       "  4  312.007996  192.692627  498.819946  269.901428    0.316026     53   \n",
       "  \n",
       "             name  \n",
       "  0          fork  \n",
       "  1         knife  \n",
       "  2         pizza  \n",
       "  3  dining table  \n",
       "  4         pizza  ,\n",
       "  'caption': ['The piece of pizza on bottom.',\n",
       "   'The pizza slice on the bottom.'],\n",
       "  'bbox_target': [315.17, 178.6, 148.99, 94.55]},\n",
       " 373: {'image_emb': tensor([[-2.7661e-01,  4.3701e-01, -5.6445e-01,  ...,  1.4883e+00,\n",
       "            2.9980e-01, -4.4983e-02],\n",
       "          [-3.9258e-01,  2.2473e-01, -1.4990e-01,  ...,  9.1943e-01,\n",
       "            9.8206e-02, -5.9357e-02],\n",
       "          [ 1.0938e-01,  1.4075e-01, -2.4792e-01,  ...,  8.3057e-01,\n",
       "           -5.5115e-02, -9.9087e-04],\n",
       "          [-1.6028e-01,  7.8369e-02, -2.2131e-01,  ...,  9.0430e-01,\n",
       "            3.1812e-01,  4.1406e-01],\n",
       "          [-3.3264e-02,  3.7451e-01, -9.9182e-02,  ...,  1.0283e+00,\n",
       "           -8.0811e-02, -4.8798e-02],\n",
       "          [-1.2079e-01,  1.9946e-01, -2.0532e-01,  ...,  6.8555e-01,\n",
       "           -1.2646e-01, -1.9608e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0594, -0.1630,  0.0531,  ..., -0.0475, -0.0615, -0.2289],\n",
       "          [-0.1217, -0.0435,  0.2439,  ...,  0.0406,  0.1515, -0.2708]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.3117e-02, 2.9517e-01, 5.9021e-02, 5.6076e-04, 2.3718e-01, 3.8501e-01],\n",
       "          [3.5038e-03, 1.9739e-01, 1.2421e-02, 2.2531e-05, 2.8271e-01, 5.0391e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.000000  381.312805   79.529938  635.014526    0.909013     41   \n",
       "  1    1.958221    1.120987  319.676727  387.342712    0.895528      0   \n",
       "  2   49.051270  262.110840  425.467285  580.933655    0.856627     53   \n",
       "  3  295.536987    1.132401  426.429993  266.212402    0.819649      0   \n",
       "  4   72.338211  240.227417  287.408875  355.502258    0.745063     53   \n",
       "  5    4.841766  550.581116  426.658569  639.162415    0.507551     60   \n",
       "  6   84.436676  223.810272  159.963715  291.093353    0.344692     43   \n",
       "  \n",
       "             name  \n",
       "  0           cup  \n",
       "  1        person  \n",
       "  2         pizza  \n",
       "  3        person  \n",
       "  4         pizza  \n",
       "  5  dining table  \n",
       "  6         knife  ,\n",
       "  'caption': ['the piece of pizza that is about to be put on the plate',\n",
       "   'A piece of pizza being slid onto a white plate.'],\n",
       "  'bbox_target': [76.22, 241.62, 195.6, 113.62]},\n",
       " 374: {'image_emb': tensor([[-0.1494,  0.8403, -0.1084,  ...,  0.8286,  0.1951,  0.1937],\n",
       "          [-0.0382,  0.4653, -0.0494,  ...,  1.0137,  0.1621,  0.0717],\n",
       "          [-0.3042,  0.1776, -0.0974,  ...,  1.1592,  0.2014, -0.2429],\n",
       "          [-0.0126,  0.4653,  0.0297,  ...,  1.0781,  0.0071,  0.1365],\n",
       "          [ 0.0922,  0.5938, -0.0166,  ...,  0.6904,  0.0444, -0.0637]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0985, -0.0313,  0.1509,  ...,  0.0561,  0.2474, -0.3933],\n",
       "          [ 0.0812,  0.1902, -0.1443,  ...,  0.5024, -0.2134, -0.3245]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.5410e-01, 1.0455e-01, 3.0780e-04, 9.1887e-04, 4.4019e-01],\n",
       "          [4.6967e-02, 7.6675e-03, 1.9083e-03, 3.4392e-05, 9.4336e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  165.066467  177.434387  464.962830  478.111389    0.936543     63  laptop\n",
       "  1   61.044785    5.544052  244.806229  222.645264    0.903800     63  laptop\n",
       "  2  386.292816  120.964386  458.478302  216.948364    0.898847     65  remote\n",
       "  3  197.463837    0.390312  360.687592  141.199127    0.828975     73    book\n",
       "  4  240.564514   89.966202  397.274109  227.860352    0.693241     63  laptop\n",
       "  5  218.340775   28.794464  259.332123   68.277405    0.342212     64   mouse,\n",
       "  'caption': ['A white laptop in a black case.',\n",
       "   'white laptop computer on a living room table'],\n",
       "  'bbox_target': [162.15, 171.45, 304.96, 302.82]},\n",
       " 375: {'image_emb': tensor([[-0.3779,  0.6182, -0.3027,  ...,  1.3096,  0.0038,  0.3311],\n",
       "          [-0.3511,  0.3206,  0.1544,  ...,  1.2725,  0.3186, -0.1060],\n",
       "          [ 0.0319,  0.4124, -0.0847,  ...,  0.9087,  0.0278,  0.1038]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1968,  0.0530, -0.0204,  ..., -0.2280,  0.0730, -0.4065],\n",
       "          [-0.0380, -0.1367, -0.3662,  ..., -0.1992, -0.2512, -0.3108]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2076, 0.3020, 0.4902],\n",
       "          [0.0223, 0.9639, 0.0137]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    3.447487   52.448822  187.863068  317.735321    0.807838     51  carrot\n",
       "  1  283.261108   60.302109  465.420654  318.148743    0.802228     51  carrot\n",
       "  2  239.059052   34.334183  331.039948  378.377686    0.595021     51  carrot\n",
       "  3  152.198380   47.886581  248.677612  408.433228    0.586810     51  carrot\n",
       "  4    5.014618   39.100021  478.000000  483.127502    0.528457     45    bowl\n",
       "  5  434.804321  172.966003  467.532959  207.651672    0.255601     51  carrot,\n",
       "  'caption': ['carrot on the very left.',\n",
       "   'a pepper to the left of some others'],\n",
       "  'bbox_target': [16.56, 49.69, 173.18, 262.03]},\n",
       " 376: {'image_emb': tensor([[ 1.3916e-02,  3.8818e-01, -4.6783e-02,  ...,  1.0361e+00,\n",
       "            1.0052e-01,  1.3153e-02],\n",
       "          [-3.9429e-01,  3.0664e-01, -8.7769e-02,  ...,  1.1143e+00,\n",
       "           -6.3171e-02, -3.0029e-01],\n",
       "          [-3.7183e-01,  3.7036e-01,  1.2769e-01,  ...,  1.2627e+00,\n",
       "           -2.7856e-01, -5.0903e-02],\n",
       "          [-8.9661e-02,  4.3457e-01, -6.0333e-02,  ...,  8.5791e-01,\n",
       "           -2.1008e-01, -1.4966e-01],\n",
       "          [-2.9834e-01,  3.0664e-01, -4.0771e-02,  ...,  1.3926e+00,\n",
       "           -4.0436e-04, -5.4297e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0494,  0.3662, -0.3176,  ...,  0.0095,  0.1083, -0.5542],\n",
       "          [-0.0268,  0.1388,  0.0267,  ...,  0.2944,  0.5537, -0.0097]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.1003e-03, 3.5465e-05, 4.3201e-04, 9.8730e-01, 7.1907e-03],\n",
       "          [6.2988e-01, 5.5134e-05, 3.4103e-03, 3.6450e-01, 1.9741e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0    0.371208  165.309601  199.662384  437.946991    0.860538     25  umbrella\n",
       "  1   39.222164  425.233521  120.013977  483.247070    0.842263     41       cup\n",
       "  2   86.165359  390.472565  176.499405  462.855103    0.820919     41       cup\n",
       "  3  114.581970   87.029114  256.401306  310.162048    0.797817     25  umbrella\n",
       "  4   52.477238  398.620056   91.830948  430.432129    0.462073     47     apple\n",
       "  5   30.278053  469.399109  108.623009  497.972229    0.253404     41       cup,\n",
       "  'caption': ['The umbrella with the wooden handle.',\n",
       "   'Handle of black umbrella that is in the garbage can.'],\n",
       "  'bbox_target': [116.36, 87.27, 139.85, 211.46]},\n",
       " 377: {'image_emb': tensor([[ 0.0567,  0.1495, -0.5112,  ...,  1.3936,  0.0927,  0.2703],\n",
       "          [-0.1990,  0.3462, -0.1035,  ...,  1.1562, -0.0186,  0.1078],\n",
       "          [ 0.0486, -0.0691, -0.3696,  ...,  1.1758,  0.1393,  0.0782],\n",
       "          [ 0.0056,  0.1986,  0.0744,  ...,  1.0117,  0.2944, -0.0708],\n",
       "          [-0.1086, -0.0671, -0.3545,  ...,  0.9077, -0.2332,  0.2460]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0862,  0.0225, -0.5527,  ..., -0.2222, -0.0545, -0.0491],\n",
       "          [ 0.0178,  0.3022, -0.5327,  ..., -0.3955, -0.1245, -0.0370]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0268, 0.7363, 0.0064, 0.0091, 0.2212],\n",
       "          [0.0446, 0.0035, 0.0204, 0.9243, 0.0070]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  124.256912   94.215179  313.190857  213.661102    0.861792     16     dog\n",
       "  1  168.480774   30.757339  318.794312  125.532104    0.815052      0  person\n",
       "  2  431.997437  101.897095  585.942139  214.039978    0.799378     16     dog\n",
       "  3  471.970459    9.041016  631.170044  212.979065    0.769120      0  person\n",
       "  4   14.919243    9.210220  132.467651   87.812256    0.372646     56   chair,\n",
       "  'caption': ['hand left side', \"A hand on a dog's head\"],\n",
       "  'bbox_target': [170.0, 29.5, 145.0, 99.0]},\n",
       " 378: {'image_emb': tensor([[ 0.4966,  0.0108, -0.1031,  ...,  0.6831, -0.1213, -0.1353],\n",
       "          [ 0.3743,  0.3525, -0.1393,  ...,  1.0137, -0.0407,  0.2812],\n",
       "          [ 0.5845,  0.0650,  0.0812,  ...,  0.7446, -0.2710,  0.1964]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3389, -0.1350, -0.0583,  ...,  0.2075, -0.2491,  0.1818],\n",
       "          [ 0.4170, -0.0916, -0.3774,  ...,  0.1105, -0.2452, -0.2351]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.7520, 0.0257, 0.2223],\n",
       "          [0.5640, 0.1201, 0.3162]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0    0.153091  113.826904  400.788635  298.129517    0.939349      7  truck\n",
       "  1  382.219025  170.370422  636.352295  287.560608    0.819356      7  truck\n",
       "  2  532.574951  149.739868  635.274780  234.720154    0.655706      7  truck,\n",
       "  'caption': ['A tow truck towing a truck.', 'truck lifting the other car'],\n",
       "  'bbox_target': [2.22, 110.91, 391.51, 183.0]},\n",
       " 379: {'image_emb': tensor([[-0.2279,  0.3293, -0.2598,  ...,  1.5371, -0.1265,  0.0124],\n",
       "          [-0.2153, -0.0549,  0.2483,  ...,  0.8179,  0.1218, -0.0072],\n",
       "          [ 0.2156,  0.0991, -0.0850,  ...,  1.0254,  0.0100,  0.0786],\n",
       "          ...,\n",
       "          [ 0.0101,  0.3164, -0.2554,  ...,  0.9692,  0.2058, -0.3303],\n",
       "          [-0.2668,  0.3071, -0.4045,  ...,  1.3262,  0.1256, -0.1207],\n",
       "          [-0.2864, -0.0311, -0.2383,  ...,  0.7998,  0.0170, -0.0903]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1030, -0.2839,  0.0404,  ...,  0.3010, -0.3091,  0.1188],\n",
       "          [ 0.2024, -0.1879,  0.0925,  ...,  0.4390, -0.0781, -0.1420]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.1400e-04, 1.6451e-03, 1.3638e-03, 1.4893e-02, 9.8047e-01, 4.5681e-04,\n",
       "           7.6485e-04],\n",
       "          [1.7285e-04, 5.5838e-04, 7.3120e-02, 6.1554e-02, 8.6328e-01, 1.2980e-03,\n",
       "           1.1337e-04]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   164.385498   80.508636  292.525543  322.208893    0.897874      0   \n",
       "  1   123.739243  144.061768  504.730103  316.450989    0.885873     13   \n",
       "  2     0.831436  185.038849  112.680481  283.149200    0.882644      2   \n",
       "  3    37.804577  178.867310  124.557556  211.115173    0.859008      2   \n",
       "  4   330.886230  118.338348  639.987976  261.067902    0.854931      2   \n",
       "  5   160.970734  132.726440  252.960938  236.591370    0.766056     26   \n",
       "  6   162.447922   78.687119  291.246155  321.316956    0.447084     26   \n",
       "  7   561.125427  156.721344  601.032288  167.348053    0.363330      2   \n",
       "  8   208.016769  104.069702  217.937881  123.105225    0.349655     67   \n",
       "  9     0.385090  134.252167   46.737484  158.908356    0.311337     25   \n",
       "  10   34.691605  132.010071  150.435806  155.012634    0.298138     25   \n",
       "  \n",
       "            name  \n",
       "  0       person  \n",
       "  1        bench  \n",
       "  2          car  \n",
       "  3          car  \n",
       "  4          car  \n",
       "  5      handbag  \n",
       "  6      handbag  \n",
       "  7          car  \n",
       "  8   cell phone  \n",
       "  9     umbrella  \n",
       "  10    umbrella  ,\n",
       "  'caption': ['A black car that is parked behind the bench.',\n",
       "   'Convertible black car.'],\n",
       "  'bbox_target': [304.04, 120.96, 335.96, 155.74]},\n",
       " 380: {'image_emb': tensor([[-0.4458,  0.0385,  0.1387,  ...,  0.7075, -0.0367, -0.6294],\n",
       "          [-0.4709, -0.0416,  0.0957,  ...,  0.7744,  0.0185, -0.5991],\n",
       "          [-0.8188,  0.2788, -0.2810,  ...,  0.5239,  0.0627, -0.1896],\n",
       "          ...,\n",
       "          [-0.0129, -0.0963, -0.3408,  ...,  0.9360, -0.1659, -0.2910],\n",
       "          [ 0.1678,  0.0513, -0.2776,  ...,  1.0205, -0.1304, -0.3003],\n",
       "          [-0.4465, -0.2852,  0.0273,  ...,  0.2649,  0.0965, -0.4985]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-4.3091e-01, -5.1758e-01, -3.9648e-01, -4.7791e-02,  1.6711e-01,\n",
       "            3.3618e-01, -8.0994e-02, -9.2432e-01, -1.2657e-02, -1.9958e-01,\n",
       "           -1.4185e-01, -2.7222e-01, -7.2815e-02, -3.8037e-01,  9.9182e-02,\n",
       "           -1.1218e-01,  2.3657e-01,  2.5854e-01, -2.3987e-01,  6.0107e-01,\n",
       "            5.6885e-01,  2.3621e-02, -2.2778e-01,  9.2224e-02, -1.3306e-01,\n",
       "           -6.3525e-01,  2.6416e-01,  2.0715e-01, -2.9761e-01,  1.7139e-01,\n",
       "            1.0040e-01, -7.5455e-03,  1.9446e-01,  1.9214e-01, -7.4170e-01,\n",
       "           -2.4146e-01,  6.6528e-02, -3.3472e-01, -2.6392e-01,  1.6064e-01,\n",
       "           -6.6162e-02, -1.2787e-02, -2.9709e-02,  1.5100e-01, -5.1178e-02,\n",
       "            1.9714e-01, -4.4873e-01, -5.6244e-02,  1.6003e-01, -5.0690e-02,\n",
       "           -2.2192e-01, -3.2978e-03, -1.9312e-01,  4.5593e-02, -6.8176e-02,\n",
       "           -5.0244e-01, -5.6946e-02,  1.2488e-01, -2.5049e-01,  1.3794e-01,\n",
       "            1.8042e-01, -4.9652e-02,  2.8613e-01,  1.8701e-01,  1.7261e-01,\n",
       "           -3.8281e-01,  1.4307e-01,  2.3315e-01, -2.9517e-01, -6.6406e-02,\n",
       "           -2.1863e-01, -1.1652e-01, -2.0789e-01,  1.5149e-01, -3.0957e-01,\n",
       "            1.1084e-01, -4.0820e-01,  3.1763e-01,  1.1865e-01,  1.7017e-01,\n",
       "           -3.8892e-01,  9.0088e-02, -1.9531e-01,  1.9385e-01,  2.4341e-01,\n",
       "           -2.3483e-02,  3.9722e-01,  3.6865e-01, -3.3911e-01, -6.0692e-03,\n",
       "           -1.8701e-01,  1.5430e-01, -1.3838e+00,  8.9551e-01, -6.7627e-01,\n",
       "           -4.2877e-02, -2.6810e-02, -4.0332e-01, -2.8491e-01,  1.3086e-01,\n",
       "            3.2471e-01,  2.8247e-01,  1.1078e-01, -4.6899e-01, -1.6541e-01,\n",
       "           -6.1859e-02, -4.4830e-02, -3.7061e-01, -3.5352e-01, -1.4050e-01,\n",
       "           -4.5020e-01,  5.9277e-01, -1.3293e-01, -3.5107e-01,  7.1716e-02,\n",
       "            2.0950e-02,  7.4219e-02, -6.0516e-02, -2.8955e-01, -2.6306e-02,\n",
       "           -1.2274e-01,  1.8640e-01, -1.4715e-03, -3.0908e-01, -2.7832e-01,\n",
       "           -2.9956e-01,  1.6431e-01,  9.8206e-02,  1.3538e-01,  4.9951e-01,\n",
       "            1.2341e-01, -3.5645e-01, -7.1472e-02,  5.5508e+00, -1.8384e-01,\n",
       "           -2.9785e-01, -4.7485e-01, -2.1423e-01,  5.0964e-02,  9.4543e-02,\n",
       "           -2.6440e-01, -2.3773e-02,  1.2830e-01,  1.6797e-01, -7.4402e-02,\n",
       "            2.1687e-03, -4.0558e-02, -2.7802e-02, -7.2461e-01,  1.6626e-01,\n",
       "            8.1909e-02,  3.8208e-01,  2.0740e-01,  1.1743e-01,  1.0797e-01,\n",
       "           -1.8164e-01,  3.9722e-01, -1.6418e-01, -6.8359e-02,  1.4587e-01,\n",
       "           -6.8588e-03, -1.8054e-01,  1.0645e-01, -1.2341e-01, -4.5563e-02,\n",
       "            2.9590e-01, -2.9761e-01,  2.7661e-01, -4.0558e-02, -1.9226e-01,\n",
       "           -9.9609e-02, -7.3814e-03,  1.0016e-01, -1.8665e-01, -7.3303e-02,\n",
       "            3.6694e-01, -1.0620e-01,  2.8052e-01, -5.1453e-02,  2.0593e-01,\n",
       "           -3.9673e-01,  5.9235e-02, -4.1406e-01,  8.4045e-02,  6.1218e-02,\n",
       "           -2.6138e-02, -2.8369e-01,  1.4246e-01,  1.1719e-01,  1.8689e-01,\n",
       "           -1.1414e-01,  2.2656e-01,  6.0645e-01, -4.3488e-02,  2.2498e-01,\n",
       "            4.0314e-02, -2.0432e-02,  2.4829e-01,  1.2976e-01,  2.5684e-01,\n",
       "            2.0728e-01, -1.1121e-01, -2.2598e-02, -2.1469e-02, -4.8943e-03,\n",
       "            1.8860e-01, -3.1281e-02, -1.4880e-01, -4.2175e-02, -1.1218e-01,\n",
       "            1.8140e-01,  3.8452e-01,  1.3293e-01,  1.8335e-01, -4.2285e-01,\n",
       "            8.5022e-02, -1.9031e-01, -5.0879e-01,  2.3712e-02,  2.2253e-01,\n",
       "           -3.8257e-01, -4.2456e-01,  3.4009e-01,  1.3159e-01, -2.4902e-01,\n",
       "           -2.6636e-01,  1.4697e-01,  4.0381e-01, -1.8848e-01, -1.5454e-01,\n",
       "           -2.7368e-01,  3.5767e-01,  1.3733e-01, -3.6816e-01,  1.5845e-01,\n",
       "           -2.4268e-01,  1.7563e-02, -2.2717e-01, -5.6580e-02,  1.9385e-01,\n",
       "           -4.8157e-02,  1.6199e-01, -3.1763e-01, -8.3069e-02, -1.6272e-01,\n",
       "            2.0279e-02, -1.5656e-02,  4.8364e-01,  2.7051e-01,  1.3283e-02,\n",
       "            1.8103e-01, -8.3191e-02, -9.7351e-02, -1.1694e-01, -2.7222e-01,\n",
       "           -1.4809e-02, -2.5928e-01, -2.3633e-01,  7.7820e-02,  1.2054e-01,\n",
       "           -3.8843e-01, -2.1790e-02,  4.2188e-01,  3.2139e-03,  1.7603e-01,\n",
       "           -2.0508e-01, -1.7212e-01,  1.4380e-01, -5.3375e-02,  9.5947e-02,\n",
       "            6.6895e-02,  1.1792e-01,  9.7839e-02, -2.4146e-01, -2.0471e-01,\n",
       "            1.7114e-01,  8.1970e-02,  2.1802e-01,  1.9421e-01, -8.5388e-02,\n",
       "           -3.8672e-01, -1.0468e-01,  3.4717e-01,  2.4231e-01,  2.8003e-01,\n",
       "           -8.3923e-02,  9.8328e-02, -5.4779e-02,  2.5244e-01,  2.5806e-01,\n",
       "            2.7319e-01,  9.3689e-02,  1.3147e-01,  1.1877e-01,  1.8201e-01,\n",
       "            3.0136e-02,  1.7242e-02,  2.1985e-01,  2.9199e-01,  7.0129e-02,\n",
       "           -2.8320e-01,  1.1993e-02, -1.2415e-01, -2.8931e-01, -4.1602e-01,\n",
       "           -3.1274e-01,  1.9272e-02,  2.7197e-01,  9.8328e-02, -5.2930e-01,\n",
       "           -3.1952e-02,  2.5073e-01,  5.5391e+00, -3.3203e-01,  4.2383e-01,\n",
       "            1.2683e-01,  4.9146e-01, -2.5708e-01,  4.8920e-02,  4.8950e-01,\n",
       "           -2.2095e-01, -2.1936e-01,  3.5352e-01, -6.6284e-02, -5.2734e-01,\n",
       "           -6.6711e-02, -9.6252e-02,  7.1838e-02, -2.7148e-01, -2.4609e+00,\n",
       "            2.0129e-01, -1.6772e-01, -2.7808e-01,  2.9053e-01, -2.7173e-01,\n",
       "           -6.0577e-02,  9.5596e-03, -1.2445e-01, -1.9318e-02,  1.3550e-01,\n",
       "            4.5557e-01, -1.3745e-01,  2.5513e-01, -2.8638e-01,  5.0873e-02,\n",
       "           -3.2007e-01,  8.6121e-02,  2.7588e-01, -8.9661e-02,  2.6917e-02,\n",
       "            5.6055e-01, -1.3000e-01,  4.7412e-01,  2.9510e-02, -1.7358e-01,\n",
       "            1.0292e-02,  3.6163e-02,  4.5227e-02, -3.3179e-01,  9.0698e-02,\n",
       "            3.8013e-01, -5.2002e-02,  8.6133e-01, -9.9854e-02, -4.6948e-01,\n",
       "            2.8809e-01, -4.5728e-01,  3.5736e-02,  2.0050e-02,  1.2323e-01,\n",
       "           -1.2164e-01,  3.7659e-02, -5.7709e-02, -6.9824e-02,  8.1360e-02,\n",
       "            1.6064e-01,  3.8794e-01, -1.9006e-01, -1.6467e-01, -1.0248e-01,\n",
       "            9.0942e-02,  1.0272e-01,  4.5013e-02, -5.1611e-01, -1.7493e-01,\n",
       "           -2.0996e-01, -1.5112e-01, -8.5449e-02, -3.1708e-02,  9.0393e-02,\n",
       "           -5.8984e-01, -1.7908e-01,  1.0205e-01,  1.6040e-01,  8.0444e-02,\n",
       "           -5.4901e-02,  8.0811e-02,  2.1326e-01,  2.6440e-01,  2.3291e-01,\n",
       "            2.1252e-01, -3.3203e-01,  3.6084e-01,  7.4829e-02, -5.6030e-02,\n",
       "           -1.6479e-02,  1.0706e-01,  3.9185e-02,  2.0493e-02, -1.2732e-01,\n",
       "            2.0251e-01, -4.1187e-01, -1.5662e-01,  1.3159e-01,  6.4575e-02,\n",
       "           -3.9429e-01,  1.1749e-01, -5.1392e-02, -2.1286e-02, -4.5746e-02,\n",
       "            2.1103e-02, -1.6028e-01,  1.1560e-01, -9.5215e-02, -1.2280e-01,\n",
       "           -3.9062e-01, -4.4373e-02, -2.6810e-02,  1.8860e-01, -4.2773e-01,\n",
       "           -2.7588e-01,  1.3977e-01,  2.6318e-01, -1.9580e-01, -3.6987e-01,\n",
       "            4.7424e-02, -2.9755e-02,  5.9033e-01,  2.4072e-01, -1.3074e-01,\n",
       "            5.4352e-02, -5.2051e-01, -3.2153e-01, -1.4270e-01,  3.8147e-02,\n",
       "           -2.6318e-01, -3.5309e-02,  2.4933e-02, -8.7433e-03,  3.2983e-01,\n",
       "            2.5894e-02,  1.1591e-01,  7.7637e-02,  3.2471e-01, -1.9739e-01,\n",
       "            9.1553e-02, -2.8784e-01, -7.7393e-02, -3.4619e-01, -5.3809e-01,\n",
       "            6.4011e-03, -1.8945e-01, -3.0945e-02, -3.7402e-01, -1.0345e-01,\n",
       "           -4.4373e-02, -1.2189e-01, -3.6526e-03, -1.2352e-02,  1.4624e-01,\n",
       "            2.0096e-02, -7.4707e-02,  1.0254e-02, -1.5747e-01, -1.9873e-01,\n",
       "            2.9443e-01,  1.0107e-01,  2.7393e-01, -2.9160e-02,  3.3130e-01,\n",
       "            8.6792e-02, -8.8428e-01,  4.0063e-01,  1.2012e-01,  1.7200e-01,\n",
       "           -1.3696e-01,  6.1035e-03, -5.1392e-02, -2.5488e-01, -7.1899e-02,\n",
       "            1.2537e-01,  2.0227e-01, -1.2878e-01,  2.9565e-01,  6.6772e-02,\n",
       "           -4.7668e-02,  7.3120e-02,  1.9946e-01,  1.0114e-01,  1.1959e-03,\n",
       "           -3.0493e-01, -5.5127e-01, -2.0544e-01, -5.8479e-03,  1.4050e-01,\n",
       "            1.8753e-02,  2.0898e-01,  1.6638e-01, -1.3586e-01,  3.0347e-01,\n",
       "           -8.7341e-02, -5.7568e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0806, 0.0537, 0.2368, 0.1707, 0.0075, 0.0013, 0.4495]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  346.293610   59.240173  552.594604  401.912415    0.946833      0   \n",
       "  1   30.686218   59.836243  235.156067  402.077209    0.944771      0   \n",
       "  2  220.252625   40.135025  298.427246  166.270325    0.866887     38   \n",
       "  3  536.355225   39.939560  615.367798  167.916656    0.858837     38   \n",
       "  4  161.456299  116.485886  180.778290  135.220352    0.851572     32   \n",
       "  5  478.430054  115.933487  497.734131  134.770218    0.817281     32   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2  tennis racket  \n",
       "  3  tennis racket  \n",
       "  4    sports ball  \n",
       "  5    sports ball  ,\n",
       "  'caption': ['tennis player in first photo on right'],\n",
       "  'bbox_target': [347.94, 56.75, 214.45, 341.37]},\n",
       " 381: {'image_emb': tensor([[ 0.0428,  0.4998, -0.4766,  ...,  0.8452,  0.1509, -0.0627],\n",
       "          [ 0.0396,  0.1343, -0.3269,  ...,  1.2324, -0.0372, -0.1536],\n",
       "          [-0.1312,  0.1709, -0.1464,  ...,  1.2900,  0.1865,  0.0191],\n",
       "          ...,\n",
       "          [-0.4299, -0.0082, -0.1956,  ...,  1.1709, -0.0291, -0.1003],\n",
       "          [ 0.1144, -0.3118, -0.2329,  ...,  0.7129, -0.1569, -0.2363],\n",
       "          [-0.4626,  0.1444, -0.4062,  ...,  1.0352,  0.6055,  0.3079]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0399,  0.1276, -0.1896,  ...,  0.4465, -0.1268,  0.2830],\n",
       "          [-0.0210,  0.0691, -0.1732,  ..., -0.4983, -0.0103, -0.3032]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.3842e-07, 3.0875e-05, 5.9605e-07, 9.8877e-01, 1.6851e-03, 5.9128e-04,\n",
       "           2.3384e-03, 4.7989e-03, 8.0466e-06, 1.5831e-03, 4.6492e-06, 1.9670e-06],\n",
       "          [1.5306e-03, 1.8463e-03, 7.2289e-04, 2.8711e-01, 4.9896e-02, 5.3101e-02,\n",
       "           4.8071e-01, 8.2275e-02, 7.1144e-04, 3.3752e-02, 8.1406e-03, 1.4460e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   461.460632  123.983246  525.677490  287.319244    0.928284      0   \n",
       "  1   543.897400  103.763580  607.229553  268.996796    0.907630      0   \n",
       "  2    56.601521  111.249237  103.311752  247.087799    0.899247      0   \n",
       "  3   466.038910  353.253937  640.000000  477.882507    0.887196     63   \n",
       "  4   218.687408  277.219604  490.461639  476.565247    0.885609      0   \n",
       "  5    34.593151  227.448456   78.452393  278.532318    0.865379     63   \n",
       "  6   210.132507  235.156189  296.554626  394.266174    0.853481     56   \n",
       "  7     0.088726  211.302612  113.800552  475.319214    0.852620      0   \n",
       "  8   111.184853  133.776627  154.221573  180.422150    0.849642      0   \n",
       "  9   100.749527  281.046753  246.663742  477.487671    0.805188     56   \n",
       "  10  545.546265  258.797089  558.052612  295.933136    0.715253     39   \n",
       "  11  389.038910  419.868103  411.376495  447.598450    0.698209     67   \n",
       "  12  572.736328  256.536255  621.159302  322.930603    0.662737     63   \n",
       "  13    0.000000  405.570282   76.853790  478.469727    0.642911     56   \n",
       "  14  258.842133   49.511551  416.198822  214.880676    0.521898     62   \n",
       "  15  605.181213  220.682922  639.881042  303.530212    0.494791      0   \n",
       "  16   30.298935  255.558807  236.980179  332.327850    0.486521     60   \n",
       "  17   16.212938  158.651688   63.122215  245.742279    0.417137     56   \n",
       "  18  572.385620  256.627838  607.968262  315.620697    0.402553     62   \n",
       "  19   85.643036  247.972992  100.852631  279.485260    0.375012     41   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3         laptop  \n",
       "  4         person  \n",
       "  5         laptop  \n",
       "  6          chair  \n",
       "  7         person  \n",
       "  8         person  \n",
       "  9          chair  \n",
       "  10        bottle  \n",
       "  11    cell phone  \n",
       "  12        laptop  \n",
       "  13         chair  \n",
       "  14            tv  \n",
       "  15        person  \n",
       "  16  dining table  \n",
       "  17         chair  \n",
       "  18            tv  \n",
       "  19           cup  ,\n",
       "  'caption': ['A silver laptop with a green and blue desktop background.',\n",
       "   'the laptop on bottom right'],\n",
       "  'bbox_target': [467.02, 351.79, 172.98, 128.21]},\n",
       " 382: {'image_emb': tensor([[-0.1698,  0.3684, -0.3257,  ...,  1.0322,  0.4116, -0.0587],\n",
       "          [-0.2935,  0.2102, -0.2715,  ...,  1.2305, -0.0879,  0.1704],\n",
       "          [ 0.0353,  0.3784, -0.3489,  ...,  1.1162, -0.0616,  0.1311],\n",
       "          [-0.1052,  0.4233, -0.4067,  ...,  1.3027,  0.2225,  0.0215],\n",
       "          [-0.5571,  0.0991, -0.0658,  ...,  0.6763,  0.3716,  0.5181]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0671,  0.1901,  0.1714,  ...,  0.6016, -0.3418, -0.2817],\n",
       "          [-0.2969, -0.0172, -0.0917,  ..., -0.1445, -0.1938, -0.3459]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.3915e-05, 8.9404e-01, 3.9721e-04, 1.0510e-01, 5.7793e-04],\n",
       "          [1.0967e-04, 9.3164e-01, 1.0242e-03, 6.4392e-02, 3.0594e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   407.998138   55.500122  543.270935  445.703857    0.881607      0   \n",
       "  1    97.495651  223.790649  236.187363  285.415649    0.870861      2   \n",
       "  2   219.297104   50.677429  316.355530  444.040649    0.869000      0   \n",
       "  3    21.765076  292.875549   85.376106  413.328308    0.863995      0   \n",
       "  4   305.781036   60.259445  418.203461  443.771179    0.690473      0   \n",
       "  5   404.721954   93.581360  470.971527  303.536743    0.673585      0   \n",
       "  6   141.690308  208.426178  236.766907  244.310272    0.546254      2   \n",
       "  7   366.817932   44.465347  426.973022  140.809280    0.518781      0   \n",
       "  8   326.554169  183.434387  418.314423  321.846008    0.469356     77   \n",
       "  9     0.628048  352.938812   73.457962  438.921265    0.453318      2   \n",
       "  10  386.083649   79.652832  466.191742  444.061890    0.411898      0   \n",
       "  11  539.044617  327.766632  638.632996  445.563965    0.399408     24   \n",
       "  12    0.603378  308.495178  219.610931  444.951355    0.364903      2   \n",
       "  13  375.959290  131.870270  414.667175  275.158661    0.346841     26   \n",
       "  14  538.461121  327.649200  638.900818  445.136841    0.286089     16   \n",
       "  15  366.029022  136.751221  414.387054  275.023499    0.271768     77   \n",
       "  \n",
       "            name  \n",
       "  0       person  \n",
       "  1          car  \n",
       "  2       person  \n",
       "  3       person  \n",
       "  4       person  \n",
       "  5       person  \n",
       "  6          car  \n",
       "  7       person  \n",
       "  8   teddy bear  \n",
       "  9          car  \n",
       "  10      person  \n",
       "  11    backpack  \n",
       "  12         car  \n",
       "  13     handbag  \n",
       "  14         dog  \n",
       "  15  teddy bear  ,\n",
       "  'caption': ['A station wagon in a parking lot.',\n",
       "   'The silvery SUV in the lower left-hand corner'],\n",
       "  'bbox_target': [0.83, 311.8, 243.09, 124.87]},\n",
       " 383: {'image_emb': tensor([[-3.5620e-01,  3.8110e-01, -1.8286e-01,  ...,  8.1641e-01,\n",
       "            2.6978e-01, -2.6758e-01],\n",
       "          [-2.2107e-01, -9.5337e-02, -5.7983e-04,  ...,  9.3164e-01,\n",
       "            3.7231e-01,  1.3086e-01],\n",
       "          [-1.6589e-01,  2.6538e-01, -1.1133e-01,  ...,  1.1260e+00,\n",
       "            9.4238e-02,  9.0271e-02],\n",
       "          [-4.3213e-02,  3.4058e-01, -2.4231e-01,  ...,  6.0059e-01,\n",
       "            2.2888e-01, -1.7554e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.4753,  0.2472, -0.1459,  ..., -0.0188, -0.2316,  0.1722],\n",
       "          [ 0.1135,  0.1104, -0.1925,  ...,  0.1895, -0.3059,  0.3206]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9951e-01, 4.0531e-06, 8.8871e-05, 2.0981e-04],\n",
       "          [9.9951e-01, 1.7464e-05, 5.7030e-04, 1.8895e-05]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  417.600220    0.000000  562.530884  142.031433    0.919014     41   \n",
       "  1  104.966957    0.000000  219.510406   68.907806    0.838121      0   \n",
       "  2  265.888611  431.643341  320.166199  479.357056    0.780211     42   \n",
       "  3    0.000000    1.278168  639.270020  474.811890    0.634955     60   \n",
       "  4   73.079315   72.310150  590.932495  474.390991    0.616909     53   \n",
       "  5  407.580444    0.240334  456.362854   67.869049    0.586322     41   \n",
       "  6  548.179321    0.242111  613.253540   52.992111    0.573039     41   \n",
       "  7  218.998566  420.100494  283.059601  479.508911    0.492999     43   \n",
       "  \n",
       "             name  \n",
       "  0           cup  \n",
       "  1        person  \n",
       "  2          fork  \n",
       "  3  dining table  \n",
       "  4         pizza  \n",
       "  5           cup  \n",
       "  6           cup  \n",
       "  7         knife  ,\n",
       "  'caption': ['A green mug behind the unknown dish', 'A green coffee cup.'],\n",
       "  'bbox_target': [418.61, 0.0, 148.07, 133.72]},\n",
       " 384: {'image_emb': tensor([[-0.0128,  0.5664, -0.0678,  ...,  0.8018,  0.0845, -0.3752],\n",
       "          [ 0.0863,  0.5801, -0.0579,  ...,  0.9702, -0.1104, -0.3396],\n",
       "          [-0.1584,  0.5166, -0.1860,  ...,  0.9795,  0.0347, -0.1714],\n",
       "          ...,\n",
       "          [ 0.2217, -0.2524, -0.5176,  ...,  1.0176, -0.1050, -0.0917],\n",
       "          [ 0.3586,  0.4536, -0.4009,  ...,  1.0967, -0.1395,  0.2522],\n",
       "          [-0.0253,  0.6118, -0.1737,  ...,  0.6880,  0.0905, -0.0191]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2646,  0.0859,  0.1947,  ...,  0.4204,  0.0296,  0.0918],\n",
       "          [-0.0052, -0.3970,  0.2810,  ...,  0.2688, -0.1326, -0.0315]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.4912e-01, 2.7197e-01, 1.2646e-01, 1.1826e-03, 5.3310e-04, 5.9462e-04,\n",
       "           1.4954e-03, 1.0443e-03, 2.4756e-01],\n",
       "          [1.1200e-01, 6.0547e-01, 1.4160e-01, 4.4098e-03, 9.6893e-04, 1.8280e-02,\n",
       "           5.3215e-03, 1.6747e-03, 1.1029e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    54.978851  108.095154  489.248260  361.506042    0.905047      3   \n",
       "  1   214.096481    0.000000  400.843201  367.517761    0.890428      0   \n",
       "  2     2.054291   74.897705  311.297577  239.707214    0.822472      2   \n",
       "  3   590.809814   63.579971  619.357422  102.809860    0.822241      0   \n",
       "  4   556.349792   60.474823  577.451355   88.594116    0.810760      0   \n",
       "  5   209.943466   47.603073  243.565414   78.855270    0.756351      0   \n",
       "  6     0.000000   36.492767   17.633944  148.398621    0.753412      0   \n",
       "  7   327.554382   48.532288  399.692078  105.972626    0.709931     24   \n",
       "  8   242.132370   52.035919  267.795898   75.375336    0.690113      0   \n",
       "  9    74.401985   48.752930  122.991875  133.028900    0.668775      0   \n",
       "  10   35.777584   65.735977  153.465988  140.038681    0.629529      3   \n",
       "  11  189.514175   52.091660  203.192581   83.032608    0.589752      0   \n",
       "  12   15.657898   66.710693  631.427673  314.026001    0.582636      2   \n",
       "  13  239.320923   97.262390  267.464172  130.965759    0.563227      0   \n",
       "  14  613.146606   89.448563  631.146973  111.861374    0.540114      0   \n",
       "  15   22.209862   52.462006   65.783813  115.091797    0.442218      3   \n",
       "  16  628.254883   90.120804  639.887939  118.156876    0.395657      0   \n",
       "  17  462.830261   30.520401  470.178223   47.432877    0.358261      0   \n",
       "  18  378.199402  104.325821  449.504639  180.568604    0.324308     24   \n",
       "  19  473.853760   28.342499  480.593872   47.507477    0.313558      0   \n",
       "  20   57.632919  107.547104  487.869995  360.460083    0.273251      0   \n",
       "  \n",
       "            name  \n",
       "  0   motorcycle  \n",
       "  1       person  \n",
       "  2          car  \n",
       "  3       person  \n",
       "  4       person  \n",
       "  5       person  \n",
       "  6       person  \n",
       "  7     backpack  \n",
       "  8       person  \n",
       "  9       person  \n",
       "  10  motorcycle  \n",
       "  11      person  \n",
       "  12         car  \n",
       "  13      person  \n",
       "  14      person  \n",
       "  15  motorcycle  \n",
       "  16      person  \n",
       "  17      person  \n",
       "  18    backpack  \n",
       "  19      person  \n",
       "  20      person  ,\n",
       "  'caption': ['A black motorcycle', 'Black sport cycle waiting at the night'],\n",
       "  'bbox_target': [59.12, 105.97, 428.04, 251.99]},\n",
       " 385: {'image_emb': tensor([[-2.6416e-01,  1.9836e-01, -8.8806e-02,  ...,  8.1934e-01,\n",
       "            2.4487e-01,  9.1736e-02],\n",
       "          [-3.0298e-01, -1.3879e-01, -6.5332e-01,  ...,  1.2285e+00,\n",
       "            4.3518e-02, -9.9487e-02],\n",
       "          [-5.1904e-01, -4.3726e-04, -1.0565e-01,  ...,  1.2627e+00,\n",
       "            2.2568e-02,  1.0696e-02],\n",
       "          [-1.8750e-01,  2.3389e-01, -2.7881e-01,  ...,  1.5225e+00,\n",
       "            1.7273e-02, -7.8491e-02],\n",
       "          [-1.9397e-01,  2.1423e-01,  1.1273e-01,  ...,  1.1338e+00,\n",
       "            1.8481e-01, -3.3765e-01],\n",
       "          [-5.3320e-01,  3.2568e-01,  2.2447e-04,  ...,  6.9775e-01,\n",
       "            3.5034e-01,  1.2573e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2095,  0.0332, -0.2489,  ...,  0.0260, -0.1489, -0.1240],\n",
       "          [-0.0305, -0.0009, -0.3796,  ..., -0.1516, -0.1113, -0.3142]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.6703e-05, 7.6866e-04, 9.8535e-01, 1.3840e-02, 2.0862e-06, 1.1069e-04],\n",
       "          [6.0225e-04, 2.1225e-02, 3.2690e-01, 6.4990e-01, 6.0225e-04, 6.7186e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  243.196732   50.910156  361.545166  304.404541    0.938273      0   \n",
       "  1  539.193665  321.546295  606.538757  425.453369    0.886581     16   \n",
       "  2  476.073059   58.172150  585.549255  280.028717    0.879407     58   \n",
       "  3  543.976807    9.739822  639.009155  418.853149    0.815020      0   \n",
       "  4  345.786804  170.657349  387.165833  204.601440    0.813109     29   \n",
       "  5  232.102448  124.016495  283.095856  182.451965    0.300160     58   \n",
       "  6  589.745728  181.457153  639.916992  423.655762    0.250837      0   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1           dog  \n",
       "  2  potted plant  \n",
       "  3        person  \n",
       "  4       frisbee  \n",
       "  5  potted plant  \n",
       "  6        person  ,\n",
       "  'caption': ['Person in a white t-shirt next to the potted plant',\n",
       "   'Person in white t-shirt.'],\n",
       "  'bbox_target': [540.74, 2.65, 99.26, 417.36]},\n",
       " 386: {'image_emb': tensor([[-0.2198,  0.4587, -0.1102,  ...,  1.1426,  0.0497, -0.1111],\n",
       "          [-0.3701,  0.3906, -0.1722,  ...,  1.1221,  0.0880, -0.4736],\n",
       "          [-0.3196,  0.3945, -0.1545,  ...,  1.2373, -0.1512, -0.3079],\n",
       "          [-0.2142,  0.2917,  0.0055,  ...,  1.0342, -0.1451, -0.2593],\n",
       "          [ 0.1958,  0.1067,  0.1252,  ...,  0.2588,  0.2446, -0.3496]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0245,  0.1261, -0.0387,  ...,  0.2515,  0.2698, -0.4680],\n",
       "          [-0.1287,  0.2352,  0.1982,  ...,  0.3875,  0.1134, -0.4062]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.3193e-03, 3.9673e-04, 2.9016e-04, 1.0614e-03, 9.9609e-01],\n",
       "          [2.4719e-03, 1.5318e-04, 1.8477e-04, 1.3304e-04, 9.9707e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin      ymin        xmax        ymax  confidence  class       name\n",
       "  0  315.077942  9.670914  363.610626  300.882385    0.892051     37  surfboard\n",
       "  1  247.702316  2.486920  298.772858  330.447327    0.890390     37  surfboard\n",
       "  2   69.842697  2.252781  154.987686  371.596680    0.886029     37  surfboard\n",
       "  3  161.855942  1.239371  228.222916  370.405090    0.823340     37  surfboard\n",
       "  4  200.094818  0.000000  229.829529  366.383179    0.451328     37  surfboard,\n",
       "  'caption': ['A surfboard to the left of three other boards.',\n",
       "   'A surfboard with an American flag.'],\n",
       "  'bbox_target': [69.62, 1.76, 85.12, 373.24]},\n",
       " 387: {'image_emb': tensor([[-0.1833,  0.2856, -0.0020,  ...,  1.1572, -0.0037,  0.2229],\n",
       "          [ 0.1122,  0.2180,  0.0269,  ...,  0.8760,  0.0586,  0.1735],\n",
       "          [ 0.0219,  0.3254,  0.1188,  ...,  1.6172, -0.1121, -0.3455],\n",
       "          [-0.2068, -0.1610,  0.0887,  ...,  0.9819,  0.0335,  0.0232],\n",
       "          [ 0.0168, -0.1389, -0.0229,  ...,  0.7812, -0.3323,  0.2006]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2664, -0.1641, -0.1940,  ...,  0.0190, -0.2146, -0.5713],\n",
       "          [-0.0704, -0.1749, -0.0738,  ...,  0.0110,  0.1181, -0.5996]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.9458, 0.0081, 0.0086, 0.0042, 0.0334],\n",
       "          [0.0075, 0.7515, 0.0220, 0.1600, 0.0589]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  108.973503  229.894897  319.083008  426.024414    0.949407      0  person\n",
       "  1    3.878693   78.726517  121.475708  199.604309    0.922339      0  person\n",
       "  2  508.738525  235.683838  596.745361  427.420837    0.887924      0  person\n",
       "  3   10.424515   42.831314  491.612488  181.139954    0.809563      8    boat\n",
       "  4  309.435364  212.015594  636.703186  425.629395    0.659670      8    boat,\n",
       "  'caption': ['a man with white hat pushing a bot',\n",
       "   'boat man ready to ship the boat'],\n",
       "  'bbox_target': [105.82, 231.93, 215.48, 193.35]},\n",
       " 388: {'image_emb': tensor([[ 0.1857,  0.2305,  0.3511,  ...,  0.3855,  0.1360,  0.0958],\n",
       "          [-0.3911,  0.3704, -0.0837,  ...,  0.0058,  0.0268,  0.1285],\n",
       "          [ 0.1331,  0.2585, -0.0119,  ...,  0.3220,  0.1176,  0.0671]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1200, -0.2328,  0.1639,  ...,  0.1548, -0.0690, -0.0327],\n",
       "          [-0.2455,  0.1134, -0.0358,  ...,  0.1602, -0.0041, -0.0730]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.3750e-01, 5.9891e-04, 5.6201e-01],\n",
       "          [1.5295e-01, 6.9351e-03, 8.3984e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0   54.461220   23.805088  610.055847  387.388916    0.915645      1  bicycle\n",
       "  1   57.567120  305.178345  364.785828  611.368347    0.773795     16      dog\n",
       "  2  334.881714  215.231445  364.182861  256.922455    0.251902     39   bottle,\n",
       "  'caption': ['A blue bicycle.', 'Handbar, seat and mud guard of a cycle'],\n",
       "  'bbox_target': [215.03, 31.01, 390.08, 325.3]},\n",
       " 389: {'image_emb': tensor([[ 0.0037,  0.2942, -0.1332,  ...,  0.7500,  0.4719,  0.0176],\n",
       "          [-0.0204,  0.2712,  0.1050,  ...,  1.1270, -0.0719, -0.1068],\n",
       "          [ 0.2120, -0.3037,  0.2703,  ...,  0.8750, -0.2002,  0.1738],\n",
       "          [ 0.0784,  0.0435,  0.4766,  ...,  0.8345, -0.2014,  0.1886],\n",
       "          [ 0.0455, -0.0331,  0.0761,  ...,  0.6665, -0.1594, -0.0519]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-1.1520e-02, -4.8853e-01,  6.1426e-01, -1.8054e-01,  2.3163e-02,\n",
       "            1.4685e-01, -2.8662e-01, -6.4697e-01, -9.8755e-02,  1.9946e-01,\n",
       "            4.2651e-01,  3.9526e-01, -2.7026e-01, -2.0923e-01,  3.5303e-01,\n",
       "           -2.4133e-01,  3.6792e-01, -1.8250e-01, -9.0866e-03,  3.4985e-01,\n",
       "            1.6748e-01,  1.5149e-01,  2.3669e-01, -2.6611e-01, -3.4155e-01,\n",
       "            1.0120e-01,  1.3513e-01, -9.4238e-02, -5.1367e-01, -3.4546e-01,\n",
       "            1.2988e-01,  2.9984e-02, -3.4277e-01,  7.8369e-02, -5.9473e-01,\n",
       "           -3.7567e-02, -5.5908e-02,  1.1365e-01, -3.3455e-03,  1.4954e-02,\n",
       "           -1.5552e-01, -1.1707e-01,  1.8127e-01, -1.5955e-01, -3.6279e-01,\n",
       "           -2.1576e-02, -1.6833e-01, -9.8877e-02,  7.2754e-02, -3.0005e-01,\n",
       "            3.3496e-01,  1.8237e-01, -6.7688e-02, -1.5823e-02, -4.3140e-01,\n",
       "           -1.3708e-01, -5.5939e-02,  2.7451e-02,  4.7339e-01, -2.8052e-01,\n",
       "           -3.1348e-01, -3.5126e-02,  7.0374e-02,  2.9541e-02,  5.9662e-02,\n",
       "            3.2227e-01,  2.0351e-03,  2.5726e-02, -3.0981e-01,  6.4514e-02,\n",
       "           -4.8401e-02, -3.2666e-01,  2.5439e-01,  1.5149e-01,  1.2128e-01,\n",
       "           -1.2901e-02,  1.2091e-01, -2.0813e-01,  6.4148e-02, -4.9219e-01,\n",
       "            7.6843e-02, -1.2311e-01,  1.8970e-01,  4.1779e-02, -1.9446e-01,\n",
       "           -7.8857e-02,  8.6136e-03, -7.1655e-02, -1.6443e-01, -5.7178e-01,\n",
       "            8.4045e-02, -2.6123e-01, -1.2207e+00,  5.9082e-01, -2.4673e-02,\n",
       "            3.5547e-01, -1.5100e-01,  7.2571e-02,  1.4893e-01, -1.4380e-01,\n",
       "            2.2998e-01,  3.9819e-01, -1.5533e-02, -2.0190e-01, -4.0649e-01,\n",
       "            1.2830e-01, -3.1769e-02, -6.1371e-02,  6.3965e-02, -2.1228e-01,\n",
       "            3.6914e-01,  1.7810e-01,  3.4033e-01,  1.6333e-01,  8.9539e-02,\n",
       "            1.8628e-01, -1.7004e-01,  3.0859e-01, -8.3984e-02, -2.5049e-01,\n",
       "           -8.5205e-02, -1.9556e-01, -5.7422e-01, -5.0812e-02, -1.9812e-01,\n",
       "            4.3983e-03,  3.6670e-01, -4.7974e-01, -1.1206e-01, -6.5674e-02,\n",
       "            2.8271e-01, -3.6499e-01,  1.4050e-01,  4.3516e+00, -2.0203e-01,\n",
       "           -3.4607e-02, -1.1505e-01, -3.4009e-01,  1.0681e-01, -2.1423e-01,\n",
       "           -8.3017e-04,  2.2491e-02,  2.4353e-01,  4.1138e-01, -1.1192e-02,\n",
       "           -1.3879e-01,  2.6489e-01, -1.8250e-01, -3.4546e-01, -7.2144e-02,\n",
       "           -4.8126e-02,  1.3354e-01, -2.1744e-02,  5.0098e-01, -3.1104e-01,\n",
       "            2.2546e-01, -2.0886e-01, -4.5386e-01,  2.4475e-02, -2.7148e-01,\n",
       "            3.3569e-01, -2.8662e-01, -4.0619e-02,  1.7285e-01, -4.1016e-01,\n",
       "           -1.3037e-01, -1.9006e-01,  6.6223e-03, -3.6743e-02,  4.9957e-02,\n",
       "           -1.9421e-01, -3.2544e-01,  1.1609e-01, -1.3599e-01,  1.3626e-02,\n",
       "            4.7272e-02, -2.2537e-02, -1.8591e-01, -3.0981e-01,  1.7151e-01,\n",
       "           -1.4160e-01,  2.6758e-01, -5.5176e-01,  2.6294e-01, -4.9951e-01,\n",
       "           -3.3356e-02,  1.6138e-01, -1.7957e-01,  6.5979e-02, -6.3379e-01,\n",
       "            1.4526e-01,  2.0081e-01, -7.0740e-02, -3.8745e-01, -3.0151e-01,\n",
       "           -3.3130e-01, -3.9673e-01,  1.4404e-01,  2.4280e-01, -4.3848e-01,\n",
       "            2.7856e-01,  2.6123e-01, -8.4656e-02, -6.4453e-02,  3.8330e-02,\n",
       "           -3.3252e-01,  5.4077e-02, -7.8064e-02, -4.4238e-01, -2.1887e-01,\n",
       "            6.0596e-01, -3.0298e-01,  2.2009e-01, -7.8918e-02, -1.0687e-01,\n",
       "            1.2421e-01,  1.5576e-01,  7.4829e-02,  2.5415e-01,  1.6394e-01,\n",
       "           -3.4058e-01,  2.5726e-02,  2.4744e-01,  5.1537e-03, -8.1360e-02,\n",
       "            6.8298e-02, -4.0771e-01, -3.3667e-01,  7.1838e-02, -3.8025e-02,\n",
       "           -1.8738e-01,  7.3059e-02,  1.2927e-01,  2.3352e-01,  1.3110e-01,\n",
       "            2.3376e-01,  1.9055e-01, -1.5894e-01, -2.6047e-02, -6.0693e-01,\n",
       "            2.7557e-02,  1.0399e-02,  1.3806e-01, -2.2302e-01, -7.4036e-02,\n",
       "            5.1086e-02, -3.2446e-01,  1.8018e-01,  4.0063e-01, -3.7134e-01,\n",
       "           -1.8591e-01, -2.5586e-01,  1.1279e-01, -7.7881e-02,  1.6187e-01,\n",
       "           -1.3538e-01,  4.5361e-01,  2.1790e-01,  3.4912e-01,  8.7097e-02,\n",
       "            1.8219e-02, -1.7900e-03, -9.7046e-02, -6.6406e-02,  1.7493e-01,\n",
       "            3.3521e-01,  2.2192e-01, -2.1155e-01, -1.7834e-01,  3.1641e-01,\n",
       "           -2.7710e-01,  6.2646e-01, -1.0046e-01,  6.1615e-02,  5.2539e-01,\n",
       "           -4.2152e-03,  5.8716e-02, -9.5642e-02,  3.7140e-02, -5.2063e-02,\n",
       "           -1.9867e-02, -5.5029e-01, -2.1252e-01,  2.5610e-01,  4.8633e-01,\n",
       "            6.2683e-02,  3.7109e-01,  8.2016e-03, -7.4585e-02, -4.9744e-02,\n",
       "           -1.9299e-01, -4.0381e-01, -1.7432e-01,  1.6968e-02, -2.9077e-01,\n",
       "            8.9478e-02, -1.9189e-01, -4.2529e-01,  1.1670e-01,  2.1558e-01,\n",
       "           -3.2562e-02, -3.3789e-01,  5.9784e-02, -2.2144e-01, -3.7744e-01,\n",
       "           -2.6562e-01,  1.6174e-01,  4.4336e-01,  4.1565e-02, -8.0017e-02,\n",
       "           -2.3169e-01,  3.0103e-01,  4.3438e+00, -6.4087e-02, -4.6460e-01,\n",
       "           -1.8799e-01, -4.4556e-02, -3.8428e-01,  2.5122e-01,  2.2839e-01,\n",
       "            7.5500e-02,  1.4075e-01,  1.2177e-01, -2.4792e-01,  2.9468e-01,\n",
       "           -2.9346e-01,  1.0529e-01,  3.3081e-02,  1.6736e-01, -1.8096e+00,\n",
       "            1.7285e-01, -2.3865e-01,  1.9287e-01,  2.6337e-02, -2.2446e-02,\n",
       "           -2.0850e-01,  2.1240e-01,  4.3042e-01,  4.8291e-01,  2.5952e-01,\n",
       "           -1.9836e-01, -5.7739e-02, -3.7476e-02, -5.8447e-01, -3.0933e-01,\n",
       "           -3.0444e-01,  2.9834e-01, -9.7778e-02, -2.4890e-01,  3.6719e-01,\n",
       "           -1.9653e-01,  1.8494e-01,  4.6326e-02, -1.9093e-03,  1.4819e-01,\n",
       "           -8.4961e-02, -6.8164e-01, -1.8188e-01,  1.4906e-03,  3.1055e-01,\n",
       "            3.2129e-01, -2.9240e-03, -2.9175e-01, -2.2485e-01, -1.1615e-01,\n",
       "           -5.7520e-01, -2.3303e-01,  4.8981e-02, -2.1021e-01,  6.4941e-02,\n",
       "           -7.0114e-03,  2.7881e-01,  3.8052e-03,  2.9565e-01, -3.6426e-01,\n",
       "            3.4717e-01, -5.3741e-02, -4.9146e-01,  2.8442e-01,  1.6174e-01,\n",
       "           -1.3725e-02,  3.1342e-02, -3.7622e-01, -1.9714e-01,  9.3079e-02,\n",
       "           -1.6711e-01, -6.8359e-02, -2.6831e-01,  1.9043e-01, -2.8589e-01,\n",
       "           -1.0176e+00,  1.3770e-01, -9.7656e-02,  1.7114e-01,  1.2103e-01,\n",
       "            5.6738e-01,  2.2852e-01, -1.5869e-01,  1.6589e-01,  5.3662e-01,\n",
       "            2.0459e-01,  3.4424e-01,  3.4515e-02,  2.1228e-01, -2.1594e-01,\n",
       "            1.8326e-02, -2.0508e-01, -4.8889e-02,  2.4463e-01,  2.7197e-01,\n",
       "           -3.7628e-02,  2.6147e-01,  1.7166e-03,  4.1797e-01,  6.8164e-01,\n",
       "            3.7524e-01, -2.2293e-02, -5.0195e-01, -4.0210e-01, -3.1836e-01,\n",
       "           -2.2571e-01,  1.2408e-01, -1.7004e-01, -2.6050e-01, -4.3396e-02,\n",
       "           -2.7197e-01,  1.0736e-01, -6.3354e-02,  8.4351e-02, -5.5298e-02,\n",
       "           -3.9185e-01, -2.6855e-02, -1.0986e-01, -8.5632e-02, -4.6570e-02,\n",
       "            1.2280e-01, -4.5532e-01, -1.9019e-01, -8.2568e-01,  4.4751e-01,\n",
       "           -1.0480e-01,  5.9814e-02, -8.1787e-03, -1.0034e-01, -1.1029e-01,\n",
       "           -3.3112e-02, -3.1299e-01,  1.1365e-01,  1.8408e-01, -3.4521e-01,\n",
       "           -2.4683e-01, -4.3945e-01, -1.2683e-01, -3.0859e-01,  1.0590e-02,\n",
       "           -5.9277e-01, -2.0898e-01,  2.2339e-01, -1.0950e-01, -4.5947e-01,\n",
       "           -3.9404e-01,  5.7568e-01, -3.7781e-02, -1.0083e-01, -1.0052e-01,\n",
       "            9.5642e-02,  2.9724e-02, -5.4840e-02, -1.7261e-01,  2.0288e-01,\n",
       "            1.0394e-01, -1.7371e-01, -4.4995e-01,  1.5637e-01,  2.2119e-01,\n",
       "            1.5796e-01,  7.8201e-03,  2.2241e-01, -8.4534e-02,  3.1885e-01,\n",
       "            2.6947e-02, -6.0742e-01,  1.3989e-01,  6.5247e-02,  1.0138e-01,\n",
       "            3.6133e-01,  3.2153e-01,  1.2299e-02,  3.3838e-01, -2.0593e-01,\n",
       "           -1.3489e-01,  4.9194e-01, -1.7114e-01,  6.5625e-01,  1.1365e-01,\n",
       "            2.4890e-01,  4.6680e-01, -1.0504e-01,  1.0364e-01,  2.3865e-02,\n",
       "           -1.3818e-01, -4.2505e-01, -1.7969e-01,  7.7820e-02, -7.8735e-02,\n",
       "            2.4072e-01,  3.3350e-01, -2.1387e-01,  8.5938e-02,  8.5205e-01,\n",
       "           -5.2393e-01,  1.1499e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.7042e-05, 2.5024e-03, 8.9111e-01, 1.6830e-02, 8.9600e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  334.711609   89.723724  442.700989  192.970917    0.936341     16       dog\n",
       "  1  178.515182   98.272064  280.868408  225.812744    0.905998      0    person\n",
       "  2   48.747681  166.432434  581.000793  388.743591    0.886312      7     truck\n",
       "  3    1.245941  272.170563  639.514648  477.131104    0.825714      7     truck\n",
       "  4  278.815063  182.840210  387.348450  230.594421    0.382237     28  suitcase,\n",
       "  'caption': ['atv fourwheelers'],\n",
       "  'bbox_target': [49.51, 165.74, 531.66, 227.09]},\n",
       " 390: {'image_emb': tensor([[ 0.2444, -0.3118, -0.0250,  ...,  0.4355,  0.0811, -0.4312],\n",
       "          [ 0.2744, -0.0373,  0.0306,  ...,  0.5312,  0.1938, -0.1791],\n",
       "          [ 0.7100, -0.1146, -0.0825,  ...,  0.5259,  0.3562, -0.0601],\n",
       "          [ 0.0514, -0.2090, -0.0706,  ...,  0.6187,  0.0693, -0.0586],\n",
       "          [ 0.0297, -0.4001,  0.0275,  ...,  0.4277,  0.0066, -0.1489]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3818, -0.1232, -0.2249,  ..., -0.0007,  0.1727, -0.2590],\n",
       "          [ 0.1287,  0.0820, -0.1748,  ..., -0.4434,  0.0768, -0.4526]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.8659e-03, 2.4866e-01, 7.4219e-01, 7.4339e-04, 5.6982e-04],\n",
       "          [5.5481e-02, 3.7329e-01, 5.3467e-01, 1.3809e-02, 2.2766e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  104.552666   82.327789  343.866394  383.037079    0.947121     23  giraffe\n",
       "  1  159.217911  260.496674  297.689270  398.206940    0.932539     14     bird\n",
       "  2  396.809998  251.934631  555.661560  398.877991    0.911339     14     bird\n",
       "  3  401.787994  168.722595  494.304352  329.830017    0.884823     23  giraffe,\n",
       "  'caption': ['An ostrich walking to the right.',\n",
       "   'The bird that is on the right of the picture'],\n",
       "  'bbox_target': [397.48, 253.15, 156.17, 154.31]},\n",
       " 391: {'image_emb': tensor([[ 0.6069,  0.2097, -0.1453,  ...,  0.6196, -0.1643,  0.4380],\n",
       "          [-0.0930,  0.4807, -0.0469,  ...,  1.2246,  0.3604,  0.2494],\n",
       "          [ 0.4526,  0.0929, -0.3257,  ...,  0.7153, -0.1948,  0.4014],\n",
       "          [-0.3662,  0.1497, -0.1388,  ...,  1.3320, -0.0584,  0.0506],\n",
       "          [-0.3735,  0.4846, -0.2316,  ...,  1.4141, -0.1057, -0.1632],\n",
       "          [ 0.4470, -0.1823, -0.3306,  ...,  0.7578,  0.0817,  0.3347]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0803, -0.0374,  0.0920,  ..., -0.0569, -0.5034, -0.0243],\n",
       "          [ 0.0645, -0.0249,  0.2959,  ..., -0.0345, -0.3301,  0.0453]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.6738e-01, 4.3809e-05, 1.9922e-01, 2.7001e-05, 2.9039e-04, 2.3291e-01],\n",
       "          [5.3516e-01, 4.2391e-04, 2.5269e-01, 1.1230e-04, 2.2202e-03, 2.0947e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    51.150467  119.522964  284.018555  312.441284    0.913774     48   \n",
       "  1    97.541313    0.000000  203.549561  121.921417    0.912671     41   \n",
       "  2   182.620544  205.889801  360.628906  356.285004    0.875867     48   \n",
       "  3     0.509609    0.898544   88.038940  221.601318    0.745573     39   \n",
       "  4   483.675537  268.403870  585.700684  382.272644    0.701531     42   \n",
       "  5     8.763611   21.027695  638.685059  419.662659    0.652650     60   \n",
       "  6   560.754333  237.309204  640.000000  408.723328    0.620236     42   \n",
       "  7   550.202820  237.883606  640.000000  423.860596    0.445765     44   \n",
       "  8   544.618835  270.453827  639.352600  397.017914    0.347377     43   \n",
       "  9     9.577129    0.000000   94.496780   52.383636    0.304029      0   \n",
       "  10  614.303955  109.880737  639.814697  174.131012    0.251817     45   \n",
       "  \n",
       "              name  \n",
       "  0       sandwich  \n",
       "  1            cup  \n",
       "  2       sandwich  \n",
       "  3         bottle  \n",
       "  4           fork  \n",
       "  5   dining table  \n",
       "  6           fork  \n",
       "  7          spoon  \n",
       "  8          knife  \n",
       "  9         person  \n",
       "  10          bowl  ,\n",
       "  'caption': ['A half eaten sandwich sitting on a plate.',\n",
       "   'A partly eaten half of a sandwich to the right side of a white plate.'],\n",
       "  'bbox_target': [180.49, 205.67, 181.17, 149.72]},\n",
       " 392: {'image_emb': tensor([[-0.0448,  0.4382, -0.1725,  ...,  1.0410, -0.2783,  0.0258],\n",
       "          [ 0.1117,  0.2817, -0.2164,  ...,  0.5942,  0.0652,  0.0641],\n",
       "          [ 0.1285,  0.2242, -0.1711,  ...,  0.9053,  0.0635, -0.2056],\n",
       "          ...,\n",
       "          [ 0.0344,  0.0865, -0.3884,  ...,  0.9722,  0.4265,  0.1405],\n",
       "          [ 0.0651,  0.4321, -0.2440,  ...,  0.5835,  0.2018,  0.1592],\n",
       "          [ 0.5117,  0.0179,  0.4670,  ...,  0.5356,  0.5083,  0.4036]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0189, -0.0638, -0.1543,  ..., -0.2184,  0.1769,  0.1885],\n",
       "          [-0.0157, -0.1218, -0.3733,  ..., -0.2983,  0.2769,  0.3733]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0633e-04, 9.9170e-01, 6.5506e-05, 5.0468e-03, 2.6588e-03, 2.7418e-06,\n",
       "           1.3447e-04, 6.6578e-05],\n",
       "          [1.4488e-02, 2.8366e-02, 3.3021e-04, 8.4229e-01, 1.0876e-01, 2.8849e-05,\n",
       "           2.0039e-04, 5.5885e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  351.086182   81.381226  488.258545  474.464966    0.935683      0   \n",
       "  1  500.729614  207.208221  605.966309  362.584930    0.924178     56   \n",
       "  2  110.999466  242.547058  161.171188  295.250793    0.891700     65   \n",
       "  3  517.098328    1.807434  617.803894  250.086060    0.891040      0   \n",
       "  4  158.486176  109.879532  262.466675  475.422546    0.882164      0   \n",
       "  5  418.301239  344.628662  453.049347  382.848083    0.815031     65   \n",
       "  6   18.738411  362.779297  190.631805  477.535339    0.713524     28   \n",
       "  7  314.164429  174.275528  334.666382  345.439453    0.647272     25   \n",
       "  8  581.829651  405.876648  639.494568  476.899109    0.497180      0   \n",
       "  9  581.586548  405.805206  639.168579  478.159668    0.261778     32   \n",
       "  \n",
       "            name  \n",
       "  0       person  \n",
       "  1        chair  \n",
       "  2       remote  \n",
       "  3       person  \n",
       "  4       person  \n",
       "  5       remote  \n",
       "  6     suitcase  \n",
       "  7     umbrella  \n",
       "  8       person  \n",
       "  9  sports ball  ,\n",
       "  'caption': ['The woman in the pink shirt up on a chair, but not the chair legs.',\n",
       "   'woman standing on chair in background wearing pink shirt'],\n",
       "  'bbox_target': [516.76, 0.0, 96.21, 243.03]},\n",
       " 393: {'image_emb': tensor([[ 0.2439, -0.1577, -0.0257,  ..., -0.1302,  0.5645,  0.0183],\n",
       "          [ 0.2751, -0.2791,  0.0293,  ...,  0.0791,  0.4048,  0.0346],\n",
       "          [ 0.2935,  0.1124, -0.2032,  ...,  0.6099,  0.2013,  0.1024],\n",
       "          [ 0.2612, -0.3130,  0.2507,  ..., -0.4216,  0.3635,  0.2185]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0110,  0.0294, -0.1932,  ..., -0.1785, -0.1176, -0.3286],\n",
       "          [ 0.2939, -0.1735, -0.3123,  ...,  0.3020,  0.0896, -0.2576]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.4282, 0.4629, 0.0163, 0.0926],\n",
       "          [0.4331, 0.4070, 0.0078, 0.1521]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  357.127716   12.691315  581.947998  403.840057    0.954245      0   \n",
       "  1  158.790482   50.700516  439.744080  410.425415    0.952897      0   \n",
       "  2  419.500214  342.073914  478.053009  400.964478    0.879453     32   \n",
       "  \n",
       "            name  \n",
       "  0       person  \n",
       "  1       person  \n",
       "  2  sports ball  ,\n",
       "  'caption': ['Man in white and blue uniform kicking a red and white soccer ball behind the man in a blue uniform.',\n",
       "   'A soccer player with the number 11 jersey.'],\n",
       "  'bbox_target': [159.51, 48.63, 281.71, 359.21]},\n",
       " 394: {'image_emb': tensor([[-3.2043e-04,  2.4915e-01,  1.5283e-01,  ...,  3.8623e-01,\n",
       "            4.1260e-01, -1.6235e-01],\n",
       "          [-1.4294e-01,  2.9346e-01, -6.3660e-02,  ...,  1.0781e+00,\n",
       "            3.3496e-01, -3.0737e-01],\n",
       "          [-1.8250e-02, -9.0942e-02,  7.5928e-02,  ...,  3.8818e-01,\n",
       "            4.6191e-01, -1.0645e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0476, -0.0328, -0.4871,  ..., -0.1168, -0.2175, -0.0075],\n",
       "          [-0.1204, -0.2595,  0.1543,  ...,  0.5820, -0.1937, -0.2915]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.3438e-01, 1.5976e-02, 2.4976e-01],\n",
       "          [5.1514e-01, 5.5790e-04, 4.8413e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0    0.278572  246.874878  161.414490  396.429382    0.932297     17  horse\n",
       "  1  192.818024  270.776886  322.057709  360.089447    0.744829     17  horse\n",
       "  2  432.018890  255.089355  574.086670  346.461365    0.564547     17  horse,\n",
       "  'caption': ['a gray horse on the left', 'white horse in stable stall'],\n",
       "  'bbox_target': [1.44, 248.81, 159.64, 163.95]},\n",
       " 395: {'image_emb': tensor([[-0.4575,  0.0553, -0.0308,  ...,  0.9033,  0.1111, -0.2415],\n",
       "          [-0.3828,  0.1710, -0.1890,  ...,  0.6543, -0.0963,  0.1578],\n",
       "          [-0.1136, -0.2128, -0.0712,  ...,  1.1143,  0.0800, -0.3970],\n",
       "          [-0.3015,  0.3772,  0.1326,  ...,  0.6396, -0.0427,  0.0663],\n",
       "          [-0.2218,  0.7354, -0.1743,  ...,  0.8638,  0.2382, -0.3091],\n",
       "          [-0.4021,  0.0703,  0.3196,  ...,  0.3489, -0.0209, -0.1195]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0691, -0.0178, -0.4141,  ...,  0.2261, -0.1418,  0.0314],\n",
       "          [ 0.2148,  0.0765, -0.2485,  ...,  0.0825,  0.0788, -0.0141]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0910e-02, 6.0498e-01, 1.5092e-04, 3.4473e-01, 2.5375e-02, 1.3580e-02],\n",
       "          [1.1682e-01, 7.3828e-01, 8.7357e-03, 1.0803e-01, 6.0368e-04, 2.7328e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.849319   20.611008  350.000793  341.127502    0.941043      0   \n",
       "  1  386.923126  143.826614  637.695068  423.864502    0.933394      0   \n",
       "  2    1.193542  303.016144  183.126877  423.307251    0.914322     13   \n",
       "  3  280.811371  231.000580  637.462402  423.498291    0.901279     13   \n",
       "  4  330.836945  202.506622  363.908539  237.534088    0.850980     67   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2       bench  \n",
       "  3       bench  \n",
       "  4  cell phone  ,\n",
       "  'caption': ['The woman in blue sweater leaning showing phone to th eother woman.',\n",
       "   'This is a woman wearing a grey cardigan and black pants.'],\n",
       "  'bbox_target': [0.0, 21.64, 351.02, 322.17]},\n",
       " 396: {'image_emb': tensor([[-0.1577,  0.0882,  0.1218,  ...,  0.7720, -0.0767, -0.1049],\n",
       "          [-0.1180,  0.0931, -0.2362,  ...,  0.8218, -0.3992,  0.4368],\n",
       "          [-0.1740, -0.0813, -0.1288,  ...,  0.9614, -0.0967,  0.4094],\n",
       "          [ 0.2109, -0.2720, -0.4990,  ...,  0.9263,  0.0416, -0.1877],\n",
       "          [ 0.1747, -0.1365, -0.3479,  ...,  0.8618,  0.1816, -0.1194],\n",
       "          [-0.0466, -0.0421,  0.1666,  ...,  0.6328, -0.3284,  0.2078]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0801, -0.3572,  0.2961,  ...,  0.0158, -0.0422, -0.3225],\n",
       "          [-0.0350, -0.3577,  0.1802,  ...,  0.0525, -0.1820, -0.2695]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.7294e-03, 7.3340e-01, 6.3438e-03, 5.3227e-05, 4.2081e-05, 2.5732e-01],\n",
       "          [1.6642e-03, 9.4678e-01, 1.9760e-03, 3.7372e-05, 3.5107e-05, 4.9408e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  284.118652  192.773010  467.154846  272.114380    0.863369      5     bus\n",
       "  1  138.835205  156.933441  269.813049  251.912689    0.862361      5     bus\n",
       "  2  235.161774  141.481201  383.267853  263.494446    0.831864      5     bus\n",
       "  3  201.185944  224.098602  214.222961  263.968231    0.775697      0  person\n",
       "  4  221.849304  220.571503  236.113129  253.735809    0.716841      0  person,\n",
       "  'caption': ['A white single story bus.', 'an old white bus'],\n",
       "  'bbox_target': [137.51, 157.84, 130.79, 92.47]},\n",
       " 397: {'image_emb': tensor([[ 0.2111,  0.7607, -0.0140,  ...,  0.8311, -0.0329, -0.0580],\n",
       "          [-0.1517,  0.1066, -0.1754,  ...,  0.6914,  0.0400,  0.0432],\n",
       "          [-0.2050,  0.4514, -0.2472,  ...,  1.0254, -0.0602, -0.2137],\n",
       "          ...,\n",
       "          [-0.2886, -0.0404, -0.3191,  ...,  1.1436, -0.2585,  0.1431],\n",
       "          [-0.1396,  0.3459, -0.6230,  ...,  1.1338,  0.0244, -0.1049],\n",
       "          [-0.0374,  0.1752, -0.1644,  ...,  0.9092, -0.0501, -0.1600]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0661,  0.1605, -0.4409,  ...,  0.5913,  0.2214, -0.1338],\n",
       "          [ 0.1787,  0.1279, -0.2347,  ...,  0.1968,  0.0373, -0.2244]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7899e-02, 3.0136e-03, 2.3723e-05, 9.7705e-01, 6.7949e-06, 4.3416e-04,\n",
       "           7.0870e-05, 2.1160e-04, 6.3777e-06, 1.8477e-05, 2.1160e-04, 9.3365e-04,\n",
       "           3.5429e-04],\n",
       "          [4.0070e-02, 2.0580e-03, 2.7597e-05, 9.5557e-01, 6.5565e-07, 5.0068e-06,\n",
       "           3.7670e-05, 1.0246e-04, 5.0068e-06, 3.4571e-06, 2.2376e-04, 1.6031e-03,\n",
       "           2.3448e-04]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   105.662491  135.265778  388.904968  345.584625    0.950401     63   \n",
       "  1   467.702057  177.970428  509.999908  239.607697    0.918745     41   \n",
       "  2     0.272602  332.351196   71.760895  400.959412    0.888070     67   \n",
       "  3   451.125214  123.604782  614.344360  241.320496    0.884738     63   \n",
       "  4   463.431335   31.263397  525.111450  102.885773    0.853345      0   \n",
       "  5   573.564148  200.990784  640.000000  238.127930    0.840836      0   \n",
       "  6   423.445435   89.723969  515.242004  167.577057    0.813404      0   \n",
       "  7    68.752472  333.800720  640.000000  476.351501    0.766792     73   \n",
       "  8   542.723877  273.421936  640.000000  391.564514    0.756295     73   \n",
       "  9   185.005005  126.534973  223.559784  136.566925    0.751421     56   \n",
       "  10  403.494476  166.596176  459.972931  210.660034    0.718401     56   \n",
       "  11    0.164337  161.625092  143.806564  335.754730    0.709522     24   \n",
       "  12  356.937164  121.303314  388.602753  146.014435    0.695143     63   \n",
       "  13  346.564056   97.255737  397.136017  140.859375    0.658745      0   \n",
       "  14  350.200684  200.510559  467.805969  238.646118    0.644292     73   \n",
       "  15   31.476427  131.747604   81.817123  167.479141    0.640948     56   \n",
       "  16    1.197903  413.464783   68.740738  478.225525    0.611286     73   \n",
       "  17  223.750061  112.533020  246.448578  135.142517    0.581847      0   \n",
       "  18    0.303050  141.813202   19.401245  168.685852    0.556758     56   \n",
       "  19   68.014290  130.256790  101.148369  167.727310    0.431649     56   \n",
       "  20  240.189758  114.294418  285.798523  136.732681    0.389774      0   \n",
       "  21    2.760406  235.901306  634.677490  478.374817    0.373970     60   \n",
       "  22    0.000000  114.708206   13.880433  134.790207    0.359163     63   \n",
       "  23  340.644928  153.865585  379.123932  213.880768    0.350167     56   \n",
       "  24  131.102631  143.717804  143.149750  179.169922    0.320794     56   \n",
       "  25    0.000000  115.271439   18.616238  166.787094    0.250893     56   \n",
       "  \n",
       "              name  \n",
       "  0         laptop  \n",
       "  1            cup  \n",
       "  2     cell phone  \n",
       "  3         laptop  \n",
       "  4         person  \n",
       "  5         person  \n",
       "  6         person  \n",
       "  7           book  \n",
       "  8           book  \n",
       "  9          chair  \n",
       "  10         chair  \n",
       "  11      backpack  \n",
       "  12        laptop  \n",
       "  13        person  \n",
       "  14          book  \n",
       "  15         chair  \n",
       "  16          book  \n",
       "  17        person  \n",
       "  18         chair  \n",
       "  19         chair  \n",
       "  20        person  \n",
       "  21  dining table  \n",
       "  22        laptop  \n",
       "  23         chair  \n",
       "  24         chair  \n",
       "  25         chair  ,\n",
       "  'caption': ['A black laptop and white cup of coffe with a hand on the touch pad',\n",
       "   'A laptop with a coffee cup.'],\n",
       "  'bbox_target': [449.8, 125.12, 165.03, 116.5]},\n",
       " 398: {'image_emb': tensor([[-0.0610,  0.1225, -0.4507,  ...,  0.5581,  0.3396,  0.1268],\n",
       "          [-0.0359,  0.2561, -0.4185,  ...,  1.1260,  0.1615,  0.0515],\n",
       "          [-0.1584,  0.0638, -0.5137,  ...,  0.6440,  0.1869,  0.1305],\n",
       "          ...,\n",
       "          [ 0.1403, -0.1078, -0.2781,  ...,  0.9482,  0.1284, -0.3618],\n",
       "          [ 0.1214,  0.0523, -0.3091,  ...,  0.6138,  0.1615,  0.1702],\n",
       "          [-0.2371, -0.0136, -0.0323,  ..., -0.1493,  0.2134,  0.0107]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0947,  0.4604,  0.1624,  ..., -0.1904, -0.3232, -0.1714],\n",
       "          [ 0.3433,  0.1848,  0.0161,  ...,  0.0305, -0.0249, -0.1083]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.3789e-01, 2.3537e-03, 1.4111e-01, 1.6846e-02, 3.0994e-06, 2.5630e-06,\n",
       "           1.6689e-06, 7.7486e-07, 3.5763e-07, 1.9817e-03],\n",
       "          [9.9268e-01, 1.1988e-03, 2.5387e-03, 3.5801e-03, 3.5763e-07, 8.7619e-06,\n",
       "           6.5565e-07, 1.7881e-07, 8.3447e-07, 2.3007e-05]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   320.645538  184.194153  521.295044  475.770081    0.923244      0   \n",
       "  1     0.433956  148.153076   92.264771  470.936279    0.914823      0   \n",
       "  2    50.700638  196.619019  273.914673  476.991638    0.903732      0   \n",
       "  3   253.179199  160.050476  393.721680  477.748535    0.854391      0   \n",
       "  4   581.808960  345.276276  622.038086  447.065430    0.822186     39   \n",
       "  5   526.763733  365.862518  561.524475  470.179932    0.777392     39   \n",
       "  6   560.692383  368.585266  587.271851  437.355896    0.758714     39   \n",
       "  7   504.237091  338.179291  524.531006  405.985382    0.753880     39   \n",
       "  8   103.735474  299.931580  130.295975  324.042480    0.702482     74   \n",
       "  9   621.783875  363.834747  639.932190  445.036438    0.618875     39   \n",
       "  10  219.741043  422.006805  266.931732  477.505859    0.560196     40   \n",
       "  11  475.861145  291.455231  523.350403  351.913055    0.495816      0   \n",
       "  12  613.892334  363.749695  631.828125  442.175232    0.353262     39   \n",
       "  13  520.740662  346.000580  573.914124  404.293182    0.305931     73   \n",
       "  \n",
       "            name  \n",
       "  0       person  \n",
       "  1       person  \n",
       "  2       person  \n",
       "  3       person  \n",
       "  4       bottle  \n",
       "  5       bottle  \n",
       "  6       bottle  \n",
       "  7       bottle  \n",
       "  8        clock  \n",
       "  9       bottle  \n",
       "  10  wine glass  \n",
       "  11      person  \n",
       "  12      bottle  \n",
       "  13        book  ,\n",
       "  'caption': ['A woman with red hair is standing next to a woman and laughing',\n",
       "   'A raucous red-haired woman in what appears to be a floral-printed dress.'],\n",
       "  'bbox_target': [321.44, 186.43, 198.47, 286.92]},\n",
       " 399: {'image_emb': tensor([[-0.0504,  0.7524, -0.0693,  ...,  0.7920, -0.0155,  0.0329],\n",
       "          [-0.1388,  0.5205, -0.2078,  ...,  0.8613,  0.0151, -0.2747],\n",
       "          [-0.4741,  0.5703,  0.1361,  ...,  0.6753,  0.0111, -0.2837]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1520,  0.0337, -0.4028,  ...,  0.1169,  0.1368, -0.2815],\n",
       "          [-0.2118,  0.3184, -0.1259,  ..., -0.1479,  0.0715, -0.2482]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0463, 0.1831, 0.7705],\n",
       "          [0.1506, 0.7651, 0.0845]], dtype=torch.float16),\n",
       "  'df_boxes':         xmin        ymin        xmax        ymax  confidence  class  name\n",
       "  0    0.24910  180.622116  280.537415  561.425720    0.949728     75  vase\n",
       "  1  316.07254  231.173035  479.853760  595.843506    0.929766     75  vase,\n",
       "  'caption': ['Narrower vase on the right', 'red vase without handles'],\n",
       "  'bbox_target': [316.4, 231.91, 163.6, 368.18]},\n",
       " 400: {'image_emb': tensor([[-5.6213e-02,  1.6626e-01,  2.5732e-01,  ...,  8.7549e-01,\n",
       "            1.4502e-01, -2.6562e-01],\n",
       "          [ 7.0251e-02,  3.4497e-01, -1.4748e-02,  ...,  7.3389e-01,\n",
       "            8.0261e-02, -9.3018e-02],\n",
       "          [-2.0972e-01,  2.3804e-01, -3.8232e-01,  ...,  8.2324e-01,\n",
       "           -4.1676e-04, -2.1533e-01],\n",
       "          [ 7.9712e-02,  3.1226e-01,  2.2827e-02,  ...,  8.6914e-01,\n",
       "            1.4819e-01, -6.2378e-02],\n",
       "          [ 1.2463e-01,  2.2168e-01,  5.0415e-02,  ...,  9.3408e-01,\n",
       "            1.3892e-01, -1.0052e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0745, -0.1610, -0.0015,  ...,  0.0747, -0.3848,  0.0117],\n",
       "          [-0.0625, -0.3040, -0.1932,  ...,  0.2172, -0.5010,  0.0471]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.8413e-05, 1.1780e-01, 1.0452e-02, 5.3613e-01, 3.3545e-01],\n",
       "          [4.0829e-05, 2.6562e-01, 1.1206e-03, 4.5898e-01, 2.7417e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  202.386612  102.123215  351.019470  313.313049    0.911723     15       cat\n",
       "  1  108.299973  135.230103  426.078247  456.150940    0.871914     28  suitcase\n",
       "  2  447.474854  141.913071  638.514648  475.574036    0.862800     28  suitcase\n",
       "  3    7.853241   48.092255  640.000000  478.700562    0.741094     59       bed\n",
       "  4   99.683746  130.530991  439.682037  471.325073    0.427268     59       bed,\n",
       "  'caption': ['A suitcase sitting on the bed.',\n",
       "   'Suitcase that does not have a cat on it.'],\n",
       "  'bbox_target': [447.57, 136.76, 190.27, 337.29]},\n",
       " 401: {'image_emb': tensor([[-0.1058,  0.0056, -0.0144,  ...,  0.8008,  0.1072,  0.3262],\n",
       "          [ 0.0307,  0.2146, -0.4424,  ...,  1.2764, -0.0845,  0.0405],\n",
       "          [-0.2402,  0.1656, -0.2791,  ...,  1.1162, -0.1242, -0.0211],\n",
       "          [ 0.1677,  0.0351, -0.4363,  ...,  0.9058, -0.2140, -0.0473],\n",
       "          [-0.1108,  0.3384, -0.1688,  ...,  1.3711, -0.1570,  0.0120],\n",
       "          [ 0.0031, -0.0172,  0.0517,  ...,  0.6865, -0.0547,  0.1404]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0179, -0.2230, -0.0047,  ..., -0.3184,  0.2162, -0.2180],\n",
       "          [-0.2681, -0.2427, -0.3574,  ..., -0.1980,  0.0414, -0.1533]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.1162e-02, 9.2920e-01, 4.6539e-03, 8.1635e-03, 1.0651e-02, 3.6041e-02],\n",
       "          [2.2681e-01, 9.8705e-04, 1.7395e-01, 2.8014e-05, 1.6089e-01, 4.3726e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   35.995224  113.088974  448.106934  421.739258    0.917973      0  person\n",
       "  1  426.404449  193.437988  610.447510  422.270142    0.846074     56   chair\n",
       "  2   86.415634  258.761932  189.215134  326.044037    0.775557     63  laptop\n",
       "  3   28.915203  404.153381   88.008820  425.812439    0.723926     64   mouse\n",
       "  4  103.014587  216.219666  189.801422  296.636902    0.723700     62      tv\n",
       "  5  237.032471  246.754028  261.004822  266.058044    0.550904     41     cup\n",
       "  6  167.849609  198.608704  325.533691  383.387695    0.503292      0  person\n",
       "  7  235.732285  246.842316  261.797211  285.989532    0.379437     41     cup\n",
       "  8  200.703430  343.963531  292.188568  387.433014    0.283392     56   chair,\n",
       "  'caption': ['The man in black looking at something',\n",
       "   'Two office workers, working on their computers.'],\n",
       "  'bbox_target': [35.51, 107.59, 416.48, 314.76]},\n",
       " 402: {'image_emb': tensor([[-0.1840,  0.0574, -0.2242,  ...,  0.4817,  0.1041, -0.0021],\n",
       "          [-0.3328, -0.0574,  0.0240,  ...,  0.6533,  0.5337,  0.0504],\n",
       "          [-0.3303,  0.1533, -0.2325,  ...,  0.4380,  0.1779, -0.0821],\n",
       "          [-0.1632, -0.1924, -0.1205,  ...,  0.6372,  0.0181,  0.1326]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-2.8955e-01, -1.1188e-01, -4.4174e-03, -2.2598e-02,  5.7251e-02,\n",
       "            1.0185e-02, -1.8262e-01, -6.6406e-01,  1.3989e-01, -1.0506e-02,\n",
       "           -1.2581e-02, -3.5156e-01,  5.7037e-02, -3.4766e-01,  3.6450e-01,\n",
       "           -7.8064e-02,  1.5356e-01, -6.6872e-03, -3.0444e-01,  3.7256e-01,\n",
       "            2.9492e-01,  5.0195e-01,  1.6589e-01, -2.7979e-01,  2.5497e-02,\n",
       "           -2.5425e-03, -2.8687e-01,  2.8662e-01, -5.4834e-01, -2.5098e-01,\n",
       "           -1.6394e-01,  6.9275e-02, -1.6431e-01,  3.2788e-01, -8.7463e-02,\n",
       "           -1.7236e-01, -1.8335e-01,  2.6782e-01, -2.8101e-01,  4.5502e-02,\n",
       "           -2.7515e-01,  5.8212e-03,  1.0797e-01,  4.0796e-01, -6.1133e-01,\n",
       "            4.2816e-02, -4.6936e-02,  3.6597e-01,  3.5962e-01, -1.0791e-01,\n",
       "           -2.1436e-01, -3.3716e-01,  1.9257e-02, -1.6260e-01, -5.6104e-01,\n",
       "           -1.5649e-01, -7.7087e-02, -5.3662e-01,  2.8296e-01,  4.8615e-02,\n",
       "            1.3428e-01, -4.1357e-01, -2.0911e-01,  1.3538e-01,  1.4832e-01,\n",
       "            2.8687e-01,  1.2189e-01, -4.1565e-02,  8.0273e-01, -8.1787e-02,\n",
       "           -5.7220e-03, -1.8250e-01,  3.9233e-01, -1.9055e-01,  4.1602e-01,\n",
       "           -7.3047e-01, -8.6365e-02,  1.5051e-01, -1.3538e-01, -2.1216e-01,\n",
       "            8.0261e-02,  4.5312e-01, -3.6377e-01,  8.2397e-02, -9.2041e-02,\n",
       "            1.8835e-01,  1.9519e-01,  1.0236e-01, -1.7871e-01, -2.2119e-01,\n",
       "            1.2201e-01, -7.0435e-02, -1.0967e+00,  2.0483e-01, -2.2949e-01,\n",
       "            3.4863e-01,  1.1823e-01, -6.2598e-01,  2.3267e-01, -1.1597e-01,\n",
       "           -9.4376e-03, -3.1885e-01, -3.1543e-01,  2.5659e-01,  5.8533e-02,\n",
       "           -3.4424e-01,  9.2346e-02,  5.5908e-01,  2.8027e-01, -8.5083e-02,\n",
       "            8.0017e-02,  9.4299e-02, -2.3071e-01, -2.4841e-01,  2.3169e-01,\n",
       "            9.5459e-02, -1.5527e-01,  4.3060e-02,  4.5807e-02,  3.0737e-01,\n",
       "           -2.9785e-01, -1.0391e+00,  4.9042e-02, -2.0825e-01,  4.7913e-02,\n",
       "            7.5111e-03, -7.5012e-02,  2.4756e-01,  3.6072e-02,  4.3066e-01,\n",
       "           -1.1774e-01, -1.1121e-01, -1.9580e-01,  4.1211e+00, -8.0261e-02,\n",
       "           -2.1838e-01, -4.0430e-01, -3.5962e-01, -6.8298e-02, -7.6782e-02,\n",
       "           -4.9194e-01,  1.8860e-02,  6.0692e-03,  1.9312e-01,  5.8746e-02,\n",
       "           -5.2948e-02,  1.4661e-01, -2.5659e-01, -1.6980e-01, -9.3506e-02,\n",
       "            3.5571e-01, -4.2114e-01,  3.9478e-01, -1.3843e-01,  2.0654e-01,\n",
       "           -5.3906e-01,  6.2061e-01, -4.0497e-02, -1.3623e-01,  1.4233e-01,\n",
       "            1.4099e-01, -2.1204e-01, -3.2666e-01, -3.0566e-01,  1.6809e-01,\n",
       "            3.0078e-01,  2.2668e-01, -5.9326e-01,  2.1378e-02,  6.5117e-03,\n",
       "            6.2225e-02, -3.9917e-01,  3.5498e-01, -1.7151e-01, -2.8516e-01,\n",
       "            2.3401e-01,  1.2903e-01,  2.8717e-02, -6.3525e-01, -8.1238e-02,\n",
       "           -3.2080e-01,  6.4453e-02,  2.5391e-01,  2.4463e-01, -3.8257e-01,\n",
       "           -1.6345e-01, -3.9746e-01, -2.9883e-01, -6.9702e-02,  9.1736e-02,\n",
       "           -3.2617e-01,  7.5867e-02, -2.0764e-01,  7.2266e-02,  2.4582e-02,\n",
       "            2.9068e-02, -2.5708e-01, -1.8585e-02, -4.9048e-01,  2.1774e-02,\n",
       "           -4.9683e-02, -3.1030e-01, -1.1804e-01, -9.5581e-02,  1.7822e-01,\n",
       "           -8.3801e-02, -4.2725e-02, -2.1350e-01,  2.4097e-01, -3.7305e-01,\n",
       "            1.4880e-01,  7.2656e-01,  3.8306e-01,  1.2817e-01,  3.8867e-01,\n",
       "            4.0161e-02, -1.3745e-01, -1.4050e-01,  7.1240e-01, -4.4037e-02,\n",
       "            2.7295e-01, -2.8671e-02,  3.5229e-01,  3.4204e-01,  7.1411e-02,\n",
       "            1.3757e-01, -4.4141e-01, -3.3374e-01, -1.3574e-01, -2.7124e-01,\n",
       "            2.2742e-01, -4.6814e-02,  1.0266e-01,  4.2090e-01,  2.4805e-01,\n",
       "           -6.9962e-03,  1.0052e-01, -4.8615e-02, -6.0010e-01, -5.0751e-02,\n",
       "            6.1981e-02,  1.3281e-01, -5.4626e-02,  1.1139e-01, -2.5269e-01,\n",
       "           -1.1664e-01, -1.8640e-01, -6.6650e-01, -8.5205e-02,  1.6617e-02,\n",
       "           -5.0635e-01,  4.4971e-01,  2.5375e-02,  2.0264e-01, -8.3984e-01,\n",
       "           -3.1348e-01,  1.7395e-01,  9.3933e-02,  2.9785e-01,  3.1543e-01,\n",
       "           -6.0730e-03,  1.1676e-01,  4.8920e-02,  1.6187e-01,  1.8811e-01,\n",
       "            3.1013e-03,  3.8062e-01, -1.0248e-01, -9.1431e-02, -2.8702e-02,\n",
       "           -5.7617e-01, -1.9495e-01, -1.7480e-01,  2.6398e-02,  8.6243e-02,\n",
       "            6.1493e-02, -1.9211e-02,  1.5674e-01,  1.1066e-01,  9.3323e-02,\n",
       "            2.1912e-01,  4.3726e-01,  2.4426e-01, -1.8738e-01,  7.4158e-03,\n",
       "            1.3818e-01,  1.8811e-04,  3.0176e-01,  5.2686e-01,  2.7908e-02,\n",
       "            1.1694e-01, -1.5967e-01,  9.1064e-02,  3.9856e-02,  3.2471e-02,\n",
       "            3.3057e-01,  1.2103e-01,  1.7175e-01, -1.2655e-03, -1.1658e-01,\n",
       "            8.8684e-02,  7.4280e-02,  3.8599e-01,  1.9653e-01, -5.6488e-02,\n",
       "           -2.5220e-01, -7.8308e-02, -2.9272e-01, -1.6516e-01, -3.3081e-01,\n",
       "            5.0195e-01,  1.7273e-01,  4.1172e+00,  5.7617e-02,  1.9543e-01,\n",
       "            1.6675e-01,  2.4744e-01, -1.4966e-01, -9.7275e-03, -2.2656e-01,\n",
       "            8.8318e-02,  2.7881e-01,  3.8306e-01,  4.3164e-01,  3.0322e-01,\n",
       "           -5.8960e-02, -5.8929e-02, -3.5327e-01,  5.0049e-01, -1.4111e+00,\n",
       "           -2.4438e-01, -6.4392e-02,  8.8745e-02, -1.1249e-01,  3.5187e-02,\n",
       "           -2.4866e-01,  1.8539e-02,  1.4917e-01,  1.4502e-01, -1.0040e-01,\n",
       "            6.2439e-02, -7.7393e-02, -7.5439e-02, -3.8544e-02,  1.0559e-02,\n",
       "           -1.4893e-01,  1.4307e-01, -2.0862e-01, -1.5234e-01, -4.8492e-02,\n",
       "            3.1372e-01,  5.3680e-02,  9.4299e-02,  2.3486e-01, -3.3264e-02,\n",
       "           -3.6133e-01,  2.0215e-01, -1.0840e-01,  1.7358e-01,  1.6418e-01,\n",
       "            5.3223e-01,  1.2494e-01, -1.4233e-01, -1.5442e-01,  3.2983e-01,\n",
       "           -4.0820e-01, -4.6997e-03, -9.9731e-02,  3.4180e-01,  4.1962e-02,\n",
       "           -5.1086e-02, -2.2107e-01,  6.2103e-02,  4.1260e-02,  4.2432e-01,\n",
       "           -1.1115e-01, -1.9604e-01, -2.3608e-01, -1.9702e-01, -4.7821e-02,\n",
       "           -5.7068e-02, -1.8469e-01,  2.2339e-01,  2.4365e-01,  1.3757e-01,\n",
       "            4.8120e-01,  2.5879e-02,  2.4084e-01, -4.7437e-01, -1.2280e-01,\n",
       "           -7.0703e-01, -3.2129e-01, -6.1328e-01,  3.1403e-02,  7.4585e-02,\n",
       "           -3.4058e-01,  4.5166e-01,  2.9395e-01, -2.0935e-01,  7.8320e-01,\n",
       "           -4.0137e-01, -2.5977e-01,  1.4209e-01,  1.2396e-01, -9.9182e-02,\n",
       "            5.2344e-01, -1.1102e-01, -2.7539e-01,  6.8652e-01,  8.0872e-03,\n",
       "            4.1357e-01,  8.8730e-03,  1.6382e-01,  3.0591e-01,  1.1737e-01,\n",
       "           -2.1521e-01,  2.8125e-01, -2.6382e-02, -1.2549e-01, -4.3408e-01,\n",
       "           -1.1688e-01, -2.8638e-01, -3.7524e-01, -3.1738e-01,  3.2642e-01,\n",
       "            2.9492e-01,  5.2612e-02, -2.1423e-01,  1.6418e-01, -2.1741e-01,\n",
       "            7.2205e-02,  4.1779e-02, -1.8286e-01,  6.9580e-02,  3.2520e-01,\n",
       "           -9.7412e-02, -2.6367e-01, -1.5320e-01, -2.7515e-01, -1.6968e-01,\n",
       "            1.5759e-01, -4.5929e-02, -6.6101e-02, -2.1765e-01,  1.7920e-01,\n",
       "            1.7456e-01,  9.0576e-02, -2.1277e-01,  2.7417e-01, -1.5222e-01,\n",
       "           -2.2192e-01,  3.3447e-01, -1.8579e-01, -2.8125e-01, -1.0278e-01,\n",
       "            2.8101e-01,  4.3066e-01, -1.0364e-01, -7.7881e-02,  2.0203e-01,\n",
       "           -3.5693e-01,  4.7058e-02, -2.5562e-01,  7.9956e-02, -6.4270e-02,\n",
       "           -2.9614e-01, -4.3457e-02,  2.7252e-02,  2.7612e-01, -2.3730e-01,\n",
       "            1.5991e-01, -1.8506e-01,  1.7212e-01,  4.7333e-02,  1.6565e-01,\n",
       "           -1.6431e-01,  1.4587e-01, -2.8833e-01, -2.1692e-01,  2.7686e-01,\n",
       "            2.1289e-01, -2.8931e-01, -2.1338e-01,  6.6711e-02, -7.5195e-02,\n",
       "           -3.6328e-01,  2.3743e-02, -2.3010e-01, -9.3811e-02,  4.8889e-02,\n",
       "            8.2825e-02, -1.6833e-01, -2.7368e-01,  7.6855e-01,  6.5625e-01,\n",
       "           -1.2311e-01,  4.8218e-01,  2.0007e-01, -3.9062e-01, -3.2495e-01,\n",
       "            2.4939e-01,  6.9519e-02,  1.7297e-01,  2.0203e-01,  1.0913e-01,\n",
       "           -1.0364e-01, -7.3013e-03,  2.6294e-01, -1.8799e-01,  2.7271e-01,\n",
       "           -1.3794e-01,  4.4092e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1110, 0.3816, 0.4900, 0.0176]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0   82.990051  121.263153  402.765533  375.145416    0.944389     22  zebra\n",
       "  1  383.249084  202.364685  626.868652  330.273376    0.929808     22  zebra\n",
       "  2    0.154266   84.994186  235.063675  226.117554    0.901680     22  zebra,\n",
       "  'caption': ['A zebra with his neck up but head not visible.'],\n",
       "  'bbox_target': [0.85, 85.17, 238.47, 151.6]},\n",
       " 403: {'image_emb': tensor([[ 0.0058,  0.2021, -0.3708,  ...,  1.1436,  0.2837,  0.1163],\n",
       "          [ 0.0526,  0.3682, -0.4202,  ...,  0.7124,  0.3665,  0.2367],\n",
       "          [ 0.0213,  0.4983, -0.2222,  ...,  0.4910,  0.2903,  0.2167]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 6.1157e-02,  1.4191e-02, -4.6606e-01,  3.9917e-02, -1.4001e-01,\n",
       "            2.7466e-01,  1.8143e-02, -1.0420e+00, -1.4478e-01, -4.8798e-02,\n",
       "           -1.6260e-01,  2.1497e-01,  1.4478e-01, -8.2153e-02, -2.6001e-02,\n",
       "            1.3721e-01, -3.1567e-01, -3.6346e-02, -2.8052e-01,  4.7021e-01,\n",
       "            2.5952e-01, -1.3867e-01, -5.6494e-01,  1.3647e-01,  3.1128e-01,\n",
       "           -2.3889e-01, -1.3855e-01,  9.7534e-02,  9.3445e-02, -1.2164e-01,\n",
       "            1.3403e-01,  8.1177e-02, -3.6523e-01,  8.9722e-02, -3.4839e-01,\n",
       "           -1.3867e-01,  1.7261e-01,  3.6450e-01, -1.5869e-02,  1.8103e-01,\n",
       "            4.7028e-02, -1.5430e-03,  1.0162e-01, -4.5093e-01,  9.9731e-02,\n",
       "            3.8757e-02,  1.6980e-01,  1.2524e-01,  2.4634e-01,  7.7393e-02,\n",
       "            6.9214e-02, -1.5637e-01,  5.2686e-01, -1.4355e-01, -5.1514e-01,\n",
       "           -1.0522e-01,  1.6711e-01,  1.9568e-01, -3.4106e-01,  4.1260e-01,\n",
       "           -2.7832e-01, -2.9724e-02, -8.5373e-03, -2.3035e-01, -2.8467e-01,\n",
       "           -1.9800e-01,  4.0479e-01, -1.1395e-01, -1.0126e-01,  2.6962e-02,\n",
       "            1.8631e-02,  6.0516e-02,  1.1438e-01, -2.7344e-02,  3.8574e-01,\n",
       "            1.9849e-01,  3.7671e-01,  1.4294e-01,  8.1543e-02,  3.1689e-01,\n",
       "           -3.9551e-01,  2.1191e-01,  5.5969e-02, -1.0260e-01,  1.1853e-01,\n",
       "            3.0640e-01, -1.5381e-02, -1.2250e-01, -5.9082e-01, -1.5100e-01,\n",
       "            3.2837e-01, -4.8047e-01, -1.3828e+00,  2.5562e-01,  3.9581e-02,\n",
       "            2.6392e-01,  3.9893e-01, -8.9600e-02, -2.0767e-02, -4.2816e-02,\n",
       "           -4.7394e-02,  1.1316e-01, -2.0496e-01, -4.6387e-01,  5.2124e-02,\n",
       "            4.5471e-02,  1.9031e-01,  5.3467e-01,  3.3301e-01, -7.6538e-02,\n",
       "            1.9336e-04,  2.3804e-01, -3.5210e-03,  2.0789e-01, -1.1560e-01,\n",
       "            1.9885e-01,  4.7485e-01, -9.8724e-03, -4.9530e-02,  5.3564e-01,\n",
       "            5.6488e-02, -1.1094e+00, -2.7734e-01, -3.1067e-02,  3.0664e-01,\n",
       "           -2.8857e-01,  1.2622e-01, -2.3853e-01,  3.9404e-01,  3.7061e-01,\n",
       "            2.2388e-01, -4.5508e-01, -3.3789e-01,  4.8203e+00, -2.8833e-01,\n",
       "            2.5073e-01, -3.0103e-01, -1.7188e-01,  8.7280e-02, -2.0020e-01,\n",
       "           -2.3169e-01,  1.5234e-01, -1.5344e-01,  5.3906e-01,  3.5693e-01,\n",
       "            3.5950e-02, -2.9068e-03, -5.6738e-01, -6.2012e-02,  8.8074e-02,\n",
       "           -1.2622e-01,  4.9469e-02,  1.8604e-01,  2.0813e-01, -1.6693e-02,\n",
       "            6.9504e-03, -5.2429e-02, -3.8354e-01,  7.1838e-02,  4.0625e-01,\n",
       "            3.3960e-01, -6.4209e-02, -2.0126e-02,  2.0154e-01,  1.0309e-01,\n",
       "            2.1082e-01,  1.6907e-01, -4.9707e-01, -1.0109e-02, -1.9324e-01,\n",
       "           -8.0200e-02, -2.1765e-01,  2.0764e-01,  2.4939e-01, -1.3257e-01,\n",
       "           -6.2485e-03,  6.5186e-02, -2.1759e-02,  2.7328e-02, -4.1699e-01,\n",
       "            1.1940e-03,  2.4390e-01,  2.1896e-02, -1.1035e-01,  1.9348e-02,\n",
       "            3.3722e-02,  2.9724e-02, -1.9849e-01,  9.0637e-02, -7.0496e-02,\n",
       "            1.4795e-01,  3.6108e-01, -1.0577e-01, -5.5518e-01, -9.0637e-02,\n",
       "            1.2927e-01, -1.6833e-01,  1.0979e-02, -2.9297e-01, -4.3384e-01,\n",
       "            2.3206e-01,  3.5864e-01, -3.4485e-02,  2.6221e-01, -5.0293e-01,\n",
       "            3.5034e-01,  1.6772e-01,  5.2826e-02, -2.4829e-01, -5.0342e-01,\n",
       "            3.0957e-01,  4.8511e-01, -1.6235e-01,  2.1716e-01, -1.0828e-01,\n",
       "           -3.3203e-01,  1.6479e-02,  9.3445e-02,  6.3525e-01, -6.4941e-02,\n",
       "            2.3743e-01,  1.2159e-03, -3.2373e-01, -8.5266e-02, -1.9073e-02,\n",
       "           -6.3232e-02,  3.7036e-01, -3.4546e-01, -4.7363e-01,  5.0079e-02,\n",
       "           -5.4932e-01,  3.6914e-01, -6.7383e-02,  2.5879e-01,  8.1406e-03,\n",
       "           -5.5969e-02, -1.1316e-01,  7.6660e-02, -4.3549e-02, -3.7445e-02,\n",
       "           -1.5857e-01,  3.2275e-01,  4.6045e-01,  2.0203e-01, -9.9365e-02,\n",
       "            1.4832e-01,  1.2482e-01, -1.7371e-01,  1.1102e-01, -6.5430e-02,\n",
       "            1.5503e-01, -5.1172e-01,  6.9580e-01, -1.3037e-01, -3.8110e-01,\n",
       "            3.0420e-01,  2.7954e-01, -4.5654e-02,  2.1286e-02, -1.9910e-01,\n",
       "           -3.0981e-01,  4.9658e-01,  3.7689e-02,  4.5319e-02, -9.5459e-02,\n",
       "           -2.5342e-01, -2.3468e-02, -1.0760e-01, -3.4473e-01,  1.4392e-01,\n",
       "           -7.1655e-02, -3.0380e-02,  2.6294e-01, -2.9810e-01,  2.0288e-01,\n",
       "           -1.4197e-01, -1.8616e-01,  2.5732e-01,  2.8168e-02, -4.6783e-02,\n",
       "            3.7671e-01,  1.6769e-02,  9.7046e-02,  3.0005e-01, -6.5979e-02,\n",
       "            2.9980e-01, -1.4417e-01, -1.7993e-01,  4.0625e-01,  1.8811e-01,\n",
       "           -1.5356e-01, -3.1421e-01, -1.9275e-01, -1.2341e-01, -6.8848e-02,\n",
       "            2.5391e-01, -1.4563e-01, -3.6255e-01, -3.4863e-01, -1.5154e-03,\n",
       "           -2.7490e-01, -1.4368e-01, -1.0913e-01,  5.3520e-03,  3.3496e-01,\n",
       "           -2.2232e-02, -2.5537e-01, -5.8008e-01, -2.6270e-01, -5.7129e-02,\n",
       "            6.1981e-02,  1.5112e-01,  4.8086e+00, -1.4771e-01, -3.7085e-01,\n",
       "            1.8555e-01,  1.8628e-01, -1.5308e-01,  2.6489e-01,  3.3398e-01,\n",
       "            3.1299e-01,  1.4575e-01, -8.8806e-02,  2.9205e-02, -1.5051e-01,\n",
       "           -1.5039e-01,  2.5903e-01, -2.3901e-01,  5.7280e-05, -1.9756e+00,\n",
       "            1.8274e-01,  1.4343e-01,  1.9519e-01, -2.6758e-01, -8.4656e-02,\n",
       "            3.9111e-01, -3.3057e-01, -6.0089e-02,  4.1016e-01,  7.6477e-02,\n",
       "           -3.7305e-01, -4.9512e-01,  1.0931e-01, -2.7026e-01, -8.7158e-02,\n",
       "           -1.3351e-02, -2.1509e-01,  1.6724e-02, -3.1152e-01, -8.0872e-02,\n",
       "            1.4319e-01, -4.2871e-01,  3.8062e-01,  1.3110e-01, -6.8893e-03,\n",
       "           -1.9568e-01, -1.9849e-01,  3.1323e-01,  1.2646e-01,  1.3940e-01,\n",
       "           -1.6882e-01, -3.2544e-01, -1.0596e-01,  1.2103e-01, -2.5070e-02,\n",
       "            5.9521e-01,  6.5613e-02, -1.5991e-01, -1.2634e-01, -1.3538e-01,\n",
       "            1.6174e-02, -1.9336e-01, -2.3450e-01, -8.2458e-02, -3.6621e-01,\n",
       "            2.2107e-01, -1.1060e-01,  1.2061e-01,  2.1948e-01, -2.3941e-02,\n",
       "            9.2224e-02,  1.6638e-01,  8.6304e-02,  1.1078e-01,  1.4758e-01,\n",
       "            1.0065e-01,  7.8674e-02,  1.7197e-02, -9.4360e-02, -1.3611e-01,\n",
       "           -8.6731e-02,  3.1274e-01, -1.6931e-01, -2.0251e-01,  7.9529e-02,\n",
       "            8.6304e-02,  2.3486e-01,  9.8324e-04, -1.4734e-01,  4.2389e-02,\n",
       "            7.6904e-01, -3.0469e-01,  1.7773e-01,  2.7246e-01, -3.3984e-01,\n",
       "           -3.2544e-01,  4.6196e-03,  3.5864e-01, -5.4004e-01, -1.0596e-01,\n",
       "            8.0811e-02,  4.1064e-01, -2.5415e-01,  1.8408e-01,  4.7668e-02,\n",
       "           -4.0942e-01, -2.7979e-01,  1.4130e-02,  3.6108e-01,  1.1938e-01,\n",
       "           -2.4704e-02, -9.6970e-03,  4.0527e-01,  6.5796e-02,  1.2512e-02,\n",
       "           -1.2646e-01,  1.2866e-01, -7.7095e-03,  3.6182e-01,  1.4319e-01,\n",
       "            1.5222e-01, -3.8062e-01, -1.5796e-01,  1.2927e-01, -1.4697e-01,\n",
       "            1.7899e-02, -1.1169e-01,  9.6680e-02, -2.9858e-01,  4.5264e-01,\n",
       "           -3.6133e-01,  2.3950e-01, -3.7354e-01,  7.2021e-02, -1.0071e-01,\n",
       "           -1.4389e-02, -2.6685e-01, -1.5083e-02,  1.8774e-01,  4.9512e-01,\n",
       "           -1.6821e-01, -1.5967e-01,  2.1423e-01,  6.2500e-02,  9.9243e-02,\n",
       "           -1.2732e-01,  4.1431e-01,  3.5303e-01,  1.3306e-02,  5.7190e-02,\n",
       "           -1.7529e-01,  3.3765e-01,  4.1699e-01,  7.1594e-02,  2.6855e-01,\n",
       "           -2.0959e-01, -3.7891e-01, -5.1544e-02,  9.9976e-02,  3.5913e-01,\n",
       "            3.7573e-01, -1.5820e-01, -1.0480e-01, -9.2651e-02, -1.1670e-01,\n",
       "           -2.3840e-01,  1.5039e-01,  1.4514e-01, -1.8457e-01,  3.1616e-01,\n",
       "            1.8860e-01, -4.6948e-01,  1.9617e-01,  1.4746e-01,  7.9529e-02,\n",
       "            1.1803e-02, -9.4238e-02, -1.5662e-01,  1.2396e-01, -5.1660e-01,\n",
       "           -4.9774e-02, -1.4026e-01,  2.8564e-02,  8.2080e-01,  2.0715e-01,\n",
       "           -1.2238e-01,  3.3398e-01, -5.3125e-01, -4.5898e-02,  1.7188e-01,\n",
       "           -1.2433e-01,  7.3792e-02, -2.2498e-01, -3.6841e-01,  2.5732e-01,\n",
       "           -9.3872e-02,  2.9926e-03, -1.3596e-02,  1.9739e-01,  1.0034e-01,\n",
       "           -3.6621e-01,  1.2112e-03]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0609, 0.5601, 0.3789]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  284.683197  190.415283  494.855011  319.092957    0.933550     16     dog\n",
       "  1   49.369232    1.567215  630.488647  419.865295    0.728401      0  person\n",
       "  2   44.772064    0.000000  631.058105  419.281372    0.408091     57   couch,\n",
       "  'caption': ['the couch, a lady sitted on it.'],\n",
       "  'bbox_target': [0.0, 2.75, 163.45, 419.66]},\n",
       " 404: {'image_emb': tensor([[-0.1095,  0.6699, -0.2883,  ...,  1.2432, -0.1337, -0.2705],\n",
       "          [ 0.2974,  0.3203,  0.0391,  ...,  0.8857, -0.0742,  0.2493],\n",
       "          [ 0.1735,  0.0983,  0.0593,  ...,  0.6650,  0.5435, -0.1288],\n",
       "          [ 0.1816, -0.1099,  0.1248,  ...,  0.5552,  0.2830, -0.1973]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0065, -0.0105, -0.0038,  ...,  0.1394,  0.3865, -0.4526],\n",
       "          [ 0.2922,  0.1534,  0.1545,  ...,  0.1938,  0.3452, -0.3472]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.1781e-04, 3.8128e-03, 9.7754e-01, 1.8188e-02],\n",
       "          [9.5367e-07, 9.7229e-02, 8.3984e-01, 6.2805e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  310.973267  102.420013  455.038086  635.563232    0.906107      0   \n",
       "  1   76.405487  222.867676  322.812286  636.736389    0.899902     31   \n",
       "  2    3.341934   35.063934  447.934692  638.559570    0.898033      0   \n",
       "  3   19.528122  138.265594  121.479202  306.116730    0.674522      0   \n",
       "  4    0.000000  141.103577  121.622345  573.696777    0.561601      0   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1  snowboard  \n",
       "  2     person  \n",
       "  3     person  \n",
       "  4     person  ,\n",
       "  'caption': ['A man holding a snowboard.',\n",
       "   'A man holding a Burton snowboard.'],\n",
       "  'bbox_target': [20.13, 35.54, 422.84, 464.95]},\n",
       " 405: {'image_emb': tensor([[ 1.5979e-01,  4.9561e-01, -1.7456e-01,  ...,  2.0056e-01,\n",
       "            4.3701e-01,  9.4360e-02],\n",
       "          [-4.6729e-01,  2.6172e-01,  3.2867e-02,  ...,  8.2373e-01,\n",
       "            1.7981e-01,  5.9937e-02],\n",
       "          [-3.5791e-01,  3.2178e-01, -1.9812e-01,  ...,  8.3496e-01,\n",
       "            2.8296e-01, -9.4986e-04],\n",
       "          [-6.5723e-01,  3.1036e-02, -3.6060e-01,  ...,  1.1250e+00,\n",
       "            4.0771e-02, -1.2708e-01],\n",
       "          [-3.6392e-03,  4.7363e-01, -1.3464e-01,  ...,  3.6133e-01,\n",
       "            3.5376e-01,  4.8523e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0448, -0.3142, -0.7197,  ...,  0.5195, -0.2400,  0.0278],\n",
       "          [ 0.2512,  0.0116, -0.1588,  ..., -0.3770, -0.2004, -0.2520]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3176, 0.0032, 0.0043, 0.0024, 0.6724],\n",
       "          [0.6025, 0.0815, 0.0206, 0.0194, 0.2759]], dtype=torch.float16),\n",
       "  'df_boxes':         xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   0.000000  101.431061  498.386230  639.749878    0.928274      0  person\n",
       "  1  18.394119   69.429291  266.692688  404.367889    0.859466      0  person\n",
       "  2  93.056290  301.802795  146.846298  374.190125    0.837035     65  remote\n",
       "  3   0.000000  411.741150   76.988998  453.770935    0.761965     65  remote\n",
       "  4   5.030746  135.892487  500.391602  638.444824    0.335274     57   couch,\n",
       "  'caption': ['asian man sitting at end of couch', 'The left most person'],\n",
       "  'bbox_target': [0.0, 81.85, 142.79, 327.21]},\n",
       " 406: {'image_emb': tensor([[-0.0764,  0.3855, -0.1163,  ...,  0.1332,  0.3149,  0.1454],\n",
       "          [-0.0041,  0.1278, -0.3140,  ...,  0.9575,  0.2544, -0.0329],\n",
       "          [ 0.2185,  0.8281, -0.3469,  ...,  0.8115,  0.1995,  0.0266],\n",
       "          ...,\n",
       "          [-0.1917,  0.3350,  0.0170,  ...,  1.1387,  0.4158,  0.1033],\n",
       "          [-0.1750,  0.6309, -0.2507,  ...,  1.3105,  0.1538, -0.2289],\n",
       "          [-0.0756,  0.4448,  0.1809,  ...,  0.3804,  0.5332, -0.0909]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1412, -0.0664, -0.0439,  ..., -0.0375, -0.4729,  0.1617],\n",
       "          [-0.3435,  0.3208, -0.1440,  ..., -0.3586, -0.1008, -0.3345]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.1750e-04, 9.7021e-01, 5.2273e-05, 1.7052e-03, 8.5735e-04, 2.6672e-02,\n",
       "           5.9247e-05],\n",
       "          [3.3798e-03, 9.8242e-01, 4.1652e-04, 5.5170e-04, 1.9569e-03, 1.0254e-02,\n",
       "           1.1501e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   334.945435   13.821793  637.700378  397.802917    0.955874      0   \n",
       "  1    79.186172  286.045380  230.856583  399.880310    0.917110      0   \n",
       "  2   341.944641  221.232635  381.623352  280.345184    0.881763     67   \n",
       "  3   235.602936  117.486496  333.701996  291.308838    0.776271     58   \n",
       "  4   354.729309    0.000000  467.622864  243.128143    0.738388     58   \n",
       "  5    54.284294  189.024658  223.781189  294.206848    0.706638     58   \n",
       "  6     0.058442  358.095673   26.788116  401.544678    0.561025      0   \n",
       "  7   615.890198  171.937531  638.110535  200.681732    0.498019     74   \n",
       "  8   476.661285  168.731384  504.893341  191.797241    0.405901     27   \n",
       "  9    10.408661  249.448212   85.610443  296.158600    0.329513     58   \n",
       "  10    0.000000  267.253448   21.490837  293.043365    0.269439     58   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2     cell phone  \n",
       "  3   potted plant  \n",
       "  4   potted plant  \n",
       "  5   potted plant  \n",
       "  6         person  \n",
       "  7          clock  \n",
       "  8            tie  \n",
       "  9   potted plant  \n",
       "  10  potted plant  ,\n",
       "  'caption': ['A lady with short brown hair.',\n",
       "   'A woman in a black shit with her hands under her chin.'],\n",
       "  'bbox_target': [78.3, 287.84, 149.6, 111.45]},\n",
       " 407: {'image_emb': tensor([[ 0.1384,  0.4192, -0.1653,  ...,  1.4834, -0.1470,  0.2598],\n",
       "          [ 0.2764,  0.1639, -0.3008,  ...,  0.5127, -0.2612,  0.4966],\n",
       "          [-0.0783,  0.1410, -0.2365,  ...,  1.0166,  0.1382, -0.1392],\n",
       "          ...,\n",
       "          [-0.3069,  0.2751, -0.2539,  ...,  0.9341, -0.2324,  0.1354],\n",
       "          [-0.0782,  0.4573, -0.3806,  ...,  1.3662, -0.0240,  0.2314],\n",
       "          [ 0.4890,  0.2993, -0.3232,  ...,  0.4746, -0.2925,  0.4478]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 4.5227e-02,  9.9304e-02, -3.8184e-01,  ...,  1.9995e-01,\n",
       "           -5.5115e-02, -4.3335e-01],\n",
       "          [-1.0107e-01, -9.4116e-02, -3.2202e-01,  ...,  2.8488e-02,\n",
       "            1.1826e-04, -5.3760e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.3793e-03, 2.0623e-05, 9.9121e-01, 5.9605e-08, 6.5565e-07, 1.1921e-07,\n",
       "           4.0352e-05, 4.2319e-06, 1.7881e-07, 1.7881e-07, 4.2458e-03],\n",
       "          [6.1095e-05, 9.5367e-07, 9.9951e-01, 5.9605e-08, 5.9605e-08, 0.0000e+00,\n",
       "           1.1325e-06, 5.9605e-08, 5.9605e-08, 0.0000e+00, 2.2340e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   469.132019    0.806969  568.938782  222.425812    0.934946      0   \n",
       "  1   194.859146    1.021744  459.646240  320.564270    0.931760      0   \n",
       "  2   489.563538  194.911987  626.046326  305.462769    0.906719     16   \n",
       "  3     0.735001  148.034073  456.090637  421.808716    0.904577     60   \n",
       "  4   406.750885   58.510132  513.714050  114.836639    0.849094      2   \n",
       "  5   215.276871  140.298004  265.169769  180.745758    0.829440     41   \n",
       "  6   137.227173   79.238480  206.216370  160.868225    0.800697     56   \n",
       "  7   137.341019   20.598732  191.788101   81.295410    0.796936      0   \n",
       "  8    67.432632  153.682526  160.852524  237.275665    0.779799     41   \n",
       "  9   594.027466   88.672180  639.799927  135.321289    0.705383      3   \n",
       "  10  103.396019   64.925171  155.710022  148.554932    0.680514     56   \n",
       "  11  112.470146  245.815735  253.932007  376.965088    0.667427     41   \n",
       "  12  548.596252   85.572037  608.282776  127.806259    0.663638      3   \n",
       "  13  412.614105  187.591644  473.669098  218.566010    0.552283     52   \n",
       "  14  447.915466  332.302765  638.363098  425.176575    0.552157      0   \n",
       "  15   51.447945   65.535767   89.462700   85.611938    0.510694     56   \n",
       "  16    0.232370  336.972626   46.010773  380.440094    0.488064     44   \n",
       "  17  345.870605  150.136658  389.337891  202.886414    0.476725     56   \n",
       "  18  246.653839  266.865845  329.276337  286.759460    0.391127     44   \n",
       "  19    0.000000   77.929245  124.450531  144.900955    0.384891     56   \n",
       "  20  110.887100  245.254181  254.119980  377.268097    0.373946     45   \n",
       "  21  158.645264  163.336792  216.745697  180.475464    0.321057     44   \n",
       "  22    0.000000  137.514191  139.032898  190.892365    0.311292     56   \n",
       "  23   91.712189   20.064049  123.804321   64.880157    0.300901      0   \n",
       "  24  167.068680   86.752365  208.908981  133.925064    0.259594     56   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2            dog  \n",
       "  3   dining table  \n",
       "  4            car  \n",
       "  5            cup  \n",
       "  6          chair  \n",
       "  7         person  \n",
       "  8            cup  \n",
       "  9     motorcycle  \n",
       "  10         chair  \n",
       "  11           cup  \n",
       "  12    motorcycle  \n",
       "  13       hot dog  \n",
       "  14        person  \n",
       "  15         chair  \n",
       "  16         spoon  \n",
       "  17         chair  \n",
       "  18         spoon  \n",
       "  19         chair  \n",
       "  20          bowl  \n",
       "  21         spoon  \n",
       "  22         chair  \n",
       "  23        person  \n",
       "  24         chair  ,\n",
       "  'caption': ['A woman in a white shirt walking a white dog.',\n",
       "   'a woman in a white shift walking a small white dog'],\n",
       "  'bbox_target': [477.16, 3.37, 93.51, 220.75]},\n",
       " 408: {'image_emb': tensor([[-0.1754,  0.2742,  0.0853,  ...,  1.1836,  0.3616,  0.1093],\n",
       "          [-0.4719,  0.0496,  0.0475,  ...,  1.0850,  0.0733, -0.0522],\n",
       "          [-0.1170,  0.1359,  0.1659,  ...,  0.8384,  0.0438,  0.2556],\n",
       "          [-0.2458,  0.1620,  0.0442,  ...,  0.8813,  0.1624, -0.2251],\n",
       "          [-0.4495,  0.5503,  0.1418,  ...,  1.0693,  0.1727,  0.1030],\n",
       "          [-0.0434,  0.2603,  0.2026,  ...,  1.1982,  0.0673,  0.0316]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2241, -0.1384, -0.5259,  ...,  0.2045,  0.1082,  0.3357],\n",
       "          [ 0.1986,  0.2534, -0.2142,  ...,  0.0614,  0.2095,  0.2795]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.3788e-02, 3.6835e-02, 8.9258e-01, 1.3292e-04, 6.7353e-06, 4.6570e-02],\n",
       "          [1.6748e-01, 4.7241e-02, 7.6270e-01, 1.8835e-05, 1.2517e-06, 2.2675e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    3.977737   56.649414  337.339966  454.405090    0.916909      0  person\n",
       "  1  432.492676    0.101456  638.456909  452.100159    0.908381      0  person\n",
       "  2  215.682938   68.925293  535.688599  454.044678    0.873397      0  person\n",
       "  3  193.317200  326.114105  274.841522  386.101044    0.871608     65  remote\n",
       "  4  109.347893  293.411011  183.833160  351.424255    0.769873     65  remote,\n",
       "  'caption': ['blond hair woman in pink top sitting next to long haird man in blue shirt',\n",
       "   'A blonde woman wearing a pink shirt.'],\n",
       "  'bbox_target': [218.18, 69.98, 321.09, 383.87]},\n",
       " 409: {'image_emb': tensor([[-0.1759, -0.3699, -0.6558,  ...,  0.7900,  0.1633,  0.3904],\n",
       "          [ 0.0586,  0.3669, -0.3188,  ...,  1.4131,  0.4045, -0.1622],\n",
       "          [-0.1186,  0.2822, -0.1456,  ...,  1.1348, -0.0848, -0.0713],\n",
       "          [-0.0770, -0.3516, -0.4014,  ...,  0.7188, -0.0634,  0.3655]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2600, -0.0851, -0.1186,  ...,  0.4124,  0.0702,  0.0446],\n",
       "          [ 0.2405,  0.0335, -0.0925,  ...,  0.2003,  0.0096,  0.0915]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.7510e-01, 3.2501e-03, 3.3915e-05, 5.2148e-01],\n",
       "          [4.0186e-01, 3.6572e-01, 9.9976e-02, 1.3245e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  145.921112   19.249252  601.196655  397.746521    0.882978      7    truck\n",
       "  1  571.201660  109.819092  639.679688  304.827271    0.823421      2      car\n",
       "  2   53.643391   90.524460  150.657257  154.774704    0.805985      7    truck\n",
       "  3    0.039391  142.156860   13.099781  164.228210    0.601615      1  bicycle,\n",
       "  'caption': ['A yellow VW truck.', 'a yellow colour truck'],\n",
       "  'bbox_target': [148.85, 33.26, 455.19, 362.43]},\n",
       " 410: {'image_emb': tensor([[ 0.1200, -0.3848, -0.0827,  ...,  0.7871, -0.2976,  0.4744],\n",
       "          [-0.0091, -0.5083, -0.0926,  ...,  0.7793, -0.3105,  0.2356],\n",
       "          [-0.0953, -0.0793, -0.0510,  ...,  0.7480, -0.2695,  0.3118]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2122, -0.1964, -0.3411,  ...,  0.0329, -0.0610, -0.0075],\n",
       "          [ 0.2771,  0.0182, -0.1567,  ...,  0.3325,  0.0831, -0.1198]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.7881, 0.1287, 0.0831],\n",
       "          [0.8745, 0.0907, 0.0350]], dtype=torch.float16),\n",
       "  'df_boxes':         xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  37.600998  181.021591  379.105957  640.000000    0.931017     21    bear\n",
       "  1  78.566772    1.526337  425.875061  634.383179    0.794108      0  person\n",
       "  2   1.067635    0.770508  151.947937  637.039246    0.573460      0  person\n",
       "  3  94.157135    0.146347  423.422607  148.543564    0.453241      0  person,\n",
       "  'caption': ['A lady holding a bear cub.', 'A man holding a Black bear'],\n",
       "  'bbox_target': [1.44, 4.07, 277.57, 629.94]},\n",
       " 411: {'image_emb': tensor([[ 0.2271,  0.3699, -0.3455,  ...,  1.0596,  0.2593, -0.1108],\n",
       "          [-0.0256,  0.3215, -0.0786,  ...,  1.0693, -0.1066,  0.2341],\n",
       "          [-0.1360,  0.2344,  0.1328,  ...,  0.8179,  0.1652,  0.0060],\n",
       "          ...,\n",
       "          [-0.0378,  0.2854, -0.2107,  ...,  0.9727,  0.1202, -0.0290],\n",
       "          [ 0.1525, -0.1016, -0.4277,  ...,  0.9404,  0.1171, -0.1799],\n",
       "          [-0.1517,  0.3035, -0.4058,  ...,  1.1523,  0.1169, -0.0394]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0465,  0.2147, -0.3904,  ...,  0.3777, -0.2988, -0.0165],\n",
       "          [ 0.0161,  0.2175, -0.3401,  ...,  0.3647, -0.1660, -0.0570]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.5807e-04, 1.6928e-05, 8.4639e-05, 1.6570e-04, 8.3268e-05, 1.5557e-04,\n",
       "           1.6727e-03, 1.8299e-05, 3.2783e-06, 2.5034e-05, 8.2016e-05, 9.9756e-01],\n",
       "          [4.5815e-03, 1.0986e-02, 1.4786e-02, 9.4910e-02, 2.6779e-02, 1.9287e-02,\n",
       "           1.1450e-01, 2.2537e-02, 7.0238e-04, 1.7563e-02, 2.4765e-02, 6.4844e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    95.274078    0.526779  276.850433  229.059647    0.921266      0   \n",
       "  1   378.163635    0.237167  451.142029  146.101166    0.910766      0   \n",
       "  2   280.952393   13.700333  377.651794  148.389755    0.895577      0   \n",
       "  3   255.771271  251.167847  295.311005  289.450562    0.889577     41   \n",
       "  4   235.346191  196.729843  262.680969  237.884445    0.888470     41   \n",
       "  5    34.803650    1.031342  219.852783  300.119141    0.887848      0   \n",
       "  6   194.281769    0.036613  343.505463  184.188080    0.880167      0   \n",
       "  7   192.378571  232.589355  227.539154  274.922394    0.876770     41   \n",
       "  8     0.208527    1.256760   71.454498  336.432556    0.845717      0   \n",
       "  9   315.015320  160.403015  335.251465  189.284424    0.845357     41   \n",
       "  10  341.566193  155.674301  358.871307  182.155716    0.784886     41   \n",
       "  11  406.892792  189.703720  479.283142  256.569641    0.694676      0   \n",
       "  12    0.654434  211.800323  478.214050  636.250854    0.666616     60   \n",
       "  13  266.984436   91.045517  309.148071  122.116852    0.596648     45   \n",
       "  14  158.602814  213.504150  479.200195  355.788696    0.562871     60   \n",
       "  15  374.538696  206.373718  406.388428  276.665741    0.486830     39   \n",
       "  16  277.447693  155.236053  480.000000  220.968445    0.480235     60   \n",
       "  17  374.082184  207.548767  406.036896  276.912079    0.431265     41   \n",
       "  18  454.314331  132.713211  476.733032  173.275955    0.328699     41   \n",
       "  19  429.029572  155.521408  455.298828  199.867813    0.319386     39   \n",
       "  20    2.352341  205.988663  118.144730  333.264160    0.270454      0   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3            cup  \n",
       "  4            cup  \n",
       "  5         person  \n",
       "  6         person  \n",
       "  7            cup  \n",
       "  8         person  \n",
       "  9            cup  \n",
       "  10           cup  \n",
       "  11        person  \n",
       "  12  dining table  \n",
       "  13          bowl  \n",
       "  14  dining table  \n",
       "  15        bottle  \n",
       "  16  dining table  \n",
       "  17           cup  \n",
       "  18           cup  \n",
       "  19        bottle  \n",
       "  20        person  ,\n",
       "  'caption': ['A yellow patterned table holding two tea kettles.',\n",
       "   'yellow table cloth under the teapot'],\n",
       "  'bbox_target': [0.0, 326.57, 425.71, 232.99]},\n",
       " 412: {'image_emb': tensor([[ 0.1031,  0.4392, -0.5586,  ...,  1.2842,  0.0909, -0.2137],\n",
       "          [-0.1589,  0.1189, -0.3799,  ...,  0.9189,  0.0196, -0.0304],\n",
       "          [ 0.1351,  0.3953, -0.3635,  ...,  0.9043,  0.1000, -0.0158],\n",
       "          [ 0.0562, -0.1133, -0.3369,  ...,  1.1963,  0.1801, -0.0726],\n",
       "          [ 0.3989,  0.3462, -0.0718,  ...,  0.3201,  0.1262,  0.0354]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2585,  0.0931, -0.5039,  ...,  0.0144,  0.0600, -0.1505],\n",
       "          [ 0.1198, -0.0488, -0.1704,  ...,  0.2394,  0.1743, -0.1898]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.3806e-01, 5.4047e-02, 1.0538e-03, 3.7193e-05, 8.0664e-01],\n",
       "          [9.3945e-01, 5.8228e-02, 6.7806e-04, 1.5366e-04, 1.2465e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  279.690186  156.673950  416.706360  568.920410    0.951304      0   \n",
       "  1   97.038513  390.395447  177.829773  578.137878    0.937969      0   \n",
       "  2   17.812607  287.009644  107.964828  425.282593    0.922986      0   \n",
       "  3   71.915298  398.288208  107.735153  427.332214    0.747225     36   \n",
       "  4  300.221710  553.793030  349.555939  571.158020    0.263959     39   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2      person  \n",
       "  3  skateboard  \n",
       "  4      bottle  ,\n",
       "  'caption': ['A man in a plaid shirt and green shorts standing next to a little boy.',\n",
       "   'A man in a plaid shirt.'],\n",
       "  'bbox_target': [278.23, 157.27, 139.49, 408.26]},\n",
       " 413: {'image_emb': tensor([[-2.4622e-01,  1.5503e-01,  2.0615e-02,  ...,  6.4551e-01,\n",
       "           -8.2153e-02,  5.9570e-02],\n",
       "          [-2.9492e-01,  2.3755e-01,  1.1548e-01,  ...,  9.9072e-01,\n",
       "           -3.7766e-04, -1.8433e-01],\n",
       "          [-2.9761e-01,  2.4036e-01, -3.6377e-02,  ...,  6.1572e-01,\n",
       "            1.8875e-02,  1.3397e-02],\n",
       "          [-1.5295e-01,  1.7554e-01, -6.7688e-02,  ...,  6.5576e-01,\n",
       "           -1.2305e-01,  1.4429e-01],\n",
       "          [-4.8309e-02, -9.8572e-02,  3.0981e-01,  ...,  6.0449e-01,\n",
       "           -2.0996e-01,  7.8064e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0542,  0.0427, -0.1161,  ...,  0.2471, -0.2507, -0.0748],\n",
       "          [-0.0098, -0.0145, -0.1858,  ..., -0.0265, -0.3772,  0.0964]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.3672e-01, 3.4082e-01, 5.3406e-03, 1.6190e-02, 6.5804e-04],\n",
       "          [6.9531e-01, 1.7041e-01, 9.1248e-02, 1.7136e-02, 2.5726e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    46.246994  158.127258  254.618759  396.792786    0.938738     56   \n",
       "  1   185.269836  132.645081  310.663696  296.292297    0.938180     56   \n",
       "  2   337.547211  120.235992  434.046844  246.219940    0.929141     56   \n",
       "  3   432.777344  172.165649  639.572510  474.503235    0.856088     57   \n",
       "  4   410.122467  220.676392  461.483917  239.703308    0.681856     73   \n",
       "  5   288.542847  222.074829  485.103455  377.476990    0.662057     60   \n",
       "  6   349.769012  243.710876  438.109650  273.704590    0.521977     73   \n",
       "  7   477.482391   95.411179  487.520355  120.068832    0.433800     75   \n",
       "  8   127.155052  174.185287  153.443207  222.290161    0.395201     73   \n",
       "  9   106.874123  184.091644  140.910339  227.166351    0.356857     73   \n",
       "  10  407.946533  231.867584  463.140869  247.773163    0.345960     73   \n",
       "  11    0.000000  152.699707   72.800308  194.512360    0.331149     73   \n",
       "  12    0.142918  153.805634   12.863493  196.360931    0.314448     73   \n",
       "  13    4.043760  199.830353   66.666321  242.953461    0.297095     73   \n",
       "  14   28.591578  155.006546   43.464050  192.633698    0.295835     73   \n",
       "  15  127.024391  164.803864  182.084686  221.754608    0.277495     73   \n",
       "  16  353.138763  103.798309  388.513153  121.720581    0.254627     56   \n",
       "  17   13.897106  157.256378   26.640614  194.831268    0.250431     73   \n",
       "  \n",
       "              name  \n",
       "  0          chair  \n",
       "  1          chair  \n",
       "  2          chair  \n",
       "  3          couch  \n",
       "  4           book  \n",
       "  5   dining table  \n",
       "  6           book  \n",
       "  7           vase  \n",
       "  8           book  \n",
       "  9           book  \n",
       "  10          book  \n",
       "  11          book  \n",
       "  12          book  \n",
       "  13          book  \n",
       "  14          book  \n",
       "  15          book  \n",
       "  16         chair  \n",
       "  17          book  ,\n",
       "  'caption': ['The Rocker chair with the green cushion, next to the all wooden chair.',\n",
       "   'A chair in a family room, near the door'],\n",
       "  'bbox_target': [186.19, 131.3, 124.84, 166.82]},\n",
       " 414: {'image_emb': tensor([[-0.2385,  0.2913,  0.1758,  ...,  0.8394, -0.1000,  0.2766],\n",
       "          [-0.0014,  0.4780,  0.0513,  ...,  0.6182, -0.1787, -0.1228],\n",
       "          [-0.1477,  0.4358,  0.0884,  ...,  0.4456, -0.1144,  0.0571]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2278,  0.3647, -0.0482,  ..., -0.4111,  0.0704, -0.1351],\n",
       "          [-0.2732,  0.2825,  0.4497,  ...,  0.2720, -0.2030, -0.1791]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0277, 0.5654, 0.4070],\n",
       "          [0.0300, 0.5229, 0.4473]], dtype=torch.float16),\n",
       "  'df_boxes':         xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   0.802904  168.670059  230.135696  284.977264    0.848637     45   \n",
       "  1   2.956289    1.483065  241.779373  175.704193    0.801455     49   \n",
       "  2   0.089085   33.606415  109.544304  182.484177    0.597709     49   \n",
       "  3   0.000000    4.132462  331.237152  491.712097    0.390103     60   \n",
       "  4  53.229923    0.525564  185.023666  104.728790    0.282912     49   \n",
       "  5  90.066620  140.884262  168.406769  176.816376    0.256279     49   \n",
       "  \n",
       "             name  \n",
       "  0          bowl  \n",
       "  1        orange  \n",
       "  2        orange  \n",
       "  3  dining table  \n",
       "  4        orange  \n",
       "  5        orange  ,\n",
       "  'caption': ['The oranges in the back.', 'Oranges stacked in a bowl.'],\n",
       "  'bbox_target': [52.94, 1.18, 188.24, 178.82]},\n",
       " 415: {'image_emb': tensor([[ 0.4038,  0.2272, -0.2742,  ...,  0.5674, -0.2174, -0.1350],\n",
       "          [ 0.6294,  0.3352, -0.1500,  ...,  0.4602, -0.1492, -0.2544],\n",
       "          [-0.0303,  0.4917, -0.1082,  ...,  1.2090,  0.1271,  0.1685],\n",
       "          [ 0.2729,  0.2251, -0.0912,  ...,  0.6191, -0.2003, -0.1675]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 2.4670e-01, -7.8735e-02, -6.8604e-02,  2.7173e-01,  1.5088e-01,\n",
       "           -5.5756e-02, -1.8384e-01, -6.4941e-01, -1.0736e-01, -2.6001e-01,\n",
       "            1.4954e-01, -2.1765e-01, -1.5723e-01, -3.5791e-01, -5.1453e-02,\n",
       "           -1.5100e-01,  1.0480e-01, -1.2659e-01,  2.0886e-01,  3.6572e-01,\n",
       "            1.6418e-01,  2.1887e-01,  7.2144e-02, -5.4108e-02,  5.8447e-01,\n",
       "           -4.1389e-03, -1.2793e-01,  2.8857e-01,  4.1699e-01,  2.0898e-01,\n",
       "            1.2134e-01,  2.2858e-02,  1.0590e-02, -1.0034e-01, -2.3535e-01,\n",
       "            1.3428e-01,  2.6880e-01,  5.5420e-02, -1.0504e-01,  1.2024e-01,\n",
       "           -1.9275e-01,  1.9177e-01, -2.2668e-01, -1.4771e-02,  6.4575e-02,\n",
       "            7.9102e-02,  3.2788e-01,  4.1235e-01,  1.6113e-02, -1.8982e-01,\n",
       "           -5.0635e-01, -2.9028e-01, -1.9852e-02,  2.3901e-01, -6.8237e-02,\n",
       "            1.9409e-01, -2.7930e-01, -3.4766e-01, -1.3818e-01,  5.5713e-01,\n",
       "           -6.5674e-02,  3.5474e-01, -2.2864e-01, -2.2681e-01, -1.3818e-01,\n",
       "           -7.2327e-02,  2.1155e-01,  3.4277e-01, -1.0696e-02,  3.4131e-01,\n",
       "            4.3896e-01,  3.1274e-01,  2.9517e-01,  2.4524e-01,  1.2781e-01,\n",
       "            2.7417e-01, -2.8271e-01,  5.9326e-02, -2.5513e-01,  2.0032e-01,\n",
       "           -6.8237e-02,  8.0811e-02,  7.4890e-02,  1.4664e-02, -1.3599e-01,\n",
       "            2.4243e-01,  3.6792e-01,  1.9302e-02,  2.7832e-01,  2.4280e-01,\n",
       "            1.6223e-01,  7.2656e-01, -1.2422e+00,  1.0205e+00, -1.9455e-03,\n",
       "            2.4304e-01,  1.7273e-01, -1.4746e-01,  1.8415e-03,  7.1411e-02,\n",
       "            4.9042e-02,  1.5808e-01,  1.2646e-01, -1.7761e-01, -2.3755e-01,\n",
       "            2.5659e-01, -7.0312e-02,  1.0582e-02,  3.4790e-02,  2.8076e-01,\n",
       "           -4.9316e-02,  7.1594e-02,  2.4979e-02, -2.5439e-01, -2.5049e-01,\n",
       "           -5.1025e-01,  1.7944e-02,  2.1606e-01, -2.6587e-01, -2.5000e-01,\n",
       "           -2.0496e-01, -4.3677e-01,  1.1792e-01,  2.2656e-01, -1.1421e-02,\n",
       "           -1.3037e-01, -1.3293e-01,  3.1738e-01, -2.4963e-01,  2.4609e-01,\n",
       "           -7.6172e-02,  5.6445e-01, -1.6907e-01,  4.6484e+00,  1.6382e-01,\n",
       "           -3.4595e-01, -4.4653e-01, -2.7075e-01, -3.1567e-01, -1.8140e-01,\n",
       "            3.8208e-01,  4.2432e-01,  1.4575e-01, -2.2888e-02, -2.4036e-01,\n",
       "           -3.0273e-02, -1.6736e-01, -6.6553e-01, -2.6416e-01, -7.8735e-03,\n",
       "           -6.9580e-01,  2.0361e-01,  4.2383e-01,  4.2114e-01,  2.5244e-01,\n",
       "           -3.7964e-01, -1.2383e-02, -6.2891e-01,  1.2433e-01, -8.0139e-02,\n",
       "           -1.9495e-01, -1.4050e-01,  1.6748e-01,  4.9019e-03,  3.8269e-02,\n",
       "           -3.0981e-01,  2.5342e-01, -9.7900e-02,  3.8062e-01, -2.7954e-01,\n",
       "           -1.3611e-02, -1.9067e-01,  1.3879e-01, -1.7651e-01, -4.0234e-01,\n",
       "           -4.4403e-02, -1.4355e-01,  1.7883e-01,  1.9604e-01,  2.2974e-01,\n",
       "            3.2471e-02,  3.0842e-03, -3.7766e-03, -5.6396e-02,  4.5288e-02,\n",
       "            1.7310e-01,  1.9849e-01, -7.5302e-03,  1.8921e-01,  2.4628e-02,\n",
       "            2.3425e-01, -4.1602e-01,  1.7529e-01, -3.0591e-01,  2.2247e-02,\n",
       "            3.4760e-02, -1.8091e-01,  3.3203e-01, -3.0835e-01,  6.7520e-03,\n",
       "           -5.5878e-02,  1.8274e-01,  4.3262e-01, -5.3613e-01, -2.5171e-01,\n",
       "           -3.4180e-02,  7.5500e-02, -4.2358e-01, -1.0071e-01, -4.0186e-01,\n",
       "           -4.9133e-02,  2.7612e-01,  1.8738e-01, -2.0544e-01, -4.6118e-01,\n",
       "            1.7383e-01, -9.9976e-02, -1.1652e-01,  3.0029e-01,  1.7297e-01,\n",
       "           -1.9669e-02,  8.8074e-02, -8.3679e-02,  6.7139e-01, -3.3789e-01,\n",
       "           -3.0908e-01,  4.1534e-02,  2.2949e-01,  2.3120e-01,  1.2756e-01,\n",
       "            2.1179e-01,  4.3799e-01, -2.6657e-02,  6.4148e-02, -8.9355e-02,\n",
       "            5.2734e-02, -2.8336e-02, -3.1006e-01,  3.5919e-02, -1.1859e-01,\n",
       "           -2.6840e-02,  2.9175e-01,  1.9028e-02, -1.0956e-01, -2.6758e-01,\n",
       "           -1.9531e-01, -1.9202e-01,  3.9990e-01, -1.4490e-01,  2.7319e-01,\n",
       "           -1.1981e-01, -6.1035e-03,  8.9417e-02, -4.7705e-01, -6.3281e-01,\n",
       "           -4.0942e-01, -2.1497e-01,  3.6713e-02,  5.5786e-02,  2.5439e-01,\n",
       "            1.1212e-01, -3.1299e-01,  1.8884e-01,  7.9956e-02,  2.3901e-01,\n",
       "           -7.9041e-02, -2.1436e-01, -1.9690e-01, -2.4878e-01,  2.6276e-02,\n",
       "           -1.3672e-01, -1.0724e-01, -2.5903e-01, -4.6338e-01, -2.3254e-01,\n",
       "            1.9678e-01,  2.4023e-01,  3.4888e-01,  2.3462e-01,  2.5903e-01,\n",
       "            1.0858e-01,  1.8018e-01,  4.2896e-01,  3.9429e-01, -2.1240e-01,\n",
       "           -1.4026e-01,  1.0059e-01, -2.1851e-01,  1.9482e-01, -4.9536e-01,\n",
       "            2.8564e-01,  6.1084e-01,  1.3953e-01, -5.8746e-02,  1.3611e-01,\n",
       "            3.0127e-01, -2.5586e-01,  5.2100e-01, -8.3191e-02, -1.9543e-01,\n",
       "           -9.3811e-02, -1.6345e-01, -1.1829e-01,  5.3009e-02,  2.5464e-01,\n",
       "           -5.0507e-03, -3.5522e-02, -1.0516e-01,  2.9590e-01,  7.1960e-02,\n",
       "           -1.5027e-01,  3.7720e-01,  4.6445e+00,  2.7271e-01,  2.8906e-01,\n",
       "            2.8223e-01,  4.0474e-03,  2.6685e-01,  2.9102e-01,  4.6313e-01,\n",
       "            4.3188e-01,  5.2917e-02,  2.4963e-01, -1.9360e-01, -2.8152e-02,\n",
       "            4.8682e-01,  3.5278e-01,  7.4707e-01, -1.1884e-01, -1.8428e+00,\n",
       "           -3.9209e-01, -6.8018e-01,  8.8654e-03, -2.2461e-02, -1.3171e-01,\n",
       "           -4.2969e-01,  2.4353e-01,  2.1460e-01,  5.0842e-02,  3.8379e-01,\n",
       "           -5.1318e-01,  1.4046e-02,  4.7168e-01, -7.2205e-02,  7.3792e-02,\n",
       "           -4.6661e-02,  2.0996e-01, -1.7078e-01,  7.2449e-02,  1.5503e-01,\n",
       "            4.1187e-01, -1.5076e-01, -2.0351e-03, -1.6022e-02, -3.3020e-02,\n",
       "           -3.1470e-01, -2.1191e-01, -6.2485e-03,  3.0908e-01,  1.2170e-01,\n",
       "           -3.2642e-01, -3.3665e-03,  2.1057e-02,  5.6122e-02, -3.5522e-02,\n",
       "            1.0559e-01, -1.1896e-01,  1.9958e-01,  3.4326e-01, -1.8396e-01,\n",
       "           -1.6235e-02,  2.0520e-01, -6.1584e-02,  2.7679e-02,  1.5222e-01,\n",
       "           -6.0596e-01, -3.2544e-01, -1.0516e-01,  3.0835e-01, -2.5879e-01,\n",
       "           -7.9407e-02, -1.3037e-01,  3.2520e-01,  7.7026e-02,  2.1228e-01,\n",
       "            2.9321e-01, -5.1025e-01, -1.5857e-01, -2.2827e-01, -1.7200e-01,\n",
       "           -5.0879e-01,  4.7192e-01, -4.1733e-03, -5.2539e-01,  2.6352e-02,\n",
       "            5.6549e-02, -2.2632e-01,  3.4619e-01, -1.2396e-01,  2.9370e-01,\n",
       "            1.9238e-01,  1.5173e-01,  1.6138e-01, -5.4779e-02, -6.7078e-02,\n",
       "            4.3359e-01, -3.6743e-01, -1.1957e-01, -1.0590e-01, -1.2140e-01,\n",
       "           -7.1045e-02,  1.5027e-01, -2.7710e-01,  3.2886e-01,  2.5732e-01,\n",
       "           -2.7075e-01, -7.2998e-02, -2.8833e-01,  3.0786e-01, -1.2469e-01,\n",
       "            1.4258e-01, -3.8965e-01, -1.2085e-01, -2.0911e-01,  3.4546e-01,\n",
       "           -3.4473e-01,  2.1683e-02,  4.2755e-02, -4.8645e-02,  5.1709e-01,\n",
       "           -4.9854e-01, -2.7197e-01, -2.2388e-01,  9.1003e-02, -8.0383e-02,\n",
       "           -1.5845e-01, -1.0602e-01, -3.6621e-02, -3.5591e-03,  1.7737e-01,\n",
       "           -9.4116e-02,  5.9319e-04, -3.8574e-02,  1.4014e-01, -3.4424e-02,\n",
       "            1.1914e-01,  1.4270e-01, -3.6035e-01,  2.9443e-01,  1.8176e-01,\n",
       "            2.4829e-01,  5.6335e-02,  1.7065e-01,  4.0833e-02, -9.5337e-02,\n",
       "           -1.8433e-01,  2.0496e-01, -2.3535e-01,  2.2021e-01, -3.2886e-01,\n",
       "            2.2107e-01, -1.5637e-01, -2.7686e-01,  4.4373e-02,  2.7863e-02,\n",
       "           -2.7197e-01, -1.5564e-01,  1.6189e-04,  2.1790e-01, -2.1460e-01,\n",
       "           -1.3684e-01,  1.8750e-01,  1.4722e-01, -6.9763e-02, -2.6074e-01,\n",
       "           -4.6295e-02,  1.5747e-01,  1.7456e-02, -3.3472e-01,  5.9766e-01,\n",
       "            2.5366e-01, -3.6401e-01,  1.9507e-01,  1.0095e-01, -1.7554e-01,\n",
       "           -8.6853e-02, -5.7716e-03,  1.1093e-02, -4.5215e-01, -4.8560e-01,\n",
       "           -1.2213e-01, -2.0117e-01, -1.6077e-01,  1.1455e+00,  4.6997e-01,\n",
       "            3.0615e-01,  2.0850e-01,  3.9429e-01, -8.7036e-02, -2.7298e-02,\n",
       "            3.8818e-01, -1.8604e-01,  2.1362e-02,  1.9141e-01, -1.7700e-01,\n",
       "           -5.9814e-01,  4.3799e-01, -2.4060e-01, -5.6190e-03, -1.5210e-01,\n",
       "           -8.9645e-03, -4.3726e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2954, 0.0735, 0.0054, 0.6255]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   305.383606   53.802505  640.000000  376.061462    0.832499     53   \n",
       "  1    46.227631   33.463120  325.476562  361.647461    0.803654     53   \n",
       "  2     0.037203  245.091888   48.805168  325.748260    0.735269     50   \n",
       "  3    66.567291  214.917297  146.961609  271.111389    0.656056     50   \n",
       "  4   393.731018  161.675446  472.882996  279.645294    0.655418     50   \n",
       "  5    45.979767   32.028229  325.577209  366.420074    0.576326     50   \n",
       "  6   177.052368  137.601822  225.600159  186.390320    0.572618     50   \n",
       "  7     6.624367  337.541473  134.088867  422.505249    0.522430     50   \n",
       "  8   211.865494   81.287750  304.172119  122.595886    0.397644     50   \n",
       "  9     7.641548  368.373291   84.495789  423.334778    0.298837     50   \n",
       "  10    9.472290    0.000000  640.000000  415.874268    0.295457     60   \n",
       "  11  266.493225   81.050110  303.147766  120.169983    0.283714     50   \n",
       "  \n",
       "              name  \n",
       "  0          pizza  \n",
       "  1          pizza  \n",
       "  2       broccoli  \n",
       "  3       broccoli  \n",
       "  4       broccoli  \n",
       "  5       broccoli  \n",
       "  6       broccoli  \n",
       "  7       broccoli  \n",
       "  8       broccoli  \n",
       "  9       broccoli  \n",
       "  10  dining table  \n",
       "  11      broccoli  ,\n",
       "  'caption': ['The three slices of pizza on the left.'],\n",
       "  'bbox_target': [0.0, 90.73, 368.65, 328.54]},\n",
       " 416: {'image_emb': tensor([[ 0.5059,  0.3567,  0.0113,  ...,  0.8281, -0.3274,  0.4392],\n",
       "          [-0.0852, -0.0095, -0.0887,  ...,  0.8135, -0.0417,  0.3018],\n",
       "          [-0.1130, -0.0229, -0.2959,  ...,  0.8867,  0.2351,  0.2927],\n",
       "          ...,\n",
       "          [-0.0925, -0.0606, -0.2783,  ...,  1.4531,  0.0275, -0.4065],\n",
       "          [ 0.2581,  0.1602,  0.1349,  ...,  0.8408, -0.1766,  0.2964],\n",
       "          [ 0.3157,  0.0850,  0.0945,  ...,  0.6089, -0.0464,  0.4004]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1316, -0.0345, -0.3657,  ...,  0.1088,  0.4373, -0.4719],\n",
       "          [ 0.2253,  0.1854, -0.1990,  ..., -0.0393,  0.2125, -0.1257]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.0526e-05, 9.7949e-01, 9.5963e-06, 3.6073e-04, 1.2917e-02, 1.0767e-03,\n",
       "           2.8253e-05, 7.6830e-05, 4.5357e-03, 1.6422e-03],\n",
       "          [1.6603e-03, 4.2578e-01, 2.6894e-04, 1.5991e-02, 3.3356e-02, 1.2195e-01,\n",
       "           1.2195e-01, 1.9913e-02, 2.2791e-01, 3.1311e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   192.519287   89.200607  420.379639  421.574768    0.951097      0   \n",
       "  1     0.276978  267.970703  193.664612  423.278137    0.942369      0   \n",
       "  2   542.330872  262.273499  622.793640  352.423218    0.904993      0   \n",
       "  3   403.571899  176.442566  576.985718  418.931030    0.890611      0   \n",
       "  4   157.224747  247.349274  211.258133  368.758759    0.879811      0   \n",
       "  5     0.398827  249.598450   33.655502  327.828552    0.877640      0   \n",
       "  6   513.984253  347.207825  544.926392  419.958862    0.874466     39   \n",
       "  7   589.434570  187.746643  640.000000  317.274475    0.873521      0   \n",
       "  8   199.986771    0.835632  638.719727  240.693970    0.835369     25   \n",
       "  9     0.163162  117.855560  258.917480  243.250763    0.685068     25   \n",
       "  10   44.115005  278.398529   90.101395  328.044891    0.631573      0   \n",
       "  11  620.261353  265.537720  640.000000  356.192627    0.575027      0   \n",
       "  12  420.853271  322.274048  552.946411  406.511414    0.478143     26   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2     person  \n",
       "  3     person  \n",
       "  4     person  \n",
       "  5     person  \n",
       "  6     bottle  \n",
       "  7     person  \n",
       "  8   umbrella  \n",
       "  9   umbrella  \n",
       "  10    person  \n",
       "  11    person  \n",
       "  12   handbag  ,\n",
       "  'caption': ['A red umbrella above the african american mans head.',\n",
       "   \"An umbrella that appears to be coming out of a man in a blue shirt's head.\"],\n",
       "  'bbox_target': [0.45, 117.73, 260.63, 152.03]},\n",
       " 417: {'image_emb': tensor([[-0.7939,  0.3894, -0.2988,  ...,  0.5371, -0.1786, -0.2032],\n",
       "          [ 0.1412,  0.3560, -0.2189,  ...,  0.4634, -0.0735,  0.5308],\n",
       "          [ 0.0468,  0.8765, -0.3037,  ...,  0.7388, -0.1415, -0.0786],\n",
       "          [-0.2791,  0.2695, -0.1860,  ...,  0.7466,  0.2103, -0.0199],\n",
       "          [ 0.0654,  0.1381,  0.1079,  ...,  0.8789,  0.0885, -0.4377],\n",
       "          [-0.0487,  0.6436,  0.1462,  ...,  0.7476,  0.1425,  0.0524]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.4583, -0.3447, -0.1561,  ...,  0.0540, -0.2365,  0.1259],\n",
       "          [ 0.1343, -0.4272, -0.3123,  ...,  0.2554, -0.3616, -0.1877]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.5367e-07],\n",
       "          [0.0000e+00, 1.0000e+00, 0.0000e+00, 5.9605e-08, 0.0000e+00, 1.4305e-06]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  118.936684    0.283116  239.057877  109.539009    0.926830     41   \n",
       "  1    0.075769  106.758186  231.000061  348.430054    0.909389     73   \n",
       "  2  115.772942  136.215027  196.263977  240.937515    0.908274     67   \n",
       "  3  395.435608    0.043869  465.101257   38.734341    0.844231     47   \n",
       "  4  273.171051    0.000000  339.945465   59.102142    0.725596     39   \n",
       "  5  237.066071   83.582390  500.000000  364.773224    0.566067     24   \n",
       "  6   80.022720   13.313532  148.616928   58.868454    0.544177     73   \n",
       "  7  258.548126   47.014141  336.452179  101.852325    0.267516     67   \n",
       "  \n",
       "           name  \n",
       "  0         cup  \n",
       "  1        book  \n",
       "  2  cell phone  \n",
       "  3       apple  \n",
       "  4      bottle  \n",
       "  5    backpack  \n",
       "  6        book  \n",
       "  7  cell phone  ,\n",
       "  'caption': ['A paperback book titled The Night Watch with a dark cover.',\n",
       "   'the book called the night watch by sarah waters'],\n",
       "  'bbox_target': [2.53, 107.87, 228.37, 241.85]},\n",
       " 418: {'image_emb': tensor([[-0.0058, -0.1199,  0.2096,  ...,  0.5054, -0.1019, -0.2159],\n",
       "          [-0.2668, -0.4478, -0.0983,  ...,  0.8867, -0.1815,  0.0553],\n",
       "          [-0.0956, -0.1315,  0.0529,  ...,  0.5615,  0.0107, -0.2927],\n",
       "          [ 0.2915, -0.3486,  0.1010,  ...,  0.0776, -0.0348, -0.2438]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0701, -0.3413, -0.0284,  ...,  0.1820, -0.3381, -0.4258],\n",
       "          [ 0.1860, -0.2842, -0.4238,  ...,  0.3550, -0.0497, -0.3760]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1484, 0.0054, 0.8276, 0.0183],\n",
       "          [0.0290, 0.0044, 0.2542, 0.7124]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0   13.062347  100.330734  221.849030  353.555359    0.928633     18  sheep\n",
       "  1  243.815430   61.315735  472.552795  186.655579    0.925015     18  sheep\n",
       "  2  191.406540  117.390747  328.763062  321.051575    0.839065     18  sheep,\n",
       "  'caption': ['A small white lamb near the fence.',\n",
       "   'A lamb next to a wire fence standing behind two other lambs.'],\n",
       "  'bbox_target': [246.97, 65.03, 227.61, 115.36]},\n",
       " 419: {'image_emb': tensor([[ 0.0889,  0.7065,  0.1353,  ...,  0.8232, -0.0368, -0.1945],\n",
       "          [ 0.2104,  0.2883, -0.2318,  ...,  1.3799, -0.1903, -0.2898],\n",
       "          [-0.0512,  0.3228, -0.0636,  ...,  0.3271, -0.4001, -0.0080],\n",
       "          ...,\n",
       "          [-0.0030, -0.3093, -0.2240,  ...,  0.7974,  0.0697, -0.2712],\n",
       "          [ 0.1892, -0.1324, -0.5347,  ...,  0.8887,  0.0098, -0.1731],\n",
       "          [-0.0289,  0.1648, -0.0273,  ...,  0.1433, -0.4248, -0.0206]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0254, -0.2457,  0.2325,  ...,  0.1764, -0.3254, -0.1580],\n",
       "          [ 0.1229,  0.0247, -0.0515,  ..., -0.2230,  0.1733,  0.0028]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.9019e-04, 3.3164e-04, 8.8623e-01, 7.5073e-02, 1.4365e-05, 1.6034e-05,\n",
       "           3.7750e-02],\n",
       "          [1.5259e-04, 2.7776e-05, 8.0322e-02, 8.3691e-01, 5.9605e-07, 9.5367e-07,\n",
       "           8.2825e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.265366  270.299774   86.840164  390.878632    0.930263      2   \n",
       "  1  554.181396  230.287506  626.734253  478.897156    0.863584      0   \n",
       "  2  188.724442  115.664383  449.558044  287.859741    0.767651      7   \n",
       "  3  144.718643  160.324432  631.157837  386.501801    0.766240      7   \n",
       "  4   13.966875  140.493820   37.389351  183.464905    0.737510      9   \n",
       "  5   16.989998  201.901855   36.748135  258.705872    0.722055      0   \n",
       "  6  569.039856    0.607727  638.881531   86.983948    0.693393      9   \n",
       "  7  530.591675  301.487366  565.247314  366.530396    0.658958     26   \n",
       "  8   38.533791  193.484863  225.534271  344.609619    0.620697      7   \n",
       "  9  508.323273  201.666473  531.920837  258.192841    0.429641      0   \n",
       "  \n",
       "              name  \n",
       "  0            car  \n",
       "  1         person  \n",
       "  2          truck  \n",
       "  3          truck  \n",
       "  4  traffic light  \n",
       "  5         person  \n",
       "  6  traffic light  \n",
       "  7        handbag  \n",
       "  8          truck  \n",
       "  9         person  ,\n",
       "  'caption': ['A white police van.', 'A police van being carried by a truck.'],\n",
       "  'bbox_target': [190.82, 115.27, 278.37, 174.37]},\n",
       " 420: {'image_emb': tensor([[-0.1331,  0.3735, -0.5625,  ...,  1.0859,  0.1510, -0.2108],\n",
       "          [ 0.1085,  0.3435, -0.1897,  ...,  1.3750, -0.2009, -0.3376],\n",
       "          [ 0.1088,  0.2480, -0.4148,  ...,  0.6597,  0.0602, -0.2471],\n",
       "          ...,\n",
       "          [-0.0284, -0.0343, -0.1731,  ...,  0.9014,  0.0409, -0.1720],\n",
       "          [ 0.0360, -0.1039, -0.2328,  ...,  0.6475,  0.3071, -0.2146],\n",
       "          [-0.1863,  0.1718, -0.4636,  ...,  0.4819,  0.3884, -0.2096]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-5.6915e-02,  2.4084e-01, -1.1841e-01, -1.5051e-01, -2.4512e-01,\n",
       "           -1.4673e-01, -2.1118e-01, -9.9268e-01, -1.7688e-01, -3.0981e-01,\n",
       "            1.4148e-01, -1.1615e-01, -3.8965e-01, -6.8115e-02, -1.3831e-01,\n",
       "           -2.0996e-02, -3.0298e-01, -3.9600e-01, -1.9409e-01,  2.4463e-01,\n",
       "            3.1281e-02,  5.4834e-01, -7.2937e-03,  7.4805e-01,  3.5376e-01,\n",
       "           -3.0029e-01, -2.0496e-01, -1.7166e-02,  1.0272e-01, -1.2036e-01,\n",
       "           -3.1177e-01, -5.9814e-01,  6.1157e-02, -2.0154e-01, -8.2080e-01,\n",
       "            2.8427e-02, -1.8201e-01, -2.8491e-01, -2.5452e-02,  1.7676e-01,\n",
       "            1.5129e-02, -1.8042e-01,  4.9707e-01, -2.6016e-02, -1.5991e-01,\n",
       "            1.8115e-01,  2.5806e-01,  1.8738e-01, -3.4033e-01,  1.9275e-01,\n",
       "            1.6125e-01, -3.2275e-01, -3.5156e-02,  1.9464e-03,  2.6657e-02,\n",
       "            3.4570e-01, -2.2144e-01, -6.1584e-02,  6.2599e-03,  2.2827e-01,\n",
       "            2.0837e-01,  2.2388e-01,  5.7959e-01, -7.0524e-04, -2.6270e-01,\n",
       "           -1.9897e-01,  8.7708e-02, -4.3213e-02, -3.1763e-01, -2.5952e-01,\n",
       "            3.4473e-01, -2.9114e-02,  3.9917e-01,  1.1591e-01,  3.1647e-02,\n",
       "           -8.5205e-02, -5.0293e-01,  1.8295e-02,  2.0496e-01, -6.4893e-01,\n",
       "           -5.1807e-01,  1.6724e-01,  9.6985e-02,  2.5635e-01,  9.6802e-02,\n",
       "           -2.6901e-02, -2.2180e-01, -2.0642e-01,  6.7200e-02,  1.1353e-01,\n",
       "            2.3279e-01, -8.6670e-03, -1.3369e+00,  3.4033e-01, -2.9639e-01,\n",
       "            1.9373e-01, -4.2871e-01, -2.3645e-01,  2.5269e-01,  1.7285e-01,\n",
       "           -4.4116e-01, -9.6680e-02, -1.9910e-01,  1.8958e-01,  2.6587e-01,\n",
       "            1.6980e-01,  9.9548e-02,  1.1578e-01, -2.4170e-01, -2.3804e-01,\n",
       "            3.3911e-01,  1.5942e-01, -1.7322e-01,  2.1497e-01, -4.1431e-01,\n",
       "            8.4778e-02, -2.0300e-01,  7.4365e-01,  6.3362e-03,  2.1033e-01,\n",
       "           -4.2554e-01, -2.9449e-02,  3.0200e-01, -8.7158e-02,  2.0642e-01,\n",
       "           -1.0999e-01,  2.7344e-01,  2.4979e-02, -3.1763e-01, -1.1011e-01,\n",
       "           -4.4891e-02,  3.8379e-01,  6.0577e-02,  3.6152e+00, -4.9707e-01,\n",
       "            1.0330e-02,  8.0566e-02, -1.7090e-01,  2.9126e-01, -1.1859e-01,\n",
       "           -5.2295e-01,  3.2471e-01, -8.8928e-02, -1.2619e-02, -2.8870e-02,\n",
       "            2.5787e-03,  1.3525e-01, -4.4043e-01, -2.3193e-01,  3.9722e-01,\n",
       "            2.7124e-01, -7.5500e-02,  5.1855e-01, -2.4158e-01, -2.8418e-01,\n",
       "            7.6416e-02, -2.8125e-01,  6.7810e-02,  4.0845e-01, -1.3269e-01,\n",
       "            4.8657e-01, -1.8774e-01, -2.4390e-04,  4.4165e-01,  1.6327e-02,\n",
       "           -2.8271e-01,  5.0439e-01,  9.8145e-02,  2.5977e-01,  2.5122e-01,\n",
       "            1.2093e-02, -7.5073e-02,  2.1399e-01, -6.7810e-02, -2.9883e-01,\n",
       "            4.9591e-02, -1.4294e-01,  2.4002e-02,  4.9268e-01,  1.1938e-01,\n",
       "           -2.4048e-01,  6.9763e-02, -2.2681e-01, -2.7466e-01, -1.5454e-01,\n",
       "            3.1030e-01, -1.9043e-01, -1.8253e-03, -3.4521e-01,  2.8394e-01,\n",
       "            2.7686e-01, -2.2144e-01, -5.2930e-01,  7.2693e-02,  2.0401e-02,\n",
       "           -2.6733e-01, -4.5746e-02,  1.6296e-01,  2.1619e-01,  3.5431e-02,\n",
       "            4.8730e-01, -1.3000e-01,  9.6313e-02,  2.2339e-02, -4.0332e-01,\n",
       "           -1.4856e-01,  2.9663e-01, -2.1497e-01,  2.5366e-01, -2.7417e-01,\n",
       "            3.0371e-01,  1.6479e-01, -4.0674e-01, -2.4329e-01, -2.1716e-01,\n",
       "           -1.1157e-01, -2.0740e-01, -5.0079e-02,  2.6978e-02, -7.8857e-02,\n",
       "           -3.2275e-01,  2.1497e-01, -2.4292e-01, -1.9882e-02, -4.5972e-01,\n",
       "           -4.1931e-02, -4.2229e-03, -2.6392e-01,  3.5840e-01,  4.1602e-01,\n",
       "           -1.3672e-01,  2.6050e-01,  9.5947e-02, -2.4402e-01,  3.7524e-01,\n",
       "            5.5542e-02,  5.9631e-02,  2.8418e-01,  1.9946e-01, -1.6455e-01,\n",
       "           -1.2598e-01, -3.7085e-01, -1.9287e-01,  2.8027e-01, -1.5015e-01,\n",
       "           -5.4150e-01,  6.3293e-02,  5.1025e-01,  1.0016e-01,  2.8369e-01,\n",
       "           -1.2329e-01,  5.0342e-01,  1.5112e-01,  2.2925e-01, -1.9424e-02,\n",
       "           -2.1835e-02,  2.6196e-01,  1.4844e-01,  2.8101e-01,  4.1504e-01,\n",
       "           -4.4983e-02, -2.8516e-01,  5.3314e-02, -5.3009e-02,  4.5703e-01,\n",
       "            1.4502e-01, -2.0599e-02,  8.4717e-02, -1.7908e-01,  1.2500e-01,\n",
       "           -1.8274e-01,  1.9165e-02, -8.3191e-02,  1.4087e-01,  9.4055e-02,\n",
       "           -1.6638e-01, -1.0315e-01, -2.5342e-01,  4.5166e-01,  1.7834e-01,\n",
       "           -8.2886e-02,  3.9642e-02,  4.4653e-01,  2.6465e-01,  2.0911e-01,\n",
       "            1.9592e-01, -1.6861e-02,  8.0627e-02, -2.1631e-01, -4.0820e-01,\n",
       "           -5.4150e-01, -8.0139e-02,  2.0361e-01, -3.1763e-01, -7.6538e-02,\n",
       "            1.3403e-01, -6.6602e-01,  4.4019e-01,  5.6671e-02,  2.0557e-01,\n",
       "           -8.6548e-02, -3.5962e-01, -3.2446e-01,  2.5464e-01, -9.6497e-02,\n",
       "            2.9694e-02,  1.2476e-01,  6.0120e-02,  5.4053e-01,  1.8811e-01,\n",
       "            2.5742e-02,  4.2700e-01,  3.6055e+00, -1.9067e-01,  1.8188e-01,\n",
       "            8.2214e-02,  1.5552e-01, -2.3694e-01, -2.3889e-01,  7.4072e-01,\n",
       "            4.6387e-02,  1.9299e-01, -2.9614e-01,  2.0676e-03, -1.0834e-01,\n",
       "            1.5796e-01,  1.3086e-01,  7.2632e-02, -1.7670e-02, -2.2832e+00,\n",
       "           -8.4351e-02, -4.3427e-02,  2.5928e-01,  1.0547e-01,  9.9243e-02,\n",
       "           -5.1367e-01, -1.3672e-01, -1.5857e-01,  3.3789e-01, -9.6436e-02,\n",
       "           -1.6443e-01,  1.5015e-01,  1.1676e-01, -2.8809e-02,  3.5858e-02,\n",
       "            6.8665e-02,  3.5767e-01, -9.1797e-02,  2.6001e-01,  2.1875e-01,\n",
       "           -7.3059e-02, -2.2742e-01,  1.0175e-01,  8.1970e-02, -1.9287e-01,\n",
       "           -2.3645e-01, -1.6748e-01, -3.6163e-02,  2.3035e-01, -1.0413e-01,\n",
       "            1.2061e-01,  6.6956e-02,  4.8901e-01,  5.6006e-01, -4.4800e-01,\n",
       "           -4.9829e-01,  6.3660e-02,  1.1975e-01, -1.5784e-01,  3.5889e-01,\n",
       "           -1.7236e-01, -4.6875e-01, -1.1371e-01, -6.6711e-02,  3.6987e-01,\n",
       "           -4.2206e-02, -1.1035e-01, -4.3976e-02,  6.9971e-01,  2.2986e-01,\n",
       "            3.1079e-01,  2.3462e-01,  5.1605e-02,  3.9917e-01,  3.2764e-01,\n",
       "            3.6108e-01,  3.0688e-01,  2.6050e-01, -2.1106e-01,  3.1543e-01,\n",
       "           -5.1318e-01, -1.0419e-01, -1.7700e-01, -3.8788e-02,  1.0199e-01,\n",
       "            1.6675e-01,  2.9370e-01, -2.3035e-01, -3.6621e-01,  9.9121e-02,\n",
       "            4.4141e-01, -3.1592e-01,  9.0820e-02, -5.8105e-01, -2.8149e-01,\n",
       "            8.2275e-02, -1.2604e-02, -5.0507e-02,  1.3519e-02,  2.7856e-01,\n",
       "            3.7427e-01, -2.6807e-01, -8.2520e-02,  1.1987e-01,  4.8999e-01,\n",
       "           -1.8298e-01,  4.5142e-01, -2.4536e-01, -2.9834e-01,  4.9622e-02,\n",
       "            2.1460e-01, -1.1780e-01,  1.8140e-01, -1.4648e-01,  4.1187e-01,\n",
       "           -1.0034e-01, -4.3449e-03, -4.5410e-02,  4.1479e-01, -1.3049e-01,\n",
       "           -3.4448e-01, -2.0764e-01, -4.4775e-01,  5.1025e-01, -3.0167e-02,\n",
       "            1.5198e-01, -6.9763e-02,  9.8633e-02, -1.7419e-01,  4.3854e-02,\n",
       "           -1.7908e-01, -1.9482e-01,  1.5149e-01,  4.2188e-01, -6.5137e-01,\n",
       "            5.9326e-01, -2.8467e-01, -7.9150e-01,  1.9205e-04,  3.2739e-01,\n",
       "            7.0129e-02, -1.3647e-01, -8.8867e-02, -5.1666e-02,  3.2520e-01,\n",
       "           -3.9307e-01,  2.2522e-01,  7.8491e-02,  2.6636e-01,  8.4900e-02,\n",
       "           -8.2092e-02, -1.0327e-01, -1.1914e-01,  1.6357e-01, -2.0123e-03,\n",
       "           -6.0364e-02,  5.7617e-01, -2.8149e-01, -8.3435e-02, -3.8086e-01,\n",
       "            1.3196e-01, -6.7932e-02,  3.1592e-01, -6.3286e-03, -3.7903e-02,\n",
       "            2.5616e-03,  1.2573e-01,  2.8687e-01,  2.6794e-02,  5.9033e-01,\n",
       "            1.4453e-01, -4.6753e-01, -9.4421e-02,  6.8237e-02, -2.0569e-01,\n",
       "           -3.7280e-01, -9.7595e-02, -3.4375e-01, -4.1382e-02,  2.0581e-01,\n",
       "            2.2363e-01,  2.8760e-01, -3.8147e-02,  5.8594e-01,  3.2764e-01,\n",
       "            1.8677e-01,  4.3427e-02, -1.5833e-01, -4.7112e-03,  1.6693e-02,\n",
       "           -3.5449e-01, -7.4646e-02, -2.4681e-03,  1.9434e-01,  7.0435e-02,\n",
       "           -2.2852e-01, -1.6754e-02,  3.2104e-02, -2.8030e-02, -1.8542e-01,\n",
       "            8.0490e-03, -6.4014e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.6989e-04, 4.8399e-05, 2.6345e-05, 2.6703e-05, 6.1214e-05, 6.8545e-06,\n",
       "           3.0589e-04, 9.8584e-01, 1.3214e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   162.276093  235.534622  265.288055  541.133545    0.918106      0   \n",
       "  1    71.590622  242.705414  144.500534  485.148712    0.897012      0   \n",
       "  2   252.121857  235.457733  296.219940  441.367584    0.862686      0   \n",
       "  3   136.535126  240.066605  193.494354  469.826843    0.854184      0   \n",
       "  4    70.113525  279.395386  125.129395  382.494385    0.846182     26   \n",
       "  5   308.129578    0.495369  403.678772  162.899109    0.820914      9   \n",
       "  6   383.805267  271.903900  404.021027  296.587006    0.726063      2   \n",
       "  7    69.322403  209.814667  153.791321  251.176208    0.723712     25   \n",
       "  8   113.932648  201.798538  282.299103  291.888916    0.667742     25   \n",
       "  9   301.562927  267.344269  344.134521  286.966278    0.660803      2   \n",
       "  10  380.056458  235.275024  425.000000  492.283325    0.617104      0   \n",
       "  11  338.131104  212.798981  424.273071  277.457733    0.589491     25   \n",
       "  12   78.927872  275.341187  100.297226  290.091370    0.552976      2   \n",
       "  13  385.944855  276.607391  424.594788  350.010468    0.520686     26   \n",
       "  14  288.026550  297.158539  299.018738  321.686493    0.455118     26   \n",
       "  15  304.378845  255.535400  384.486084  286.035828    0.362478      5   \n",
       "  16  380.537231  346.524780  422.018005  410.706055    0.261979     26   \n",
       "  \n",
       "               name  \n",
       "  0          person  \n",
       "  1          person  \n",
       "  2          person  \n",
       "  3          person  \n",
       "  4         handbag  \n",
       "  5   traffic light  \n",
       "  6             car  \n",
       "  7        umbrella  \n",
       "  8        umbrella  \n",
       "  9             car  \n",
       "  10         person  \n",
       "  11       umbrella  \n",
       "  12            car  \n",
       "  13        handbag  \n",
       "  14        handbag  \n",
       "  15            bus  \n",
       "  16        handbag  ,\n",
       "  'caption': ['the red and the white umbrellas'],\n",
       "  'bbox_target': [67.6, 208.1, 357.4, 60.4]},\n",
       " 421: {'image_emb': tensor([[ 0.2881,  0.2822,  0.0388,  ...,  0.3059,  0.3110,  0.0126],\n",
       "          [-0.1826,  0.2595, -0.3215,  ...,  1.0244,  0.4929, -0.0796],\n",
       "          [-0.0417,  0.2947, -0.2040,  ...,  0.3372,  0.5854, -0.1599],\n",
       "          ...,\n",
       "          [-0.0751,  0.2827, -0.3560,  ...,  0.7539,  0.2991,  0.0358],\n",
       "          [ 0.1790,  0.3679,  0.0183,  ...,  0.2214,  0.2991,  0.0578],\n",
       "          [-0.0923,  0.1652, -0.1439,  ...,  0.7534,  0.5327, -0.0134]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0120,  0.2917, -0.4302,  ...,  0.2529,  0.1035, -0.1812],\n",
       "          [ 0.1566,  0.0576, -0.3042,  ...,  0.5493,  0.3103, -0.0262]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7224e-01, 8.9355e-02, 3.3905e-02, 9.1248e-03, 5.8365e-04, 9.2163e-02,\n",
       "           1.4668e-03, 6.0107e-01],\n",
       "          [9.6664e-03, 1.0077e-01, 5.6519e-02, 2.2141e-02, 1.5152e-04, 6.6101e-02,\n",
       "           1.2565e-04, 7.4463e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     0.268579  111.000313  261.031219  248.374420    0.933481     39   \n",
       "  1   271.863953   22.511303  499.962219  300.691650    0.920186      0   \n",
       "  2   210.589935  238.765961  295.674561  371.311859    0.905749     40   \n",
       "  3   415.372131  134.385162  464.711517  221.364883    0.812598     40   \n",
       "  4   247.544724  257.699646  498.520752  367.922058    0.782546      0   \n",
       "  5    24.727524  223.823929   76.320694  299.601654    0.734204     40   \n",
       "  6     0.000000   64.944420  205.289032  192.121445    0.713775      0   \n",
       "  7     0.250691  251.316833   69.184395  356.210938    0.689057      0   \n",
       "  8   397.288666  180.563385  499.318848  305.201538    0.601964      0   \n",
       "  9     0.000000   58.258461  196.416336  365.839386    0.479304      0   \n",
       "  10    2.623099  237.182755  184.972885  369.403687    0.372112      0   \n",
       "  \n",
       "            name  \n",
       "  0       bottle  \n",
       "  1       person  \n",
       "  2   wine glass  \n",
       "  3   wine glass  \n",
       "  4       person  \n",
       "  5   wine glass  \n",
       "  6       person  \n",
       "  7       person  \n",
       "  8       person  \n",
       "  9       person  \n",
       "  10      person  ,\n",
       "  'caption': ['A hand holds a wine bottle while pouring the wine.',\n",
       "   'A hand pours wine from a bottle into an upheld glass.'],\n",
       "  'bbox_target': [4.18, 66.04, 205.64, 119.54]},\n",
       " 422: {'image_emb': tensor([[-0.0958, -0.0789, -0.3088,  ...,  0.4414,  0.4094, -0.1696],\n",
       "          [-0.1923,  0.3003, -0.3000,  ...,  0.5889,  0.0111,  0.1322],\n",
       "          [-0.7598,  0.4858,  0.0792,  ...,  0.5176,  0.1122, -0.0047]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0710,  0.1630,  0.0523,  ...,  0.6240,  0.0588,  0.0752],\n",
       "          [ 0.1304,  0.2136, -0.5029,  ...,  0.0941, -0.0345,  0.3650]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0042, 0.9888, 0.0071],\n",
       "          [0.1121, 0.8413, 0.0467]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  159.364761  119.563339  285.498444  364.745728    0.923329      0  person\n",
       "  1   99.071571   23.819855  214.304352  366.782440    0.903586      0  person\n",
       "  2  162.132767  145.453766  171.536880  169.696381    0.326926     39  bottle,\n",
       "  'caption': ['A woman wearing a green jacket is skiing.',\n",
       "   'the female who is smiling at the camera with the green shirt'],\n",
       "  'bbox_target': [100.98, 21.73, 110.64, 343.34]},\n",
       " 423: {'image_emb': tensor([[ 4.6924e-01,  6.4880e-02, -3.3398e-01,  1.4233e-01, -1.4026e-01,\n",
       "           -3.7817e-01,  2.1619e-01,  1.9714e-01, -4.6631e-01,  2.3755e-01,\n",
       "            2.9443e-01,  3.8525e-01,  5.7275e-01,  2.6709e-01,  1.3611e-01,\n",
       "           -4.6216e-01,  3.2935e-01, -2.8564e-01, -1.8420e-01,  2.9492e-01,\n",
       "            2.7368e-01,  1.6846e-01, -5.5176e-02, -2.2461e-01, -2.8687e-01,\n",
       "            3.5840e-01, -1.0602e-01,  2.0837e-01,  3.2080e-01, -2.4622e-01,\n",
       "            3.2910e-01,  1.5698e-01, -3.1470e-01,  6.0059e-01,  2.3645e-01,\n",
       "            5.1514e-01, -1.5271e-01, -1.9180e-02, -1.6333e-01,  1.5068e+00,\n",
       "            3.1433e-02,  2.2363e-01,  5.1117e-02, -3.6719e-01,  4.7925e-01,\n",
       "           -2.0527e+00,  3.1836e-01,  3.1177e-01, -2.4988e-01,  3.8965e-01,\n",
       "            1.9958e-01, -3.4155e-01,  2.4658e-01,  1.1224e-01,  1.0284e-01,\n",
       "            3.6841e-01, -7.8418e-01, -1.7017e-01, -4.2847e-01, -9.8877e-02,\n",
       "            3.1250e-01, -4.0527e-01, -1.1072e-01, -1.2622e-01, -2.6343e-01,\n",
       "           -6.0120e-02,  6.5527e-01, -5.0171e-02,  6.1646e-02, -2.5830e-01,\n",
       "           -1.2042e-01, -1.3257e-01,  4.9780e-01,  2.5537e-01, -1.8359e-01,\n",
       "            3.2324e-01,  2.3730e-01, -1.5205e-02,  1.1102e-01, -2.6611e-01,\n",
       "            1.8591e-01,  4.1797e-01, -6.7529e-01, -1.4697e-01,  4.5288e-02,\n",
       "            1.1505e-01,  2.7856e-01, -5.5127e-01,  3.8965e-01, -2.9834e-01,\n",
       "            1.4429e-01,  3.3130e-01, -5.7695e+00,  1.9421e-01,  2.8271e-01,\n",
       "            4.7510e-01, -2.1814e-01,  7.1228e-02,  5.3809e-01, -4.8022e-01,\n",
       "            8.3191e-02,  3.3276e-01,  5.4047e-02,  2.5195e-01,  7.5867e-02,\n",
       "            2.6782e-01, -2.3320e+00, -3.2275e-01,  3.6182e-01,  2.6147e-01,\n",
       "           -7.0679e-02, -3.6523e-01, -2.4109e-01, -7.3547e-02, -1.8616e-01,\n",
       "           -2.7954e-01,  1.1859e-01, -7.4561e-01,  6.8896e-01,  1.2103e-01,\n",
       "           -1.8570e-02,  3.9282e-01, -1.6754e-02,  2.6660e-01, -2.5220e-01,\n",
       "            7.0068e-02, -1.2042e-01,  5.3516e-01,  6.3354e-02, -5.1709e-01,\n",
       "           -2.2876e-01,  1.8311e-01, -2.3804e-01,  7.7734e-01,  1.4481e-02,\n",
       "            3.5376e-01,  3.3862e-01,  2.9761e-01,  6.7578e-01, -9.2163e-03,\n",
       "            1.3037e-01,  3.9600e-01, -3.9087e-01,  4.2358e-02,  1.5404e-02,\n",
       "           -1.7593e-02, -3.2495e-01,  3.8818e-01, -4.6606e-01,  1.4502e-01,\n",
       "            2.7734e-01,  6.3574e-01,  1.8066e-01, -1.7017e-01, -8.0750e-02,\n",
       "           -4.1821e-01,  1.3214e-02, -7.9468e-02,  1.0956e-02, -1.3306e-01,\n",
       "           -3.0981e-01, -2.6074e-01,  6.7505e-02,  2.0466e-03, -2.3010e-02,\n",
       "           -1.0760e-01,  6.1279e-01,  3.2910e-01,  4.5679e-01, -1.0461e-01,\n",
       "            4.9219e-01, -1.5900e-02,  1.6589e-01,  7.2571e-02,  3.1714e-01,\n",
       "           -5.8740e-01, -9.9023e-01,  3.6792e-01, -5.0342e-01,  2.0154e-01,\n",
       "            4.8022e-01, -5.8398e-01, -6.0242e-02,  8.8867e-02,  9.9976e-02,\n",
       "            6.2408e-02,  4.2305e-03, -2.2925e-01, -1.4673e-01, -1.6983e-02,\n",
       "            4.4019e-01, -2.0789e-01, -1.0382e-01,  2.5073e-01, -3.4448e-01,\n",
       "            1.7749e-01,  6.1646e-02,  1.2213e-01, -1.1553e+00,  2.1057e-02,\n",
       "            5.2521e-02,  3.1177e-01,  5.7959e-01, -3.0786e-01,  1.1652e-01,\n",
       "            7.8308e-02,  7.8857e-02, -1.5063e-01, -2.8442e-01, -2.1277e-01,\n",
       "            3.3057e-01, -3.5547e-01, -1.2817e-01, -2.8320e-01, -3.2080e-01,\n",
       "           -6.6162e-01, -2.1683e-02,  5.0000e-01, -7.6514e-01, -4.1870e-02,\n",
       "           -2.6760e-03, -1.0059e+00,  3.1067e-02, -2.9346e-01,  2.1802e-01,\n",
       "           -6.0498e-01,  5.9033e-01, -1.1163e-01, -5.6335e-02, -6.8542e-02,\n",
       "           -6.2744e-01,  5.1270e-01,  2.2607e-01,  8.1726e-02,  1.0229e-01,\n",
       "           -3.4985e-01,  2.0508e-02, -1.5503e-01,  5.7275e-01,  1.6565e-01,\n",
       "            5.8203e-01,  1.4441e-01,  3.5278e-02, -6.9482e-01, -4.0771e-01,\n",
       "            6.8311e-01,  3.5400e-01, -4.5898e-01,  1.4001e-01,  1.5320e-01,\n",
       "           -5.3564e-01,  3.3997e-02,  1.2183e-01, -2.7145e-02,  2.1622e-02,\n",
       "            1.7090e-02, -2.6562e-01,  2.0422e-01,  8.3740e-01, -2.3767e-01,\n",
       "            4.0512e-03, -1.6821e-01,  1.5030e-02,  7.8027e-01,  7.6294e-02,\n",
       "           -4.7046e-01,  2.0837e-01,  2.4255e-01,  5.0439e-01,  3.0396e-01,\n",
       "           -5.8929e-02, -1.0468e-01,  1.3855e-01,  3.8501e-01,  1.6321e-01,\n",
       "           -4.7668e-02,  1.4844e-01, -2.2791e-01,  1.2596e-02, -3.1543e-01,\n",
       "            9.2346e-02,  3.4821e-02, -2.8613e-01, -2.4805e-01,  1.5527e-01,\n",
       "            6.4795e-01, -5.8685e-02, -8.6288e-03, -2.3682e-01,  2.1271e-02,\n",
       "           -6.0645e-01, -2.4109e-01,  9.8389e-02,  6.9824e-02, -4.2773e-01,\n",
       "            7.8369e-02,  1.6675e-01,  5.4785e-01,  3.1494e-01,  1.6040e-01,\n",
       "            2.6172e-01, -4.8169e-01, -5.7666e-01,  3.2275e-01,  1.1725e-01,\n",
       "           -1.1426e-01, -4.1412e-02,  2.6538e-01,  3.3105e-01, -1.9263e-01,\n",
       "           -1.9958e-01,  4.0405e-01,  7.7539e-01,  4.0796e-01,  3.9038e-01,\n",
       "            5.0635e-01, -1.0394e-01, -4.5929e-02, -1.5637e-01,  6.5308e-02,\n",
       "            6.4209e-02,  4.3677e-01, -4.3896e-01,  3.3600e-02, -3.4082e-01,\n",
       "            1.0223e-01,  5.1165e-04, -5.0439e-01,  6.8848e-02, -1.5686e-01,\n",
       "           -3.9642e-02,  2.5610e-01,  2.1497e-01,  3.0127e-01,  1.6861e-02,\n",
       "            4.1431e-01, -1.3647e-01,  1.0846e-01, -2.3877e-01, -1.6431e-01,\n",
       "           -3.2910e-01, -3.6450e-01,  8.7830e-02, -4.0845e-01,  3.2196e-02,\n",
       "           -5.6543e-01,  1.5027e-01,  3.2013e-02, -2.3108e-01,  2.4536e-01,\n",
       "           -3.9844e-01, -1.7944e-01,  8.1299e-02,  1.4722e-01, -2.1130e-01,\n",
       "           -6.3904e-02,  7.8247e-02,  3.8818e-01, -1.9727e-01, -4.9854e-01,\n",
       "           -4.7632e-01,  4.2944e-01, -2.6074e-01, -1.7773e-01, -2.2571e-01,\n",
       "            2.6270e-01, -2.9443e-01, -1.2910e+00, -2.2742e-01, -4.0698e-01,\n",
       "            1.3037e-01,  2.3877e-01, -1.7371e-01, -1.5234e-01,  1.1035e-01,\n",
       "            3.0469e-01,  6.0938e-01, -1.6632e-02, -1.9836e-01,  3.0396e-01,\n",
       "            6.4354e-03,  3.5980e-02, -6.9153e-02,  2.8955e-01, -1.5222e-01,\n",
       "            1.3306e-01, -2.4963e-01,  2.2791e-01, -1.5088e-01, -1.0858e-01,\n",
       "            3.9600e-01,  6.4026e-02, -4.3121e-02, -1.8115e-01, -1.4343e-01,\n",
       "            1.6064e-01, -2.8030e-02,  3.3667e-01, -7.8076e-01,  4.0918e-01,\n",
       "            4.4580e-01,  9.4360e-02,  2.5684e-01, -2.5952e-01, -2.5586e-01,\n",
       "           -4.7876e-01, -5.2376e-03, -2.2852e-01, -3.8965e-01, -4.6729e-01,\n",
       "            7.8979e-02,  4.5557e-01, -1.8140e-01,  5.0293e-01, -3.7769e-01,\n",
       "           -3.2886e-01,  6.3049e-02,  2.6245e-01, -6.3281e-01, -1.1011e-01,\n",
       "           -6.7017e-02, -1.6394e-01,  7.1436e-01, -1.7223e-03,  9.0942e-02,\n",
       "            2.1375e-01, -1.3916e-01,  1.9006e-01, -7.5195e-02,  1.8530e-01,\n",
       "           -9.5276e-02,  1.2903e-01,  2.3230e-01, -2.8320e-01, -1.4001e-01,\n",
       "           -5.0830e-01, -3.8330e-02,  4.8340e-01,  1.2097e-01,  2.8833e-01,\n",
       "           -3.8635e-02,  2.8076e-01,  2.6758e-01,  1.6235e-01,  9.5520e-03,\n",
       "           -4.0131e-02,  8.2764e-02, -3.7659e-02, -1.3940e-01,  3.3447e-02,\n",
       "           -1.3074e-01, -2.1997e-01,  1.3379e-01, -7.9651e-02, -3.3276e-01,\n",
       "            3.1934e-01,  1.0278e-01, -7.6367e-01,  1.8970e-01, -3.7329e-01,\n",
       "            1.1177e-02,  9.8816e-02,  8.1543e-02, -2.8833e-01,  1.8494e-01,\n",
       "           -6.8604e-02,  7.3291e-01, -2.8687e-01,  1.4355e-01,  6.0516e-02,\n",
       "            1.3623e-01,  4.6313e-01,  9.3201e-02, -5.9863e-01, -1.9409e-02,\n",
       "           -7.6074e-01,  1.3660e-01,  1.0437e-01, -2.3120e-01,  2.0996e-01,\n",
       "            3.3508e-02,  5.2521e-02,  3.4448e-01,  5.0830e-01,  1.7371e-01,\n",
       "           -1.3220e-01,  4.7974e-01,  6.9043e-01, -6.2354e-01,  1.3474e-02,\n",
       "           -2.2980e-02, -1.5088e-01,  4.8584e-01, -8.8428e-01, -5.1221e-01,\n",
       "           -2.5098e-01, -5.9418e-02,  4.6173e-02, -2.4475e-01,  3.1128e-01,\n",
       "            3.9331e-01, -1.1749e-01,  1.5454e-01,  6.1066e-02,  6.1816e-01,\n",
       "            3.1403e-02, -6.2061e-01, -1.2866e-01, -5.0598e-02,  2.0874e-01,\n",
       "            1.2939e-01,  9.0942e-03]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.4397, -0.1200, -0.1630,  ...,  0.2025,  0.1335, -0.2871],\n",
       "          [ 0.1965,  0.0901,  0.1803,  ..., -0.0690, -0.2664, -0.1375]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.],\n",
       "          [1.]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin       xmax        ymax  confidence  class      name\n",
       "  0  152.852203  358.800507  294.28067  405.451782    0.657314      4  airplane\n",
       "  1   59.504333  166.826630  637.31311  418.561157    0.544288      4  airplane,\n",
       "  'caption': ['a black and silver plane inside a big building',\n",
       "   'A blue and white airplane with three small circles visible on the body.'],\n",
       "  'bbox_target': [72.24, 171.69, 399.92, 246.03]},\n",
       " 424: {'image_emb': tensor([[-0.0097,  0.2998,  0.0277,  ...,  0.8174, -0.0268, -0.1935],\n",
       "          [-0.0742,  0.4387, -0.0879,  ...,  0.6416,  0.0871, -0.0109],\n",
       "          [ 0.1353, -0.0464, -0.5464,  ...,  0.6646, -0.0956, -0.2286],\n",
       "          [-0.2622,  0.0967, -0.0013,  ...,  0.6538,  0.1765,  0.0163]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3025, -0.1011, -0.0234,  ...,  0.4773,  0.4321, -0.4441],\n",
       "          [ 0.0570,  0.0349, -0.2742,  ...,  0.3569,  0.4565, -0.3740]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.9365e-01, 2.9028e-01, 3.0470e-04, 2.1570e-01],\n",
       "          [7.2119e-01, 1.6602e-01, 2.2733e-04, 1.1237e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class       name\n",
       "  0  201.348114  44.774628  295.406158  312.938507    0.915652      0     person\n",
       "  1   91.200935  46.757965  294.877960  255.401581    0.884285     37  surfboard\n",
       "  2   74.234062  89.234756   84.703926  119.712067    0.792661      0     person,\n",
       "  'caption': ['surfer with wet toes',\n",
       "   'man wearing surfing suit and holding surfboard as we walks out of water'],\n",
       "  'bbox_target': [201.5, 42.14, 97.34, 274.83]},\n",
       " 425: {'image_emb': tensor([[-0.2329,  1.0605,  0.0153,  ...,  0.8037,  0.0519, -0.2817],\n",
       "          [-0.1012,  0.3379, -0.3042,  ...,  1.2871,  0.0467,  0.2656],\n",
       "          [ 0.0451,  0.7451, -0.3865,  ...,  0.8877,  0.0124, -0.0706],\n",
       "          [-0.1161,  0.5649, -0.3103,  ...,  0.8540,  0.2820, -0.3699],\n",
       "          [ 0.0425,  0.9067, -0.1650,  ...,  0.4609, -0.3733, -0.0884]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-1.8384e-01,  1.5723e-01, -5.6836e-01,  8.3679e-02,  2.3804e-01,\n",
       "            1.6553e-01,  5.2032e-02, -2.7661e-01, -4.3896e-01,  1.5271e-01,\n",
       "           -5.4932e-01, -1.6309e-01, -2.0264e-01, -1.2213e-01,  1.1188e-01,\n",
       "            2.7563e-01,  2.6270e-01,  2.0642e-01, -8.7891e-02,  4.7272e-02,\n",
       "            1.7883e-02,  2.1021e-01,  5.6580e-02,  3.4473e-01,  1.2744e-01,\n",
       "           -9.2651e-02,  4.8438e-01,  1.1346e-01, -1.1086e-02, -7.5928e-02,\n",
       "           -3.7689e-03,  1.7834e-01, -1.0986e-01,  7.2144e-02, -4.5898e-01,\n",
       "           -3.6816e-01,  6.6345e-02, -9.9243e-02, -3.2227e-01,  1.5405e-01,\n",
       "           -3.7500e-01,  4.9756e-01,  2.9297e-01,  2.3438e-02, -5.8807e-02,\n",
       "            1.0254e-01,  3.8647e-01, -9.0027e-02, -2.2437e-01, -1.8469e-01,\n",
       "           -1.5820e-01,  2.1088e-02,  2.2534e-01,  4.1333e-01, -1.9531e-01,\n",
       "            1.9580e-01, -1.9397e-01, -2.6703e-02,  4.1797e-01,  3.5767e-01,\n",
       "           -9.0088e-02,  7.0862e-02, -1.2408e-01,  4.3604e-01,  1.0284e-01,\n",
       "            1.3074e-01,  6.4209e-02,  8.5645e-01, -2.4976e-01,  3.0518e-01,\n",
       "            4.1382e-01, -1.4551e-01,  4.2114e-01,  3.0350e-02, -5.0568e-02,\n",
       "           -2.5195e-01, -2.5659e-01, -1.4832e-01, -1.4722e-01,  1.2006e-01,\n",
       "            1.0437e-01,  3.7451e-01,  5.6055e-01, -1.7432e-01,  5.1660e-01,\n",
       "           -3.1714e-01, -6.8542e-02, -1.3501e-01,  1.1487e-01, -1.7737e-01,\n",
       "           -2.8076e-02, -1.4612e-01, -9.1016e-01,  6.7236e-01, -2.0691e-01,\n",
       "            3.0591e-01,  6.1493e-03,  1.2744e-01,  5.4590e-01,  2.1375e-01,\n",
       "            4.1724e-01, -5.8685e-02, -1.3318e-01, -7.0605e-01, -1.2573e-01,\n",
       "            1.9373e-01, -9.7656e-03,  3.8330e-01,  3.2568e-01,  1.8811e-01,\n",
       "           -2.1985e-01, -9.6497e-02,  2.4377e-01,  3.9215e-03, -5.9906e-02,\n",
       "           -3.5205e-01,  5.7678e-02,  4.3144e-03, -7.8308e-02,  9.7412e-02,\n",
       "            2.4048e-01, -8.9990e-01,  2.8613e-01,  2.6416e-01,  3.6670e-01,\n",
       "            1.8274e-01,  1.2048e-01, -4.6265e-01,  7.4280e-02, -4.6362e-01,\n",
       "           -1.1768e-01, -8.4473e-02, -1.3550e-01,  3.6250e+00, -5.1660e-01,\n",
       "            7.8735e-02,  3.0842e-03, -6.0693e-01,  1.0394e-01, -5.5664e-01,\n",
       "            2.0218e-03,  4.7363e-01,  2.2876e-01, -1.1951e-01,  4.2786e-02,\n",
       "           -4.0552e-01, -1.7615e-01, -3.8818e-01, -2.3239e-02,  2.8076e-01,\n",
       "           -4.2188e-01,  2.1899e-01,  1.8250e-01, -6.2895e-04, -1.5783e-03,\n",
       "            3.5474e-01,  5.2551e-02,  4.2816e-02, -3.3984e-01,  2.5732e-01,\n",
       "            1.8738e-01, -4.9835e-02,  2.7637e-01, -2.2168e-01,  1.1530e-01,\n",
       "           -5.8228e-02,  3.3911e-01, -3.6646e-01,  6.5674e-02,  1.3489e-01,\n",
       "            3.5669e-01, -2.9077e-01,  3.7256e-01,  1.1444e-01, -4.1797e-01,\n",
       "           -3.6469e-02,  5.1025e-02, -5.7739e-02,  9.5703e-01, -1.1823e-01,\n",
       "           -2.1289e-01,  1.3574e-01,  2.1408e-02, -6.4148e-02, -4.3848e-01,\n",
       "            2.9419e-01,  1.5955e-01, -3.1433e-02, -2.2598e-02, -3.8306e-01,\n",
       "            5.1025e-01, -3.1982e-01, -3.7231e-01, -7.2632e-02,  2.2705e-01,\n",
       "            6.2988e-02,  1.5726e-03, -8.9905e-02, -4.4495e-02, -1.2524e-01,\n",
       "            1.6418e-01, -1.3501e-01, -1.7810e-01, -5.9448e-02, -3.9642e-02,\n",
       "            8.5083e-02,  1.1115e-01, -4.9829e-01,  1.7090e-01, -1.2500e-01,\n",
       "            4.4043e-01,  8.1604e-02,  2.1393e-02, -4.2456e-01,  1.0120e-01,\n",
       "            1.4221e-01, -3.3887e-01, -2.0477e-02,  3.1006e-01, -6.1676e-02,\n",
       "            5.2686e-01,  5.0000e-01,  4.1229e-02, -2.4292e-01,  3.1128e-02,\n",
       "           -4.1748e-01,  2.6807e-01,  1.6650e-01, -3.2642e-01, -5.9509e-02,\n",
       "            1.6895e-01, -8.8120e-03, -3.0640e-01, -2.3279e-01,  1.3770e-01,\n",
       "            1.7981e-01,  9.4666e-02,  2.3669e-01, -2.0300e-01,  1.0535e-01,\n",
       "           -1.7197e-02,  4.8364e-01,  1.4441e-01, -1.9666e-01, -3.2959e-01,\n",
       "           -1.7212e-01, -1.2413e-02,  2.7563e-01, -1.6309e-01,  1.0449e-01,\n",
       "           -2.2168e-01, -3.7207e-01,  1.8250e-01, -1.1798e-01, -2.1887e-01,\n",
       "            3.8623e-01, -4.1113e-01, -9.7229e-02, -1.2195e-01,  2.0654e-01,\n",
       "           -3.3789e-01,  3.3545e-01, -2.1530e-02,  7.6714e-03, -2.6318e-01,\n",
       "            1.5466e-01,  3.1299e-01, -1.2512e-01, -2.2839e-01,  5.3906e-01,\n",
       "            1.9519e-01,  5.9296e-02,  1.0156e-01,  2.1509e-01,  1.6748e-01,\n",
       "            2.6245e-01,  8.7158e-02, -3.8330e-02, -4.7900e-01,  4.4975e-03,\n",
       "           -2.3230e-01, -2.3401e-01,  3.0518e-01,  1.2305e-01, -2.6855e-01,\n",
       "           -1.2476e-01, -6.0547e-02, -1.2764e-02, -1.6333e-01,  4.8828e-01,\n",
       "            2.5391e-01,  6.5918e-01, -1.7712e-01, -7.1526e-03, -2.7637e-01,\n",
       "           -1.9507e-01, -3.1836e-01,  1.8701e-01, -3.4973e-02, -4.6973e-01,\n",
       "           -2.5586e-01,  1.7181e-02, -1.4929e-01,  6.9580e-02,  3.5571e-01,\n",
       "           -3.2153e-01, -6.5979e-02, -2.5122e-01,  2.3779e-01,  1.8234e-02,\n",
       "            1.3184e-01,  3.5107e-01,  3.6230e+00,  2.3547e-01,  1.3657e-02,\n",
       "           -4.6906e-02, -4.1199e-02,  9.9792e-02,  8.1482e-03,  2.8052e-01,\n",
       "           -1.2500e-01, -5.1575e-02,  3.5742e-01, -6.7566e-02, -6.2402e-01,\n",
       "            6.1914e-01,  3.0786e-01,  2.6050e-01,  1.1768e-01, -9.4287e-01,\n",
       "           -1.6464e-02, -1.2952e-01,  1.3855e-01,  2.2461e-01,  1.1188e-01,\n",
       "           -6.9702e-02,  9.3384e-02, -1.6638e-01, -2.8882e-01,  1.2585e-01,\n",
       "           -7.0117e-01, -4.3921e-01,  2.9640e-03,  3.3130e-01,  4.0356e-01,\n",
       "            7.0724e-03,  3.3618e-01, -7.3120e-02, -3.9746e-01,  5.9906e-02,\n",
       "           -7.1220e-03,  1.3245e-01,  3.0957e-01,  2.0813e-01, -2.5342e-01,\n",
       "           -1.2093e-02,  7.9468e-02,  1.6016e-01, -4.6600e-02,  4.0112e-01,\n",
       "           -1.9397e-01, -4.3970e-01,  2.0984e-01,  3.2739e-01,  2.9028e-01,\n",
       "           -2.3547e-01, -2.2302e-01,  2.5928e-01, -1.4929e-01, -2.2839e-01,\n",
       "           -1.8494e-01, -2.8857e-01, -3.1885e-01,  2.0142e-01,  3.8403e-01,\n",
       "           -1.6699e-01, -1.7627e-01, -2.8833e-01,  3.5583e-02,  1.0834e-01,\n",
       "           -2.6440e-01, -5.0146e-01,  7.3291e-01, -9.9365e-02, -1.2695e-01,\n",
       "           -4.6875e-01, -1.3367e-01, -9.1614e-02, -2.4805e-01, -1.8713e-01,\n",
       "           -4.6826e-01,  1.5613e-01, -3.4326e-01,  2.5562e-01,  3.3960e-01,\n",
       "           -2.6416e-01,  2.0691e-01, -2.2021e-01, -3.2349e-01,  3.2544e-01,\n",
       "           -1.6891e-02,  1.1212e-01,  1.2524e-01, -2.7319e-01, -2.1484e-01,\n",
       "           -1.4832e-01,  2.8296e-01,  2.3010e-01,  7.3047e-01, -2.0218e-02,\n",
       "           -1.8921e-01, -1.5503e-01, -6.8726e-02, -1.3074e-01,  7.4524e-02,\n",
       "            3.4424e-01, -5.6946e-02,  2.6099e-01,  5.5762e-01, -1.1053e-01,\n",
       "            1.6724e-01, -1.0376e-01,  3.5693e-01,  3.1677e-02,  3.9551e-01,\n",
       "            1.6626e-01, -2.5879e-01, -1.0309e-01,  1.9153e-01,  9.4528e-03,\n",
       "           -4.8462e-01,  2.1497e-01, -7.7026e-02,  3.9209e-01,  3.5248e-02,\n",
       "           -2.0569e-01, -5.0928e-01,  5.1953e-01, -2.6245e-01,  8.1726e-02,\n",
       "           -1.8906e-02, -2.3486e-01,  1.1414e-01, -5.0171e-02, -5.7324e-01,\n",
       "            1.6357e-01,  2.2913e-01, -3.0078e-01, -1.8262e-01, -1.6492e-01,\n",
       "            1.8933e-01,  1.6992e-01,  3.1714e-01, -1.8579e-01,  5.1660e-01,\n",
       "           -6.9946e-02,  2.9492e-01, -3.5889e-01,  1.5479e-01,  2.1118e-01,\n",
       "            5.2734e-02,  6.9885e-02, -3.8647e-01, -1.4392e-01,  7.7934e-03,\n",
       "           -9.1171e-03,  3.1128e-02, -3.5962e-01, -8.2910e-01, -3.6401e-01,\n",
       "            1.6956e-01,  9.1187e-02, -2.4948e-02, -4.5715e-02,  2.8229e-03,\n",
       "           -3.2788e-01,  2.5366e-01, -2.4097e-01, -1.0016e-01,  6.6162e-01,\n",
       "           -5.9052e-02, -2.4365e-01, -3.3423e-01, -3.3051e-02,  1.2408e-01,\n",
       "           -3.6353e-01, -1.8994e-01, -2.0947e-01,  6.1401e-02, -1.3269e-01,\n",
       "           -2.4512e-01, -3.8403e-01, -1.6553e-01,  4.8413e-01,  2.7832e-01,\n",
       "            7.6660e-02, -1.1896e-01, -9.3384e-02, -3.0029e-01,  3.7964e-02,\n",
       "           -2.4072e-01, -3.0469e-01,  1.1902e-01,  1.9507e-01,  2.7344e-01,\n",
       "            2.7246e-01,  6.3965e-02,  2.7930e-01, -9.4788e-02,  2.0581e-01,\n",
       "           -2.8174e-01, -1.5173e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0481e-03, 2.6488e-04, 7.3047e-01, 1.5640e-02, 2.5244e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  420.378082   79.662201  621.080627  347.746368    0.940792     41      cup\n",
       "  1    0.000000    1.593719  110.619415  306.616028    0.879053     41      cup\n",
       "  2  134.470886  316.277557  443.988708  445.884705    0.777892     52  hot dog\n",
       "  3  117.004868  257.351715  414.719666  370.881805    0.775412     52  hot dog,\n",
       "  'caption': ['A hot dog with saurkraut sitting closest to a bottle of ketchup.'],\n",
       "  'bbox_target': [109.55, 262.02, 294.78, 99.39]},\n",
       " 426: {'image_emb': tensor([[-8.3618e-02,  4.6558e-01, -1.7969e-01,  ...,  1.0654e+00,\n",
       "            4.4952e-02,  3.2446e-01],\n",
       "          [-2.4292e-01,  1.9507e-01, -1.6418e-01,  ...,  7.4951e-01,\n",
       "           -1.6479e-01,  2.7441e-01],\n",
       "          [-1.4008e-02,  4.7412e-01, -4.2511e-02,  ...,  1.0908e+00,\n",
       "           -1.4844e-01, -2.2400e-01],\n",
       "          ...,\n",
       "          [-1.2352e-02,  2.0764e-01, -3.3960e-01,  ...,  9.8877e-01,\n",
       "            7.9041e-02,  2.9388e-02],\n",
       "          [ 7.7148e-02, -2.8467e-01, -5.3857e-01,  ...,  9.4189e-01,\n",
       "           -1.6162e-01, -2.1948e-01],\n",
       "          [ 1.0843e-03,  4.8193e-01, -8.9478e-02,  ...,  6.1279e-01,\n",
       "            4.7638e-02,  2.2046e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1901,  0.0885, -0.0453,  ...,  0.1365, -0.0378, -0.0486],\n",
       "          [ 0.3203,  0.3835, -0.2297,  ..., -0.2363,  0.0005,  0.0499]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.0544e-01, 4.9292e-01, 6.2065e-03, 9.0790e-04, 1.6022e-04, 1.7285e-06,\n",
       "           1.7881e-07, 2.9443e-01],\n",
       "          [1.7151e-01, 8.1787e-01, 8.0185e-03, 3.1090e-04, 7.3850e-05, 2.2173e-05,\n",
       "           2.9802e-07, 2.1915e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    38.701889  121.667511  203.331589  474.945435    0.949335      0   \n",
       "  1   369.527161  115.761887  528.866516  475.440552    0.936209      0   \n",
       "  2   221.819519  149.237305  294.059875  354.540527    0.925050      0   \n",
       "  3   272.648682  171.943237  312.591980  327.203796    0.841764      0   \n",
       "  4   500.150299  297.208984  566.804993  377.996338    0.751407     71   \n",
       "  5   217.658844  107.899231  256.835938  163.645355    0.745836     58   \n",
       "  6   553.016296  203.952515  566.570007  283.180298    0.736700     39   \n",
       "  7   305.295044  275.783081  351.468262  365.618042    0.692302     56   \n",
       "  8   165.172119  161.092972  208.497406  274.162231    0.553405     69   \n",
       "  9   525.997498  229.894928  547.697937  283.991608    0.447891     39   \n",
       "  10  311.619507  243.238373  332.446228  259.526520    0.424661     45   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3         person  \n",
       "  4           sink  \n",
       "  5   potted plant  \n",
       "  6         bottle  \n",
       "  7          chair  \n",
       "  8           oven  \n",
       "  9         bottle  \n",
       "  10          bowl  ,\n",
       "  'caption': ['A woman in a red sweater in front of the sink.',\n",
       "   'A woman with short dark hair wearing a long red shirt'],\n",
       "  'bbox_target': [369.15, 118.51, 158.2, 354.08]},\n",
       " 427: {'image_emb': tensor([[-0.0402,  0.7393, -0.0673,  ...,  0.9297, -0.0895, -0.2461],\n",
       "          [-0.0507,  0.4429, -0.0620,  ...,  0.7417,  0.2593,  0.1017],\n",
       "          [-0.0511,  0.3599, -0.2150,  ...,  0.7148,  0.0442,  0.2522]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3674, -0.2942, -0.1891,  ...,  0.1299,  0.1342, -0.2236],\n",
       "          [-0.1527,  0.1526,  0.2578,  ...,  0.2076, -0.2136, -0.2805]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.8968e-05, 2.7176e-02, 9.7266e-01],\n",
       "          [9.4385e-01, 8.9722e-03, 4.6997e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  128.342529  147.022293  260.767822  473.615540    0.913339      0  person\n",
       "  1  239.857880  209.610443  508.009491  474.750244    0.747779     69    oven\n",
       "  2    0.025635  209.571899  154.676453  268.604004    0.654155     71    sink\n",
       "  3  241.003632  295.076782  354.195831  474.080017    0.621028     69    oven,\n",
       "  'caption': ['A closed white oven in a kitchen that a little boy is standing next to.',\n",
       "   'A white over door across from a boy in a hooded sweatshirt.'],\n",
       "  'bbox_target': [246.43, 296.11, 107.74, 172.13]},\n",
       " 428: {'image_emb': tensor([[-0.5249,  0.4641, -0.2045,  ...,  1.0215,  0.1229, -0.3162],\n",
       "          [-0.2258,  0.5449, -0.1532,  ...,  0.7720, -0.1329, -0.1349],\n",
       "          [ 0.0945,  0.0015, -0.3408,  ...,  0.8140,  0.0446, -0.0149],\n",
       "          [-0.3889,  0.3584,  0.2067,  ...,  1.2051,  0.0598,  0.0506],\n",
       "          [-0.4626,  0.4258, -0.1414,  ...,  0.8120,  0.1230, -0.1990]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.5205, -0.3489,  0.1021,  ...,  0.0506, -0.4160, -0.4067],\n",
       "          [-0.3015, -0.1552, -0.0183,  ..., -0.1139, -0.3958, -0.5244]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.1426e-01, 7.0007e-02, 3.1424e-04, 1.4992e-03, 3.1372e-01],\n",
       "          [3.0396e-01, 4.0497e-02, 6.5470e-04, 1.1421e-02, 6.4355e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  146.386383    0.000000  275.914368  138.137955    0.909501     44   spoon\n",
       "  1  201.395584  110.999443  457.391449  325.740173    0.790649     45    bowl\n",
       "  2  278.412567  206.403427  306.440948  231.454727    0.778860     51  carrot\n",
       "  3  383.101837  239.029175  499.665649  328.346954    0.702189      0  person\n",
       "  4    0.448155    0.000000  497.706421  325.703827    0.695133     69    oven,\n",
       "  'caption': ['A stove with soup cooking on it', 'The stove, pot and bowl.'],\n",
       "  'bbox_target': [0.0, 2.05, 488.96, 326.25]},\n",
       " 429: {'image_emb': tensor([[-0.4766,  0.3660, -0.2072,  ...,  0.8438,  0.1831, -0.1755],\n",
       "          [-0.1759,  0.1851,  0.0812,  ...,  0.8306, -0.0119,  0.0453],\n",
       "          [-0.0859,  0.0603, -0.2050,  ...,  0.7163,  0.2673, -0.2026],\n",
       "          [-0.1279,  0.2605, -0.4192,  ...,  0.6211,  0.2485, -0.2019]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1044, -0.0309, -0.2410,  ..., -0.1131, -0.0674, -0.2700],\n",
       "          [-0.0237, -0.0245, -0.3545,  ..., -0.1292, -0.0513, -0.2515]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.8848e-01, 1.9014e-05, 4.5923e-01, 3.5205e-01],\n",
       "          [2.0459e-01, 3.7694e-04, 4.0063e-01, 3.9429e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  174.139099   38.627396  620.451843  354.163147    0.940162     17   horse\n",
       "  1  509.939453   90.194946  622.579712  254.742859    0.938113      0  person\n",
       "  2   11.175293   97.319366  319.707458  356.857391    0.850748     17   horse\n",
       "  3  595.062378  171.348877  629.977661  265.866394    0.279463     56   chair,\n",
       "  'caption': ['The horse to the right of another horse.',\n",
       "   'The horse to the right'],\n",
       "  'bbox_target': [171.91, 40.93, 452.5, 315.29]},\n",
       " 430: {'image_emb': tensor([[ 0.0143,  0.3540,  0.0243,  ...,  0.9995,  0.1007, -0.1360],\n",
       "          [-0.4441,  0.4312, -0.2024,  ...,  0.9941,  0.2805, -0.2671],\n",
       "          [-0.2954,  0.2529, -0.3103,  ...,  1.6367,  0.1083, -0.1826],\n",
       "          [ 0.3765,  0.1091, -0.2722,  ...,  0.4978,  0.3696, -0.1771],\n",
       "          [ 0.4080,  0.0909, -0.2487,  ...,  0.4580,  0.2263, -0.1681]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1306,  0.2698,  0.0415,  ..., -0.1879,  0.0092,  0.0714],\n",
       "          [-0.1281,  0.3071, -0.1593,  ..., -0.0395, -0.0879,  0.1245]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.6286e-05, 9.9951e-01, 5.0354e-04, 1.1146e-05, 1.8895e-05],\n",
       "          [2.5034e-06, 1.0000e+00, 1.1235e-04, 1.0967e-05, 2.3186e-05]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  295.345428  121.446106  515.441162  298.469482    0.945505     41   \n",
       "  1  412.104431   31.565872  553.203857  172.131088    0.936229     41   \n",
       "  2  256.765381  119.943604  327.903259  302.680115    0.842115     44   \n",
       "  3    0.000000    5.495895  639.213379  476.754089    0.761108     60   \n",
       "  4    0.449463  132.389282  134.286957  320.329590    0.631594     55   \n",
       "  5  257.818268  210.914062  319.814178  301.822388    0.444031     44   \n",
       "  6   45.214668    0.209572  232.996552   87.022736    0.343081     45   \n",
       "  7  191.036087   85.909912  391.617371  344.358826    0.272432     60   \n",
       "  \n",
       "             name  \n",
       "  0           cup  \n",
       "  1           cup  \n",
       "  2         spoon  \n",
       "  3  dining table  \n",
       "  4          cake  \n",
       "  5         spoon  \n",
       "  6          bowl  \n",
       "  7  dining table  ,\n",
       "  'caption': ['A glass of water', 'A glass of water on the table.'],\n",
       "  'bbox_target': [409.89, 30.2, 142.38, 142.38]},\n",
       " 431: {'image_emb': tensor([[-0.0161,  0.2249,  0.0466,  ...,  0.6147, -0.0886,  0.1112],\n",
       "          [-0.0196,  0.1061, -0.1318,  ...,  0.9238, -0.0453,  0.3289],\n",
       "          [-0.0705,  0.2318,  0.1874,  ...,  0.5225, -0.2383,  0.0912],\n",
       "          ...,\n",
       "          [-0.1372,  0.2430,  0.1962,  ...,  0.4673, -0.2505,  0.3013],\n",
       "          [-0.2871,  0.2646, -0.0233,  ...,  1.4121, -0.2324,  0.2556],\n",
       "          [-0.1678,  0.0795,  0.0865,  ...,  0.6943, -0.0773,  0.2190]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0306,  0.0252, -0.4519,  ..., -0.1826,  0.2042,  0.3318],\n",
       "          [-0.2058, -0.1584, -0.2676,  ..., -0.3689,  0.2729, -0.2203]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0372, 0.4673, 0.0366, 0.0800, 0.0613, 0.2424, 0.0751],\n",
       "          [0.1193, 0.2817, 0.0649, 0.1193, 0.0758, 0.2159, 0.1230]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  261.692474   67.102432  544.004883  325.428772    0.921934      0  person\n",
       "  1  304.346771    0.560806  418.996857  122.556015    0.857805      0  person\n",
       "  2  140.120255   68.670670  490.101807  421.488586    0.853971      0  person\n",
       "  3  450.305298  118.496277  610.945923  271.073303    0.843736     63  laptop\n",
       "  4    0.000000   86.514465  369.401276  424.431213    0.838111      0  person\n",
       "  5  346.062134  321.263092  520.262939  425.325439    0.749110     63  laptop\n",
       "  6  427.127594  228.501953  602.471313  420.837891    0.632115     63  laptop\n",
       "  7  527.085815  230.665741  600.654907  416.265503    0.468235     63  laptop,\n",
       "  'caption': ['Black laptop in the background',\n",
       "   'computer on the right in the back in the right hand picture'],\n",
       "  'bbox_target': [452.91, 122.66, 158.32, 144.89]},\n",
       " 432: {'image_emb': tensor([[-0.0063,  0.2605,  0.0391,  ...,  1.0791,  0.2976, -0.0475],\n",
       "          [-0.1060,  0.3604, -0.3540,  ...,  0.9155,  0.3921, -0.3271],\n",
       "          [-0.0685, -0.1110, -0.0101,  ...,  0.7637,  0.2229, -0.0975],\n",
       "          [ 0.2343,  0.0207, -0.1855,  ...,  0.6777,  0.1522, -0.4543],\n",
       "          [-0.2435,  0.2754,  0.0584,  ...,  0.8618,  0.0592, -0.2920],\n",
       "          [-0.4006, -0.1005,  0.0591,  ...,  1.4092,  0.2266,  0.0384]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1472,  0.3506, -0.6841,  ...,  0.0945,  0.0734, -0.1825],\n",
       "          [-0.4268, -0.0365, -0.3933,  ...,  0.3132,  0.0312, -0.3464]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.2695e-01, 3.6865e-01, 5.9605e-08, 2.6855e-03, 5.9307e-05, 1.7605e-03],\n",
       "          [9.4580e-01, 5.3345e-02, 0.0000e+00, 5.5134e-05, 3.3975e-06, 7.8535e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    39.179031   86.360596  346.841248  637.521545    0.949922      0  person\n",
       "  1   254.586029  151.902267  436.255310  411.113831    0.885757      0  person\n",
       "  2   334.726288  405.299011  361.189423  478.633606    0.828086     39  bottle\n",
       "  3   323.269928  356.807251  345.844269  377.698425    0.757990     65  remote\n",
       "  4   395.787476  253.894836  479.784668  424.659851    0.752853     56   chair\n",
       "  5   402.235168  245.183350  446.178528  275.745850    0.554677     65  remote\n",
       "  6   395.023315  184.723404  436.354309  229.892593    0.447461     73    book\n",
       "  7   251.548248  350.513519  339.861115  439.108978    0.446674     62      tv\n",
       "  8   251.347229  349.590942  338.698120  438.480530    0.422686     56   chair\n",
       "  9   421.614471  244.998505  445.691833  263.947144    0.407505     65  remote\n",
       "  10  432.846985  182.991104  461.336853  228.219986    0.311453     73    book,\n",
       "  'caption': ['A man holding remote with lines shirt',\n",
       "   'A man with white lined shirt holding the joystick'],\n",
       "  'bbox_target': [255.68, 151.78, 187.5, 260.53]},\n",
       " 433: {'image_emb': tensor([[-0.1036,  0.0051,  0.2042,  ...,  1.0156,  0.1348,  0.1453],\n",
       "          [-0.2115,  0.1757, -0.0911,  ...,  0.6978,  0.2411,  0.4944],\n",
       "          [ 0.1174,  0.2389, -0.1189,  ...,  0.4204,  0.3345,  0.1326],\n",
       "          ...,\n",
       "          [-0.4541,  0.8022, -0.1796,  ...,  0.7920,  0.0414, -0.0752],\n",
       "          [ 0.0650,  0.0122, -0.3252,  ...,  0.6548,  0.2430,  0.0552],\n",
       "          [-0.1237, -0.1774, -0.0712,  ...,  0.8833,  0.3066,  0.1633]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0447,  0.3093, -0.2412,  ..., -0.2297, -0.2913,  0.2089],\n",
       "          [ 0.0538, -0.0829, -0.1912,  ..., -0.1855, -0.1617, -0.1081]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.3144e-03, 2.1470e-04, 8.7643e-04, 3.2961e-05, 5.5134e-05, 1.0841e-02,\n",
       "           3.9697e-05, 1.2517e-05, 6.7863e-03, 2.5630e-05, 6.5136e-04, 2.2292e-05,\n",
       "           9.7607e-01],\n",
       "          [4.2343e-03, 1.5345e-03, 4.5815e-03, 1.0710e-03, 2.0008e-03, 1.5015e-02,\n",
       "           6.8092e-04, 1.7488e-04, 1.1696e-02, 9.3994e-03, 5.0316e-03, 9.0170e-04,\n",
       "           9.4385e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    17.386364  147.303497  299.845367  477.825134    0.936610      0   \n",
       "  1   398.177582  314.106720  463.114105  481.527344    0.909269     40   \n",
       "  2    83.815811  322.369904  175.723526  541.274170    0.883700     40   \n",
       "  3   338.449310  339.019653  382.026123  455.403534    0.874932     40   \n",
       "  4   504.574554  333.858948  543.718140  435.951935    0.863013     40   \n",
       "  5   414.392090  192.699722  533.388062  428.852997    0.845193      0   \n",
       "  6   373.557373  341.815979  415.418976  463.663605    0.836709     40   \n",
       "  7    47.767216  344.379730  104.383690  500.712341    0.831824     40   \n",
       "  8   243.198471  204.012619  423.003204  446.818237    0.828916      0   \n",
       "  9   181.954575  262.595184  298.142426  332.614716    0.783612     58   \n",
       "  10  366.841431  463.334045  422.868561  561.422974    0.746160     41   \n",
       "  11  543.179199  337.863129  576.478088  434.903534    0.723887     40   \n",
       "  12   14.552649  412.840607  578.765991  590.160706    0.657322     60   \n",
       "  13  176.124481  330.538940  253.244888  400.770477    0.630767     56   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1     wine glass  \n",
       "  2     wine glass  \n",
       "  3     wine glass  \n",
       "  4     wine glass  \n",
       "  5         person  \n",
       "  6     wine glass  \n",
       "  7     wine glass  \n",
       "  8         person  \n",
       "  9   potted plant  \n",
       "  10           cup  \n",
       "  11    wine glass  \n",
       "  12  dining table  \n",
       "  13         chair  ,\n",
       "  'caption': ['A man laughing next to two women at a table.',\n",
       "   'A man in black dress sitting next to two girls.'],\n",
       "  'bbox_target': [410.39, 193.77, 134.39, 232.99]},\n",
       " 434: {'image_emb': tensor([[-0.3733,  0.6582,  0.1512,  ...,  1.1035,  0.3438, -0.4055],\n",
       "          [-0.4460,  0.4956, -0.0945,  ...,  1.0459,  0.3901,  0.1721],\n",
       "          [-0.0348,  0.6221, -0.1566,  ...,  0.7241,  0.4038, -0.3665],\n",
       "          [-0.2124,  0.1486,  0.0381,  ...,  0.4788,  0.1846, -0.2266],\n",
       "          [-0.3291,  0.4731,  0.1720,  ...,  0.6777,  0.0872, -0.2080]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0576,  0.3096, -0.1982,  ...,  0.1152,  0.1620, -0.2200],\n",
       "          [-0.0076,  0.2285, -0.1826,  ...,  0.1065,  0.2690, -0.1680]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2734, 0.0063, 0.0275, 0.5439, 0.1487],\n",
       "          [0.1182, 0.0121, 0.0196, 0.7241, 0.1259]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  110.987816  136.099152  289.998108  389.718872    0.830145     75   \n",
       "  1    0.020897  250.014023  120.214951  426.399414    0.825650     75   \n",
       "  2  257.214508  309.950623  401.939240  463.697906    0.810584     75   \n",
       "  3  110.783699    1.067114  381.178955  386.793304    0.763097     58   \n",
       "  4  255.577759  111.394585  402.408112  466.973816    0.662861     58   \n",
       "  \n",
       "             name  \n",
       "  0          vase  \n",
       "  1          vase  \n",
       "  2          vase  \n",
       "  3  potted plant  \n",
       "  4  potted plant  ,\n",
       "  'caption': ['a yellow vase with a plant in it',\n",
       "   'yellow pot and plant in the right hand picture'],\n",
       "  'bbox_target': [110.11, 1.12, 291.01, 408.99]},\n",
       " 435: {'image_emb': tensor([[-3.4448e-01,  3.5889e-01,  5.0262e-02, -5.5237e-02,  2.8369e-01,\n",
       "           -1.3855e-01, -2.8003e-01, -3.5156e-01,  9.9219e-01, -6.4087e-02,\n",
       "            1.6809e-01,  3.9795e-02,  8.3105e-01, -9.1919e-02,  1.8280e-02,\n",
       "           -4.4531e-01,  4.3750e-01,  1.3733e-01,  1.1530e-01, -1.2488e-01,\n",
       "            6.3232e-01,  3.9941e-01,  1.9971e-01,  5.4346e-01,  6.8054e-02,\n",
       "            6.6797e-01,  3.2666e-01, -3.1738e-01,  2.0667e-01,  2.3059e-01,\n",
       "            1.3635e-01,  2.8296e-01, -3.3472e-01,  1.8677e-01, -2.3523e-01,\n",
       "            1.7957e-01,  1.1487e-01, -6.2683e-02,  4.2285e-01,  6.0840e-01,\n",
       "           -2.6764e-02, -3.1519e-01, -6.0181e-02, -5.6348e-01, -2.5073e-01,\n",
       "           -2.4961e+00, -1.4880e-01,  3.2129e-01, -3.0005e-01, -8.6182e-02,\n",
       "           -1.7188e-01, -3.2593e-01,  5.7715e-01,  2.5488e-01,  2.4341e-01,\n",
       "           -1.3281e-01, -8.3789e-01, -4.0332e-01,  1.0620e-01, -1.6943e-01,\n",
       "           -6.0938e-01,  1.9867e-02, -1.3098e-01,  8.2886e-02, -4.2017e-01,\n",
       "           -1.2366e-01,  4.4409e-01,  1.4246e-01, -2.6953e-01,  2.7222e-01,\n",
       "           -2.6953e-01,  3.8184e-01, -3.3325e-02,  2.0764e-01, -1.6760e-01,\n",
       "            4.6021e-01, -6.1719e-01, -4.3872e-01,  3.3105e-01, -1.8994e-01,\n",
       "            9.4177e-02, -4.3164e-01,  4.0796e-01,  8.3203e-01,  3.6035e-01,\n",
       "           -4.4670e-03,  4.4263e-01, -3.9551e-01, -6.4795e-01, -9.1324e-03,\n",
       "           -1.5857e-01, -1.0608e-01, -5.1211e+00,  3.9160e-01, -5.2930e-01,\n",
       "           -1.1993e-01,  4.1943e-01,  3.9453e-01,  5.6396e-01,  4.4946e-01,\n",
       "            4.2534e-03,  2.9980e-01, -5.2094e-02,  1.5454e-01,  1.0544e-02,\n",
       "            4.0344e-02, -2.0605e-01,  1.8445e-01, -1.8030e-01,  1.3550e-01,\n",
       "           -1.9678e-01, -2.3718e-01,  7.1106e-02,  1.5732e-02,  3.4082e-01,\n",
       "           -2.7686e-01,  3.2568e-01,  7.4316e-01, -6.6589e-02,  1.1957e-01,\n",
       "           -1.7590e-01,  1.1322e-01,  1.9592e-01, -2.5684e-01,  3.6084e-01,\n",
       "            2.9053e-01,  1.5015e-01,  2.6831e-01, -2.6685e-01,  3.8623e-01,\n",
       "            6.6956e-02, -2.0447e-01, -7.3364e-02,  6.8457e-01, -1.8896e-01,\n",
       "            8.1055e-02,  1.7883e-01,  6.1377e-01,  2.3730e-01,  2.9028e-01,\n",
       "            8.0505e-02,  2.6904e-01,  4.5752e-01, -2.6147e-01,  1.1359e-01,\n",
       "            7.6294e-02, -2.8101e-01,  6.1670e-01, -4.2627e-01, -1.1401e-01,\n",
       "           -2.6953e-01,  2.0569e-01,  7.2900e-01, -3.1226e-01,  2.7002e-01,\n",
       "            2.6636e-01, -2.9419e-01,  3.4149e-02,  2.5000e-01, -6.8909e-02,\n",
       "            2.3987e-01,  8.8930e-04, -1.4209e-01, -1.8173e-02,  3.7354e-01,\n",
       "            7.0557e-02, -1.1530e-01, -6.2988e-02,  3.0151e-01,  4.7974e-02,\n",
       "           -4.3823e-01, -1.3953e-01, -2.8271e-01, -2.3376e-01, -1.4355e-01,\n",
       "            4.1260e-01,  1.3750e+00, -1.9934e-01,  2.1106e-01,  2.8101e-01,\n",
       "           -1.0364e-01, -7.6758e-01, -3.4637e-02,  6.6223e-02, -1.2964e-01,\n",
       "            7.0984e-02, -1.7365e-02,  1.7163e-01, -3.0176e-01, -3.3472e-01,\n",
       "            5.5054e-02, -3.9453e-01,  3.1250e-01,  1.7566e-01, -4.4458e-01,\n",
       "           -6.8054e-03, -2.2412e-01,  2.9199e-01, -1.2715e+00,  4.7168e-01,\n",
       "            1.2231e-01,  1.2323e-01,  1.8445e-01,  4.6692e-02,  1.6895e-01,\n",
       "           -5.5084e-02,  2.6245e-01,  1.0632e-01, -1.2903e-01, -6.2164e-02,\n",
       "           -3.3716e-01,  8.1934e-01,  3.4698e-02,  7.2754e-02,  1.6650e-01,\n",
       "           -4.9341e-01,  4.1473e-02,  4.6631e-02,  8.2080e-01,  1.6052e-02,\n",
       "            3.8232e-01,  2.3657e-01,  1.5049e-03, -3.3789e-01, -2.2742e-01,\n",
       "            1.9824e-01, -2.4155e-02, -6.1218e-02, -2.0776e-01, -1.5234e-01,\n",
       "            1.8396e-01,  2.6093e-02,  7.6408e-03,  2.1851e-01,  3.1396e-01,\n",
       "            6.6260e-01,  1.1316e-01,  2.5732e-01, -1.3831e-01, -3.1323e-01,\n",
       "            7.3303e-02, -2.6318e-01, -4.7461e-01, -8.2336e-02,  2.8351e-02,\n",
       "            1.1982e+00, -2.7173e-01,  4.2310e-01,  2.3206e-01, -4.0234e-01,\n",
       "            8.3838e-01,  4.3237e-01, -8.1848e-02,  3.8055e-02, -6.7810e-02,\n",
       "            4.9316e-02, -5.2246e-02,  1.0022e-01,  2.9712e-01,  5.0781e-01,\n",
       "           -2.4158e-01, -1.8005e-01,  3.7573e-01,  1.6768e+00,  2.1655e-01,\n",
       "            1.3611e-01,  1.0101e-01,  5.1123e-01, -1.3879e-01,  3.4277e-01,\n",
       "            1.3855e-01, -3.1567e-01,  2.8369e-01,  3.7866e-01,  2.3206e-01,\n",
       "           -1.6370e-01,  2.8418e-01, -5.1318e-01, -3.2690e-01,  1.3086e-01,\n",
       "            4.6478e-02, -9.4788e-02, -3.9612e-02,  3.1274e-01,  2.9932e-01,\n",
       "           -2.3853e-01,  1.0146e+00,  1.9531e-01, -1.7053e-01, -4.2603e-01,\n",
       "            6.2561e-02,  4.2017e-01,  3.5181e-01, -7.9285e-02, -5.5939e-02,\n",
       "           -6.7749e-02,  1.2494e-01,  2.7490e-01, -8.3008e-02, -1.4941e-01,\n",
       "           -5.1416e-01,  1.2427e-01, -2.4365e-01,  1.8494e-01, -1.7566e-01,\n",
       "           -3.2446e-01, -1.1230e-01, -1.0858e-01,  4.0503e-01,  4.2896e-01,\n",
       "           -3.6163e-02,  4.7021e-01,  6.8213e-01, -8.5327e-02, -2.9541e-01,\n",
       "            7.1484e-01,  2.3242e-01,  7.9492e-01, -1.6516e-01, -7.0361e-01,\n",
       "            3.6816e-01,  1.0420e+00, -2.6074e-01,  7.1899e-02, -2.1692e-01,\n",
       "            1.8909e-01,  3.4277e-01, -3.8525e-01,  8.2092e-02, -5.1807e-01,\n",
       "            3.5736e-02, -3.4033e-01, -6.6223e-02, -1.8494e-01, -4.6118e-01,\n",
       "            3.2300e-01,  8.0444e-02, -1.0718e-01,  1.4978e-01, -2.4207e-01,\n",
       "            8.3008e-02, -4.3066e-01, -1.4050e-01,  2.2119e-01,  4.4263e-01,\n",
       "           -5.4248e-01,  8.7830e-02, -4.6967e-02,  5.4346e-01, -2.5537e-01,\n",
       "            1.1240e+00,  1.1951e-01, -3.8696e-01,  1.0022e-01,  5.3589e-02,\n",
       "           -7.1436e-01, -4.0088e-01,  7.7490e-01, -1.0706e-01, -1.4185e-01,\n",
       "           -4.9878e-01,  8.7280e-02,  1.1542e-01,  9.2139e-01, -4.1553e-01,\n",
       "            4.0527e-01,  1.1034e-03,  3.0103e-01,  5.4840e-02, -3.6865e-01,\n",
       "           -1.9141e-01, -3.1299e-01,  3.9648e-01, -3.1470e-01,  4.3140e-01,\n",
       "            3.5474e-01,  1.0566e+00, -7.9773e-02, -3.8403e-01,  4.3750e-01,\n",
       "           -1.5100e-01, -3.3643e-01,  2.5952e-01, -5.1208e-02,  4.6338e-01,\n",
       "           -4.0015e-01, -1.7969e-01, -3.7915e-01, -2.2070e-01,  3.8770e-01,\n",
       "            2.2754e-01,  4.2725e-01,  1.5820e-01,  3.2324e-01, -5.3857e-01,\n",
       "           -5.8301e-01,  4.3457e-01, -1.6968e-01,  7.7441e-01,  4.0601e-01,\n",
       "           -2.1582e-01,  4.7314e-01,  1.1357e+00,  1.1346e-01, -4.8462e-02,\n",
       "           -2.1271e-02,  8.0933e-02, -5.2734e-01,  1.7099e-03, -1.3367e-01,\n",
       "           -3.1689e-01,  4.8511e-01,  8.2886e-02,  3.1372e-01,  6.6795e-03,\n",
       "            5.4883e-01, -2.7539e-01, -4.2534e-03, -4.7607e-01,  4.3030e-02,\n",
       "           -1.2396e-01,  1.0022e-01,  3.2275e-01,  2.9346e-01,  9.3750e-02,\n",
       "            5.4004e-01,  5.0316e-03, -1.4473e-02, -2.7109e+00,  7.6218e-03,\n",
       "           -1.5735e-01, -1.1017e-01,  6.5381e-01,  1.1780e-01, -1.1603e-01,\n",
       "           -1.5576e-01,  3.9787e-03,  1.2891e-01,  8.3191e-02, -1.2939e-01,\n",
       "            4.8340e-02,  6.5613e-02,  6.1371e-02,  2.6318e-01, -2.5488e-01,\n",
       "           -3.6621e-01, -2.0065e-02,  8.2764e-02,  2.3511e-01, -8.4570e-01,\n",
       "            7.2266e-01,  9.4360e-02, -1.1810e-01,  7.0850e-01,  1.0425e-01,\n",
       "            2.2437e-01,  3.1641e-01, -2.9565e-01,  6.2927e-02,  3.7866e-01,\n",
       "            2.7344e-01,  1.2708e-01, -2.1301e-01,  3.1909e-01, -3.3545e-01,\n",
       "            6.2317e-02,  1.3647e-01, -3.7842e-01, -2.1545e-01, -2.0447e-01,\n",
       "           -2.1045e-01, -8.5266e-02,  3.4839e-01, -5.9570e-01, -2.9028e-01,\n",
       "            2.5562e-01, -3.0054e-01, -4.4373e-02,  1.9019e-01,  2.1228e-01,\n",
       "            3.4828e-03, -5.8350e-01,  3.4888e-01,  4.3579e-01,  3.2349e-01,\n",
       "            7.3242e-01,  1.7761e-01,  2.6929e-01,  2.1362e-01,  4.0820e-01,\n",
       "           -3.6768e-01,  1.8188e-02,  3.9771e-01, -8.1738e-01,  1.7993e-01,\n",
       "           -6.1493e-02, -1.6675e-01,  4.5459e-01, -1.0425e-01, -1.8311e-01,\n",
       "           -1.3196e-01, -2.2058e-01,  5.0635e-01,  5.3662e-01, -2.5806e-01,\n",
       "            1.4551e-01, -1.1603e-01,  2.5537e-01, -5.9717e-01, -1.0025e-02,\n",
       "           -2.8003e-01, -5.8301e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0872,  0.2581,  0.1605,  ...,  0.0352,  0.2491, -0.3953],\n",
       "          [ 0.1294, -0.0563, -0.3508,  ..., -0.0107,  0.0742, -0.3289]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.],\n",
       "          [1.]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  322.547577  218.824554  417.359772  387.981659    0.666953     69   \n",
       "  1  541.082947  101.889412  599.537109  153.012558    0.665417     68   \n",
       "  2  535.979980  129.912231  599.182739  258.100616    0.550246     69   \n",
       "  3  105.053940  294.795258  171.995117  304.734070    0.491482     71   \n",
       "  4   40.607059  314.622833   47.729221  336.143677    0.482729     39   \n",
       "  5  278.764099  298.942444  316.434723  317.916504    0.268788     71   \n",
       "  \n",
       "          name  \n",
       "  0       oven  \n",
       "  1  microwave  \n",
       "  2       oven  \n",
       "  3       sink  \n",
       "  4     bottle  \n",
       "  5       sink  ,\n",
       "  'caption': ['An undercounter refrigerator with a glass door in a white cabinet.',\n",
       "   'An oven in a house.'],\n",
       "  'bbox_target': [322.7, 222.92, 91.68, 164.5]},\n",
       " 436: {'image_emb': tensor([[ 0.1925,  0.1473, -0.0947,  ...,  0.6353,  0.2048,  0.0291],\n",
       "          [-0.3752,  0.3474, -0.1249,  ...,  1.2402,  0.2169,  0.0180],\n",
       "          [-0.1600,  0.2390, -0.3176,  ...,  0.9487, -0.1070, -0.0427],\n",
       "          [-0.6494,  0.3789, -0.0536,  ...,  1.1729,  0.1626, -0.4568],\n",
       "          [ 0.0227,  0.1460,  0.0326,  ...,  0.7197, -0.0438, -0.2329]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1759,  0.0551,  0.1277,  ...,  0.5581, -0.4136, -0.1420],\n",
       "          [-0.3188, -0.1332,  0.1750,  ...,  0.2262,  0.3623, -0.2703]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.1338e-01, 8.3084e-03, 1.8244e-03, 1.8143e-02, 2.5830e-01],\n",
       "          [1.2756e-01, 8.9228e-05, 3.3665e-04, 4.1485e-05, 8.7207e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  138.631409  156.517700  379.950134  313.972595    0.918675     53   pizza\n",
       "  1   91.705017  302.865540  192.255768  422.033447    0.802869     39  bottle\n",
       "  2  185.138565  331.513428  272.548584  422.781311    0.790013     39  bottle\n",
       "  3   32.955513  299.836395  130.480286  420.993225    0.753075     39  bottle\n",
       "  4   11.924019  352.776001  105.252312  422.437561    0.685801     39  bottle\n",
       "  5  575.413330   63.194885  639.642944  140.783691    0.673400     53   pizza\n",
       "  6    0.137331  328.030518   34.345078  420.939209    0.656188     39  bottle\n",
       "  7    0.000000  298.721924   43.407837  356.288330    0.466642     39  bottle,\n",
       "  'caption': ['a food on tabule',\n",
       "   'A slice of pizza with three olives on an otherwise empty pan.'],\n",
       "  'bbox_target': [138.16, 155.15, 238.2, 158.17]},\n",
       " 437: {'image_emb': tensor([[ 4.8096e-02,  2.5366e-01, -2.5659e-01,  ...,  1.2695e+00,\n",
       "            3.2642e-01, -1.6504e-01],\n",
       "          [-3.3423e-01,  4.0723e-01,  3.3855e-04,  ...,  1.2295e+00,\n",
       "            2.6636e-01, -1.2891e-01],\n",
       "          [ 1.1914e-01,  1.2964e-01, -2.8809e-01,  ...,  8.0029e-01,\n",
       "            1.6980e-01, -3.2227e-02],\n",
       "          ...,\n",
       "          [-4.4617e-02,  6.5430e-01, -4.0771e-02,  ...,  1.0615e+00,\n",
       "           -4.2694e-02,  3.0371e-01],\n",
       "          [-1.3623e-01,  1.1084e-01, -1.6187e-01,  ...,  1.0479e+00,\n",
       "            1.6956e-01, -1.1823e-01],\n",
       "          [-3.0029e-01,  4.5068e-01, -2.0203e-01,  ...,  6.1182e-01,\n",
       "            4.7705e-01, -1.7261e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0122,  0.0724, -0.1746,  ..., -0.1337,  0.1453, -0.2325],\n",
       "          [ 0.0220,  0.3398, -0.3616,  ..., -0.5483,  0.2413,  0.1680]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0956e-02, 9.1736e-02, 1.3989e-01, 2.7256e-03, 7.8888e-03, 1.4591e-03,\n",
       "           2.2964e-03, 2.7256e-03, 2.6688e-02, 6.5002e-02, 8.1406e-03, 1.8066e-02,\n",
       "           2.6538e-01, 3.5718e-01],\n",
       "          [6.0005e-03, 5.1562e-01, 3.6194e-02, 1.1816e-03, 3.1624e-03, 3.5477e-04,\n",
       "           4.9257e-04, 1.4794e-04, 7.5439e-02, 1.9684e-02, 3.9577e-04, 2.6318e-01,\n",
       "           2.3010e-02, 5.5176e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    62.147171  188.613342   99.699707  252.796814    0.913360     41   \n",
       "  1   277.772034   24.093937  362.689972  207.791489    0.910070      0   \n",
       "  2   203.876678   50.683140  265.149933  127.990028    0.861075      0   \n",
       "  3   207.668350  222.875763  247.661682  279.163147    0.851934     41   \n",
       "  4   244.225647  195.045380  279.585846  259.558258    0.842853     41   \n",
       "  5   292.187561  179.575211  324.354492  232.858139    0.830667     41   \n",
       "  6   155.006927  227.476028  178.599350  266.615356    0.820149     41   \n",
       "  7   187.854355  103.916992  230.533722  230.313538    0.818343     39   \n",
       "  8    41.436695   36.740982  129.165329  125.163177    0.816655     62   \n",
       "  9    28.020403   85.728714   95.134880  222.681015    0.783399     56   \n",
       "  10    0.410540   87.445091   38.831150  242.594360    0.783101     56   \n",
       "  11  330.666412  101.869026  499.543427  318.503326    0.760853      0   \n",
       "  12   83.770554  119.756653  117.207184  232.509567    0.730162     39   \n",
       "  13  102.694832  197.574280  152.946747  230.113937    0.656982     45   \n",
       "  14  244.238403  125.116013  263.453033  193.152710    0.655028     39   \n",
       "  15  274.635040  226.391464  310.163910  274.914795    0.653891     41   \n",
       "  16  129.789078  135.387344  154.028458  191.244202    0.647551     39   \n",
       "  17  119.034195  156.156128  139.428131  215.053864    0.630312     39   \n",
       "  18    6.090963  109.817230  338.272522  320.513672    0.627866     60   \n",
       "  19  260.587524  109.583603  280.275360  177.298187    0.612040     39   \n",
       "  20  262.251831  179.074234  292.299683  226.879410    0.610543     41   \n",
       "  21  276.705261  127.596786  297.651917  181.604263    0.573648     39   \n",
       "  22  297.104034  111.751816  310.805786  135.796997    0.556573     39   \n",
       "  23   36.225361  220.993515   71.202667  246.961456    0.550046     45   \n",
       "  24  187.993317  100.267220  203.408356  138.604599    0.464798     39   \n",
       "  25  151.926498  198.604401  186.779602  236.681992    0.461563     41   \n",
       "  26  223.511124  109.502708  245.417068  187.602402    0.448414     39   \n",
       "  27  379.841400  107.807335  493.225098  169.762604    0.358865     57   \n",
       "  28  379.924591  107.369888  496.848114  169.339996    0.346219     56   \n",
       "  29  103.410301  197.082230  153.542084  230.062439    0.319809     41   \n",
       "  \n",
       "              name  \n",
       "  0            cup  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3            cup  \n",
       "  4            cup  \n",
       "  5            cup  \n",
       "  6            cup  \n",
       "  7         bottle  \n",
       "  8             tv  \n",
       "  9          chair  \n",
       "  10         chair  \n",
       "  11        person  \n",
       "  12        bottle  \n",
       "  13          bowl  \n",
       "  14        bottle  \n",
       "  15           cup  \n",
       "  16        bottle  \n",
       "  17        bottle  \n",
       "  18  dining table  \n",
       "  19        bottle  \n",
       "  20           cup  \n",
       "  21        bottle  \n",
       "  22        bottle  \n",
       "  23          bowl  \n",
       "  24        bottle  \n",
       "  25           cup  \n",
       "  26        bottle  \n",
       "  27         couch  \n",
       "  28         chair  \n",
       "  29           cup  ,\n",
       "  'caption': ['the man in black tshirt in the right hand picture',\n",
       "   'A man in the background wearing a black t-shirt with a picture on it.'],\n",
       "  'bbox_target': [275.45, 22.3, 87.44, 186.55]},\n",
       " 438: {'image_emb': tensor([[ 0.0375,  0.1292, -0.1257,  ...,  0.1393,  0.3953,  0.3252],\n",
       "          [-0.0704, -0.1759,  0.0630,  ...,  0.5054,  0.1698,  0.1884],\n",
       "          [ 0.0423,  0.5601,  0.0568,  ...,  0.8135,  0.3843, -0.0242],\n",
       "          [ 0.0121,  0.1345,  0.1120,  ...,  1.0371,  0.1023, -0.0733],\n",
       "          [ 0.0817, -0.2791,  0.0409,  ..., -0.0850, -0.0361,  0.3508]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0348,  0.2059, -0.0536,  ...,  0.2751, -0.0071,  0.0038],\n",
       "          [-0.1750, -0.1354,  0.0117,  ...,  0.1296, -0.0448, -0.1934]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.6230e-01, 9.5749e-03, 1.5008e-04, 3.8505e-05, 1.2817e-01],\n",
       "          [6.8896e-01, 2.7145e-02, 8.4543e-04, 7.7426e-05, 2.8271e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  268.444977  163.226807  449.573761  494.277405    0.936189     20  elephant\n",
       "  1  128.541428  309.668396  296.529175  472.356995    0.902794     20  elephant\n",
       "  2  395.098877  294.698944  504.133240  495.460876    0.888900     20  elephant\n",
       "  3  109.224289  415.517578  238.356735  467.023987    0.764495     13     bench,\n",
       "  'caption': [\"Largest elephant showing it's rear.\",\n",
       "   'An adult elephant standing in front of to small elephants.'],\n",
       "  'bbox_target': [269.33, 165.03, 182.22, 328.93]},\n",
       " 439: {'image_emb': tensor([[ 0.1141, -0.1793, -0.3901,  ...,  0.7314, -0.4719, -0.1960],\n",
       "          [ 0.1742,  0.7603, -0.1461,  ...,  1.2861,  0.1284, -0.0675],\n",
       "          [ 0.2507,  0.2389, -0.0895,  ...,  1.1621,  0.0926, -0.0662],\n",
       "          [ 0.2664,  0.4949,  0.2756,  ...,  0.6431,  0.0058,  0.0230]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-1.2976e-01,  2.6025e-01, -2.8662e-01, -4.7412e-01, -1.6931e-01,\n",
       "           -1.5540e-01, -2.9175e-01, -5.7983e-02, -2.1985e-01,  1.6736e-01,\n",
       "           -4.1321e-02, -3.2471e-01,  4.1748e-01,  7.9041e-02,  9.9304e-02,\n",
       "            1.0431e-01, -3.8428e-01, -5.2979e-01, -3.6719e-01,  2.2449e-01,\n",
       "            1.4099e-01,  2.0605e-01, -4.4263e-01,  3.8037e-01,  3.8159e-01,\n",
       "           -1.0199e-01,  2.2754e-01,  2.3877e-01, -1.4844e-01, -3.6896e-02,\n",
       "           -5.3906e-01,  2.0618e-01, -1.8689e-01, -3.7134e-01, -2.3450e-01,\n",
       "           -2.3474e-01, -2.9541e-01, -2.4316e-01,  1.2421e-01,  1.5051e-01,\n",
       "            4.5679e-01,  2.0569e-01,  2.1338e-01,  9.1980e-02, -1.2683e-01,\n",
       "           -2.1500e-02,  3.0151e-01,  3.0441e-02, -1.8103e-01,  1.7810e-01,\n",
       "            2.9785e-01,  1.7700e-01, -1.9812e-01, -4.1479e-01, -3.5474e-01,\n",
       "           -1.0162e-01, -9.0332e-02,  5.9326e-02,  7.2754e-02,  2.9956e-01,\n",
       "            1.1456e-01, -7.1777e-02,  5.4138e-02, -1.4246e-01, -6.8408e-01,\n",
       "           -1.2274e-01,  2.5269e-01,  4.4019e-01,  5.7007e-02, -3.3813e-01,\n",
       "            7.6599e-02, -6.2988e-02,  3.4961e-01, -1.7163e-01,  1.9678e-01,\n",
       "            8.0729e-04,  2.0935e-02,  2.8516e-01, -6.7932e-02,  1.3232e-01,\n",
       "           -3.3569e-01,  1.8213e-01,  1.7236e-01, -7.1838e-02,  4.5563e-02,\n",
       "            1.4673e-01,  5.9387e-02, -2.8687e-01, -7.9932e-01,  1.8066e-01,\n",
       "            4.6729e-01, -3.2422e-01, -9.5264e-01,  3.0664e-01,  1.3638e-03,\n",
       "            1.8811e-01, -1.9971e-01,  1.5576e-01,  2.0325e-01,  1.5747e-01,\n",
       "            5.7251e-02,  3.6011e-02, -1.6162e-01,  1.4990e-01,  2.9077e-01,\n",
       "            1.9128e-01, -1.4819e-01, -9.7778e-02, -2.5558e-02,  3.3813e-02,\n",
       "           -3.7036e-01,  7.9468e-02,  9.3811e-02,  2.7881e-01, -8.4305e-03,\n",
       "            2.3132e-01, -2.5000e-01,  3.5327e-01, -3.1519e-01,  3.1348e-01,\n",
       "            1.9189e-01, -8.3691e-01, -2.4731e-01, -1.2805e-01,  6.6589e-02,\n",
       "           -5.6122e-02,  1.5125e-01, -2.7368e-01, -2.2705e-01,  2.2461e-01,\n",
       "           -1.7792e-02, -2.7368e-01, -4.3262e-01,  3.6328e+00, -1.3623e-01,\n",
       "            2.3303e-01,  2.8589e-01, -6.6895e-01,  2.2363e-01, -3.3984e-01,\n",
       "           -5.7959e-01,  3.5889e-01, -1.0858e-01, -4.0466e-02, -1.0437e-01,\n",
       "           -1.8091e-01,  1.4478e-01, -2.3743e-01,  4.4067e-02, -5.1544e-02,\n",
       "           -2.2766e-01,  5.8807e-02,  3.4497e-01, -9.1370e-02, -8.5632e-02,\n",
       "           -3.5889e-02, -4.0649e-01, -7.0459e-01, -1.5479e-01,  1.8433e-01,\n",
       "            4.0234e-01,  8.9233e-02,  1.0278e-01,  2.5806e-01,  1.3452e-01,\n",
       "           -3.9429e-01,  8.4521e-01, -1.1414e-01,  1.1975e-01, -3.8147e-02,\n",
       "           -2.0432e-02,  1.7664e-01,  3.8647e-01,  9.6924e-02, -4.2505e-01,\n",
       "            3.0786e-01,  1.4417e-01,  7.8125e-02, -1.3171e-01, -2.3181e-01,\n",
       "            1.4551e-01,  2.2705e-01, -3.4326e-01, -1.2122e-01, -1.8201e-01,\n",
       "            6.2305e-01,  2.3392e-02, -1.9055e-01, -2.3514e-02, -8.2275e-02,\n",
       "            2.0398e-01,  3.4302e-02, -3.2080e-01, -4.2328e-02,  4.3579e-02,\n",
       "           -3.8971e-02, -2.1631e-01,  1.1774e-01,  1.3940e-01,  5.2344e-01,\n",
       "            2.6099e-01,  1.3318e-01, -2.1411e-01,  3.7866e-01, -6.1523e-01,\n",
       "           -7.0740e-02,  1.2085e-01, -1.9092e-01,  8.5144e-02, -4.8340e-01,\n",
       "            5.3772e-02,  4.4263e-01, -1.7664e-01,  1.1002e-02, -1.4355e-01,\n",
       "           -6.2866e-02, -1.5381e-01, -1.6418e-01,  2.1777e-01, -6.5234e-01,\n",
       "            5.4688e-01, -2.7954e-01, -6.2598e-01, -1.3519e-02, -3.7964e-01,\n",
       "            1.0999e-01, -1.4087e-01, -2.7490e-01,  1.0254e-01,  5.5908e-02,\n",
       "            1.3110e-01,  3.3350e-01,  5.7715e-01,  5.9631e-02,  2.0972e-01,\n",
       "           -3.4882e-02,  1.7834e-01,  1.4404e-01, -2.4182e-01, -1.2482e-01,\n",
       "            3.2031e-01, -6.7017e-02, -1.8799e-01,  3.3398e-01,  1.0474e-01,\n",
       "            3.1226e-01, -3.8696e-02,  4.0007e-04, -3.1433e-02,  3.4033e-01,\n",
       "           -2.4011e-01, -7.1167e-02,  1.7654e-02,  4.4067e-01, -2.9810e-01,\n",
       "           -3.5095e-02, -2.2736e-02,  1.3135e-01,  1.6943e-01,  1.3721e-01,\n",
       "           -1.1249e-01, -2.5467e-02, -1.8225e-01,  2.0544e-01, -5.2124e-02,\n",
       "           -1.4856e-01,  5.4230e-02,  2.9510e-02, -4.2554e-01,  1.8372e-01,\n",
       "           -3.6792e-01, -1.8274e-01,  4.7095e-01,  1.1426e-01, -1.2915e-01,\n",
       "           -3.0420e-01, -2.8662e-01,  2.5073e-01,  1.2708e-01,  1.3184e-01,\n",
       "            2.7374e-02, -1.0144e-01,  6.0333e-02,  2.4646e-01, -5.0262e-02,\n",
       "            2.1021e-01, -5.2216e-02,  8.6670e-03,  2.6489e-01, -2.2302e-01,\n",
       "            3.5864e-01, -1.5454e-01,  6.3049e-02,  1.5051e-01, -1.2634e-01,\n",
       "            1.0785e-01, -2.7930e-01,  4.3530e-01, -1.6943e-01, -3.2471e-01,\n",
       "           -1.0492e-01, -4.3530e-01, -2.9517e-01, -4.2572e-02, -5.0635e-01,\n",
       "           -4.7437e-01, -1.3770e-01,  8.5510e-02, -3.2593e-02,  7.6714e-03,\n",
       "           -3.1567e-01,  6.3428e-01,  3.6348e+00, -3.3154e-01,  1.4534e-02,\n",
       "           -3.7012e-01,  2.6318e-01, -2.1191e-01,  1.0510e-01,  1.8579e-01,\n",
       "            1.1581e-02,  1.4563e-01,  5.9668e-01,  1.0052e-01, -4.5435e-01,\n",
       "           -1.6943e-01,  4.9536e-01, -6.4209e-02,  1.8701e-01, -1.1406e+00,\n",
       "            7.0605e-01, -1.4270e-01,  9.1125e-02, -1.7883e-01, -1.8176e-01,\n",
       "           -4.8920e-02, -6.1005e-02, -1.5784e-01,  2.1887e-01,  1.8103e-01,\n",
       "           -6.0181e-02, -5.1689e-04,  3.6499e-01, -6.8420e-02,  2.9297e-01,\n",
       "            4.3701e-02,  4.4678e-02,  4.5386e-01,  3.4448e-01, -6.9641e-02,\n",
       "            1.0846e-01, -6.8018e-01,  1.4954e-01,  2.1228e-01,  1.1987e-01,\n",
       "           -5.7031e-01,  2.0386e-01,  1.4856e-01,  8.7158e-02,  9.5154e-02,\n",
       "           -1.4246e-01, -5.4810e-02,  6.8474e-03, -5.8136e-02,  5.4352e-02,\n",
       "           -2.7344e-01,  1.9580e-01,  1.4575e-01,  4.2542e-02,  3.5327e-01,\n",
       "           -3.0200e-01, -1.1041e-01, -1.7798e-01,  4.7058e-02, -9.6436e-03,\n",
       "           -1.1908e-01,  2.8351e-02, -1.7200e-01, -1.6052e-01, -3.9282e-01,\n",
       "            3.6768e-01,  2.2644e-01, -3.1006e-02,  1.9592e-01, -7.0618e-02,\n",
       "            4.8120e-01,  3.6084e-01,  1.0449e-01, -2.0374e-01,  1.2970e-02,\n",
       "           -7.3340e-01,  2.4207e-01, -4.3896e-01, -1.5918e-01, -3.4497e-01,\n",
       "           -1.7896e-01,  1.5588e-01, -8.4106e-02,  2.1629e-03,  3.4888e-01,\n",
       "            6.6162e-01,  6.3057e-03,  1.8982e-01, -2.3328e-01, -1.2054e-01,\n",
       "           -2.4756e-01,  1.3586e-01, -2.0886e-01, -3.1714e-01,  3.4961e-01,\n",
       "           -3.3252e-01, -3.7817e-01,  2.1753e-01,  1.3806e-01, -1.8372e-01,\n",
       "           -4.3433e-01, -2.1774e-02, -3.8544e-02,  2.8809e-01, -1.4038e-01,\n",
       "           -2.1179e-01, -3.3691e-01,  1.8970e-01, -2.3401e-01,  7.7972e-03,\n",
       "            2.1667e-02, -1.6724e-01,  3.4863e-01,  1.8384e-01,  2.0325e-01,\n",
       "           -3.8501e-01,  3.5736e-02, -9.0088e-02,  1.4502e-01, -2.1594e-01,\n",
       "           -4.5020e-01, -1.5430e-01, -9.4238e-02, -5.5664e-01, -1.7651e-01,\n",
       "            2.2659e-02,  1.9543e-01,  4.4067e-02,  8.0811e-02, -1.5710e-01,\n",
       "            1.6589e-01, -1.5771e-01, -2.2656e-01,  2.1411e-01,  2.4365e-01,\n",
       "           -4.8389e-01, -7.9285e-02,  2.9053e-01, -1.1023e-01,  3.2031e-01,\n",
       "           -1.7419e-01, -1.0504e-01,  2.8955e-01,  7.7820e-02, -2.9126e-01,\n",
       "           -1.6528e-01,  3.8184e-01,  4.9591e-02, -1.4600e-01, -2.8961e-02,\n",
       "           -2.3804e-01,  2.4158e-01, -1.8640e-01,  7.0068e-02, -2.8061e-02,\n",
       "            2.5732e-01, -2.3328e-01, -1.9092e-01, -1.0156e-01,  3.4546e-01,\n",
       "            5.3101e-03,  1.2854e-01,  1.1664e-01, -7.2021e-01,  5.4102e-01,\n",
       "            4.0430e-01, -7.0947e-01, -4.0283e-02,  2.3987e-01,  4.0918e-01,\n",
       "           -1.9910e-01, -2.2156e-01, -1.6516e-01, -2.3651e-02, -7.2693e-02,\n",
       "            2.7206e-02, -2.5269e-01,  1.8042e-01,  9.8193e-01,  6.7383e-01,\n",
       "           -2.7100e-01,  5.3418e-01, -4.9341e-01, -2.0789e-01, -1.4172e-01,\n",
       "           -2.7856e-01, -7.0419e-03, -2.8671e-02,  1.4600e-01,  4.5312e-01,\n",
       "           -1.0730e-01, -1.2622e-01,  1.1676e-01, -1.0669e-01, -6.9189e-01,\n",
       "           -5.7892e-02, -4.1699e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.5700e-04, 1.2213e-01, 1.7493e-01, 7.0264e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  479.415741  325.082458  487.608124  350.598633    0.819578      0    person\n",
       "  1  336.433167    0.752274  488.595398  408.088928    0.811731     25  umbrella\n",
       "  2  203.615265  326.867920  569.485718  474.994690    0.755459     56     chair\n",
       "  3  611.903809  356.631317  640.000000  387.467743    0.638384     56     chair\n",
       "  4  394.958191  386.176392  618.183472  479.365479    0.546398     56     chair\n",
       "  5  589.903381  325.233643  638.395691  353.599426    0.426223     56     chair\n",
       "  6  555.067139  312.943237  561.328613  326.869568    0.323869      0    person\n",
       "  7  620.356812  323.864929  639.952759  349.432129    0.314394     56     chair,\n",
       "  'caption': ['The chair with the unopened umbrella connected to it'],\n",
       "  'bbox_target': [222.95, 324.02, 345.17, 142.71]},\n",
       " 440: {'image_emb': tensor([[-0.0805,  0.1120,  0.0618,  ...,  0.2551,  0.0066, -0.0716],\n",
       "          [-0.3799,  0.2898, -0.2837,  ...,  0.6699, -0.0159, -0.5972],\n",
       "          [-0.0801,  0.0341,  0.0768,  ...,  0.3311,  0.1699, -0.2051],\n",
       "          [-0.0830, -0.4060,  0.0437,  ..., -0.1969,  0.2668,  0.1194]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0275, -0.0964, -0.0943,  ...,  0.5898, -0.0114, -0.0277],\n",
       "          [-0.0538,  0.2646, -0.3137,  ...,  0.0385,  0.1683, -0.3059]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.9268, 0.0072, 0.0276, 0.0383],\n",
       "          [0.6519, 0.0089, 0.3228, 0.0166]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  201.386230   67.518936  431.552734  458.205261    0.939378      0  person\n",
       "  1  122.054779  163.790405  208.319580  518.035339    0.926398      0  person\n",
       "  2  204.314087  191.931702  388.880493  615.912903    0.923752     17   horse,\n",
       "  'caption': ['Senior adult enjoying horse riding',\n",
       "   'The woman riding the horse.'],\n",
       "  'bbox_target': [196.48, 66.09, 242.71, 392.96]},\n",
       " 441: {'image_emb': tensor([[-0.5742,  0.2942, -0.1831,  ...,  0.5825, -0.0848, -0.2812],\n",
       "          [ 0.0935,  0.3372, -0.0304,  ...,  1.0439,  0.0959, -0.0251],\n",
       "          [-0.1533,  0.3127, -0.0908,  ...,  1.1924,  0.1499,  0.3279],\n",
       "          [-0.3955,  0.5195, -0.3252,  ...,  1.1416,  0.0139,  0.0241],\n",
       "          [-0.1266,  0.4958, -0.3152,  ...,  0.9873,  0.3918, -0.3457],\n",
       "          [-0.1870,  0.0721, -0.1896,  ...,  0.6230, -0.0549,  0.0530]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0072,  0.2471,  0.2040,  ...,  0.1541, -0.0995, -0.0925],\n",
       "          [-0.1077,  0.0437,  0.0606,  ...,  0.1365, -0.1370,  0.0966]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.4485e-02, 1.7236e-01, 6.0156e-01, 1.1487e-01, 4.2319e-06, 7.6477e-02],\n",
       "          [7.3486e-02, 5.3467e-01, 1.7908e-01, 2.0288e-01, 7.1526e-06, 9.7961e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  192.176437  238.387817  374.026398  306.115234    0.905402     28   \n",
       "  1  198.648315  304.137146  375.000000  378.848755    0.904700     28   \n",
       "  2  178.393051  364.406555  375.000000  458.704132    0.898984     28   \n",
       "  3  153.242996  424.681458  374.829865  499.868408    0.889078     28   \n",
       "  4  104.195366  307.067047  164.037155  355.179260    0.801267      2   \n",
       "  5  228.417252  144.733643  266.234253  232.697815    0.675971     40   \n",
       "  \n",
       "           name  \n",
       "  0    suitcase  \n",
       "  1    suitcase  \n",
       "  2    suitcase  \n",
       "  3    suitcase  \n",
       "  4         car  \n",
       "  5  wine glass  ,\n",
       "  'caption': ['A maroon suitcase with a blue label.',\n",
       "   'Dark red suitcase with round symbol under the handle.'],\n",
       "  'bbox_target': [200.0, 307.87, 174.16, 70.78]},\n",
       " 442: {'image_emb': tensor([[ 0.2133,  0.5054,  0.2374,  ...,  0.7988,  0.0292, -0.3704],\n",
       "          [-0.5356,  0.1001, -0.1078,  ...,  0.7808, -0.0096, -0.4270],\n",
       "          [-0.1818,  0.5718,  0.1862,  ...,  1.5850, -0.1318, -0.4006],\n",
       "          ...,\n",
       "          [ 0.1293,  0.0487, -0.4429,  ...,  1.4160, -0.2101,  0.0308],\n",
       "          [-0.2500,  0.1954, -0.0993,  ...,  0.7031, -0.1316, -0.4956],\n",
       "          [-0.0254,  0.1163, -0.0895,  ...,  0.7095, -0.0576, -0.2372]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1456,  0.0367, -0.2690,  ..., -0.0751,  0.0027, -0.5371],\n",
       "          [-0.1371, -0.1760,  0.0930,  ..., -0.1067, -0.3110, -0.4314]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.3677e-01, 2.3880e-02, 1.5576e-01, 2.8351e-02, 2.9541e-01, 5.3883e-05,\n",
       "           8.3447e-05, 3.4273e-05, 5.7936e-04, 5.9113e-02],\n",
       "          [7.8174e-01, 6.1333e-05, 6.8298e-02, 4.2305e-03, 1.4453e-01, 4.4608e-04,\n",
       "           6.3896e-04, 2.8074e-05, 2.8074e-05, 3.6061e-05]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     0.604683  202.014984  160.750458  383.421234    0.920559      2   \n",
       "  1   201.621964   69.684128  388.026794  158.103394    0.898739     25   \n",
       "  2   342.900604   64.053528  478.393890  130.707336    0.888514      2   \n",
       "  3   507.265259   47.667511  639.972046  113.992157    0.886222      2   \n",
       "  4     0.000000   86.626801  222.994110  181.255402    0.846068      2   \n",
       "  5    49.412697   77.944275   72.169762  113.027710    0.841180      0   \n",
       "  6   178.043701  268.001465  281.931488  426.348816    0.833534     28   \n",
       "  7   159.506744   73.440460  203.633911  192.604889    0.817022      0   \n",
       "  8   225.132080  105.584427  379.176025  420.812927    0.794449      0   \n",
       "  9   112.914139   81.018463  143.236374  174.404602    0.614610      0   \n",
       "  10  292.842712   54.164261  320.898438   81.273453    0.603563      0   \n",
       "  11  310.748840  164.988647  390.855713  312.952698    0.563425     26   \n",
       "  12  118.223633   97.142670  143.096466  133.889221    0.342492     26   \n",
       "  13  336.904663   66.907104  354.029358   91.412659    0.257512      0   \n",
       "  14  257.669891  108.749695  296.399872  222.267334    0.254690      0   \n",
       "  \n",
       "          name  \n",
       "  0        car  \n",
       "  1   umbrella  \n",
       "  2        car  \n",
       "  3        car  \n",
       "  4        car  \n",
       "  5     person  \n",
       "  6   suitcase  \n",
       "  7     person  \n",
       "  8     person  \n",
       "  9     person  \n",
       "  10    person  \n",
       "  11   handbag  \n",
       "  12   handbag  \n",
       "  13    person  \n",
       "  14    person  ,\n",
       "  'caption': ['Front of white car nearest woman with umbrella.',\n",
       "   'White car where only its front end is visible'],\n",
       "  'bbox_target': [0.96, 200.05, 160.62, 186.59]},\n",
       " 443: {'image_emb': tensor([[ 0.4700,  0.1774, -0.2137,  ...,  0.1736,  0.1776, -0.3164],\n",
       "          [ 0.1788,  0.1462, -0.1868,  ...,  0.4138,  0.2087,  0.1499],\n",
       "          [ 0.5132, -0.0201, -0.0395,  ...,  0.3662, -0.2659,  0.2517],\n",
       "          ...,\n",
       "          [ 0.1849, -0.2319, -0.5449,  ...,  0.8384, -0.0827, -0.1951],\n",
       "          [ 0.4685,  0.1631, -0.5273,  ...,  0.8301,  0.0599,  0.1674],\n",
       "          [ 0.1467,  0.2063, -0.0518,  ..., -0.4717, -0.0993,  0.1205]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1881,  0.1400, -0.2288,  ...,  0.4089, -0.0770, -0.0119],\n",
       "          [ 0.0194, -0.0229, -0.3662,  ...,  0.3987,  0.0510,  0.2729]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.2285e-01, 2.3735e-04, 6.0852e-02, 1.1436e-02, 6.0022e-05, 2.8181e-04,\n",
       "           1.0252e-05, 3.9744e-04, 4.0131e-03],\n",
       "          [3.0884e-01, 2.6321e-03, 5.7715e-01, 3.2043e-02, 1.3866e-03, 1.8663e-03,\n",
       "           8.6784e-04, 1.8845e-02, 5.6274e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   66.955933  251.360443  212.105560  381.781158    0.929076     16     dog\n",
       "  1  347.655518   59.986877  447.629456  331.989197    0.913376      0  person\n",
       "  2  148.575638  222.704468  274.720673  342.872742    0.882935     16     dog\n",
       "  3  290.593567  224.422150  370.095398  336.817047    0.847514     15     cat\n",
       "  4  334.945557  134.651794  357.030396  221.469299    0.831781      0  person\n",
       "  5  247.955215  285.240326  294.073181  342.996002    0.808289     15     cat\n",
       "  6  214.147675  156.030670  225.645599  185.321960    0.768395      0  person\n",
       "  7   47.424507  164.793320   67.468445  215.154907    0.709938      0  person\n",
       "  8   19.407101  157.637909   32.031563  190.348297    0.577330      0  person\n",
       "  9  178.919495  153.881149  189.774536  185.413269    0.567070      0  person,\n",
       "  'caption': ['A black dog with one white paw and a collar standing on the sidewalk.',\n",
       "   'the black dog who is the furthest away from the man'],\n",
       "  'bbox_target': [65.38, 252.08, 147.11, 131.79]},\n",
       " 444: {'image_emb': tensor([[ 0.1055,  0.3357, -0.4775,  ...,  1.2598,  0.3171,  0.0327],\n",
       "          [ 0.0697,  0.5581, -0.1481,  ...,  1.1514,  0.1448,  0.0720],\n",
       "          [ 0.0168,  0.2512, -0.3333,  ...,  0.7891,  0.1206, -0.1948],\n",
       "          [-0.0378,  0.5083, -0.2583,  ...,  1.0449,  0.2646,  0.0725],\n",
       "          [ 0.3542,  0.5249, -0.1025,  ...,  0.7554, -0.1306, -0.4097],\n",
       "          [ 0.2822,  0.4172, -0.1736,  ...,  0.7617, -0.0644, -0.0799]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0534,  0.1913, -0.0645,  ...,  0.1890,  0.0042,  0.1882],\n",
       "          [-0.2369, -0.1407, -0.1562,  ..., -0.3047,  0.1272, -0.0096]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.6328e-01, 3.4571e-05, 1.7524e-05, 1.3660e-01, 1.2457e-05, 1.6761e-04],\n",
       "          [2.5977e-01, 4.8065e-02, 1.5515e-01, 4.5605e-01, 6.3660e-02, 1.7410e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  344.042694    0.000000  512.709473  157.656189    0.906720     40   \n",
       "  1  106.027016   30.861977  336.262970  130.292297    0.836382     45   \n",
       "  2  137.836319  239.648010  444.877075  383.376038    0.806412     50   \n",
       "  3    0.000000    0.213722  111.646408  176.078949    0.791002     39   \n",
       "  4  147.866821   95.157394  436.372681  384.036011    0.782802     48   \n",
       "  5  502.204681    1.758698  604.348022  198.886353    0.690971     39   \n",
       "  6  584.813232    3.613144  639.807251  295.408325    0.561710     41   \n",
       "  \n",
       "           name  \n",
       "  0  wine glass  \n",
       "  1        bowl  \n",
       "  2    broccoli  \n",
       "  3      bottle  \n",
       "  4    sandwich  \n",
       "  5      bottle  \n",
       "  6         cup  ,\n",
       "  'caption': ['A blue drinking glass.',\n",
       "   'Blue image on very far right of picture.'],\n",
       "  'bbox_target': [587.18, 2.47, 52.49, 290.16]},\n",
       " 445: {'image_emb': tensor([[-0.4714,  0.0435,  0.0629,  ...,  0.9736,  0.0703,  0.1660],\n",
       "          [-0.0901, -0.0469, -0.1344,  ...,  1.1270, -0.0772,  0.0891],\n",
       "          [-0.5796, -0.1170, -0.1816,  ...,  0.7915,  0.1591,  0.4070],\n",
       "          ...,\n",
       "          [ 0.0788,  0.0484, -0.2966,  ...,  1.4668,  0.2605,  0.1021],\n",
       "          [-0.3137,  0.0575, -0.1231,  ...,  0.9917,  0.2080,  0.2026],\n",
       "          [-0.3384,  0.0169, -0.2157,  ...,  0.8389, -0.0429,  0.1157]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3784, -0.1184, -0.4026,  ..., -0.0059, -0.2499, -0.0151],\n",
       "          [-0.2930,  0.0148, -0.1901,  ..., -0.1464, -0.1604, -0.0977]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.4124e-02, 2.4506e-02, 8.0322e-02, 1.7881e-07, 2.0837e-01, 4.1723e-07,\n",
       "           4.0221e-04, 6.6211e-01],\n",
       "          [3.2715e-02, 8.6182e-02, 4.3091e-01, 1.0133e-05, 3.3032e-01, 1.4305e-06,\n",
       "           2.7013e-04, 1.1963e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   450.869537   94.639282  639.640686  346.161316    0.947074      0   \n",
       "  1   165.242004   61.809326  474.793884  381.297058    0.944544      0   \n",
       "  2     0.437523   21.844238  235.394073  451.601868    0.935681      0   \n",
       "  3   409.526611   63.129776  458.210693  124.557755    0.886618     74   \n",
       "  4   284.710632   73.913834  438.843323  336.512817    0.848988      0   \n",
       "  5   492.910889  336.605988  541.260254  378.555939    0.798406     41   \n",
       "  6     6.980835  300.029510  637.274902  474.461670    0.790804     60   \n",
       "  7   310.147125  401.021423  476.179474  478.970642    0.668546     45   \n",
       "  8   553.523071  358.668243  606.080566  395.710236    0.614063     41   \n",
       "  9   130.681473  123.226593  221.303574  250.227753    0.575059     58   \n",
       "  10  170.214890  415.679932  223.950333  465.839966    0.551554     45   \n",
       "  11  170.494995  415.944031  224.061188  466.089661    0.503139     41   \n",
       "  12  151.776199  185.950348  176.340561  249.984100    0.466651     75   \n",
       "  13  340.549164  409.107056  406.622467  478.521606    0.357374     44   \n",
       "  14  401.127777  134.051361  426.072418  189.392426    0.273921     75   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3          clock  \n",
       "  4         person  \n",
       "  5            cup  \n",
       "  6   dining table  \n",
       "  7           bowl  \n",
       "  8            cup  \n",
       "  9   potted plant  \n",
       "  10          bowl  \n",
       "  11           cup  \n",
       "  12          vase  \n",
       "  13         spoon  \n",
       "  14          vase  ,\n",
       "  'caption': ['A man in a red apron preparing food at a table with other men in aprons',\n",
       "   'The man in red apron.'],\n",
       "  'bbox_target': [285.2, 74.91, 153.9, 258.29]},\n",
       " 446: {'image_emb': tensor([[ 0.0999,  0.4297, -0.2080,  ...,  1.2012, -0.1017, -0.1918],\n",
       "          [ 0.1681,  0.2238, -0.0567,  ...,  1.2842,  0.1511, -0.0729],\n",
       "          [-0.1643,  0.2600, -0.1688,  ...,  0.9092,  0.3220,  0.1293],\n",
       "          [-0.0941,  0.3789, -0.2040,  ...,  1.0635,  0.3374,  0.0662],\n",
       "          [ 0.3320,  0.3098, -0.0196,  ...,  0.5493,  0.0624,  0.3430]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.5078,  0.3054, -0.2832,  ..., -0.0751, -0.0150, -0.0106],\n",
       "          [-0.1915, -0.1010, -0.4385,  ..., -0.0296, -0.2627, -0.2133]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7792e-02, 2.4939e-01, 6.3672e-01, 9.4666e-02, 1.3504e-03],\n",
       "          [9.3460e-03, 2.5659e-01, 7.1973e-01, 1.4030e-02, 3.0041e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  313.116150  168.034821  638.988831  421.655518    0.915000     16      dog\n",
       "  1  225.553253  120.320801  359.447968  285.154663    0.854398     29  frisbee\n",
       "  2  234.194916    0.461525  429.061676  202.313873    0.847760      0   person\n",
       "  3   72.313133    0.000000  285.044800  401.207214    0.847491      0   person\n",
       "  4    0.074799  301.589447  395.431000  423.766602    0.573397     16      dog\n",
       "  5  226.760651  272.749146  372.402008  414.405457    0.540477     57    couch\n",
       "  6    0.287354   93.055176  106.305649  328.376648    0.401001      0   person\n",
       "  7  239.206802   24.040237  275.755432   51.108200    0.304689     41      cup\n",
       "  8  228.730652  120.238037  359.345886  289.831909    0.257614      0   person,\n",
       "  'caption': ['this is a woman wearing a brown shirt',\n",
       "   'The chest and face of the woman sitting in the brown chair.'],\n",
       "  'bbox_target': [243.73, 0.0, 190.95, 201.51]},\n",
       " 447: {'image_emb': tensor([[ 1.9800e-01, -3.3105e-01, -6.1475e-01,  ...,  7.8857e-01,\n",
       "            9.9182e-05,  4.2993e-01],\n",
       "          [-6.4392e-02,  3.9136e-01, -3.0615e-01,  ...,  1.1621e+00,\n",
       "           -1.7029e-02, -2.9004e-01],\n",
       "          [ 6.2805e-02, -1.3562e-01, -2.8296e-01,  ...,  7.9541e-01,\n",
       "            5.0690e-02,  1.0962e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0743,  0.6284, -0.1040,  ..., -0.0682, -0.0072, -0.0186],\n",
       "          [ 0.0787,  0.1748, -0.2123,  ..., -0.2515, -0.0167, -0.3394]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7881e-07, 1.0000e+00, 1.7953e-04],\n",
       "          [2.0266e-06, 1.0000e+00, 2.1994e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  181.749069   77.085739  360.912292  227.978943    0.921501      6     train\n",
       "  1   96.677040  148.494751  225.146271  402.928833    0.915514      0    person\n",
       "  2  102.853905  203.545135  116.545326  262.398773    0.353007     24  backpack,\n",
       "  'caption': ['A man in a black jacket and khakis',\n",
       "   'A man with black hair, tan pants wearing sunglasses and a pack on his back with a water bottle sticking out of it.'],\n",
       "  'bbox_target': [97.08, 151.01, 125.12, 259.96]},\n",
       " 448: {'image_emb': tensor([[-0.0345,  0.9004,  0.4758,  ...,  1.1641,  0.2130,  0.0403],\n",
       "          [-0.0312, -0.0117, -0.0764,  ...,  0.6895, -0.2551, -0.1202],\n",
       "          [-0.9180,  0.5786,  0.3613,  ...,  1.2041,  0.1648, -0.0994],\n",
       "          [ 0.0015,  0.6436,  0.1543,  ...,  0.6152,  0.0221, -0.0754],\n",
       "          [ 0.3147,  0.0490,  0.2118,  ...,  0.6367, -0.0936, -0.0784]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0500,  0.1224,  0.0819,  ..., -0.3740, -0.2925, -0.1240],\n",
       "          [-0.0967, -0.0635,  0.1987,  ..., -0.0441, -0.5732, -0.1646]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.0081e-04, 3.7549e-01, 1.8872e-01, 1.6495e-02, 4.1870e-01],\n",
       "          [3.2640e-04, 6.0889e-01, 3.1281e-02, 7.3128e-03, 3.5229e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  269.621460    0.054024  410.334900  175.899231    0.929042     41   \n",
       "  1  189.209137  210.930328  469.190887  397.616211    0.874952     48   \n",
       "  2  511.374664   94.424011  639.813660  302.900024    0.739749     45   \n",
       "  3   10.843613    0.000000  273.982544  211.068420    0.721285     41   \n",
       "  4  392.306366    1.577087  635.295532  122.728851    0.672372     45   \n",
       "  5    0.000000    1.803421  639.208374  393.384583    0.486561     60   \n",
       "  6    0.000000  273.766968   34.763489  398.657532    0.353796     42   \n",
       "  \n",
       "             name  \n",
       "  0           cup  \n",
       "  1      sandwich  \n",
       "  2          bowl  \n",
       "  3           cup  \n",
       "  4          bowl  \n",
       "  5  dining table  \n",
       "  6          fork  ,\n",
       "  'caption': ['A bowl with bread in it.', 'Plain pieces of toast in a bowl.'],\n",
       "  'bbox_target': [360.45, 0.9, 279.55, 118.65]},\n",
       " 449: {'image_emb': tensor([[-0.2546,  0.2524, -0.1337,  ...,  0.7974, -0.4033, -0.2345],\n",
       "          [-0.2400,  0.3362, -0.0677,  ...,  0.8813, -0.3477,  0.1752],\n",
       "          [-0.0614,  0.2688, -0.4812,  ...,  1.0820, -0.0677, -0.1213],\n",
       "          [-0.0394,  0.1255, -0.0559,  ...,  0.9531, -0.0914,  0.1405],\n",
       "          [ 0.2498,  0.2771, -0.2465,  ...,  1.0674,  0.1302,  0.2732],\n",
       "          [ 0.1729,  0.1091, -0.1058,  ...,  0.7627, -0.1624, -0.1479]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0712,  0.0031, -0.2629,  ..., -0.0499, -0.0586,  0.1371],\n",
       "          [ 0.0870,  0.0796, -0.1678,  ...,  0.2561, -0.1116, -0.1553]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.9150e-01, 1.0400e-01, 1.1802e-04, 1.9951e-03, 3.4082e-01, 2.6147e-01],\n",
       "          [9.6560e-06, 1.7762e-05, 2.9206e-06, 2.9206e-06, 9.9951e-01, 3.2496e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  184.291199  177.508484  243.078827  258.716187    0.883026     56   \n",
       "  1   59.457920  181.172668  145.732468  276.520996    0.865617     56   \n",
       "  2  237.430756  267.385773  286.844391  307.310211    0.855528     63   \n",
       "  3  139.266525  285.346283  241.592911  360.424286    0.821671      0   \n",
       "  4    2.384613  253.063629  529.654297  420.765137    0.723250     57   \n",
       "  5  307.501007  121.438278  338.506683  187.271851    0.630499      0   \n",
       "  6  255.336548  163.958984  266.364746  180.921204    0.629833     75   \n",
       "  7  411.680267  280.514984  504.701813  332.286041    0.432598     24   \n",
       "  8  248.776138  136.405014  277.924774  181.630219    0.399554     58   \n",
       "  \n",
       "             name  \n",
       "  0         chair  \n",
       "  1         chair  \n",
       "  2        laptop  \n",
       "  3        person  \n",
       "  4         couch  \n",
       "  5        person  \n",
       "  6          vase  \n",
       "  7      backpack  \n",
       "  8  potted plant  ,\n",
       "  'caption': ['A section of the couch to the right of the plank of wood',\n",
       "   'Floral couch with a laptop on the computer.'],\n",
       "  'bbox_target': [170.91, 247.69, 358.28, 170.91]},\n",
       " 450: {'image_emb': tensor([[-0.3208,  0.4500,  0.2568,  ...,  0.8970, -0.2052, -0.3713],\n",
       "          [-0.0703,  0.5518, -0.1759,  ...,  1.0381,  0.0998,  0.2810],\n",
       "          [-0.1455,  0.4714, -0.1115,  ...,  0.7393, -0.0715, -0.3884],\n",
       "          [-0.0735,  0.9678,  0.3523,  ...,  0.6665,  0.2283, -0.3726],\n",
       "          [-0.4634,  0.1444,  0.3174,  ...,  1.0928,  0.6123, -0.3616]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.4001,  0.0089,  0.1027,  ...,  0.2095, -0.1113, -0.4131],\n",
       "          [-0.3687,  0.1726,  0.0044,  ...,  0.3999, -0.2450, -0.2571]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0000e+00, 5.9605e-08, 6.3002e-05, 1.7226e-05, 8.1658e-06],\n",
       "          [9.9951e-01, 1.1921e-07, 5.5265e-04, 5.7340e-05, 9.1672e-05]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  126.220215  334.260773  356.765259  427.005463    0.937756     66  keyboard\n",
       "  1  139.094086  206.899872  254.056122  282.744537    0.907209     63    laptop\n",
       "  2  235.001434   41.440048  417.289398  206.434875    0.904442     62        tv\n",
       "  3  314.169250   50.281067  599.995850  232.280365    0.890221     63    laptop\n",
       "  4   66.980324  202.594543   85.597557  253.076965    0.696519     39    bottle\n",
       "  5  121.130096  175.589401  150.769012  257.329346    0.674686     39    bottle\n",
       "  6   84.733459  203.561646  106.158737  249.223999    0.627196     39    bottle\n",
       "  7   45.550110  200.571442   69.222694  264.875641    0.611654     39    bottle\n",
       "  8  118.316940  327.176056  182.387985  350.729340    0.600774     64     mouse\n",
       "  9   39.024872  201.130310   54.851669  266.512329    0.470977     39    bottle,\n",
       "  'caption': ['A white computer keyboard.',\n",
       "   'A white color keyboard placed on a computer table.'],\n",
       "  'bbox_target': [125.83, 336.1, 229.04, 90.75]},\n",
       " 451: {'image_emb': tensor([[-0.0865, -0.0610, -0.1071,  ...,  1.2480,  0.2817, -0.3567],\n",
       "          [ 0.4414, -0.1461, -0.0046,  ...,  0.4399, -0.4941, -0.1339]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3262,  0.0562,  0.2739,  ...,  0.0749, -0.0802, -0.3108],\n",
       "          [ 0.4451, -0.0831,  0.2827,  ...,  0.9360, -0.4438,  0.2432]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.4607e-02, 9.6533e-01],\n",
       "          [1.0133e-06, 1.0000e+00]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  172.886566  194.337555  253.577179  226.065338    0.706506      7     truck\n",
       "  1   47.506989   84.717926  602.328979  243.298553    0.687212      4  airplane\n",
       "  2  295.312408   61.780029  428.577667  142.774902    0.585247      4  airplane\n",
       "  3  585.972046  166.052826  629.264526  191.027863    0.385807      7     truck,\n",
       "  'caption': [\"The airplane that's not being blocked from view by anything else.\",\n",
       "   'a Delta airplane is about to board passengers'],\n",
       "  'bbox_target': [194.38, 83.08, 410.74, 163.6]},\n",
       " 452: {'image_emb': tensor([[ 0.0261, -0.3618,  0.2180,  ...,  0.7432, -0.0031, -0.1727],\n",
       "          [ 0.0514, -0.2915,  0.2959,  ...,  0.8452,  0.1404, -0.1340],\n",
       "          [-0.1176, -0.3752,  0.1759,  ...,  0.5039, -0.1954, -0.1410]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0361, -0.1416, -0.0299,  ...,  0.1161, -0.3625, -0.0851],\n",
       "          [ 0.1545, -0.2084,  0.1356,  ...,  0.0338, -0.1812, -0.1586]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2922, 0.4971, 0.2106],\n",
       "          [0.5918, 0.1833, 0.2247]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  name\n",
       "  0  112.905304  159.902313  443.599335  334.768707    0.936772     21  bear\n",
       "  1  479.761353  244.945709  615.660889  336.224823    0.914707     21  bear\n",
       "  2  321.801086  253.780273  490.729614  332.966553    0.511758     21  bear,\n",
       "  'caption': ['The bear cub that is half hidden by the adult bear.',\n",
       "   'Baby bear partly blocked by adult bear.'],\n",
       "  'bbox_target': [320.21, 256.8, 177.92, 77.69]},\n",
       " 453: {'image_emb': tensor([[ 7.1621e-04,  1.9995e-01, -3.7207e-01,  ...,  1.0088e+00,\n",
       "            1.9568e-01,  4.8486e-01],\n",
       "          [-1.5356e-01,  3.3667e-01, -3.0005e-01,  ...,  1.2803e+00,\n",
       "            1.7566e-01,  2.1851e-01],\n",
       "          [-9.6924e-02,  4.8169e-01, -9.2224e-02,  ...,  8.0518e-01,\n",
       "            2.7783e-01, -2.1835e-02],\n",
       "          ...,\n",
       "          [ 8.1726e-02,  3.3057e-01, -1.3708e-01,  ...,  6.2158e-01,\n",
       "            2.5415e-01,  1.3831e-01],\n",
       "          [-3.9868e-01,  1.3281e-01, -8.8379e-02,  ...,  7.6318e-01,\n",
       "            1.9800e-01, -1.7432e-01],\n",
       "          [-4.0552e-01,  1.8860e-01, -5.3345e-02,  ...,  4.8193e-01,\n",
       "            1.3989e-01,  4.7119e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2458,  0.0315, -0.2739,  ..., -0.2246,  0.2494, -0.3445],\n",
       "          [ 0.3379,  0.3323, -0.2430,  ..., -0.2351,  0.2524, -0.3145]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.9160e-03, 1.9312e-04, 2.6108e-02, 1.8689e-01, 9.0179e-03, 6.4258e-01,\n",
       "           1.2842e-01],\n",
       "          [6.5269e-03, 6.3286e-03, 1.9995e-01, 4.2334e-01, 1.0597e-02, 3.5083e-01,\n",
       "           2.3651e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  171.445801  223.407043  334.608398  443.800171    0.920892      0    person\n",
       "  1  297.322632   81.679596  387.310486  428.220428    0.918449      0    person\n",
       "  2  314.723389   44.489799  448.755676  218.131165    0.916914     25  umbrella\n",
       "  3   80.032196   92.142944  216.606049  422.108887    0.883157      0    person\n",
       "  4  110.770874  203.118896  295.931091  354.241882    0.840691     25  umbrella\n",
       "  5  162.548584   50.060158  321.338440  197.263611    0.813288     25  umbrella\n",
       "  6  205.578979   92.072357  302.073975  364.520416    0.607251      0    person,\n",
       "  'caption': ['The black umbrella the woman to the farthest right is holding.',\n",
       "   'darkest umbrella'],\n",
       "  'bbox_target': [317.48, 47.27, 129.67, 170.99]},\n",
       " 454: {'image_emb': tensor([[ 0.0699, -0.0782, -0.2510,  ...,  1.2363,  0.1670,  0.1503],\n",
       "          [-0.3848,  0.4189,  0.1040,  ...,  1.3066,  0.0353, -0.0492],\n",
       "          [-0.1865,  0.3093,  0.0831,  ...,  0.9790, -0.1866, -0.2422],\n",
       "          ...,\n",
       "          [ 0.1781,  0.3721, -0.1403,  ...,  1.0410, -0.2490, -0.0057],\n",
       "          [ 0.3474,  0.3823, -0.2898,  ...,  0.9580, -0.5166,  0.4646],\n",
       "          [ 0.2062,  0.3240, -0.2781,  ...,  0.6250, -0.2808, -0.1193]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-1.7615e-01,  2.3961e-04,  1.3641e-02, -1.2927e-01, -1.9653e-01,\n",
       "            3.9697e-01, -3.2318e-02, -5.3271e-01, -1.6565e-01, -3.8879e-02,\n",
       "            6.5552e-02,  2.8247e-01,  1.3892e-01,  1.0504e-01,  2.5854e-01,\n",
       "           -2.3145e-01, -3.6621e-02,  2.5732e-01, -4.2993e-01, -3.1396e-01,\n",
       "            2.7222e-01,  3.7781e-02,  2.1765e-01,  6.4392e-02, -2.9028e-01,\n",
       "            1.3660e-01, -5.8105e-02,  4.9683e-01, -5.2582e-02, -1.9238e-01,\n",
       "           -4.9341e-01,  5.1172e-01, -1.1249e-01, -8.0627e-02, -4.3286e-01,\n",
       "           -4.1772e-01,  1.0144e-01,  7.7477e-03,  7.6180e-03, -3.6108e-01,\n",
       "            9.3262e-02,  1.0913e-01,  1.9739e-01,  4.6814e-02,  3.0713e-01,\n",
       "           -7.1106e-02,  7.8735e-02, -4.0741e-03, -3.1714e-01, -1.3725e-02,\n",
       "            4.5776e-02,  1.5967e-01,  5.1575e-02, -6.6650e-02, -4.3311e-01,\n",
       "            1.8225e-01,  1.7319e-02,  2.0325e-01, -1.1255e-01,  8.6975e-02,\n",
       "           -2.7783e-01,  8.1848e-02, -1.3538e-01,  1.9318e-02,  2.8896e-03,\n",
       "            1.9385e-01,  7.8125e-02,  3.0664e-01, -3.5181e-01,  2.2107e-01,\n",
       "            2.5903e-01, -1.4221e-01, -1.5161e-01, -4.0332e-01, -1.0822e-01,\n",
       "           -1.7590e-01, -7.9102e-02,  3.1299e-01, -8.9905e-02, -1.0571e-01,\n",
       "           -4.2480e-01,  4.6082e-02,  8.6060e-02,  3.4082e-01, -5.2783e-01,\n",
       "            6.3782e-02,  2.1497e-01, -4.1699e-01, -3.9014e-01,  2.1777e-01,\n",
       "            9.4971e-02, -1.7969e-01, -1.1006e+00,  5.6885e-01, -3.5059e-01,\n",
       "            2.0032e-01,  4.4800e-02, -2.6047e-02,  1.7249e-01,  2.1252e-01,\n",
       "            2.5562e-01,  3.7964e-01, -6.4758e-02, -2.0728e-01, -1.0669e-01,\n",
       "           -2.2369e-02, -8.3435e-02,  5.8807e-02, -1.6272e-01,  7.3059e-02,\n",
       "           -5.3320e-01,  5.9418e-02,  3.2129e-01, -3.7939e-01, -4.5215e-01,\n",
       "           -1.6675e-01,  1.5576e-01,  6.5332e-01, -5.3711e-01,  3.3911e-01,\n",
       "            3.6157e-01, -6.8018e-01, -1.7493e-01,  3.5858e-02,  3.2440e-02,\n",
       "           -1.2378e-01, -2.3560e-02, -5.0488e-01,  2.5952e-01,  2.5375e-02,\n",
       "           -5.2734e-02, -2.6587e-01,  1.9958e-01,  4.3789e+00, -4.4037e-02,\n",
       "            4.8035e-02, -8.0200e-02, -4.9365e-01,  2.0996e-01, -3.0176e-01,\n",
       "            2.3010e-01,  2.4573e-01, -3.2324e-01,  2.7759e-01, -8.8013e-02,\n",
       "           -1.2469e-01,  3.1143e-02, -3.0737e-01,  3.0713e-01, -1.1957e-01,\n",
       "           -1.8274e-01, -3.8794e-01, -1.4685e-01,  1.2122e-01,  3.1177e-01,\n",
       "           -2.3376e-01,  6.6284e-02, -7.9688e-01, -2.5317e-01, -1.6678e-02,\n",
       "            3.0884e-01, -2.7319e-01,  2.9099e-02, -1.8921e-01, -1.2903e-01,\n",
       "           -1.2189e-01,  1.1884e-01,  1.1377e-01,  1.3086e-01,  1.0773e-01,\n",
       "            3.5229e-01, -8.9111e-02,  4.2603e-02, -1.0712e-01, -3.1543e-01,\n",
       "           -6.7383e-02,  1.3794e-01, -1.3074e-01,  1.6052e-01, -2.2705e-01,\n",
       "           -1.4746e-01, -8.2642e-02, -1.5759e-01,  2.4567e-02, -2.6489e-01,\n",
       "            6.7139e-02, -8.7357e-04,  2.4854e-01,  3.8843e-01, -2.5659e-01,\n",
       "           -1.3684e-01,  2.5879e-01,  3.3301e-01, -3.6774e-02,  1.0938e-01,\n",
       "           -4.8615e-02, -2.5659e-01,  1.6785e-01, -3.6835e-02, -1.2286e-01,\n",
       "            3.8232e-01,  3.6597e-01, -3.0078e-01,  2.2363e-01, -4.6631e-02,\n",
       "            2.7612e-01,  6.4575e-02, -2.0593e-01,  5.4283e-03, -1.7029e-01,\n",
       "            4.5557e-01,  4.4067e-01, -7.8491e-02,  1.1792e-01,  1.8750e-01,\n",
       "           -8.3984e-02,  3.9771e-01,  4.3286e-01,  2.7881e-01, -8.9111e-02,\n",
       "            5.0879e-01,  8.0627e-02, -3.4058e-01, -3.5571e-01,  3.0859e-01,\n",
       "           -3.6060e-01, -2.3178e-02,  4.4289e-03,  1.5417e-01,  3.0804e-03,\n",
       "           -8.3008e-02, -6.7078e-02, -1.2952e-01,  6.9702e-02,  4.1847e-03,\n",
       "           -1.9055e-01,  7.7271e-02, -4.8706e-02, -5.5225e-01, -1.8997e-02,\n",
       "            6.6650e-02,  3.0273e-01,  3.2812e-01, -2.1899e-01, -1.9397e-01,\n",
       "            5.2295e-01, -5.2307e-02,  2.3694e-01, -5.7709e-02,  3.1201e-01,\n",
       "           -3.3105e-01, -1.1865e-01, -3.7720e-02,  1.3513e-01,  4.6492e-06,\n",
       "           -5.7068e-02,  3.1274e-01, -7.6416e-02, -1.9641e-01,  3.0835e-01,\n",
       "           -1.6467e-01,  1.9470e-01, -5.0195e-01,  1.7334e-01,  1.2976e-01,\n",
       "            1.2708e-01,  3.2178e-01, -4.1870e-02,  2.5684e-01,  8.4839e-03,\n",
       "            6.2286e-02, -3.6108e-01,  4.9487e-01,  7.2449e-02, -3.0469e-01,\n",
       "           -2.0386e-01, -2.0374e-01,  1.4877e-02,  2.9907e-01,  2.7051e-01,\n",
       "            4.4336e-01,  9.2697e-03,  6.7444e-02,  1.7297e-01, -3.5583e-02,\n",
       "           -2.7441e-01,  3.1891e-02,  1.6992e-01,  2.4048e-01,  2.4811e-02,\n",
       "            2.4402e-01, -1.6382e-01,  4.1748e-01,  1.7624e-02, -3.4033e-01,\n",
       "            3.2422e-01, -3.9380e-01, -3.6768e-01,  1.4478e-01, -1.9531e-02,\n",
       "           -4.3823e-02,  1.3635e-01, -1.1055e-02, -7.6599e-02,  7.1472e-02,\n",
       "           -2.5806e-01, -6.4819e-02,  2.7026e-01, -3.9697e-01, -2.1179e-01,\n",
       "           -1.7493e-01,  4.2822e-01,  4.3750e+00, -2.7393e-01, -3.6426e-01,\n",
       "           -3.2379e-02,  4.5947e-01,  1.0583e-01,  3.8257e-01,  4.4769e-02,\n",
       "            4.4214e-01,  2.8809e-01,  4.7314e-01,  6.1371e-02, -2.1399e-01,\n",
       "            1.6495e-02,  2.7734e-01,  1.0492e-01,  5.9229e-01, -1.5527e+00,\n",
       "           -9.8083e-02,  2.1130e-01, -6.7688e-02, -4.4531e-01,  1.8677e-01,\n",
       "           -5.6038e-03, -3.3722e-02,  8.3252e-02,  4.7266e-01, -8.2825e-02,\n",
       "           -1.3745e-01, -1.4246e-01,  4.1504e-01, -3.5547e-01,  2.6270e-01,\n",
       "            2.8296e-01,  4.0161e-02,  1.5747e-01, -1.0132e-01,  6.0669e-02,\n",
       "           -2.3572e-01, -2.6782e-01, -2.5635e-01, -1.3770e-01, -7.2656e-01,\n",
       "           -3.3691e-01, -3.9429e-01, -2.4121e-01, -1.9788e-01, -1.9760e-02,\n",
       "           -6.5967e-01, -2.7002e-01, -1.1432e-01,  2.2681e-01,  5.0934e-02,\n",
       "            6.4087e-02,  2.0520e-01,  4.9390e-01, -6.8542e-02, -1.0590e-01,\n",
       "            1.8298e-01,  1.2433e-01, -1.0425e-01,  3.4497e-01, -2.4341e-01,\n",
       "            1.0760e-01,  1.7078e-01,  8.3130e-02,  4.1504e-02, -5.6641e-01,\n",
       "           -4.2773e-01, -2.8076e-01,  4.6045e-01, -6.8359e-01,  2.0508e-01,\n",
       "           -2.9810e-01, -1.7212e-01,  6.4209e-02, -9.1492e-02,  6.3171e-02,\n",
       "           -3.0078e-01,  4.7681e-01, -2.1094e-01,  7.6904e-02,  1.1151e-01,\n",
       "            3.0322e-01,  1.9604e-01, -2.9761e-01, -2.8667e-03,  4.6289e-01,\n",
       "            3.7256e-01,  2.4866e-01,  5.2490e-01, -2.4216e-02, -2.6099e-01,\n",
       "           -3.4595e-01,  1.4929e-01, -1.5686e-01, -3.1152e-01,  3.9444e-03,\n",
       "            2.2131e-01, -2.4902e-01,  2.8955e-01, -5.9143e-02,  1.7932e-01,\n",
       "            2.1875e-01, -3.7549e-01, -1.4587e-01,  6.0645e-01,  1.0242e-01,\n",
       "           -3.8696e-01,  4.1046e-03, -1.3831e-01, -4.2896e-01, -1.6882e-01,\n",
       "           -1.4197e-01, -2.3816e-01, -6.2500e-02, -1.4233e-01,  1.3538e-01,\n",
       "           -1.0669e-01,  3.0176e-01,  2.8296e-01, -2.2412e-01, -1.7776e-02,\n",
       "           -4.3384e-01, -1.3989e-01,  1.5881e-01, -2.7298e-02, -1.9812e-01,\n",
       "           -1.3354e-01, -2.5391e-01, -5.6592e-01,  2.7124e-01, -1.0016e-01,\n",
       "           -5.9723e-02, -4.4019e-01,  2.8955e-01,  2.9419e-01,  3.1714e-01,\n",
       "           -2.5830e-01,  2.6489e-01,  6.7322e-02, -1.6992e-01,  1.6174e-01,\n",
       "            1.1194e-01,  2.7197e-01,  2.1924e-01, -1.3733e-01, -1.2939e-01,\n",
       "           -1.7224e-01,  7.8857e-02,  4.0234e-01,  1.4966e-01,  5.9033e-01,\n",
       "            2.4524e-01, -1.5430e-01, -2.5049e-01,  1.7908e-01, -3.8330e-02,\n",
       "           -2.0422e-01,  2.7612e-01,  1.6418e-01, -1.4990e-01,  2.8931e-02,\n",
       "            3.1738e-01,  9.6313e-02,  4.8218e-02,  1.0876e-01,  2.4451e-01,\n",
       "            1.8030e-01, -2.9590e-01,  1.7114e-01,  1.1208e-02, -9.1858e-02,\n",
       "           -2.1497e-01, -4.4067e-02, -8.7219e-02,  5.1221e-01,  1.7090e-01,\n",
       "            1.3269e-01, -7.0984e-02,  2.3511e-01,  8.9746e-01,  5.2783e-01,\n",
       "           -6.0742e-01,  5.0879e-01, -1.5063e-01,  2.1960e-01, -3.1543e-01,\n",
       "            6.6147e-03,  4.2480e-02,  5.7373e-01,  4.5776e-01,  3.8306e-01,\n",
       "            2.0178e-01, -2.1631e-01,  2.3291e-01, -3.1470e-01, -9.0454e-02,\n",
       "           -2.7313e-02, -2.4194e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0760, 0.0021, 0.0063, 0.0036, 0.1123, 0.4172, 0.0130, 0.0159, 0.1794,\n",
       "           0.1740]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    61.121803  363.381592  123.952850  478.054016    0.847634     75   \n",
       "  1   307.903809  112.541832  359.982117  164.206177    0.783939     55   \n",
       "  2   211.064240  123.152641  263.147736  166.712524    0.781347     55   \n",
       "  3   164.971069  110.210480  217.292969  157.873199    0.766360     55   \n",
       "  4   282.865417  442.650055  464.024536  521.537476    0.748341     48   \n",
       "  5   185.393356  493.725159  330.133484  560.589661    0.734374     48   \n",
       "  6   261.324005  123.439682  313.349030  167.431473    0.708999     55   \n",
       "  7   184.922424  225.637589  236.184631  283.159363    0.705207     55   \n",
       "  8     0.000000  354.879333  402.320587  639.231812    0.700783     60   \n",
       "  9   350.190277  489.992798  450.688904  569.748291    0.690240     48   \n",
       "  10  355.352020  191.123337  402.352570  250.905594    0.690135     55   \n",
       "  11  255.049072  470.941132  355.937134  553.751099    0.655958     48   \n",
       "  12  227.245636   72.771187  259.595490  117.131676    0.643463     55   \n",
       "  13    1.004761  146.309402  129.631226  244.302902    0.634685     45   \n",
       "  14  440.724457  456.963531  512.122009  508.658905    0.565016     48   \n",
       "  15  136.920761  216.671799  190.561508  266.104156    0.560339     55   \n",
       "  16  162.004639   91.703835  212.237946  129.291748    0.558150     55   \n",
       "  17  236.716614  238.815155  285.627563  273.714508    0.547912     55   \n",
       "  18  340.740540  397.120789  475.266052  458.845764    0.546151     48   \n",
       "  19  324.759888  214.695206  375.051880  268.123047    0.452880     55   \n",
       "  20  283.557007  236.181015  335.510559  278.614716    0.426802     55   \n",
       "  21    4.379105    6.386948  478.244690  343.383301    0.415737     60   \n",
       "  22  150.106003  361.015747  193.609039  398.061951    0.365740     55   \n",
       "  23  453.457275  602.247437  512.052490  640.000000    0.306513     48   \n",
       "  24  287.734436   75.349716  341.116333  117.920547    0.304998     55   \n",
       "  25  211.735291  107.048164  244.059875  138.017365    0.292151     55   \n",
       "  26  378.352051  615.433289  459.350708  639.677063    0.253790     48   \n",
       "  \n",
       "              name  \n",
       "  0           vase  \n",
       "  1           cake  \n",
       "  2           cake  \n",
       "  3           cake  \n",
       "  4       sandwich  \n",
       "  5       sandwich  \n",
       "  6           cake  \n",
       "  7           cake  \n",
       "  8   dining table  \n",
       "  9       sandwich  \n",
       "  10          cake  \n",
       "  11      sandwich  \n",
       "  12          cake  \n",
       "  13          bowl  \n",
       "  14      sandwich  \n",
       "  15          cake  \n",
       "  16          cake  \n",
       "  17          cake  \n",
       "  18      sandwich  \n",
       "  19          cake  \n",
       "  20          cake  \n",
       "  21  dining table  \n",
       "  22          cake  \n",
       "  23      sandwich  \n",
       "  24          cake  \n",
       "  25          cake  \n",
       "  26      sandwich  ,\n",
       "  'caption': ['A TABLE WITH DRAWER'],\n",
       "  'bbox_target': [1.66, 1.66, 468.78, 339.12]},\n",
       " 455: {'image_emb': tensor([[-0.1719,  0.4319,  0.0944,  ...,  0.7871, -0.1261, -0.3674],\n",
       "          [-0.1444,  0.3452, -0.1631,  ...,  0.9419, -0.0807, -0.0121],\n",
       "          [-0.1812,  0.3428,  0.2712,  ...,  1.2354,  0.0720, -0.2656],\n",
       "          [-0.2947,  0.0461, -0.1957,  ...,  1.0459,  0.1058, -0.3342],\n",
       "          [-0.2690,  0.2169, -0.0586,  ...,  0.9429, -0.0020, -0.5737]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1331,  0.0909,  0.2544,  ..., -0.2114, -0.0850, -0.3130],\n",
       "          [-0.0856, -0.0215,  0.0328,  ..., -0.1893, -0.1242, -0.4282]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.8926e-01, 7.0691e-05, 1.0490e-02, 5.9605e-07, 2.2948e-05],\n",
       "          [9.4727e-01, 1.9159e-03, 4.5715e-02, 1.0157e-04, 5.1270e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  472.406921  346.549835  611.952087  507.968628    0.935892     56   \n",
       "  1  515.165039  121.398407  626.817871  369.549469    0.934387     72   \n",
       "  2  344.708374  393.902344  520.302917  507.735840    0.865048     56   \n",
       "  3  351.657257  221.433136  415.259430  270.653107    0.781878     69   \n",
       "  4  130.610153  224.821320  259.038757  250.556366    0.567483     71   \n",
       "  5   88.711884  242.348938  611.159302  504.357971    0.470630     60   \n",
       "  6    3.956451  258.618622  147.238388  445.876740    0.327052     72   \n",
       "  7  184.414856  225.993591  255.203094  241.303284    0.308679     71   \n",
       "  \n",
       "             name  \n",
       "  0         chair  \n",
       "  1  refrigerator  \n",
       "  2         chair  \n",
       "  3          oven  \n",
       "  4          sink  \n",
       "  5  dining table  \n",
       "  6  refrigerator  \n",
       "  7          sink  ,\n",
       "  'caption': ['A stool who barely has any legs visible.',\n",
       "   'white stool on the left'],\n",
       "  'bbox_target': [345.74, 394.75, 172.21, 115.25]},\n",
       " 456: {'image_emb': tensor([[ 0.1489, -0.1354,  0.2629,  ...,  0.8931,  0.1630, -0.3169],\n",
       "          [ 0.1886, -0.1530,  0.3455,  ...,  0.7739,  0.1272, -0.1678],\n",
       "          [-0.1759,  0.3037,  0.0958,  ...,  1.3848,  0.0820, -0.3267],\n",
       "          [-0.2477,  0.4412, -0.2224,  ...,  1.6396,  0.1741,  0.1227],\n",
       "          [ 0.0555, -0.4497,  0.0237,  ...,  0.3020, -0.0497, -0.1296]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0353,  0.2664, -0.0822,  ..., -0.0077,  0.3091,  0.0039],\n",
       "          [ 0.0238,  0.1317, -0.0590,  ..., -0.1039, -0.0750, -0.2402]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.6817e-03, 1.1559e-03, 6.7871e-01, 2.7856e-01, 3.2227e-02],\n",
       "          [7.2876e-02, 1.3908e-02, 1.1353e-02, 2.7108e-04, 9.0186e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  369.293518  289.655701  539.326111  501.978210    0.940180     18   sheep\n",
       "  1  222.609100  258.084351  412.447784  441.855225    0.930571     18   sheep\n",
       "  2  509.481812   41.857346  610.187622  419.068848    0.872138      0  person\n",
       "  3  580.048828   23.521858  640.000000  197.536896    0.725829      0  person\n",
       "  4    9.285114  152.160812   91.983368  193.693863    0.402230     13   bench\n",
       "  5  395.624817  274.697144  478.027222  346.203064    0.367773     18   sheep\n",
       "  6  585.410034  282.221466  639.482422  404.199799    0.344093     18   sheep,\n",
       "  'caption': ['Girl wearing a black coat.', 'the woman close to the goats'],\n",
       "  'bbox_target': [510.16, 42.98, 99.55, 377.82]},\n",
       " 457: {'image_emb': tensor([[ 0.1356,  0.5181,  0.0072,  ...,  0.5845,  0.1774, -0.2032],\n",
       "          [ 0.0717,  0.3577,  0.0262,  ...,  1.0625,  0.2627, -0.0309],\n",
       "          [-0.0265,  0.0558, -0.0779,  ...,  0.9658,  0.2893, -0.1671],\n",
       "          [-0.0372,  0.3313,  0.0042,  ...,  0.6055,  0.1271, -0.0646],\n",
       "          [ 0.1747,  0.3918,  0.1554,  ...,  0.5049,  0.2664, -0.2439]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2407,  0.2429, -0.0728,  ...,  0.1462,  0.4973, -0.0041],\n",
       "          [ 0.1128, -0.2275, -0.0116,  ...,  0.7964,  0.2578, -0.2812]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3127, 0.0315, 0.0080, 0.6416, 0.0060],\n",
       "          [0.1307, 0.0336, 0.6846, 0.1327, 0.0182]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   65.781853  165.872864  207.056915  425.931519    0.938871      0   \n",
       "  1    2.704132  233.860565  163.818466  329.016083    0.917235     37   \n",
       "  2  437.440857  200.884613  518.800110  320.415314    0.908259     37   \n",
       "  3  284.720154  131.891357  488.324158  425.113281    0.862224      0   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1  surfboard  \n",
       "  2  surfboard  \n",
       "  3     person  ,\n",
       "  'caption': ['A woman holding pink surfboard reaching out her arm.',\n",
       "   'boy holding red wakeboard'],\n",
       "  'bbox_target': [287.82, 134.36, 198.85, 286.91]},\n",
       " 458: {'image_emb': tensor([[ 0.0017,  0.2046,  0.1671,  ...,  0.6860, -0.0960, -0.1890],\n",
       "          [ 0.0195,  0.3611,  0.0991,  ...,  0.5151,  0.0614,  0.0271],\n",
       "          [ 0.0312,  0.3262,  0.1493,  ...,  0.5371,  0.0870,  0.0599]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2712,  0.1094, -0.3955,  ..., -0.2734, -0.2725, -0.4990],\n",
       "          [-0.1353, -0.0632, -0.4131,  ..., -0.0324, -0.0961, -0.4172]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0584, 0.4524, 0.4893],\n",
       "          [0.0298, 0.4248, 0.5454]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  219.799957   86.223236  637.782349  391.928131    0.808329      0  person\n",
       "  1    1.803497    4.125290  640.000000  471.103333    0.740252     59     bed\n",
       "  2    0.000000  142.866409  640.000000  477.126465    0.544707      0  person,\n",
       "  'caption': ['mother and a baby lying on a bed',\n",
       "   'Woman lying next to a little baby.'],\n",
       "  'bbox_target': [0.0, 120.92, 638.99, 359.08]},\n",
       " 459: {'image_emb': tensor([[ 1.3641e-02,  4.3896e-01,  3.0078e-01,  ...,  9.8389e-01,\n",
       "            2.8931e-01,  5.3271e-01],\n",
       "          [ 7.6180e-03, -2.3499e-02, -1.9495e-01,  ...,  1.2842e+00,\n",
       "            1.4917e-01, -1.3232e-01],\n",
       "          [ 3.7476e-01,  2.8101e-01,  7.7629e-04,  ...,  1.0312e+00,\n",
       "            1.0962e-01,  3.3228e-01],\n",
       "          [ 3.6011e-01,  2.6587e-01,  1.2158e-01,  ...,  5.5127e-01,\n",
       "            1.8945e-01,  4.4116e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1350,  0.1962, -0.1555,  ...,  0.0335,  0.1203, -0.1265],\n",
       "          [-0.1110,  0.1161,  0.0098,  ..., -0.1027, -0.0005,  0.0101]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.8799, 0.0054, 0.0064, 0.1084],\n",
       "          [0.4512, 0.3799, 0.0583, 0.1106]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  226.282104   67.829193  427.898865  325.432709    0.882224      0  person\n",
       "  1   30.090019  232.285919   79.376144  283.990753    0.857565      0  person\n",
       "  2  226.206573   28.159809  333.911957  104.535416    0.787395     46  banana\n",
       "  3  208.607239  378.304077  334.608948  566.253723    0.695289     46  banana\n",
       "  4  365.206207  229.622925  435.830261  339.305420    0.586972     46  banana\n",
       "  5    0.000000  328.119324   96.248627  467.295532    0.516422     46  banana\n",
       "  6  102.968811  361.075409  229.849121  492.011993    0.502690     46  banana\n",
       "  7  287.626740  449.232574  441.802429  618.628845    0.489960     46  banana\n",
       "  8   76.107544  290.803894  196.328094  385.819031    0.330000     46  banana\n",
       "  9  100.317322  472.153839  204.095795  533.590393    0.257725     46  banana,\n",
       "  'caption': ['A man in a pink cap holding a bunch of bananas in each hand.',\n",
       "   'man with bananas'],\n",
       "  'bbox_target': [221.48, 61.84, 211.42, 267.51]},\n",
       " 460: {'image_emb': tensor([[ 0.3074,  0.2419, -0.1829,  ...,  0.7812,  0.3474, -0.1340],\n",
       "          [ 0.3401,  0.1906,  0.0015,  ...,  0.7124,  0.0924, -0.5952],\n",
       "          [ 0.3179,  0.3142, -0.0847,  ...,  0.7896,  0.5469, -0.0684]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1389, -0.0042, -0.2103,  ...,  0.0266, -0.1543, -0.3267],\n",
       "          [ 0.0636,  0.2485, -0.6924,  ...,  0.2700, -0.1759, -0.2756]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5693, 0.0011, 0.4297],\n",
       "          [0.5151, 0.0007, 0.4841]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  133.275772   33.760941  366.109070  369.721161    0.936419      0  person\n",
       "  1  193.483978  180.241440  251.051331  373.179352    0.905554     39  bottle\n",
       "  2    0.425757  286.638336   35.159241  373.452759    0.587722      0  person\n",
       "  3    0.947714  134.173553  441.466278  371.183289    0.537053     57   couch\n",
       "  4  363.234558  261.083527  444.486298  312.542511    0.527361     73    book,\n",
       "  'caption': ['A man with wine bottle',\n",
       "   'A man holding some wine and a phone on a couch.'],\n",
       "  'bbox_target': [131.17, 31.21, 234.58, 340.52]},\n",
       " 461: {'image_emb': tensor([[ 0.0967,  0.3130, -0.1567,  ...,  1.3252,  0.1333,  0.3032],\n",
       "          [ 0.1543,  0.1621, -0.0428,  ...,  1.0771,  0.1166, -0.0760],\n",
       "          [ 0.0821,  0.3164, -0.3555,  ...,  1.0693, -0.0528,  0.1082],\n",
       "          ...,\n",
       "          [ 0.0591,  0.2937, -0.1759,  ...,  1.1562, -0.1978, -0.0564],\n",
       "          [-0.0450,  0.1536, -0.0555,  ...,  0.9883,  0.1063, -0.3127],\n",
       "          [-0.1631,  0.1707, -0.1211,  ...,  1.0557,  0.3013,  0.2238]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0552, -0.1879, -0.2510,  ...,  0.0688, -0.0446,  0.1356],\n",
       "          [-0.0859, -0.2383, -0.3066,  ...,  0.1899,  0.1689,  0.2495]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.5436e-02, 2.8992e-03, 6.2332e-03, 3.4618e-04, 3.9220e-04, 9.2480e-01,\n",
       "           1.2993e-02, 2.3529e-02, 3.1834e-03],\n",
       "          [4.0576e-01, 7.3853e-02, 2.1629e-03, 1.8860e-01, 2.4219e-01, 1.8967e-02,\n",
       "           1.1154e-02, 3.5461e-02, 2.1835e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   163.185364  132.059036  248.774109  305.139954    0.923864      0   \n",
       "  1    41.199837  195.876160  266.364929  421.981689    0.921637      0   \n",
       "  2   241.793823  178.142456  303.374512  366.897705    0.880174     56   \n",
       "  3    70.092087  123.539230  160.939285  205.907471    0.857822     62   \n",
       "  4   510.869446  187.887817  586.466370  252.486694    0.837014     63   \n",
       "  5   365.757568  205.790283  497.072937  419.254700    0.767534     56   \n",
       "  6   262.323730  292.028259  380.671021  423.542542    0.721295     56   \n",
       "  7   432.865967  158.188599  530.784485  354.125000    0.700635      0   \n",
       "  8   372.823700  173.410828  424.626740  196.401550    0.557123     73   \n",
       "  9     0.109121  348.158478   63.892159  423.086060    0.557019     66   \n",
       "  10   41.282482  317.162903   84.356079  341.311829    0.526946     64   \n",
       "  11  311.369049   92.638535  341.980377  124.051254    0.423031     74   \n",
       "  12    0.547543  343.761108   90.929947  424.564209    0.391295     63   \n",
       "  13   49.895195  316.454834   83.207054  325.988281    0.331970     64   \n",
       "  14  260.471069  132.787430  297.697876  159.421265    0.307918     58   \n",
       "  15    0.248983   49.644226   42.539551  164.071625    0.295274     62   \n",
       "  16    0.115753   49.313843   41.952229  164.653900    0.275591     56   \n",
       "  17  361.604126   89.931290  418.478027  117.577927    0.255424     73   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2          chair  \n",
       "  3             tv  \n",
       "  4         laptop  \n",
       "  5          chair  \n",
       "  6          chair  \n",
       "  7         person  \n",
       "  8           book  \n",
       "  9       keyboard  \n",
       "  10         mouse  \n",
       "  11         clock  \n",
       "  12        laptop  \n",
       "  13         mouse  \n",
       "  14  potted plant  \n",
       "  15            tv  \n",
       "  16         chair  \n",
       "  17          book  ,\n",
       "  'caption': ['A child in a blue shirt sittinf in a chair.',\n",
       "   'little boy wearing blue shirt sits in a computer chair  on a laptop computer'],\n",
       "  'bbox_target': [431.53, 158.9, 100.84, 198.52]},\n",
       " 462: {'image_emb': tensor([[ 0.2352,  0.1727,  0.1403,  ...,  0.5928, -0.2152, -0.2646],\n",
       "          [-0.0558,  0.3225, -0.0543,  ...,  0.7422, -0.2417, -0.2917],\n",
       "          [-0.1428,  0.3096, -0.1925,  ...,  0.9204, -0.2108, -0.0865],\n",
       "          [-0.1130,  0.1232, -0.5239,  ...,  0.4705, -0.0074, -0.3857],\n",
       "          [ 0.1066, -0.2925, -0.0290,  ..., -0.2371,  0.0726, -0.3628]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0936, -0.1234, -0.3713,  ..., -0.1199,  0.0942, -0.2476],\n",
       "          [ 0.0432,  0.2072, -0.3240,  ..., -0.3228, -0.1516, -0.0508]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2783, 0.0221, 0.0026, 0.6470, 0.0499],\n",
       "          [0.0012, 0.0084, 0.0044, 0.9688, 0.0175]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  197.854919   22.783348  288.231445  113.503990    0.941067     32   \n",
       "  1   65.476501   93.320160  340.759277  640.000000    0.932678      0   \n",
       "  2  438.765869  409.858246  525.851196  639.237671    0.922335      0   \n",
       "  3  251.625397  234.630600  469.062500  637.449158    0.900772      0   \n",
       "  4  212.893860  220.623672  340.623901  637.313904    0.350368      0   \n",
       "  \n",
       "            name  \n",
       "  0  sports ball  \n",
       "  1       person  \n",
       "  2       person  \n",
       "  3       person  \n",
       "  4       person  ,\n",
       "  'caption': ['A man heading a soccer ball.',\n",
       "   'A man wearing green t shirt  throwing the ball with his head'],\n",
       "  'bbox_target': [69.03, 94.92, 277.58, 545.08]},\n",
       " 463: {'image_emb': tensor([[-0.3499,  0.3374, -0.1348,  ...,  0.6323,  0.2145, -0.0018],\n",
       "          [-0.3262,  0.1570, -0.1276,  ...,  0.6431,  0.0220,  0.0831]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3582, -0.0492, -0.2805,  ..., -0.0381,  0.1688, -0.4097],\n",
       "          [-0.3613,  0.2703,  0.1084,  ...,  0.2664,  0.1385, -0.0122]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.8438, 0.1561],\n",
       "          [0.5737, 0.4263]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   61.151810  155.680725  508.231750  396.594666    0.861097     51  carrot\n",
       "  1   53.014412  170.790527   91.518852  369.997925    0.668494     51  carrot\n",
       "  2   75.440994  155.730225  143.437851  395.903015    0.506850     51  carrot\n",
       "  3    0.130989    2.752563  170.957397  269.796356    0.444093      0  person\n",
       "  4  459.801392  179.189575  510.553162  396.586548    0.291255     51  carrot,\n",
       "  'caption': ['Front most carrot on the left side',\n",
       "   'A half of a carrot that is the front part of some sort of carrot/leaf shelter.'],\n",
       "  'bbox_target': [72.34, 158.74, 73.34, 241.13]},\n",
       " 464: {'image_emb': tensor([[ 0.0527,  0.4128, -0.4478,  ...,  1.2725, -0.1370, -0.2971],\n",
       "          [ 0.0537,  0.1153, -0.1063,  ...,  0.8291,  0.2196,  0.0442],\n",
       "          [-0.0438,  0.1013, -0.3694,  ...,  0.6733,  0.1442,  0.2163],\n",
       "          [ 0.1038,  0.1096, -0.3528,  ...,  0.6011, -0.0988,  0.1046],\n",
       "          [-0.1726,  0.2686, -0.1231,  ...,  0.8564,  0.1931,  0.1749]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2712, -0.2656, -0.3210,  ...,  0.5137, -0.3406, -0.1476],\n",
       "          [-0.1022, -0.2239, -0.4116,  ...,  0.1675, -0.2747, -0.1986]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1255, 0.0122, 0.0165, 0.0013, 0.8442],\n",
       "          [0.4336, 0.0487, 0.0237, 0.0025, 0.4915]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0     0.000000   83.239563  133.371048  637.448975    0.930353      0  person\n",
       "  1    48.443665  199.510056  140.587219  378.951233    0.838170      0  person\n",
       "  2   429.677338  206.784836  469.221802  319.393250    0.817174     43   knife\n",
       "  3   333.256744  309.878632  357.486725  384.734589    0.748609     39  bottle\n",
       "  4   180.405670  381.378510  206.916779  410.973419    0.690865     47   apple\n",
       "  5   170.338440  318.700073  478.487366  638.477905    0.645281     69    oven\n",
       "  6   265.888947  327.990448  315.708344  373.239716    0.548283     46  banana\n",
       "  7   309.538879  357.008057  334.260132  382.171692    0.511976     49  orange\n",
       "  8   352.987885  311.538330  369.710846  382.468140    0.492184     39  bottle\n",
       "  9   466.260986  226.406021  480.000000  267.647614    0.388322     43   knife\n",
       "  10  173.741486  349.966827  311.324860  406.289764    0.287524     73    book\n",
       "  11  307.398529  332.382233  330.325836  358.343536    0.285357     49  orange\n",
       "  12  283.448822  332.632965  311.851105  361.050018    0.271889     46  banana,\n",
       "  'caption': ['Two dish towels, one blue and one striped, hanging on the front of a stove.',\n",
       "   'Two blue dish towels.'],\n",
       "  'bbox_target': [162.44, 430.55, 285.1, 209.45]},\n",
       " 465: {'image_emb': tensor([[ 0.4729,  0.1123,  0.3994,  ...,  0.7495,  0.1470, -0.1023],\n",
       "          [ 0.1153,  0.7075,  0.1949,  ...,  1.2656, -0.0336,  0.0096],\n",
       "          [ 0.3040,  0.3433, -0.1871,  ...,  1.2441,  0.4070,  0.0618],\n",
       "          [ 0.1356,  0.5601, -0.2886,  ...,  1.0820, -0.0169,  0.0284],\n",
       "          [ 0.3989,  0.2947,  0.4441,  ...,  0.5142,  0.2003, -0.1959]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3281, -0.0065,  0.0476,  ...,  0.2432,  0.2532, -0.0674],\n",
       "          [ 0.0869,  0.0378,  0.0883,  ..., -0.2930,  0.1147, -0.1610]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.1943e-01, 6.4969e-06, 4.7326e-05, 1.6093e-05, 8.0322e-02],\n",
       "          [9.0918e-01, 2.6512e-04, 3.1471e-04, 3.4750e-05, 9.0027e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   63.228760  127.176147  339.562622  397.197449    0.935898     19   \n",
       "  1  470.186737  126.259827  639.582764  310.279602    0.928880      2   \n",
       "  2  398.674561   46.515274  499.951294  393.144287    0.901338      0   \n",
       "  3  387.750031  125.614990  502.922516  231.763550    0.718892      2   \n",
       "  4  346.478210  147.910339  387.400085  199.229370    0.654475      3   \n",
       "  5  600.006287  359.131958  616.688293  383.658447    0.540792     14   \n",
       "  6  168.989471  279.427673  236.824707  345.373779    0.262507     19   \n",
       "  \n",
       "           name  \n",
       "  0         cow  \n",
       "  1         car  \n",
       "  2      person  \n",
       "  3         car  \n",
       "  4  motorcycle  \n",
       "  5        bird  \n",
       "  6         cow  ,\n",
       "  'caption': ['A black cow standing on a sidewalk',\n",
       "   \"The cow which shows the other cow's legs behind it.\"],\n",
       "  'bbox_target': [64.63, 131.04, 273.1, 269.45]},\n",
       " 466: {'image_emb': tensor([[ 0.0412,  0.2671, -0.2783,  ...,  0.6777,  0.0184, -0.0281],\n",
       "          [ 0.1849,  0.0818, -0.1191,  ...,  0.7974, -0.0247, -0.0705],\n",
       "          [ 0.4189, -0.0522, -0.3411,  ...,  0.8188,  0.0388, -0.2136],\n",
       "          [ 0.4375, -0.1368, -0.3220,  ...,  0.7729,  0.1040, -0.1726],\n",
       "          [-0.0555,  0.2240, -0.1849,  ...,  0.5596,  0.0892, -0.1667],\n",
       "          [ 0.2316, -0.0370,  0.0473,  ...,  0.8223, -0.2522, -0.0423]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0995,  0.0257, -0.0213,  ..., -0.1781, -0.0625, -0.1505],\n",
       "          [ 0.0574, -0.0819, -0.1057,  ..., -0.3672,  0.1177, -0.2625]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.1921e-07, 9.9707e-01, 3.9339e-06, 2.9683e-05, 2.8896e-03, 7.0095e-05],\n",
       "          [9.9897e-05, 4.5410e-01, 6.3782e-03, 9.1324e-03, 4.7607e-01, 5.4230e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   241.195419  163.292114  270.523010  227.806137    0.907611      0   \n",
       "  1   347.586212   90.046082  499.366150  184.792664    0.903912      4   \n",
       "  2   199.390182  161.715057  218.873566  225.932526    0.881340      0   \n",
       "  3   217.329269  161.551620  233.007095  202.880432    0.878247      0   \n",
       "  4   200.676392  127.936218  316.608154  169.066376    0.732015      4   \n",
       "  5    73.693291  141.149231  153.942703  173.245758    0.523458     28   \n",
       "  6    89.125473  166.725342  110.512558  184.329895    0.402327     28   \n",
       "  7   111.068031  177.194046  122.858498  185.823700    0.369503     28   \n",
       "  8   154.892349  154.017059  175.680237  170.535233    0.327022     28   \n",
       "  9   149.637161  163.002426  167.355042  172.858261    0.310547     28   \n",
       "  10  408.927338  168.538635  415.054047  182.980469    0.305342      0   \n",
       "  11   56.571419  161.272934  196.334671  245.205811    0.292442      7   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1   airplane  \n",
       "  2     person  \n",
       "  3     person  \n",
       "  4   airplane  \n",
       "  5   suitcase  \n",
       "  6   suitcase  \n",
       "  7   suitcase  \n",
       "  8   suitcase  \n",
       "  9   suitcase  \n",
       "  10    person  \n",
       "  11     truck  ,\n",
       "  'caption': ['The back half of the white plane with red letters.',\n",
       "   'the plane on the right'],\n",
       "  'bbox_target': [349.94, 89.39, 150.06, 94.96]},\n",
       " 467: {'image_emb': tensor([[-0.2930,  0.1815, -0.5215,  ...,  1.0391, -0.1251,  0.0258],\n",
       "          [-0.2406,  0.1472, -0.2048,  ...,  1.3125,  0.0546, -0.1381],\n",
       "          [-0.2335,  0.1858, -0.1843,  ...,  1.0918,  0.0305, -0.0446],\n",
       "          [-0.0069,  0.3574, -0.1251,  ...,  0.7075,  0.0996,  0.1541],\n",
       "          [-0.0826,  0.5029, -0.1483,  ...,  0.8706, -0.1373,  0.3704],\n",
       "          [-0.0368,  0.4260, -0.0838,  ...,  0.9038, -0.1309,  0.3833]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0453,  0.1926, -0.4219,  ..., -0.0681, -0.1324, -0.1959],\n",
       "          [-0.2037,  0.0609, -0.3835,  ...,  0.3936, -0.3318, -0.2986]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0000e+00, 2.6226e-06, 3.3975e-06, 7.7486e-07, 6.8604e-01, 3.1396e-01],\n",
       "          [2.2829e-05, 2.5868e-05, 5.5611e-05, 5.6624e-06, 5.3516e-01, 4.6484e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   105.537994  268.357025  328.165436  343.004303    0.898123     67   \n",
       "  1   376.836548    7.185528  435.629211  106.043350    0.861019     56   \n",
       "  2   408.591248   84.251984  439.784119  129.861923    0.853725     41   \n",
       "  3   432.927917   94.422150  463.483459  133.925415    0.836980     41   \n",
       "  4     3.391159    0.812347  466.083923  635.595703    0.822191      0   \n",
       "  5   371.929291  153.290497  479.107117  598.069580    0.568003     60   \n",
       "  6   325.383667  323.031189  418.167786  480.643311    0.514203     56   \n",
       "  7   434.208069    0.560772  479.651794  108.950729    0.370134      0   \n",
       "  8   102.810440  265.730042  329.699036  345.166443    0.337225      0   \n",
       "  9     0.000000    1.387604   91.120529  602.539551    0.260734      0   \n",
       "  10  412.730164  116.954552  479.721863  164.958801    0.258409     60   \n",
       "  \n",
       "              name  \n",
       "  0     cell phone  \n",
       "  1          chair  \n",
       "  2            cup  \n",
       "  3            cup  \n",
       "  4         person  \n",
       "  5   dining table  \n",
       "  6          chair  \n",
       "  7         person  \n",
       "  8         person  \n",
       "  9         person  \n",
       "  10  dining table  ,\n",
       "  'caption': ['A small child wearing a red shirt on the phone.',\n",
       "   'little boy talking on cell phone'],\n",
       "  'bbox_target': [3.11, 0.0, 461.42, 640.0]},\n",
       " 468: {'image_emb': tensor([[-0.1127,  0.1833, -0.3850,  ...,  1.3096,  0.3384,  0.0432],\n",
       "          [-0.3293,  0.1785, -0.4148,  ...,  0.8774, -0.0802, -0.3601],\n",
       "          [-0.2935,  0.2727, -0.1599,  ...,  0.9287, -0.0912,  0.1404],\n",
       "          ...,\n",
       "          [ 0.1411, -0.1057, -0.3123,  ...,  0.7031,  0.0062,  0.1199],\n",
       "          [ 0.0316,  0.2817, -0.2216,  ...,  1.0293, -0.0527, -0.0263],\n",
       "          [-0.1698, -0.0246, -0.0790,  ...,  1.0654,  0.0740,  0.3965]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0304,  0.2861, -0.6636,  ...,  0.3499, -0.0313, -0.1328],\n",
       "          [-0.1802,  0.1307, -0.3989,  ..., -0.0193,  0.0552,  0.0652]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.8242e-01, 1.5747e-02, 1.3208e-04, 3.7573e-01, 1.1921e-07, 2.9802e-07,\n",
       "           1.1921e-07, 1.2585e-01],\n",
       "          [5.7324e-01, 1.2585e-01, 4.8065e-03, 2.1082e-01, 1.5850e-03, 6.6719e-03,\n",
       "           6.0177e-04, 7.6355e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    0.173098    8.880758  185.093735  499.089294    0.931159      0  person\n",
       "  1  208.743454  158.388580  284.549255  429.539124    0.930916      0  person\n",
       "  2  290.818604  161.102066  368.977722  440.490387    0.924900      0  person\n",
       "  3  118.139717   96.659042  235.492157  498.980011    0.885749      0  person\n",
       "  4  167.868332  354.870880  190.774582  367.430389    0.806544     65  remote\n",
       "  5  187.294937  304.308990  209.064606  321.510468    0.797331     65  remote\n",
       "  6  260.946777  268.033478  304.447205  381.156433    0.750277     56   chair,\n",
       "  'caption': ['A man in a plaid shirt standing next to a man in a grey shirt holding a wii controller',\n",
       "   'The man in the checkered shirt'],\n",
       "  'bbox_target': [2.25, 7.87, 179.77, 492.13]},\n",
       " 469: {'image_emb': tensor([[-0.0160,  0.1038, -0.4644,  ...,  1.2275,  0.0306,  0.0218],\n",
       "          [ 0.2107,  0.4302, -0.1278,  ...,  0.6245, -0.0276, -0.0403]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0039,  0.2460,  0.0875,  ..., -0.4277, -0.0701, -0.3191],\n",
       "          [ 0.2371,  0.0981,  0.1005,  ..., -0.1866, -0.1868, -0.2372]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6440, 0.3557],\n",
       "          [0.2910, 0.7090]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    1.842300  309.541748  436.125488  476.157532    0.903914      8    boat\n",
       "  1    1.028355    0.416290  118.643921   69.822418    0.650447      8    boat\n",
       "  2    1.200325   62.722183  239.060608  139.468430    0.519926      8    boat\n",
       "  3  402.737122   41.585503  432.565979   78.023346    0.471597      0  person\n",
       "  4  595.486389   26.433411  639.766052   97.796112    0.456119      8    boat\n",
       "  5  555.054443  109.062271  639.580688  154.087662    0.409138      8    boat\n",
       "  6  485.101196  132.993011  614.864258  171.489746    0.341344      8    boat,\n",
       "  'caption': ['The boat with the cover over it.', 'A covered boat on land'],\n",
       "  'bbox_target': [0.0, 58.25, 235.15, 75.5]},\n",
       " 470: {'image_emb': tensor([[ 0.0971,  0.1172, -0.0371,  ...,  0.2561,  0.0522,  0.0176],\n",
       "          [ 0.2084,  0.8276, -0.2008,  ...,  0.5654,  0.0571,  0.0088],\n",
       "          [ 0.2654, -0.0132, -0.4260,  ...,  0.7754,  0.0739,  0.0142],\n",
       "          [ 0.2479, -0.2881, -0.4207,  ...,  0.4636,  0.0460, -0.0307],\n",
       "          [ 0.0497,  0.1311, -0.4575,  ...,  0.5146,  0.2399, -0.4385],\n",
       "          [ 0.0737,  0.1256, -0.0487,  ..., -0.2678,  0.0994, -0.0962]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0373, -0.2573, -0.1963,  ..., -0.1147,  0.0174, -0.2104],\n",
       "          [ 0.3235, -0.1218, -0.1249,  ...,  0.4036, -0.0577, -0.0791]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7767e-03, 9.4922e-01, 2.0385e-05, 1.0908e-05, 4.7272e-02, 1.4725e-03],\n",
       "          [1.8237e-01, 1.7126e-01, 2.0266e-06, 1.3113e-06, 4.8047e-01, 1.6602e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   91.606354  170.230225  562.582642  368.966553    0.939058      5     bus\n",
       "  1  439.260284  281.823273  639.510193  474.470947    0.936720      2     car\n",
       "  2    0.000000  341.554016   27.359810  403.262024    0.841632      2     car\n",
       "  3  597.410828  181.092926  620.210754  204.486908    0.829423     74   clock\n",
       "  4  561.804077  266.175751  617.673950  294.102570    0.788100      2     car\n",
       "  5  394.736633  253.817169  411.568542  278.022919    0.387329      0  person\n",
       "  6  552.515747  306.496643  588.535278  340.081543    0.347195      0  person,\n",
       "  'caption': ['The red taxi on the right.',\n",
       "   'A red taxi with a yellow taxi sign on top sits on the street across from a blue and yellow bus.'],\n",
       "  'bbox_target': [439.1, 275.93, 200.9, 203.99]},\n",
       " 471: {'image_emb': tensor([[-2.8198e-01, -1.7395e-01,  1.4966e-01,  ...,  1.1387e+00,\n",
       "            7.6233e-02,  1.1063e-02],\n",
       "          [-2.3218e-01,  1.3037e-01, -7.3776e-03,  ...,  9.4482e-01,\n",
       "            2.2278e-01,  2.8244e-02],\n",
       "          [-3.1860e-01,  5.9387e-02, -2.2949e-01,  ...,  8.9795e-01,\n",
       "           -7.6294e-05,  8.3008e-02],\n",
       "          [-4.3799e-01, -8.0200e-02, -2.9434e-02,  ...,  6.6748e-01,\n",
       "           -3.1464e-02, -1.5466e-01],\n",
       "          [-3.9014e-01, -1.0565e-01,  9.3506e-02,  ...,  5.8545e-01,\n",
       "            2.2937e-01,  2.6001e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3069, -0.3042, -0.1958,  ...,  0.2996, -0.3457, -0.1744],\n",
       "          [-0.1699, -0.2085, -0.0886,  ..., -0.3137, -0.2173, -0.1052]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.9150e-01, 8.6517e-03, 1.7881e-07, 0.0000e+00, 2.0007e-01],\n",
       "          [2.3267e-01, 1.5839e-02, 4.3535e-04, 7.5638e-05, 7.5098e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  169.027740  150.090271  411.062592  363.176086    0.936501      0   \n",
       "  1  132.347488    0.569443  425.358276  300.822815    0.936322      0   \n",
       "  2    0.800537  194.818542  138.267929  475.835449    0.815339     60   \n",
       "  3    0.173901  207.985168   81.010208  290.780823    0.725048     55   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2  dining table  \n",
       "  3          cake  ,\n",
       "  'caption': ['Small baby with grey sweater sitting on top of a rug.',\n",
       "   'baby on the floor'],\n",
       "  'bbox_target': [174.74, 146.7, 238.38, 227.59]},\n",
       " 472: {'image_emb': tensor([[-0.2793,  0.0135, -0.0197,  ...,  0.7896, -0.2285,  0.2307],\n",
       "          [-0.3586, -0.0067, -0.2357,  ...,  0.4109, -0.2311,  0.0158],\n",
       "          [-0.0883, -0.0267, -0.1249,  ...,  0.4893, -0.2568,  0.2769]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1320,  0.1342, -0.0845,  ...,  0.0290, -0.2883,  0.4858],\n",
       "          [-0.0217,  0.3511,  0.0944,  ...,  0.3755, -0.1105,  0.4351]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5356, 0.1293, 0.3352],\n",
       "          [0.5317, 0.1454, 0.3228]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  155.937973  147.968201  572.913757  421.203186    0.947163     20  elephant\n",
       "  1  102.523254    2.300270  509.835144  214.702087    0.907197     20  elephant,\n",
       "  'caption': ['A very large elephant standing behind some trees',\n",
       "   'Largest elephant partially behind bushes'],\n",
       "  'bbox_target': [103.77, 0.0, 406.23, 196.49]},\n",
       " 473: {'image_emb': tensor([[ 0.0354,  0.4600, -0.3101,  ...,  0.4214,  0.2284,  0.0018],\n",
       "          [-0.3123,  0.7607, -0.3159,  ...,  0.5698,  0.4189, -0.4900],\n",
       "          [ 0.2732,  0.1816, -0.3989,  ...,  0.5225,  0.0992,  0.0768],\n",
       "          [-0.0204,  0.7637, -0.2397,  ...,  0.5771, -0.1174, -0.5020]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2549,  0.2998, -0.4219,  ..., -0.0454,  0.1232, -0.8047],\n",
       "          [-0.0602,  0.5103, -0.4404,  ...,  0.0196,  0.0211, -0.3362]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0950e-01, 4.2627e-01, 9.9670e-02, 3.6450e-01],\n",
       "          [1.0738e-03, 2.5073e-01, 5.3465e-05, 7.4805e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  name\n",
       "  0  157.697495   90.033249  266.563141  360.604431    0.926889     75  vase\n",
       "  1  348.979919   30.942490  490.449341  364.084961    0.915509     75  vase\n",
       "  2  249.626053  221.942657  347.535706  404.544159    0.843180     75  vase,\n",
       "  'caption': ['The vase with a lid on the right of the vase with balls in it',\n",
       "   'A tall decanter on a table to the right of a glass container and a vase'],\n",
       "  'bbox_target': [350.92, 35.96, 140.95, 313.52]},\n",
       " 474: {'image_emb': tensor([[ 0.3660,  0.4036,  0.1882,  ...,  0.5312, -0.0136, -0.3408],\n",
       "          [ 0.3667,  0.4946,  0.1705,  ...,  0.2881, -0.0954, -0.4119],\n",
       "          [ 0.0587,  0.2231, -0.0307,  ...,  1.0938,  0.1477,  0.0961],\n",
       "          [ 0.2825,  0.4587,  0.0925,  ...,  0.6655, -0.0083, -0.3887]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 1.8457e-01,  2.2131e-01, -3.7817e-01,  6.7932e-02, -5.0244e-01,\n",
       "           -8.6230e-01, -6.1768e-01, -7.2900e-01, -3.6353e-01,  5.9509e-03,\n",
       "            4.2450e-02, -7.5195e-02,  4.8248e-02,  1.5161e-01, -3.0029e-02,\n",
       "            3.0884e-02, -4.1943e-01, -2.8369e-01, -4.2065e-01,  3.1006e-01,\n",
       "           -4.6967e-02,  4.2114e-01, -4.8438e-01,  7.3145e-01,  5.6543e-01,\n",
       "           -3.5815e-01, -2.7295e-01, -8.1604e-02,  1.2500e-01,  7.7026e-02,\n",
       "           -6.5479e-01, -6.0059e-01, -2.9297e-01,  8.8318e-02, -1.3342e-01,\n",
       "           -2.6245e-01, -1.5515e-01, -2.6978e-01, -2.5098e-01,  1.3818e-01,\n",
       "            6.3574e-01,  1.3550e-01,  2.9858e-01, -4.6417e-02, -1.2848e-02,\n",
       "           -6.4087e-02,  3.8940e-01, -5.9998e-02, -1.7786e-01,  1.8127e-01,\n",
       "            1.0919e-01, -4.8584e-01,  5.3101e-02,  5.0140e-02, -1.3928e-01,\n",
       "            7.0923e-02,  6.8176e-02, -1.4612e-01,  2.5952e-01,  2.1021e-01,\n",
       "            2.3523e-01,  1.4819e-01, -1.9150e-02, -1.3660e-01,  1.1511e-01,\n",
       "           -7.3364e-02,  1.6638e-01,  3.6621e-01, -1.4392e-01, -5.0293e-01,\n",
       "            4.6768e-03,  6.1127e-02,  2.1338e-01,  3.0005e-01,  4.9146e-01,\n",
       "           -1.2311e-01, -1.6052e-02,  1.7786e-01, -1.2903e-01, -3.7567e-02,\n",
       "           -3.8574e-01, -7.7637e-02, -2.5293e-01,  4.5020e-01,  4.7144e-01,\n",
       "            7.2937e-02, -3.3789e-01, -3.7866e-01, -3.9093e-02, -5.3741e-02,\n",
       "            2.9150e-01, -3.1226e-01, -9.6191e-01, -1.5381e-01, -1.3123e-01,\n",
       "            1.2197e-03, -1.4233e-01, -6.1462e-02,  4.4006e-02, -1.2781e-01,\n",
       "           -2.8418e-01,  1.1603e-01, -3.4821e-02,  2.5000e-01,  2.9736e-01,\n",
       "            5.7312e-02, -1.8616e-01, -9.7168e-02, -4.1187e-01, -1.3928e-01,\n",
       "            3.8300e-02,  7.7820e-02, -3.3911e-01, -6.2447e-03, -1.6260e-01,\n",
       "           -2.4750e-02,  1.3684e-01,  1.1774e-01, -1.5808e-01,  2.9175e-01,\n",
       "            7.8735e-02, -6.9482e-01, -3.7201e-02,  5.3070e-02, -2.2119e-01,\n",
       "            5.1514e-02,  9.8511e-02,  1.2317e-01,  1.5955e-01,  1.4795e-01,\n",
       "           -2.1509e-01,  4.1846e-01,  1.7188e-01,  3.0723e+00, -1.5723e-01,\n",
       "           -8.2397e-02, -7.2217e-01, -3.3228e-01,  4.5312e-01, -2.5742e-02,\n",
       "           -3.5474e-01,  3.4229e-01, -3.4717e-01,  2.4182e-01, -2.0386e-01,\n",
       "           -3.1641e-01,  3.3643e-01, -1.6040e-01, -7.3242e-02,  2.9028e-01,\n",
       "           -2.4628e-02, -3.8025e-02,  3.6060e-01, -7.0984e-02, -3.0396e-01,\n",
       "           -9.6741e-02, -1.3647e-01, -1.9214e-01, -1.1163e-01,  1.1493e-01,\n",
       "            4.5093e-01, -1.0486e-01,  9.1064e-02,  1.9580e-01,  4.4434e-02,\n",
       "           -1.3757e-01,  4.6997e-01,  1.6418e-01,  8.6914e-02,  3.7646e-01,\n",
       "           -1.1652e-01,  1.6504e-01,  2.2888e-01, -1.2671e-01, -1.0223e-01,\n",
       "            7.0898e-01, -7.5562e-02, -1.9104e-01, -5.0323e-02,  1.0822e-01,\n",
       "           -1.9983e-01,  1.1938e-01, -2.1057e-01, -1.7981e-01, -2.7075e-01,\n",
       "            1.4880e-01, -1.1011e-01,  1.4795e-01,  2.9028e-01,  1.6638e-01,\n",
       "            9.1736e-02,  4.6326e-02, -1.4624e-01, -1.8604e-01, -9.5581e-02,\n",
       "           -1.8286e-01,  5.4962e-02,  4.7461e-01,  1.7664e-01, -2.2546e-01,\n",
       "            1.1658e-01,  7.6355e-02, -8.6212e-03,  4.2017e-01, -9.6741e-02,\n",
       "           -2.0203e-01,  2.5537e-01, -1.5039e-01,  3.8843e-01, -1.5869e-01,\n",
       "           -5.8496e-01, -4.3243e-02, -3.9795e-01, -2.0386e-01, -5.6274e-02,\n",
       "            2.7686e-01, -3.6591e-02, -2.7393e-01,  1.6327e-02, -3.8037e-01,\n",
       "            2.0911e-01, -1.9812e-01, -3.1299e-01,  2.7905e-01, -2.8394e-01,\n",
       "           -1.6663e-01, -3.9600e-01, -1.1658e-01,  1.8750e-01, -1.2433e-01,\n",
       "            1.4542e-02,  3.2202e-01,  3.2422e-01, -1.9531e-01,  4.0796e-01,\n",
       "           -9.2590e-02,  2.5342e-01, -1.2433e-01,  1.1734e-02, -5.4395e-01,\n",
       "            6.4160e-01,  1.4990e-01, -1.2152e-01,  9.2590e-02, -1.1456e-01,\n",
       "           -7.0648e-03, -1.4091e-02,  4.8804e-01,  2.2144e-01,  3.0176e-01,\n",
       "           -2.9199e-01, -1.9202e-01,  1.6516e-01,  1.4868e-01,  7.0238e-04,\n",
       "           -3.3911e-01, -2.2998e-01,  2.6245e-01,  3.4106e-01,  3.5400e-01,\n",
       "            2.9678e-03, -3.9624e-01, -4.1748e-01,  1.2244e-01,  8.9844e-02,\n",
       "           -2.3438e-02,  1.5503e-01, -4.2419e-02, -4.0527e-01,  1.7114e-01,\n",
       "           -9.8145e-02,  2.9761e-01,  4.2023e-02,  1.2695e-01, -5.1178e-02,\n",
       "            6.0107e-01, -9.3567e-02, -2.0984e-01,  4.7290e-01, -1.6089e-01,\n",
       "            4.3884e-02,  4.4214e-01, -1.1639e-01,  2.6367e-01,  3.2739e-01,\n",
       "            2.1997e-01, -1.5735e-01, -1.7175e-01,  2.2009e-01, -1.1578e-01,\n",
       "           -1.2634e-01,  1.8530e-01,  3.6768e-01,  3.6774e-02,  1.6748e-01,\n",
       "            4.1040e-01, -1.4014e-01,  2.3108e-01, -4.1473e-02,  7.8308e-02,\n",
       "           -3.0884e-01, -3.4473e-01, -1.2170e-01,  4.7089e-02, -3.1567e-01,\n",
       "           -1.0547e-01,  5.4150e-01,  2.8369e-01,  4.1235e-01,  9.2407e-02,\n",
       "           -4.8047e-01,  3.2446e-01,  3.0703e+00, -4.5312e-01,  8.2779e-03,\n",
       "            1.7993e-01, -2.5537e-01, -2.7393e-01,  1.1829e-01,  3.3472e-01,\n",
       "           -2.1741e-01,  2.9517e-01, -3.4082e-01,  1.6748e-01,  2.3590e-02,\n",
       "            3.3911e-01,  1.2321e-02, -3.1616e-01,  2.9102e-01, -1.5547e+00,\n",
       "            5.3027e-01, -4.1406e-01,  5.7812e-01, -1.0565e-01,  7.4707e-02,\n",
       "           -5.5762e-01, -6.5430e-02,  2.3425e-01, -1.4746e-01,  4.2090e-01,\n",
       "            1.6184e-03,  6.0181e-02,  1.3757e-01, -3.4814e-01,  4.1504e-01,\n",
       "            7.1045e-02,  1.7151e-01,  3.6816e-01,  2.3865e-02, -2.9175e-01,\n",
       "           -1.5271e-01,  2.5806e-01,  1.6699e-01,  9.0820e-02,  3.5522e-01,\n",
       "           -3.8452e-01, -3.3081e-02,  3.1934e-01, -1.8506e-01, -5.1544e-02,\n",
       "           -1.8262e-01, -6.5979e-02,  5.6787e-01,  7.4341e-02, -4.1919e-01,\n",
       "            5.4413e-02, -1.6150e-01,  1.3416e-01,  2.0471e-01, -6.3354e-02,\n",
       "           -6.2646e-01,  3.5083e-01, -9.8206e-02,  1.9458e-01,  5.1221e-01,\n",
       "            7.9102e-02, -1.2067e-01,  2.2632e-01, -5.3375e-02,  1.2683e-01,\n",
       "            2.6489e-01,  8.5388e-02,  3.3997e-02, -9.5398e-02,  7.0007e-02,\n",
       "            5.1221e-01,  7.8979e-02, -4.9591e-03, -2.4707e-01,  1.4746e-01,\n",
       "           -5.0488e-01, -2.1362e-02, -5.1709e-01, -1.1353e-01,  2.1851e-01,\n",
       "            9.8877e-03,  2.7832e-01, -4.8523e-02,  2.4353e-01,  2.3328e-01,\n",
       "            2.5635e-01, -2.2021e-01, -1.4819e-01, -6.2402e-01, -1.8689e-01,\n",
       "           -9.2102e-02,  3.0762e-01, -3.4326e-01,  9.6802e-02,  1.7407e-01,\n",
       "            1.3464e-01,  8.8120e-03, -2.0190e-01,  1.6992e-01,  2.8857e-01,\n",
       "           -1.3342e-01, -1.6553e-01,  4.1797e-01, -3.4644e-01, -3.9575e-01,\n",
       "            3.5156e-01, -4.6899e-01,  2.7222e-01, -8.8477e-01,  1.8005e-01,\n",
       "            1.7151e-01, -2.5879e-01,  1.1523e-01,  2.4927e-01,  9.4666e-02,\n",
       "           -1.5405e-01, -9.7412e-02, -1.1322e-02,  3.2959e-01, -8.4900e-02,\n",
       "           -4.0649e-01,  3.2623e-02, -1.6727e-03, -4.7900e-01,  1.5747e-02,\n",
       "           -2.0081e-01,  2.7740e-02,  2.2620e-01,  1.2903e-01, -3.5571e-01,\n",
       "            4.1895e-01, -1.1249e-01, -1.1719e-01, -3.4180e-01, -3.1209e-04,\n",
       "            1.9849e-01, -2.3303e-01, -5.1270e-02, -1.7761e-01,  3.1769e-02,\n",
       "            1.5503e-01, -3.7866e-01, -2.9077e-01,  4.8242e-01, -2.5772e-02,\n",
       "           -3.2446e-01, -6.4026e-02,  3.1665e-01,  2.0276e-01,  4.1675e-01,\n",
       "           -1.0492e-01,  3.2837e-01, -2.3853e-01, -3.8477e-01, -3.8110e-01,\n",
       "            2.9126e-01, -2.7710e-01,  2.2205e-01,  2.1423e-01,  2.3608e-01,\n",
       "            2.1899e-01, -6.5186e-02, -5.0488e-01,  9.9243e-02,  4.7437e-01,\n",
       "            4.5128e-03, -3.9185e-01, -2.8946e-02,  1.2769e-01, -3.0225e-01,\n",
       "           -2.0679e-01,  8.1604e-02, -2.4683e-01,  5.8937e-03,  5.2155e-02,\n",
       "            2.2546e-01, -1.1078e-01,  2.1362e-01,  5.9766e-01,  3.0225e-01,\n",
       "            1.5161e-01,  1.8896e-01, -3.9893e-01, -1.0828e-01,  3.2616e-03,\n",
       "            1.3428e-01, -1.5820e-01,  3.4082e-01,  3.1372e-01,  1.2384e-01,\n",
       "           -1.2305e-01, -3.9258e-01,  1.6117e-03, -1.2097e-01,  1.1676e-01,\n",
       "            1.8164e-01, -4.2944e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.2095e-01, 6.4941e-01, 4.1127e-06, 1.2988e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  115.697525   17.045761  423.350098  394.961426    0.936581     25  umbrella\n",
       "  1    0.914185  144.892044  299.629456  633.813721    0.922509     25  umbrella\n",
       "  2  229.986664  329.186310  417.275879  638.926514    0.810853     25  umbrella\n",
       "  3    0.658318  438.973022   70.704498  639.930054    0.406954     25  umbrella,\n",
       "  'caption': ['Yellow object beneath the two rainbow umbrellas.'],\n",
       "  'bbox_target': [231.88, 520.46, 181.62, 118.67]},\n",
       " 475: {'image_emb': tensor([[-0.1322,  0.1212, -0.0532,  ...,  0.3860,  0.0074, -0.0620],\n",
       "          [-0.4031,  0.1241, -0.2336,  ...,  0.5903, -0.1064, -0.2169],\n",
       "          [-0.2817,  0.1080, -0.2754,  ...,  0.6729,  0.1644, -0.3923],\n",
       "          ...,\n",
       "          [ 0.3833,  0.0619, -0.3481,  ...,  0.9839,  0.0718, -0.2094],\n",
       "          [-0.0396, -0.0562, -0.1470,  ...,  1.3428, -0.0981, -0.2024],\n",
       "          [-0.1697,  0.2583, -0.0352,  ...,  0.2084, -0.2345,  0.0181]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 2.7905e-01,  1.9714e-01, -7.6965e-02,  1.8463e-02, -4.6777e-01,\n",
       "           -1.2354e-01, -2.7039e-02, -8.4717e-01, -5.9326e-01,  1.9617e-01,\n",
       "            1.9745e-02,  2.3727e-02,  2.4121e-01,  5.2948e-02,  8.8318e-02,\n",
       "            5.4810e-02,  1.1469e-01, -1.6077e-01, -2.4414e-01,  4.6143e-02,\n",
       "            4.1138e-01,  4.1821e-01, -5.7007e-02, -2.4933e-02, -8.5083e-02,\n",
       "           -1.9055e-01, -3.2837e-01,  3.9917e-01, -2.0935e-01,  2.6465e-01,\n",
       "            2.8906e-01, -4.0619e-02, -6.6284e-02,  2.7661e-01, -7.1338e-01,\n",
       "            5.1788e-02,  6.6895e-02,  6.9946e-02,  3.7231e-01,  9.4788e-02,\n",
       "            3.8086e-01, -2.6001e-02, -8.9788e-04,  8.3008e-02,  5.8838e-01,\n",
       "            1.5625e-01,  1.6357e-02,  1.7297e-01, -3.7451e-01, -9.3567e-02,\n",
       "            1.6953e-02, -3.9282e-01,  1.5674e-01, -9.6924e-02, -1.5833e-01,\n",
       "            2.9272e-01, -6.9580e-02,  1.1163e-01, -4.0869e-01, -7.7820e-02,\n",
       "            1.7224e-01,  2.1106e-01, -3.7506e-02,  9.3140e-02,  8.5938e-02,\n",
       "            3.4937e-01, -1.3574e-01,  1.8726e-01, -4.8511e-01, -8.6975e-02,\n",
       "           -9.2407e-02, -2.6001e-01,  3.8770e-01,  2.3950e-01,  3.8818e-01,\n",
       "           -4.0063e-01,  2.8149e-01,  2.7734e-01,  1.3023e-02,  6.2622e-02,\n",
       "           -1.7212e-01, -1.1377e-01, -2.6025e-01, -2.3438e-02, -2.0178e-01,\n",
       "           -3.2446e-01,  3.3478e-02,  2.4841e-01, -3.4961e-01, -2.0020e-01,\n",
       "           -1.1884e-01,  1.8701e-01, -1.3623e+00,  2.5244e-01, -1.9629e-01,\n",
       "            4.0625e-01, -3.1174e-02,  6.4355e-01,  3.3350e-01, -1.0168e-01,\n",
       "           -2.8589e-01, -6.2561e-02,  5.6641e-01, -5.7129e-01, -3.4839e-01,\n",
       "           -2.2293e-02, -1.0956e-01,  1.2671e-01, -2.9004e-01,  1.5723e-01,\n",
       "            1.8848e-01, -1.9791e-02,  1.6541e-01,  1.9421e-01,  1.8372e-01,\n",
       "            1.9104e-01,  3.5553e-02, -4.9072e-02, -7.5500e-02,  4.8096e-02,\n",
       "            7.3730e-02, -2.0154e-01,  1.3940e-01, -3.2007e-01,  2.2180e-01,\n",
       "           -9.2041e-02, -9.4299e-02, -3.5492e-02,  1.2671e-01,  1.3416e-01,\n",
       "           -3.8818e-01, -1.4961e-02,  1.7163e-01,  5.5859e+00, -4.2603e-01,\n",
       "           -5.1331e-02, -1.6956e-01, -4.8462e-02,  2.4219e-01, -3.2520e-01,\n",
       "           -6.2195e-02,  3.6328e-01, -4.8926e-01,  8.5144e-02, -2.3071e-01,\n",
       "            4.1772e-01, -1.5491e-01, -1.2195e-01, -1.4404e-01, -2.4316e-01,\n",
       "           -3.0127e-01, -2.4414e-01,  4.6768e-03,  3.6621e-01, -2.9785e-01,\n",
       "           -2.3169e-01,  2.5757e-01, -2.3633e-01,  9.6497e-02,  1.2115e-01,\n",
       "            1.4313e-02,  2.0923e-01, -1.8237e-01,  2.9614e-01, -2.5391e-01,\n",
       "            2.2797e-02,  1.4087e-01,  4.2334e-01, -8.7524e-02,  2.5439e-01,\n",
       "           -1.8811e-01, -2.2668e-01,  3.7842e-01,  4.6851e-01, -9.7168e-02,\n",
       "           -2.3108e-01, -4.2017e-01,  2.2424e-01, -2.9831e-02, -5.8044e-02,\n",
       "           -8.0688e-02, -2.0435e-01, -4.3335e-01,  2.1375e-01, -1.7792e-02,\n",
       "           -7.2388e-02,  1.2311e-01, -2.5537e-01, -3.0957e-01,  2.2107e-01,\n",
       "            5.4474e-02,  2.7051e-01, -5.2930e-01, -9.7961e-03,  8.7463e-02,\n",
       "            1.7700e-01, -5.7227e-01,  5.8789e-01, -2.4451e-01, -1.2671e-01,\n",
       "           -8.6975e-02,  4.4580e-01,  3.2983e-01, -1.2585e-01,  3.4454e-02,\n",
       "           -1.5259e-01, -2.0654e-01,  2.8259e-02, -5.4150e-01,  1.3196e-01,\n",
       "            2.2375e-01,  9.8206e-02,  1.8628e-01,  3.8306e-01,  1.2561e-01,\n",
       "           -2.8885e-02, -2.5955e-02, -9.8816e-02, -7.2205e-02, -2.4231e-01,\n",
       "            1.6565e-01, -4.3384e-01, -8.6609e-02,  9.6512e-03,  1.4685e-01,\n",
       "            7.8918e-02,  1.4954e-01,  5.2734e-02,  1.4557e-02, -3.5791e-01,\n",
       "           -5.8929e-02,  1.4490e-01,  1.6919e-01,  1.2170e-01, -1.7896e-01,\n",
       "           -2.6428e-02, -1.6870e-01, -2.1851e-02,  1.2549e-01, -1.2299e-02,\n",
       "           -3.5352e-01, -3.8574e-01, -3.3740e-01, -3.8666e-02, -5.5566e-01,\n",
       "           -1.1101e-02,  6.2866e-02, -4.5020e-01,  1.2244e-01,  1.7883e-01,\n",
       "           -2.9834e-01, -1.8845e-02,  2.1716e-01,  3.3032e-01, -2.2473e-01,\n",
       "            3.6011e-02,  2.3132e-02,  2.4329e-01,  2.2314e-01,  9.6497e-02,\n",
       "            4.4942e-04,  1.0748e-01, -1.4636e-01,  1.9690e-01, -8.0505e-02,\n",
       "            1.4990e-01,  4.3884e-02, -2.9224e-01, -2.6764e-02,  2.3022e-01,\n",
       "           -3.6774e-02,  3.7811e-02,  8.6975e-02,  8.2703e-02, -2.5879e-01,\n",
       "           -8.2092e-03, -2.7637e-01, -6.3721e-02,  4.8364e-01, -5.4291e-02,\n",
       "            2.8003e-01,  1.2878e-01, -1.7090e-02,  2.3117e-02, -3.4839e-01,\n",
       "            2.1802e-01,  5.6427e-02,  1.9067e-01,  4.6362e-01, -3.4809e-03,\n",
       "            2.1896e-02, -3.9868e-01,  3.9795e-02, -1.7090e-01, -3.2788e-01,\n",
       "            2.0154e-01, -3.9404e-01, -5.4199e-02, -2.6685e-01,  1.6528e-01,\n",
       "           -1.9263e-01, -3.7842e-01, -1.4551e-01, -7.0068e-02, -2.7856e-01,\n",
       "            2.3584e-01, -3.7695e-01, -2.8345e-01, -1.0992e-01, -2.1497e-01,\n",
       "           -1.5674e-01,  1.2217e-03,  5.5859e+00, -1.6943e-01, -3.7365e-03,\n",
       "            3.0908e-01,  2.4963e-01, -7.1338e-01,  7.4524e-02,  1.6516e-01,\n",
       "            1.7798e-01,  1.7639e-01,  4.2578e-01, -1.0773e-02, -4.0674e-01,\n",
       "            6.7383e-02,  4.0955e-02, -5.5762e-01,  1.5808e-01, -1.8867e+00,\n",
       "           -3.7891e-01,  3.5889e-01,  7.2899e-03,  2.0227e-01, -7.8979e-02,\n",
       "           -8.2581e-02, -8.8562e-02, -9.1858e-02, -1.6321e-01, -1.3989e-01,\n",
       "           -3.3667e-01,  2.7393e-01, -1.2396e-01, -2.5879e-01,  4.1089e-01,\n",
       "           -9.0759e-02,  3.2300e-01,  1.2384e-01,  1.0669e-01,  1.1932e-01,\n",
       "           -2.8491e-01, -6.5479e-01,  2.4011e-01,  1.1835e-01, -2.2095e-01,\n",
       "           -2.1887e-01,  1.4575e-01,  3.2251e-01,  3.8306e-01, -3.5645e-01,\n",
       "           -1.1334e-01, -1.1646e-01, -5.2539e-01, -1.3391e-01, -2.4872e-02,\n",
       "            1.5247e-01, -7.6721e-02,  9.5398e-02, -5.6946e-02, -2.7734e-01,\n",
       "           -6.1310e-02, -1.3965e-01, -3.4375e-01,  3.4058e-01, -2.7783e-01,\n",
       "            3.1641e-01, -1.4136e-01,  1.7900e-03, -3.9746e-01,  1.3257e-01,\n",
       "            1.6370e-01,  2.6514e-01, -2.0874e-01, -5.3076e-01, -1.6443e-01,\n",
       "            9.6069e-02, -8.0933e-02,  1.6052e-01,  8.8989e-02, -7.4036e-02,\n",
       "           -8.4229e-02,  5.9229e-01, -2.0471e-01,  6.6223e-02,  1.0382e-01,\n",
       "           -1.1060e-01, -2.8872e-04,  2.5562e-01, -3.6597e-01,  4.8901e-01,\n",
       "            4.4067e-02,  1.0727e-02, -1.4648e-01, -2.6367e-01, -2.2351e-01,\n",
       "           -1.4783e-01, -3.2684e-02,  2.0117e-01, -2.2766e-01, -1.2566e-02,\n",
       "            3.6841e-01, -3.5889e-01,  1.3293e-01,  2.0905e-02,  1.1035e-01,\n",
       "           -1.2000e-01, -1.6968e-01, -2.0251e-01,  4.3457e-02, -5.9418e-02,\n",
       "            3.7793e-01, -2.7026e-01,  7.5378e-02, -3.7109e-01, -4.4708e-02,\n",
       "            2.1021e-01,  7.7637e-02,  5.8014e-02,  4.2578e-01,  1.2891e-01,\n",
       "           -1.0327e-01,  3.0426e-02, -2.3718e-01, -3.1719e-03, -3.1323e-01,\n",
       "           -1.1469e-01, -2.2192e-01,  3.1934e-01,  1.9180e-02, -2.6123e-01,\n",
       "            4.0601e-01, -2.7881e-01,  1.7065e-01, -3.4009e-01,  1.0962e-01,\n",
       "            5.2490e-01, -1.4954e-01, -1.5674e-01, -9.3323e-02, -2.4429e-02,\n",
       "           -5.6641e-02,  2.6108e-02, -1.6858e-01,  3.2202e-01, -2.1899e-01,\n",
       "           -5.7031e-01,  1.4929e-01,  1.9150e-02,  2.1521e-01,  9.3323e-02,\n",
       "            8.6548e-02, -7.0740e-02,  6.0333e-02,  4.7534e-01, -1.2711e-02,\n",
       "           -3.2324e-01,  3.8330e-01,  6.5369e-02, -3.3905e-02,  6.5918e-02,\n",
       "           -8.9966e-02, -4.8706e-02, -2.4521e-02, -1.5833e-01, -6.8787e-02,\n",
       "            1.3184e-01, -7.8491e-02,  3.9600e-01, -3.4644e-01, -2.8125e-01,\n",
       "            3.0103e-01, -4.6362e-01, -9.6924e-02, -7.0312e-02,  1.8542e-01,\n",
       "            3.2440e-02, -4.0100e-02,  1.4612e-01, -4.3652e-01, -1.6418e-01,\n",
       "           -8.0414e-03,  3.0884e-01,  2.6001e-01,  6.5723e-01,  2.8198e-01,\n",
       "            3.2104e-02,  5.8936e-01,  7.6233e-02,  3.6865e-02,  1.4429e-01,\n",
       "           -1.4136e-01,  2.2369e-02,  8.1970e-02,  2.3108e-01,  3.3569e-01,\n",
       "           -2.9956e-01, -2.0721e-02,  9.4910e-02,  9.5558e-04,  2.2034e-01,\n",
       "           -2.0544e-01,  2.0813e-02]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.7393e-01, 2.9785e-02, 2.2009e-01, 7.2718e-05, 3.9911e-04, 1.7440e-04,\n",
       "           9.5215e-03, 4.6606e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    61.764130  215.533432  354.341461  371.400208    0.921694      4   \n",
       "  1   228.196991  185.295670  304.450043  215.002579    0.881182      4   \n",
       "  2   331.521545  498.740417  424.350098  620.152405    0.879962      7   \n",
       "  3   203.003448  557.209412  217.211578  600.037415    0.855758      0   \n",
       "  4   156.332611  583.355164  215.783417  616.866028    0.799787      2   \n",
       "  5   225.304565  545.474670  266.839355  603.345520    0.742507      2   \n",
       "  6     0.000000  461.855804   38.308441  524.022583    0.725109      7   \n",
       "  7     0.000000  398.389709   41.190445  449.185364    0.652357      7   \n",
       "  8   376.731201  393.388855  424.243225  427.174744    0.645238      7   \n",
       "  9   225.211853  545.414917  266.630920  603.174927    0.551674      7   \n",
       "  10   76.021347  233.280991  100.466019  242.109360    0.458735      2   \n",
       "  \n",
       "          name  \n",
       "  0   airplane  \n",
       "  1   airplane  \n",
       "  2      truck  \n",
       "  3     person  \n",
       "  4        car  \n",
       "  5        car  \n",
       "  6      truck  \n",
       "  7      truck  \n",
       "  8      truck  \n",
       "  9      truck  \n",
       "  10       car  ,\n",
       "  'caption': ['The airplane at the gate.'],\n",
       "  'bbox_target': [64.06, 216.63, 291.97, 165.17]},\n",
       " 476: {'image_emb': tensor([[ 6.6406e-02,  8.9111e-02, -2.1240e-01,  ...,  1.1494e+00,\n",
       "            9.3699e-04,  1.1591e-01],\n",
       "          [-9.3628e-02,  3.8940e-01, -6.0211e-02,  ...,  9.5557e-01,\n",
       "            9.9426e-02, -4.7943e-02],\n",
       "          [-2.1655e-01,  5.0342e-01,  7.4036e-02,  ...,  8.9160e-01,\n",
       "            3.1586e-02, -1.2372e-01],\n",
       "          ...,\n",
       "          [ 1.9519e-01,  7.1716e-02, -4.2358e-02,  ...,  9.1797e-01,\n",
       "            8.7769e-02, -3.0566e-01],\n",
       "          [-1.1459e-02,  1.4233e-01,  7.7057e-03,  ...,  1.0068e+00,\n",
       "            1.2164e-01, -3.0396e-01],\n",
       "          [-1.8042e-01, -2.0410e-01, -2.1375e-01,  ...,  8.4863e-01,\n",
       "           -3.0225e-01,  3.2288e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0503, -0.1249, -0.2732,  ..., -0.0229, -0.1530, -0.2781],\n",
       "          [-0.0139, -0.1288, -0.2083,  ..., -0.1039, -0.0437, -0.1599]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.2987e-04, 1.2878e-01, 3.2910e-01, 3.2449e-04, 4.8633e-01, 7.3552e-05,\n",
       "           5.3711e-02, 1.3657e-03],\n",
       "          [1.5259e-03, 3.0005e-01, 2.1277e-01, 1.6856e-04, 3.7939e-01, 9.6023e-05,\n",
       "           1.1635e-02, 9.4421e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   487.013123  146.583618  594.210266  360.716309    0.935298      0  person\n",
       "  1    42.792347  114.477386  157.365921  328.623383    0.923102      0  person\n",
       "  2    15.288876  236.534790  134.687836  434.817322    0.907014     17   horse\n",
       "  3   470.586761  186.572845  488.006073  233.177521    0.878992      0  person\n",
       "  4   486.745026  270.602905  578.803345  455.389282    0.872865     17   horse\n",
       "  5   410.703705  182.319427  427.789581  216.441925    0.761094      0  person\n",
       "  6   344.361206  190.205597  363.773865  218.818451    0.745909     17   horse\n",
       "  7   587.473511  183.070496  596.683960  206.047363    0.676430      0  person\n",
       "  8   323.949677  187.256500  340.938263  220.934540    0.578176     17   horse\n",
       "  9   348.258942  179.048920  358.242767  195.440704    0.573430      0  person\n",
       "  10  247.330109  183.354553  254.015930  201.660828    0.550450      0  person\n",
       "  11  322.871124  172.173523  340.315948  205.790039    0.521860      0  person\n",
       "  12  257.535095  174.210983  264.978699  201.653870    0.516592      0  person\n",
       "  13  295.384491  182.416595  303.888153  201.746796    0.489944      0  person\n",
       "  14  275.613647  171.882645  289.231201  188.272461    0.280270      0  person\n",
       "  15  143.989960  168.584442  154.202423  200.662506    0.275999      0  person,\n",
       "  'caption': ['horse in the left side', 'The horse to the far left.'],\n",
       "  'bbox_target': [19.36, 237.2, 116.08, 198.44]},\n",
       " 477: {'image_emb': tensor([[ 0.0098,  0.4868, -0.2445,  ...,  0.5894,  0.3984, -0.2163],\n",
       "          [-0.1881,  0.6934, -0.1940,  ...,  0.6299,  0.6973, -0.0270],\n",
       "          [-0.1349,  0.6577, -0.5718,  ...,  0.7500,  0.1081, -0.2852],\n",
       "          [-0.4792,  0.4309,  0.0629,  ...,  0.6401,  0.0845, -0.0085]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1255,  0.2935, -0.1702,  ..., -0.2173,  0.1616, -0.4238],\n",
       "          [-0.1422,  0.3125, -0.2113,  ...,  0.0586,  0.2324, -0.4810]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.1284e-01, 1.6675e-01, 7.5607e-03, 4.1284e-01],\n",
       "          [1.2527e-02, 7.4806e-03, 1.6785e-04, 9.7998e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  name\n",
       "  0  164.315369   30.447693  328.609558  414.365173    0.942505     75  vase\n",
       "  1  318.224731   80.811523  452.334229  369.596985    0.937677     75  vase\n",
       "  2  425.691315  163.586731  517.066650  332.961548    0.904505     75  vase,\n",
       "  'caption': ['the vase on the middle',\n",
       "   'The middle vase shown in the picture. It is medium in size.'],\n",
       "  'bbox_target': [322.41, 83.48, 130.5, 294.58]},\n",
       " 478: {'image_emb': tensor([[ 0.1898,  0.4619, -0.0682,  ...,  1.1396, -0.2246, -0.3030],\n",
       "          [ 0.1274,  0.0339, -0.5117,  ...,  0.6929,  0.1552, -0.0585],\n",
       "          [-0.2101,  0.3467, -0.5386,  ...,  0.8940, -0.2883, -0.0244],\n",
       "          [-0.1545,  0.1979, -0.1881,  ...,  1.2930, -0.0508, -0.0635],\n",
       "          [-0.2683,  0.3137, -0.4297,  ...,  0.7822, -0.2173, -0.1130]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0785, -0.1493, -0.3037,  ...,  0.4558, -0.0866, -0.1169],\n",
       "          [ 0.2686,  0.2294, -0.3303,  ..., -0.2407,  0.2455, -0.2749]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.9337e-02, 3.6743e-01, 3.2104e-02, 5.0195e-01, 5.9052e-02],\n",
       "          [7.2571e-02, 9.5749e-04, 5.9605e-08, 9.2627e-01, 1.1921e-07]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0   329.086792  218.134583  451.415649  476.640869    0.944142      0   person\n",
       "  1   322.064911  119.816528  399.270294  230.221344    0.893021      0   person\n",
       "  2     3.075226   23.030609  638.103638  475.576050    0.868100      7    truck\n",
       "  3     6.147223  279.419952   65.054703  418.292999    0.834218     26  handbag\n",
       "  4   259.540955  198.620544  271.134766  226.678284    0.560768     39   bottle\n",
       "  5   270.687866  195.432648  282.600586  226.353973    0.534313     39   bottle\n",
       "  6   292.019165  199.465088  301.723572  226.326294    0.526133     39   bottle\n",
       "  7   320.740143  205.839172  329.631012  230.135437    0.494887     39   bottle\n",
       "  8     2.721466  251.675903   66.283295  454.162476    0.481474      0   person\n",
       "  9   396.507416  144.952942  435.744720  200.667084    0.423555      0   person\n",
       "  10  275.023956  196.293884  290.704865  226.558105    0.352194     39   bottle\n",
       "  11  503.649597  248.530212  517.052856  284.165833    0.320224     39   bottle,\n",
       "  'caption': ['A woman with a purse near  food truck.',\n",
       "   'A woman in a black coat holding a bright red bag.'],\n",
       "  'bbox_target': [0.0, 232.12, 66.11, 247.88]},\n",
       " 479: {'image_emb': tensor([[ 1.1238e-02,  6.1475e-01, -2.4280e-01,  2.2485e-01,  9.7839e-02,\n",
       "            2.6782e-01,  2.6660e-01,  3.1494e-01, -4.4678e-02,  2.1130e-01,\n",
       "           -5.6519e-02, -3.4790e-01, -1.2720e-01, -2.5488e-01,  1.9910e-01,\n",
       "           -2.7930e-01,  1.0039e+00,  1.5820e-01,  5.0488e-01, -5.9692e-02,\n",
       "           -2.0293e+00,  1.4355e-01, -7.0898e-01, -3.1311e-02,  5.8398e-01,\n",
       "            5.5127e-01, -2.8397e-02,  2.0215e-01,  1.6028e-01,  1.5845e-01,\n",
       "            2.4182e-01,  1.5247e-01, -4.5581e-01,  2.1899e-01,  3.2227e-01,\n",
       "           -1.6394e-01, -1.7847e-01, -1.9299e-01, -3.6694e-01,  1.0098e+00,\n",
       "           -1.3501e-01,  1.3647e-01,  6.2549e-01, -1.5454e-01, -8.3862e-02,\n",
       "           -4.7632e-01,  5.8398e-01,  1.0962e-01, -3.0811e-01, -1.8188e-01,\n",
       "            3.4937e-01, -3.3960e-01,  9.4604e-02, -3.2379e-02, -1.0669e-01,\n",
       "            5.5713e-01,  4.7437e-01, -3.5132e-01, -1.1060e-01,  7.9285e-02,\n",
       "           -1.2830e-01,  5.1807e-01, -1.3257e-01, -5.3711e-01, -3.4546e-01,\n",
       "            5.7275e-01,  2.8580e-02,  1.8516e+00,  1.8567e-01,  4.1534e-02,\n",
       "           -3.5339e-02, -3.2642e-01, -5.0049e-01,  3.1934e-01,  5.2734e-01,\n",
       "           -5.1562e-01, -1.6467e-01,  6.1615e-02, -4.3604e-01, -3.0981e-01,\n",
       "           -2.8839e-02, -4.2554e-01,  4.2041e-01, -1.8896e-01,  1.9580e-01,\n",
       "           -1.1395e-01, -1.8481e-01,  3.3862e-01,  8.2373e-01, -8.3887e-01,\n",
       "            3.1934e-01,  1.3391e-01, -6.5742e+00,  5.0342e-01,  5.4443e-01,\n",
       "           -6.4746e-01, -8.2080e-01,  3.6646e-01, -3.3521e-01,  1.1902e-01,\n",
       "            4.0649e-02,  7.1106e-02, -7.5391e-01, -1.6785e-01, -5.1709e-01,\n",
       "            4.6094e-01,  3.8965e-01,  2.8149e-01, -5.2338e-02, -6.2305e-01,\n",
       "            1.2482e-01, -2.7881e-01, -8.1787e-02,  1.5564e-01, -3.9581e-02,\n",
       "           -2.9102e-01,  5.5615e-01,  3.9902e-03,  1.5869e-01, -5.4248e-01,\n",
       "            6.7529e-01,  8.3740e-02,  1.3477e-01,  3.4326e-01,  1.9043e-01,\n",
       "           -2.1887e-01,  2.1423e-01, -3.3550e-03,  2.5098e-01, -2.4658e-01,\n",
       "            1.8787e-01, -8.2031e-02,  2.9761e-01,  8.9600e-01,  2.8101e-01,\n",
       "            3.1812e-01, -2.3291e-01, -4.4067e-01, -6.7383e-02,  1.4807e-01,\n",
       "            2.3718e-01,  4.0552e-01,  1.6260e-01,  2.0471e-01, -3.5767e-01,\n",
       "           -4.2267e-02,  2.0667e-01, -1.1078e-01, -4.3384e-01,  1.8103e-01,\n",
       "           -6.9141e-01, -1.9043e-01,  2.8857e-01, -1.0059e-01, -1.2817e-01,\n",
       "           -6.1719e-01,  1.5198e-01, -2.9251e-02,  6.1646e-02, -1.7419e-01,\n",
       "           -2.3926e-01, -5.8777e-02, -4.2603e-01,  5.1971e-02, -3.0151e-01,\n",
       "           -7.0703e-01,  1.3745e-01,  6.4453e-01, -2.7756e-02,  3.7061e-01,\n",
       "            3.1787e-01,  1.4270e-01,  1.9446e-01,  2.1033e-01,  3.1219e-02,\n",
       "            7.3730e-01, -9.6045e-01, -1.3428e-01,  2.3560e-01,  1.4417e-01,\n",
       "            2.5171e-01, -1.4990e-01, -2.3755e-01,  4.8462e-01,  2.7710e-01,\n",
       "            5.4004e-01, -2.8052e-01,  2.5098e-01,  1.4648e-01, -3.7842e-02,\n",
       "           -7.4524e-02,  5.8929e-02,  3.1958e-01,  4.8065e-02,  6.9092e-01,\n",
       "            3.6108e-01, -4.5605e-01,  1.0400e-01, -3.5059e-01, -9.7961e-02,\n",
       "            6.3599e-02,  4.4670e-03,  2.7905e-01,  4.6326e-02,  2.0044e-01,\n",
       "           -5.6885e-01, -8.2336e-02, -2.8125e-01, -6.7627e-02,  2.0935e-01,\n",
       "           -7.9529e-02,  4.2212e-01, -6.6040e-02,  3.7817e-01, -4.2511e-02,\n",
       "            4.3140e-01, -7.2754e-01,  1.8323e-01,  1.4258e-01,  1.2421e-01,\n",
       "            4.7943e-02,  1.3086e-01, -1.1737e-01, -3.7646e-01,  2.4011e-01,\n",
       "            3.8037e-01, -2.3999e-01, -5.3467e-01,  5.7251e-02, -1.1975e-01,\n",
       "            3.8483e-02, -1.8959e-03,  2.5635e-01, -7.1411e-02,  2.3987e-01,\n",
       "           -1.3107e-02,  1.8823e-01, -7.5684e-02,  1.6187e-01,  2.3621e-01,\n",
       "           -1.6138e-01,  6.5613e-02,  3.6084e-01, -2.8345e-01, -8.0017e-02,\n",
       "           -3.5059e-01,  2.7075e-01,  3.6377e-01,  7.7087e-02, -1.3452e-01,\n",
       "            4.0527e-02,  2.9883e-01,  2.0776e-01,  1.9690e-01, -1.2488e-01,\n",
       "            1.0338e-02, -3.8550e-01,  2.2913e-01, -2.3462e-01,  1.7139e-01,\n",
       "            2.7863e-02,  3.0200e-01,  4.4800e-01,  2.7686e-01,  1.1187e-03,\n",
       "           -2.5562e-01,  5.1727e-02,  3.1055e-01,  1.3208e-01, -2.2656e-01,\n",
       "            1.3354e-01,  3.6084e-01,  5.6427e-02,  3.8770e-01,  4.2908e-02,\n",
       "            3.9014e-01,  5.4291e-02,  3.2275e-01,  3.4033e-01,  2.9346e-01,\n",
       "            4.6973e-01, -5.5969e-02,  1.5283e-01,  1.5205e-02, -2.0984e-01,\n",
       "            3.6719e-01, -6.2793e-01,  4.3549e-02,  2.6562e-01, -1.8384e-01,\n",
       "            2.6904e-01, -1.6882e-01,  2.6855e-01, -6.2408e-02, -2.6074e-01,\n",
       "            1.9250e-01,  6.7200e-02, -2.5684e-01,  9.0393e-02,  1.7212e-01,\n",
       "           -4.5715e-02, -1.3818e-01, -4.3164e-01, -5.7471e-01,  1.0913e-01,\n",
       "           -2.8824e-02,  1.3501e-01,  6.0059e-01,  7.1472e-02, -1.6174e-01,\n",
       "           -1.6638e-01,  6.3110e-02,  8.9502e-01, -1.5442e-01,  7.7454e-02,\n",
       "            3.6328e-01,  4.2065e-01, -3.5217e-02,  5.6787e-01,  3.1787e-01,\n",
       "           -9.5215e-02,  1.5449e+00, -1.9629e-01, -3.2397e-01, -4.9048e-01,\n",
       "            2.0801e-01,  1.9885e-01,  9.3811e-02, -3.3417e-02,  4.0619e-02,\n",
       "           -2.4182e-01, -4.3140e-01,  2.0898e-01, -2.4780e-01, -4.7150e-02,\n",
       "            1.4424e-04, -4.0381e-01, -1.6907e-01,  1.3867e-01,  6.3818e-01,\n",
       "            5.2734e-02, -3.7817e-01, -5.9082e-02,  2.6147e-01, -3.8525e-01,\n",
       "           -3.9246e-02, -3.9819e-01,  4.0359e-03,  4.4214e-01,  2.1072e-02,\n",
       "            4.5996e-01,  1.4319e-01, -3.7861e-03, -3.2153e-01, -5.3320e-01,\n",
       "            2.4609e-01, -3.2544e-01,  6.6309e-01, -5.9795e-04,  4.2896e-01,\n",
       "           -8.4521e-01,  3.0981e-01, -8.0200e-02,  2.7295e-01,  8.1055e-01,\n",
       "           -1.2451e-02, -3.5840e-01,  3.7201e-02, -5.2637e-01, -4.9756e-01,\n",
       "            9.4971e-02, -1.8713e-01,  3.4204e-01,  1.6345e-01,  3.2739e-01,\n",
       "            2.2644e-01,  1.2861e+00,  1.8262e-01, -9.3408e-01, -1.9617e-01,\n",
       "           -3.8550e-01, -9.4238e-01,  2.5730e-03, -3.9398e-02, -9.5154e-02,\n",
       "           -5.7959e-01,  2.9053e-01,  2.9126e-01, -7.5586e-01, -5.1611e-01,\n",
       "            3.0859e-01,  4.8755e-01,  7.9651e-02,  2.4915e-01,  4.7791e-02,\n",
       "           -4.1077e-02,  1.3391e-01, -2.8519e-02,  3.8147e-02, -1.1487e-01,\n",
       "            3.9502e-01,  8.2703e-02, -5.2783e-01, -9.1919e-02,  2.6660e-01,\n",
       "           -5.5371e-01, -1.9188e-03,  1.5918e-01,  5.4346e-01, -3.3228e-01,\n",
       "           -5.1611e-01,  9.4055e-02,  4.0619e-02,  3.4985e-01, -7.0996e-01,\n",
       "            2.1289e-01, -2.7612e-01,  1.2610e-01, -2.7148e-01, -1.1017e-01,\n",
       "            9.1675e-02, -2.4487e-01,  2.1497e-01,  6.7383e-01, -1.3586e-01,\n",
       "           -2.3450e-01, -6.1523e-01, -1.8433e-01, -2.4453e+00,  7.7026e-02,\n",
       "           -3.1519e-01,  4.7180e-02,  9.6045e-01, -3.0591e-01, -4.9365e-01,\n",
       "            2.4878e-01, -2.9468e-01,  3.5205e-01, -1.3782e-01, -2.5830e-01,\n",
       "            3.3813e-01,  3.6084e-01,  9.6741e-02, -1.4014e-01, -4.3213e-01,\n",
       "            4.7485e-01, -2.4817e-01, -3.3417e-03,  3.0493e-01, -1.6846e-01,\n",
       "           -1.1017e-01, -2.5122e-01, -3.2764e-01, -1.6309e-01,  3.4277e-01,\n",
       "            9.9182e-02,  1.9873e-01, -3.6646e-01, -2.9980e-01, -5.7831e-03,\n",
       "           -3.2373e-01, -1.7041e-01, -5.7617e-01, -4.1650e-01,  3.6646e-01,\n",
       "           -1.5881e-01, -3.2910e-01,  1.8823e-01, -3.1226e-01,  3.3716e-01,\n",
       "            4.2896e-01,  3.7598e-01, -1.4600e-01, -7.6221e-01, -1.4294e-01,\n",
       "           -1.5991e-01, -3.5791e-01,  2.1033e-01,  2.5488e-01, -3.0713e-01,\n",
       "            1.1401e-01,  1.5918e-01, -3.2520e-01,  5.4053e-01, -2.4548e-01,\n",
       "           -2.5830e-01,  6.8130e-03,  2.0740e-01, -7.5439e-02, -3.7573e-01,\n",
       "            3.9795e-01, -1.0016e-01,  8.8501e-02, -6.3086e-01, -2.5586e-01,\n",
       "           -8.7524e-02,  3.3911e-01, -1.2036e-01,  6.9641e-02,  8.8501e-02,\n",
       "            2.8638e-01, -5.8075e-02, -1.9177e-01,  3.8257e-01, -2.6221e-01,\n",
       "           -6.2939e-01,  3.3264e-02, -3.7817e-01,  2.1204e-01,  5.1416e-01,\n",
       "           -6.5369e-02,  2.6392e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2554,  0.0355, -0.4246,  ...,  0.0051,  0.0644, -0.1316],\n",
       "          [ 0.1176,  0.0209,  0.0433,  ..., -0.1908,  0.1982,  0.0590]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.],\n",
       "          [1.]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0   63.692444  174.775497  372.945557  381.543213    0.686118     48  sandwich\n",
       "  1    1.506775  361.766113  271.667603  520.615967    0.609559     46    banana\n",
       "  2    1.954376  308.958191  427.000000  619.831238    0.482584     46    banana\n",
       "  3  219.549438  363.643738  374.963928  535.192993    0.398633     46    banana,\n",
       "  'caption': ['this is a banana under a biscuit', 'The banana'],\n",
       "  'bbox_target': [218.35, 355.39, 164.14, 186.73]},\n",
       " 480: {'image_emb': tensor([[-0.0901,  0.5840, -0.5308,  ...,  0.4868,  0.1177,  0.0203],\n",
       "          [-0.7217,  0.1187, -0.1688,  ...,  1.0547,  0.2405, -0.1624],\n",
       "          [-0.0854,  0.4819, -0.4849,  ...,  0.8047,  0.0946,  0.0296],\n",
       "          [ 0.1637,  0.2966, -0.7974,  ...,  0.7114,  0.1231,  0.2384]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2275,  0.3582,  0.2817,  ...,  0.0383,  0.1313, -0.2256],\n",
       "          [-0.2186,  0.3682, -0.0548,  ..., -0.1641,  0.0666, -0.1836]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9561e-01, 4.3983e-03, 1.3709e-04, 3.7491e-05],\n",
       "          [9.9756e-01, 6.5923e-05, 1.4315e-03, 7.5436e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  492.190094    0.468277  628.554932  117.455185    0.878582     41   \n",
       "  1   63.629093    1.391891  190.180542  225.626709    0.852060     42   \n",
       "  2  278.815186    0.721344  567.435181  189.144440    0.847451     48   \n",
       "  3  252.084076    0.186890  318.150726   37.463165    0.663930     41   \n",
       "  4  156.185181  128.203156  458.790833  384.881378    0.618441     48   \n",
       "  5    5.248779    0.000000  640.000000  465.963745    0.393095     60   \n",
       "  \n",
       "             name  \n",
       "  0           cup  \n",
       "  1          fork  \n",
       "  2      sandwich  \n",
       "  3           cup  \n",
       "  4      sandwich  \n",
       "  5  dining table  ,\n",
       "  'caption': ['A styrofoam cup.', 'the white Styrofoam cup behind the bun'],\n",
       "  'bbox_target': [490.76, 1.08, 138.84, 116.23]},\n",
       " 481: {'image_emb': tensor([[-0.4563, -0.2539, -0.6895,  ...,  0.7910,  0.4451, -0.1876],\n",
       "          [-0.1238,  0.1970, -0.4897,  ...,  1.1172, -0.2908, -0.0922],\n",
       "          [-0.0276, -0.1986, -0.3296,  ...,  1.4404,  0.0360,  0.1858],\n",
       "          [-0.0583,  0.2238, -0.2761,  ...,  1.4717, -0.1221, -0.1470],\n",
       "          [-0.3777, -0.0284, -0.6343,  ...,  0.9624,  0.0575, -0.1941]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3650,  0.1067,  0.1764,  ...,  0.8096,  0.0518, -0.2598],\n",
       "          [-0.4539, -0.0944,  0.2024,  ...,  1.1270,  0.2957, -0.2939]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.1514e-01, 7.9870e-06, 6.2227e-04, 2.9564e-05, 4.8413e-01],\n",
       "          [3.2080e-01, 1.6093e-06, 6.1333e-05, 5.7817e-06, 6.7920e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  105.657471   73.709854  630.143555  345.599487    0.911381      7     truck\n",
       "  1  199.689713  211.126312  252.221725  380.193146    0.900498      0    person\n",
       "  2    0.379829  148.576782  108.781662  365.269226    0.886808      7     truck\n",
       "  3  133.069214  226.204956  194.209076  396.628845    0.885257      0    person\n",
       "  4  122.834961  275.923004  172.011414  321.747345    0.675268     24  backpack\n",
       "  5  573.782227  140.265533  640.000000  277.771027    0.484653      7     truck\n",
       "  6  463.817993  181.039856  488.494751  240.297546    0.306068      0    person,\n",
       "  'caption': ['truck with hawaiianese street food',\n",
       "   'The Hawaiianese Street food truck.'],\n",
       "  'bbox_target': [108.7, 71.03, 521.97, 267.98]},\n",
       " 482: {'image_emb': tensor([[ 0.3054, -0.2396, -0.0613,  ...,  0.3984, -0.0411, -0.5259],\n",
       "          [ 0.3003, -0.4182,  0.0503,  ...,  0.6367,  0.1595, -0.4819],\n",
       "          [-0.0375,  0.1821, -0.4397,  ...,  0.7192,  0.2450, -0.0101],\n",
       "          [ 0.3899, -0.2268,  0.0158,  ...,  0.6396,  0.1941, -0.3264],\n",
       "          [ 0.2930, -0.0591,  0.0659,  ...,  0.4011, -0.0194, -0.3638]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1500, -0.1731,  0.0648,  ...,  0.4402, -0.0254, -0.3643],\n",
       "          [ 0.2345, -0.0652,  0.2803,  ...,  0.3442, -0.4246,  0.2379]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.2234e-03, 9.6207e-03, 1.0729e-06, 8.6621e-01, 1.2097e-01],\n",
       "          [3.7718e-04, 4.9210e-04, 6.7139e-01, 3.1226e-01, 1.5305e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   49.657120  215.466125  394.958008  409.152893    0.937978     18   \n",
       "  1  189.983078  151.593140  523.575439  380.973572    0.879680     18   \n",
       "  2  602.506348  161.767654  639.615845  217.887451    0.836083      2   \n",
       "  3    0.155411   74.026764  457.053955  328.167511    0.720584      3   \n",
       "  4  579.327148  121.454773  590.590332  131.200348    0.680222     18   \n",
       "  5  368.955048   97.423676  472.057037  281.315582    0.490779      3   \n",
       "  6   46.134003  213.066711  400.802094  408.823120    0.374461      3   \n",
       "  \n",
       "           name  \n",
       "  0       sheep  \n",
       "  1       sheep  \n",
       "  2         car  \n",
       "  3  motorcycle  \n",
       "  4       sheep  \n",
       "  5  motorcycle  \n",
       "  6  motorcycle  ,\n",
       "  'caption': ['Motorcycle behind two white sheep.', 'a blue motorcycle'],\n",
       "  'bbox_target': [1.08, 86.41, 314.22, 239.46]},\n",
       " 483: {'image_emb': tensor([[-0.4534,  0.0112, -0.0942,  ...,  0.6865,  0.0415,  0.3525],\n",
       "          [ 0.1483,  0.1084,  0.0881,  ...,  0.9746, -0.2896,  0.2573],\n",
       "          [ 0.0641,  0.3484, -0.2520,  ...,  1.2031,  0.1588,  0.0813],\n",
       "          [-0.4458,  0.1398, -0.0058,  ...,  1.1670,  0.1661,  0.1851],\n",
       "          [-0.0413,  0.1078, -0.1652,  ...,  1.2324,  0.1228,  0.2441],\n",
       "          [-0.4158,  0.2615,  0.0766,  ...,  0.9434, -0.2849,  0.0030]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2637, -0.1108, -0.1938,  ..., -0.1486, -0.0572,  0.0528],\n",
       "          [-0.2435, -0.1592, -0.2588,  ...,  0.1899, -0.2119, -0.0396]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6050, 0.0044, 0.3501, 0.0101, 0.0073, 0.0235],\n",
       "          [0.9395, 0.0011, 0.0275, 0.0017, 0.0037, 0.0266]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    85.858551  205.706268  254.775330  370.326752    0.876255     77   \n",
       "  1    16.726551  144.577057   70.095398  208.234100    0.837120     77   \n",
       "  2   246.583572   32.023605  320.498535  149.923904    0.772187     58   \n",
       "  3   486.405945  148.736618  545.144958  215.909729    0.741480     77   \n",
       "  4   433.832275  184.756836  481.188477  243.386414    0.715916     77   \n",
       "  5   489.563354  210.496002  585.280884  255.291534    0.653660     28   \n",
       "  6   450.653595  146.878021  491.766388  191.230988    0.581776     77   \n",
       "  7   543.002075  150.864868  591.476562  207.131287    0.556776     77   \n",
       "  8   509.477966  253.385803  545.718323  273.677490    0.509287     45   \n",
       "  9     0.327984  203.600647   74.082855  270.200073    0.491744     60   \n",
       "  10  240.384094  129.443634  286.239319  216.619659    0.477532      0   \n",
       "  11  475.142548  125.333649  507.317596  154.320404    0.447704     77   \n",
       "  12  349.178467   97.548065  371.255005  110.881439    0.431638      0   \n",
       "  13    0.108418  228.769287   40.562149  419.807922    0.359195     56   \n",
       "  14   71.699821   87.755569  105.222298  136.997360    0.354732     77   \n",
       "  15  314.884430  173.872498  419.286163  307.264099    0.342624     77   \n",
       "  16  387.795319  114.111725  451.780365  174.872681    0.341920     58   \n",
       "  17  249.224777  171.256165  285.839905  286.688232    0.317263     17   \n",
       "  18  617.802856  146.166443  640.000000  185.167114    0.294120     77   \n",
       "  19  579.329651  123.606018  612.414734  171.816589    0.291000     77   \n",
       "  \n",
       "              name  \n",
       "  0     teddy bear  \n",
       "  1     teddy bear  \n",
       "  2   potted plant  \n",
       "  3     teddy bear  \n",
       "  4     teddy bear  \n",
       "  5       suitcase  \n",
       "  6     teddy bear  \n",
       "  7     teddy bear  \n",
       "  8           bowl  \n",
       "  9   dining table  \n",
       "  10        person  \n",
       "  11    teddy bear  \n",
       "  12        person  \n",
       "  13         chair  \n",
       "  14    teddy bear  \n",
       "  15    teddy bear  \n",
       "  16  potted plant  \n",
       "  17         horse  \n",
       "  18    teddy bear  \n",
       "  19    teddy bear  ,\n",
       "  'caption': ['The green box', 'A green toy chest.'],\n",
       "  'bbox_target': [70.07, 165.26, 187.75, 127.75]},\n",
       " 484: {'image_emb': tensor([[ 0.1490,  0.1006, -0.1844,  ...,  1.0566,  0.0418, -0.0714],\n",
       "          [-0.0606,  0.2163, -0.0594,  ...,  1.0654,  0.3767,  0.1581],\n",
       "          [-0.2421,  0.2534,  0.0290,  ...,  1.0654,  0.4526,  0.2285],\n",
       "          ...,\n",
       "          [-0.0295,  0.0020, -0.3062,  ...,  1.1748,  0.0442, -0.1322],\n",
       "          [-0.2397,  0.0329, -0.1748,  ...,  1.3662, -0.1724, -0.1615],\n",
       "          [-0.1753, -0.1214, -0.2659,  ...,  0.9717, -0.1008, -0.0439]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0305,  0.2534, -0.5791,  ...,  0.5220,  0.3606, -0.2002],\n",
       "          [ 0.3013,  0.1069, -0.4661,  ...,  0.0649, -0.0238,  0.1027]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.6101e-02, 8.4375e-01, 1.1665e-02, 5.7434e-02, 2.0477e-02, 3.4690e-04,\n",
       "           6.9380e-05, 2.9206e-06, 1.8120e-05],\n",
       "          [1.1871e-02, 7.6953e-01, 1.4858e-03, 9.8419e-03, 2.6913e-03, 7.0190e-04,\n",
       "           3.1769e-02, 1.7175e-01, 2.4629e-04]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   253.157913  133.831757  414.001770  401.850739    0.936926      0   \n",
       "  1    24.387924  142.855011  214.520920  383.961884    0.886831      0   \n",
       "  2   553.040833  225.367218  612.027893  314.392181    0.822473     40   \n",
       "  3   360.954193  205.038757  442.169586  293.427368    0.799625     40   \n",
       "  4   426.547577  150.621155  604.047607  420.240967    0.798622      0   \n",
       "  5    70.585838  175.731598  110.446388  257.329987    0.789016     40   \n",
       "  6     1.238800  344.315430  636.463867  477.322449    0.736733     60   \n",
       "  7   173.589020  197.577942  226.023895  358.628418    0.728529     56   \n",
       "  8     0.186184  147.784683   43.391674  259.635712    0.685764     56   \n",
       "  9   533.879639  310.478729  639.781494  387.150787    0.606298      0   \n",
       "  10  420.782928  245.335815  439.496429  335.001526    0.468696     56   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2     wine glass  \n",
       "  3     wine glass  \n",
       "  4         person  \n",
       "  5     wine glass  \n",
       "  6   dining table  \n",
       "  7          chair  \n",
       "  8          chair  \n",
       "  9         person  \n",
       "  10         chair  ,\n",
       "  'caption': ['A chinese man wearing a T-shirt holds a glass of wine',\n",
       "   'a man in a dark tshirt and dark hair'],\n",
       "  'bbox_target': [29.19, 145.95, 198.92, 238.91]},\n",
       " 485: {'image_emb': tensor([[-0.5171, -0.3323, -0.1737,  ...,  0.7524,  0.0090, -0.0389],\n",
       "          [-0.4204, -0.1230, -0.1768,  ...,  0.6074,  0.0634, -0.0406],\n",
       "          [-0.6616, -0.3689, -0.2068,  ...,  0.5576,  0.0120,  0.2133]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.4177, -0.1923, -0.2067,  ..., -0.1676, -0.0734,  0.3167],\n",
       "          [-0.3164, -0.1687, -0.1215,  ...,  0.0532,  0.0766,  0.5420]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0468, 0.8042, 0.1488],\n",
       "          [0.1199, 0.7583, 0.1218]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class   name\n",
       "  0    0.000000   0.000000  251.055603  268.797760    0.949069     22  zebra\n",
       "  1  181.178329  24.103119  414.534607  404.210052    0.945614     22  zebra,\n",
       "  'caption': ['AN ADULT ZEBRA STANDING IN THE BACKGROUND', 'zebra in back'],\n",
       "  'bbox_target': [0.0, 0.0, 251.23, 266.61]},\n",
       " 486: {'image_emb': tensor([[-0.1456,  0.2910, -0.1637,  ...,  0.9966,  0.1053, -0.1521],\n",
       "          [-0.1554,  0.4431, -0.0264,  ...,  0.6567, -0.0125, -0.2308],\n",
       "          [-0.5122, -0.0420,  0.1412,  ...,  1.1055,  0.1414, -0.0873],\n",
       "          ...,\n",
       "          [-0.1221,  0.2842, -0.0063,  ...,  0.6592,  0.0797, -0.2236],\n",
       "          [-0.3359,  0.4966,  0.0269,  ...,  1.2422,  0.1412, -0.3623],\n",
       "          [-0.0473,  0.3062,  0.1923,  ...,  0.6919, -0.0255, -0.1808]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0530,  0.0977, -0.0761,  ..., -0.1069,  0.3196, -0.7686],\n",
       "          [-0.0086, -0.0169,  0.0412,  ..., -0.1459,  0.5352, -0.7544]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.5352e-01, 1.5053e-02, 2.9027e-05, 6.9284e-04, 3.5352e-01, 4.5624e-02,\n",
       "           2.3181e-01],\n",
       "          [8.8037e-01, 7.1955e-04, 3.5763e-06, 2.3735e-04, 2.0706e-02, 8.7158e-02,\n",
       "           1.0574e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.044721    0.324976  109.224152  170.655914    0.938146     39   \n",
       "  1  112.949753   98.193451  403.917603  324.370758    0.787697     53   \n",
       "  2    0.063854  208.856796  138.929245  289.033112    0.759962     43   \n",
       "  3  471.057037   52.510643  499.966064  194.669174    0.735732     41   \n",
       "  4    3.336239    0.000000  492.763611  366.526306    0.710777     60   \n",
       "  5  107.326126    0.000000  186.663391   71.915558    0.704041     39   \n",
       "  6   70.115593    0.044197  108.263275   60.569908    0.505815     39   \n",
       "  7    4.214478    1.766872  496.662750  375.000000    0.312994     53   \n",
       "  \n",
       "             name  \n",
       "  0        bottle  \n",
       "  1         pizza  \n",
       "  2         knife  \n",
       "  3           cup  \n",
       "  4  dining table  \n",
       "  5        bottle  \n",
       "  6        bottle  \n",
       "  7         pizza  ,\n",
       "  'caption': ['a bottle of water next to a small pizza',\n",
       "   'a bottle of water with  label  next to a pizza'],\n",
       "  'bbox_target': [0.0, 0.49, 108.43, 172.29]},\n",
       " 487: {'image_emb': tensor([[-0.0662,  0.1588, -0.1486,  ...,  1.4375,  0.1964, -0.1558],\n",
       "          [-0.1396,  0.2737, -0.5996,  ...,  1.0303, -0.1531, -0.4402],\n",
       "          [-0.1074,  0.1541, -0.1105,  ...,  1.2383, -0.0955, -0.3687],\n",
       "          ...,\n",
       "          [ 0.2499,  0.2209, -0.2986,  ...,  1.4453, -0.0491,  0.0548],\n",
       "          [-0.2500, -0.0420, -0.1283,  ...,  0.8911, -0.0132, -0.4265],\n",
       "          [-0.2886,  0.2727, -0.0199,  ...,  1.1787, -0.2382, -0.0330]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0762, -0.0849, -0.4861,  ...,  0.4429, -0.4006,  0.3115],\n",
       "          [-0.1908, -0.1378,  0.1189,  ...,  0.2164,  0.1081,  0.0240]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.1154e-05, 1.9431e-04, 9.3994e-01, 6.6102e-05, 1.1292e-02, 6.8903e-04,\n",
       "           4.7546e-02],\n",
       "          [8.2779e-03, 9.4788e-02, 8.8525e-01, 5.4061e-05, 9.2392e-03, 9.0027e-04,\n",
       "           1.5802e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  397.358063  237.165359  467.493530  294.489471    0.887011     64   \n",
       "  1  205.247879  220.856049  266.699890  309.158325    0.881803     67   \n",
       "  2  261.146454  146.615677  409.241089  230.326538    0.849951     26   \n",
       "  3    4.857630   77.950760   66.562874  148.534149    0.826444     41   \n",
       "  4   52.150963    0.749165  445.961426  160.298874    0.816366     26   \n",
       "  5  439.165894  165.921204  499.780182  282.552917    0.726334     73   \n",
       "  \n",
       "           name  \n",
       "  0       mouse  \n",
       "  1  cell phone  \n",
       "  2     handbag  \n",
       "  3         cup  \n",
       "  4     handbag  \n",
       "  5        book  ,\n",
       "  'caption': ['BLUE COLOR HAND BAG IN THE TABLE',\n",
       "   'A very small blue item that is sitting on top of the foundation makeup.'],\n",
       "  'bbox_target': [261.91, 146.67, 145.92, 83.06]},\n",
       " 488: {'image_emb': tensor([[-0.1019,  0.4019, -0.0462,  ...,  0.3320,  0.2905, -0.0293],\n",
       "          [-0.4551,  0.3442, -0.2191,  ...,  0.4199,  0.6328,  0.1068],\n",
       "          [ 0.0994,  0.0849, -0.1152,  ...,  0.8672,  0.0505, -0.2761],\n",
       "          [ 0.1078,  0.1959, -0.1238,  ...,  0.8501,  0.1589,  0.0553]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 1.7651e-01,  2.0386e-01, -1.7944e-01, -1.1462e-01,  3.7134e-01,\n",
       "            1.0229e-01,  2.3584e-01, -4.3506e-01, -1.0516e-01,  3.0371e-01,\n",
       "           -1.4636e-01, -1.2695e-01,  2.3560e-01, -3.3179e-01,  5.6152e-02,\n",
       "            3.8940e-01,  2.4219e-01,  2.9541e-01, -2.9736e-01, -1.5796e-01,\n",
       "            3.9771e-01, -2.7637e-01,  1.8875e-02,  1.1444e-01, -1.8250e-01,\n",
       "           -1.7700e-01,  8.8013e-02,  1.6089e-01,  1.6846e-01,  1.7505e-01,\n",
       "           -7.2144e-02, -1.1787e-03, -2.1631e-01, -2.7710e-01, -2.7734e-01,\n",
       "           -3.8013e-01,  8.3374e-02, -7.7576e-02,  7.0679e-02,  2.5781e-01,\n",
       "            2.0178e-01, -1.2488e-01,  2.2485e-01, -2.4011e-01, -7.8918e-02,\n",
       "           -2.6382e-02,  3.7671e-01,  1.4307e-01,  3.9062e-02, -2.2266e-01,\n",
       "           -2.8931e-01, -9.9365e-02,  2.4463e-01, -5.5420e-01, -5.1880e-02,\n",
       "           -2.3999e-01, -2.5171e-01,  1.1664e-01, -2.8223e-01,  3.9185e-01,\n",
       "           -1.3196e-01, -4.7119e-01, -1.0211e-01, -1.6431e-01, -6.4331e-02,\n",
       "            1.4648e-01,  2.6685e-01,  1.6711e-01,  5.1453e-02,  2.8247e-01,\n",
       "            1.8982e-01, -2.3865e-01,  1.5955e-01, -2.8839e-02,  5.5225e-01,\n",
       "           -2.0996e-01, -1.1133e-01, -1.8187e-03, -1.1902e-01, -2.0325e-01,\n",
       "           -3.0933e-01,  4.0112e-01,  2.6929e-01,  9.6008e-02, -2.1753e-01,\n",
       "            6.7444e-03,  1.2793e-01, -1.2866e-01, -9.7412e-02,  2.0386e-02,\n",
       "            2.7026e-01, -3.6499e-02, -1.1123e+00,  3.2886e-01, -3.7671e-01,\n",
       "           -1.7383e-01, -2.3645e-01,  6.5002e-02,  2.6758e-01, -2.2247e-02,\n",
       "            2.3120e-01, -1.3110e-01,  2.2888e-01,  4.7882e-02, -9.3262e-02,\n",
       "            2.1301e-01,  2.1729e-01, -3.1738e-01,  2.8711e-01,  2.7271e-01,\n",
       "           -1.5771e-01,  4.0863e-02,  5.8105e-01,  2.7634e-02, -1.1719e-01,\n",
       "            2.0398e-01, -5.5078e-01,  7.6843e-02, -2.4731e-01, -7.0801e-02,\n",
       "            1.0095e-01, -9.8730e-01,  3.8501e-01, -1.9910e-01,  5.6738e-01,\n",
       "           -2.9395e-01, -1.2854e-01, -4.7388e-01, -1.8701e-01,  1.7627e-01,\n",
       "           -6.6833e-03, -5.7031e-01, -2.0447e-01,  4.5586e+00, -4.4312e-02,\n",
       "           -3.6719e-01, -5.8740e-01, -5.9521e-01, -2.5806e-01, -4.2773e-01,\n",
       "           -6.4453e-01,  3.9429e-01, -3.9893e-01,  7.8979e-02, -2.1558e-01,\n",
       "           -2.5543e-02,  1.8433e-01, -2.6550e-02, -1.6565e-01, -1.9653e-02,\n",
       "           -1.8799e-01,  3.2959e-01,  3.0762e-01,  1.9446e-01, -3.2129e-01,\n",
       "           -9.2285e-02,  2.1350e-01, -2.1875e-01,  1.4893e-01,  1.1792e-01,\n",
       "            1.5173e-01, -2.3047e-01,  4.7791e-02, -2.2241e-01, -4.8071e-01,\n",
       "           -2.6587e-01, -2.1255e-02, -2.5220e-01,  3.9062e-01,  2.2461e-01,\n",
       "            6.5857e-02, -3.7158e-01,  5.2295e-01,  8.6594e-03, -2.6758e-01,\n",
       "            1.3428e-02, -2.0215e-01,  2.1301e-01,  1.3721e-01, -5.0812e-02,\n",
       "           -2.0828e-02,  1.8848e-01, -2.0728e-01, -5.4626e-02, -4.3213e-02,\n",
       "            3.3179e-01,  1.3227e-03, -1.3074e-01,  8.8989e-02,  3.1714e-01,\n",
       "            2.6562e-01,  7.1289e-02,  1.2109e-01, -2.2552e-02,  4.0918e-01,\n",
       "            1.0626e-01,  1.5344e-01, -6.7932e-02, -2.1863e-01, -2.0435e-01,\n",
       "           -4.1968e-01, -1.3232e-01, -5.3320e-01,  4.0649e-01,  4.8767e-02,\n",
       "           -4.2749e-01,  3.4668e-01, -1.2244e-01,  2.3547e-01, -5.3516e-01,\n",
       "            2.4927e-01,  5.8057e-01, -1.6455e-01,  1.9836e-01,  5.6982e-01,\n",
       "           -1.4307e-01,  4.7510e-01, -1.0596e-01,  6.7261e-02, -2.5391e-02,\n",
       "            2.9150e-01,  2.3230e-01,  1.8311e-01, -6.5967e-01,  2.9565e-01,\n",
       "           -3.1055e-01, -3.3594e-01, -5.3406e-02,  7.0923e-02,  3.2202e-01,\n",
       "            9.3933e-02,  6.8481e-02,  8.4167e-02, -5.5859e-01, -4.4751e-01,\n",
       "            2.0630e-01,  1.4026e-01, -1.0748e-01, -2.5610e-01, -1.5100e-01,\n",
       "           -4.2627e-01, -1.6321e-01, -1.2408e-01, -2.4765e-02, -1.8762e-01,\n",
       "            7.6660e-02,  3.2074e-02,  1.5833e-01, -9.1187e-02, -6.9695e-03,\n",
       "           -4.8315e-01,  6.5369e-02, -1.5625e-01, -3.4546e-02, -4.2725e-01,\n",
       "            3.7134e-01,  3.7556e-03, -5.2002e-02,  6.9580e-03,  3.8330e-01,\n",
       "           -9.7778e-02,  7.2693e-02, -2.3669e-01,  1.4990e-01,  1.0187e-01,\n",
       "           -8.2764e-02,  1.1804e-01, -1.7969e-01, -3.6353e-01, -4.8267e-01,\n",
       "           -1.1700e-01,  1.1700e-01,  2.6465e-01, -2.8125e-01, -6.2439e-02,\n",
       "           -1.2891e-01,  1.3257e-01,  3.4399e-01,  1.0962e-01,  1.3123e-01,\n",
       "           -6.6895e-02,  1.2573e-01,  4.6635e-04, -3.1525e-02, -1.2276e-02,\n",
       "           -1.2451e-01, -1.2054e-01,  3.0884e-01,  2.9517e-01, -3.0249e-01,\n",
       "            5.4297e-01, -1.4453e-01,  5.6915e-03,  3.2446e-01, -8.0933e-02,\n",
       "            1.3403e-01, -2.0947e-01,  8.0933e-02,  1.8408e-01, -4.0405e-02,\n",
       "           -1.4856e-01, -2.3669e-01, -1.1768e-01,  8.1177e-02, -1.2006e-01,\n",
       "            3.4027e-02,  1.8140e-01,  1.7432e-01,  2.8290e-02, -1.2024e-01,\n",
       "            1.8845e-02,  1.0083e-01,  4.5508e+00, -2.7222e-01,  1.7383e-01,\n",
       "            3.3936e-02,  1.8036e-02,  9.3628e-02,  4.6631e-01,  6.3538e-02,\n",
       "            2.8784e-01,  4.0771e-01, -5.1361e-02,  4.6631e-01, -3.8892e-01,\n",
       "           -1.4636e-01,  2.2229e-01, -1.6357e-01,  2.2266e-01, -1.2959e+00,\n",
       "           -1.3953e-01,  1.9629e-01, -5.6445e-01, -1.0590e-01,  5.8929e-02,\n",
       "           -6.2622e-02, -1.6699e-01, -2.8955e-01, -1.5710e-01,  1.9556e-01,\n",
       "           -2.5781e-01,  3.2074e-02,  5.4639e-01, -6.5308e-02,  3.1665e-01,\n",
       "           -1.9177e-01,  9.4299e-02,  1.1462e-01, -5.2197e-01,  7.5623e-02,\n",
       "           -2.8198e-01, -2.8613e-01,  1.2115e-01,  3.2593e-01, -5.1172e-01,\n",
       "           -4.1724e-01,  4.2305e-03, -2.6025e-01,  1.3831e-01,  2.2766e-01,\n",
       "           -6.1230e-01, -5.6689e-01,  7.7553e-03,  2.5659e-01,  1.7810e-01,\n",
       "           -2.3804e-01,  3.5278e-01,  2.9248e-01, -5.8105e-02,  4.3555e-01,\n",
       "           -1.0139e-02, -4.7638e-02, -4.9561e-01,  3.1738e-01, -2.0947e-01,\n",
       "           -3.0908e-01, -1.0077e-01,  3.1403e-02, -1.4014e-01, -4.8730e-01,\n",
       "           -1.9775e-01, -2.7637e-01,  3.1226e-01, -2.1899e-01,  1.8347e-01,\n",
       "           -4.5990e-02, -1.1127e-01,  5.0262e-02, -2.2559e-01, -5.6427e-02,\n",
       "           -2.9443e-01,  1.2262e-01, -6.4307e-01, -2.6514e-01, -2.2864e-01,\n",
       "            1.4001e-01, -4.4983e-02,  4.3983e-03,  4.3365e-02,  4.8511e-01,\n",
       "            3.0859e-01,  1.4990e-01,  4.2847e-01, -1.4221e-01, -4.5190e-01,\n",
       "           -1.9055e-01,  7.5134e-02,  2.5903e-01,  3.5693e-01,  1.0852e-01,\n",
       "           -1.9019e-01, -4.3726e-01, -1.8372e-01,  1.2708e-01,  1.8997e-02,\n",
       "           -1.8689e-01, -2.3376e-01, -3.1372e-01,  8.6441e-03,  6.7993e-02,\n",
       "           -1.4612e-01,  2.6264e-03,  3.9307e-01, -6.5723e-01, -3.2153e-01,\n",
       "           -2.4368e-02,  6.7627e-02, -2.7979e-01,  9.0576e-02,  9.0088e-02,\n",
       "           -4.4263e-01,  3.6049e-03,  1.3892e-01,  3.0420e-01,  2.2217e-01,\n",
       "           -3.1421e-01, -6.2109e-01,  3.8403e-01, -1.3290e-02,  4.4800e-02,\n",
       "           -9.4055e-02, -5.5518e-01,  5.0751e-02, -3.1299e-01,  2.3145e-01,\n",
       "           -1.7810e-01, -2.0068e-01, -3.3691e-01,  1.2915e-01,  4.3896e-01,\n",
       "           -1.2939e-01, -3.0005e-01,  3.0200e-01, -4.9170e-01, -1.7029e-02,\n",
       "            1.0065e-01, -2.3206e-01, -2.3132e-02,  1.0883e-01, -4.5996e-01,\n",
       "           -5.5206e-02, -4.6692e-02,  1.4392e-01,  5.1849e-02, -1.5759e-01,\n",
       "            1.2408e-01,  4.0015e-01, -4.6021e-01, -1.7627e-01,  2.2607e-01,\n",
       "           -1.0553e-01,  4.7302e-02,  2.2266e-01, -3.8623e-01,  1.1194e-01,\n",
       "            1.2146e-01, -1.2732e-01, -2.5830e-01,  1.1353e-01,  1.5564e-01,\n",
       "            2.8662e-01, -2.2815e-01,  1.3908e-02, -1.4917e-01, -1.6577e-01,\n",
       "            3.1281e-02, -3.3051e-02,  2.7832e-01,  6.2378e-02, -1.1102e-01,\n",
       "           -8.7036e-02, -3.1665e-01,  3.6963e-01,  9.0918e-01,  5.0720e-02,\n",
       "            2.3560e-01,  4.0576e-01,  2.0093e-01, -2.1179e-01, -2.1021e-01,\n",
       "            2.0483e-01,  3.3569e-02, -1.8797e-03,  2.1210e-02,  5.2979e-01,\n",
       "            1.5479e-01,  2.4329e-01, -4.4409e-01, -9.4971e-02, -1.0242e-01,\n",
       "            1.6345e-01,  2.0862e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3389, 0.6040, 0.0473, 0.0101]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  405.008911  200.509018  611.709961  543.573364    0.921889     40   \n",
       "  1  127.948738    0.492906  264.438354  173.487091    0.865698     40   \n",
       "  2  199.101654    1.608098  612.000000  302.268860    0.750335     45   \n",
       "  3    1.518887   21.540895  611.519043  609.068542    0.568682     60   \n",
       "  4    0.454422  194.903259  244.821701  611.346619    0.471262     48   \n",
       "  \n",
       "             name  \n",
       "  0    wine glass  \n",
       "  1    wine glass  \n",
       "  2          bowl  \n",
       "  3  dining table  \n",
       "  4      sandwich  ,\n",
       "  'caption': ['A glass behind a bread basket'],\n",
       "  'bbox_target': [126.25, 0.72, 133.76, 175.72]},\n",
       " 489: {'image_emb': tensor([[ 0.1439,  0.2172, -0.3176,  ...,  0.5854, -0.1153, -0.0502],\n",
       "          [ 0.2094, -0.0570, -0.0967,  ...,  0.8125,  0.1490, -0.2874],\n",
       "          [-0.3074,  0.7471, -0.1150,  ...,  0.6963, -0.0686, -0.2124],\n",
       "          ...,\n",
       "          [ 0.2247, -0.0531,  0.0495,  ...,  1.0381, -0.1327, -0.0721],\n",
       "          [ 0.1870, -0.0869, -0.0957,  ...,  0.8813,  0.0130, -0.0862],\n",
       "          [ 0.3777,  0.6685, -0.1315,  ...,  0.5112, -0.2229, -0.0277]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3442,  0.2705, -0.1237,  ...,  0.3809, -0.1743, -0.3462],\n",
       "          [-0.0505,  0.1052,  0.0316,  ...,  0.7227, -0.5049, -0.4048]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.2077e-02, 0.0000e+00, 8.6487e-02, 0.0000e+00, 5.9605e-08, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 9.0137e-01],\n",
       "          [2.7686e-01, 0.0000e+00, 6.4355e-01, 0.0000e+00, 3.5763e-07, 1.8477e-06,\n",
       "           0.0000e+00, 0.0000e+00, 7.9346e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    11.598877  196.543427  634.902649  470.607178    0.902379      2   \n",
       "  1   599.667114  126.269562  638.504028  158.867828    0.878195      0   \n",
       "  2   412.454651  160.830780  639.916931  354.924469    0.821780      2   \n",
       "  3   467.987274  140.541672  493.918915  185.719604    0.813881      0   \n",
       "  4   303.235718  278.504913  377.218872  394.876434    0.789597     77   \n",
       "  5   418.431305  295.156281  462.109222  377.532257    0.789220     77   \n",
       "  6   551.832764  131.600037  580.243896  163.319336    0.785586      0   \n",
       "  7   521.183899  132.056610  546.053162  166.855469    0.716199      0   \n",
       "  8     0.038283  372.445862   22.257645  479.052795    0.667246      2   \n",
       "  9   188.092010  282.660278  254.536469  357.777771    0.661349     77   \n",
       "  10  241.745331  118.158112  303.928375  178.684784    0.594218     77   \n",
       "  11  152.681061  299.080200  182.342255  358.427734    0.564717     77   \n",
       "  12  239.510147  291.435272  290.537964  382.282990    0.514563     77   \n",
       "  13  304.697510  350.634399  350.896545  417.275879    0.513201     77   \n",
       "  14  105.682976  278.626129  153.979248  358.469330    0.482440     77   \n",
       "  15  303.026978  268.104034  335.772339  319.493256    0.467226     77   \n",
       "  16  273.740173  272.894775  312.598450  336.645752    0.359645     77   \n",
       "  17  360.233154  285.506775  386.840820  332.793213    0.355973     77   \n",
       "  18  400.469177  300.444885  429.558777  363.679810    0.349241     77   \n",
       "  19  304.387909  276.994019  375.242889  393.515686    0.309532      2   \n",
       "  20  374.792358  303.563721  406.199158  348.156616    0.305872     77   \n",
       "  21  451.176086  312.311707  481.318481  349.949768    0.262561     77   \n",
       "  \n",
       "            name  \n",
       "  0          car  \n",
       "  1       person  \n",
       "  2          car  \n",
       "  3       person  \n",
       "  4   teddy bear  \n",
       "  5   teddy bear  \n",
       "  6       person  \n",
       "  7       person  \n",
       "  8          car  \n",
       "  9   teddy bear  \n",
       "  10  teddy bear  \n",
       "  11  teddy bear  \n",
       "  12  teddy bear  \n",
       "  13  teddy bear  \n",
       "  14  teddy bear  \n",
       "  15  teddy bear  \n",
       "  16  teddy bear  \n",
       "  17  teddy bear  \n",
       "  18  teddy bear  \n",
       "  19         car  \n",
       "  20  teddy bear  \n",
       "  21  teddy bear  ,\n",
       "  'caption': ['A car covered with stuffed toys is parked in front of a perfume store.',\n",
       "   'a car cobered in stuffed animals sitting in a parking lot'],\n",
       "  'bbox_target': [13.0, 190.97, 621.94, 269.8]},\n",
       " 490: {'image_emb': tensor([[ 0.1704,  0.5195, -0.1432,  ...,  1.1963,  0.4746, -0.0300],\n",
       "          [ 0.0362,  0.5312, -0.2554,  ...,  0.6899,  0.2620, -0.1082]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3162,  0.0159,  0.1560,  ..., -0.2449, -0.0919, -0.4241],\n",
       "          [-0.1345,  0.2798,  0.0293,  ...,  0.1206, -0.1871, -0.3003]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0096, 0.9902],\n",
       "          [0.0065, 0.9937]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  name\n",
       "  0    0.000000  393.989990  155.193863  639.227417    0.878605     75  vase\n",
       "  1  282.084991  267.238281  401.324738  598.592896    0.658470     75  vase,\n",
       "  'caption': ['the white flower in the middle',\n",
       "   'A refelction of a tall vase of flowers.'],\n",
       "  'bbox_target': [287.54, 159.02, 116.52, 169.12]},\n",
       " 491: {'image_emb': tensor([[-0.0812, -0.2502,  0.0533,  ...,  0.8726, -0.2142,  0.0615],\n",
       "          [ 0.2034, -0.1159,  0.1299,  ...,  0.5508, -0.3420, -0.2024],\n",
       "          [ 0.0967, -0.4365, -0.4619,  ...,  0.9019, -0.2074, -0.2507],\n",
       "          ...,\n",
       "          [-0.1360,  0.2590,  0.0474,  ...,  1.1602, -0.2566, -0.1536],\n",
       "          [-0.1078,  0.0709,  0.0521,  ...,  1.2041,  0.0040, -0.0420],\n",
       "          [ 0.2085, -0.1949,  0.1311,  ...,  0.6982, -0.4854, -0.2195]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1571, -0.0339, -0.2649,  ...,  0.1804, -0.1433,  0.0023],\n",
       "          [ 0.1738,  0.2812, -0.1183,  ...,  0.0184, -0.0332, -0.2612]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.1103e-02, 3.9185e-01, 3.5763e-07, 1.9073e-06, 2.4277e-02, 2.0654e-01,\n",
       "           6.5002e-02, 2.9126e-01],\n",
       "          [5.8479e-03, 2.3364e-01, 3.4571e-04, 5.2738e-04, 9.6436e-03, 3.2446e-01,\n",
       "           2.1948e-01, 2.0618e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    39.826553  276.650940  216.626160  545.313660    0.944651     56   \n",
       "  1     1.029343   66.566101  478.785522  529.811462    0.810721     25   \n",
       "  2   432.501831  322.980316  442.406128  338.219879    0.807805      0   \n",
       "  3   448.746155  331.456451  478.455017  349.468781    0.792815      0   \n",
       "  4   207.314972  273.071503  404.361969  528.757690    0.757411     56   \n",
       "  5   190.488892  312.828156  231.494019  357.616913    0.735608     56   \n",
       "  6   214.190155  312.157135  248.853668  352.361359    0.704121     56   \n",
       "  7   361.577545  310.231598  374.811127  336.614899    0.679037      0   \n",
       "  8    26.348541  322.281403   55.928467  370.440216    0.676279     56   \n",
       "  9     0.000000  287.189514   53.471771  318.431519    0.578225     25   \n",
       "  10    0.000000  324.684235   28.648811  373.100800    0.572235     56   \n",
       "  11  458.109985  404.089111  479.777344  445.039673    0.552090     56   \n",
       "  \n",
       "          name  \n",
       "  0      chair  \n",
       "  1   umbrella  \n",
       "  2     person  \n",
       "  3     person  \n",
       "  4      chair  \n",
       "  5      chair  \n",
       "  6      chair  \n",
       "  7     person  \n",
       "  8      chair  \n",
       "  9   umbrella  \n",
       "  10     chair  \n",
       "  11     chair  ,\n",
       "  'caption': ['The blue umbrella that is set in between the two chairs that are in the foreground.',\n",
       "   'blue color umbrella in the image'],\n",
       "  'bbox_target': [1.44, 66.16, 478.56, 467.41]},\n",
       " 492: {'image_emb': tensor([[-0.2207,  0.4048,  0.2244,  ...,  0.5625,  0.0776, -0.0535],\n",
       "          [ 0.3928,  0.2898, -0.2808,  ...,  0.9917,  0.0147, -0.0267],\n",
       "          [-0.0159,  0.4224, -0.2664,  ...,  1.0947,  0.1484, -0.1909],\n",
       "          [ 0.0328,  0.2281, -0.0268,  ...,  0.1152, -0.0442,  0.1002]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0231, -0.0094,  0.0655,  ...,  0.5830,  0.2367, -0.2336],\n",
       "          [ 0.0120,  0.0147,  0.0021,  ...,  0.3831,  0.0526, -0.4355]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2654, 0.0111, 0.0022, 0.7212],\n",
       "          [0.3313, 0.0087, 0.0012, 0.6587]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  207.716537   24.556023  375.519531  199.851318    0.955178      0   \n",
       "  1   65.572533  187.102325  102.641022  239.115768    0.892407      0   \n",
       "  2  239.440369  168.327591  340.946411  211.227295    0.853174     36   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2  skateboard  ,\n",
       "  'caption': ['a person in a black hoodie is riding a skateboard down a ramp',\n",
       "   'skater boy on top of ramp'],\n",
       "  'bbox_target': [208.78, 25.44, 166.87, 172.12]},\n",
       " 493: {'image_emb': tensor([[-0.3071,  0.4438, -0.2922,  ...,  0.6069,  0.1533,  0.1233],\n",
       "          [-0.2421,  0.2556, -0.2690,  ...,  0.5762,  0.0846,  0.1395]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0746,  0.4192, -0.0456,  ..., -0.1130,  0.1815, -0.1180],\n",
       "          [-0.4192,  0.4131, -0.0925,  ...,  0.2177,  0.2207, -0.2866]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2338, 0.7661],\n",
       "          [0.4072, 0.5928]], dtype=torch.float16),\n",
       "  'df_boxes':         xmin      ymin        xmax        ymax  confidence  class  name\n",
       "  0  98.150482  1.154282  528.414551  422.066345    0.909334     75  vase,\n",
       "  'caption': ['A vase with a poem written on it.',\n",
       "   'The showing part of the vase with the verse on.'],\n",
       "  'bbox_target': [96.69, 0.96, 428.87, 419.3]},\n",
       " 494: {'image_emb': tensor([[-0.0919, -0.2571,  0.0505,  ...,  0.7881,  0.1671,  0.1749],\n",
       "          [-0.1719, -0.0411,  0.1156,  ...,  0.7354,  0.1503, -0.0989],\n",
       "          [ 0.0931, -0.0950, -0.1963,  ...,  1.0264, -0.1858, -0.1799],\n",
       "          [-0.3562, -0.4846,  0.0365,  ...,  0.4231,  0.1058, -0.0037]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1102, -0.4131,  0.0735,  ...,  0.6123,  0.0356,  0.2339],\n",
       "          [-0.1582, -0.1396,  0.1779,  ...,  0.2330,  0.2256,  0.1022]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7480e-01, 4.1534e-02, 1.4603e-05, 7.8369e-01],\n",
       "          [6.9482e-01, 2.3633e-01, 3.3557e-05, 6.8787e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  251.411911  145.568604  430.937317  375.559692    0.949386     20  elephant\n",
       "  1    1.064842   56.381821  202.542236  289.227295    0.944907     20  elephant\n",
       "  2  491.585449  152.125717  523.000916  210.986237    0.730195     23   giraffe,\n",
       "  'caption': ['Baby elephant walking on dry dirt picking up a leaf.',\n",
       "   'baby elephant'],\n",
       "  'bbox_target': [249.17, 148.85, 184.45, 227.6]},\n",
       " 495: {'image_emb': tensor([[-2.0703e-01,  2.3083e-01, -2.1411e-01,  ...,  9.1895e-01,\n",
       "           -5.9128e-04,  2.6392e-01],\n",
       "          [ 4.8553e-02,  1.1475e-01, -7.0496e-02,  ...,  6.3037e-01,\n",
       "            3.3252e-01, -1.5173e-01],\n",
       "          [-8.3862e-02,  1.7651e-01, -1.7419e-01,  ...,  6.9189e-01,\n",
       "            4.3799e-01,  3.9764e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.4280, -0.1594, -0.1355,  ..., -0.7202,  0.0744,  0.0428],\n",
       "          [ 0.0292, -0.2534, -0.3779,  ...,  0.3638,  0.1586, -0.1693]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0164, 0.5264, 0.4573],\n",
       "          [0.3635, 0.4595, 0.1771]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  235.904572  273.934845  310.301361  424.473389    0.919198     27      tie\n",
       "  1   50.691879    1.099319  468.849976  421.558289    0.894982      0   person\n",
       "  2    0.457895   65.868225   62.341034  417.888123    0.690354      0   person\n",
       "  3  317.447296    0.359207  636.929810  283.533569    0.680716      0   person\n",
       "  4    0.239510    0.215607  187.911957  236.812469    0.635498      0   person\n",
       "  5  458.202759  200.111755  639.141602  420.963806    0.471550     26  handbag,\n",
       "  'caption': ['A man in white in the back ground',\n",
       "   'The back of a man who is wearing a white dress shirt along with a black belt and slacks.'],\n",
       "  'bbox_target': [323.1, 0.0, 310.92, 283.26]},\n",
       " 496: {'image_emb': tensor([[ 0.2686,  0.1458, -0.2350,  ...,  0.5767, -0.0378, -0.4236],\n",
       "          [ 0.0474,  0.5522,  0.3530,  ...,  0.7393, -0.0872, -0.0410],\n",
       "          [ 0.1572,  0.1229,  0.0456,  ...,  0.5864, -0.0855,  0.0906],\n",
       "          ...,\n",
       "          [-0.2197,  0.2474, -0.0797,  ...,  1.2812, -0.0317, -0.1133],\n",
       "          [ 0.1439,  0.2517,  0.1403,  ...,  0.6758,  0.1060, -0.1682],\n",
       "          [ 0.2273,  0.1359,  0.0361,  ...,  0.2883, -0.0409, -0.2515]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2296,  0.3201, -0.2173,  ...,  0.0882, -0.2310, -0.2319],\n",
       "          [-0.3296,  0.1395,  0.0907,  ...,  0.1724, -0.2986,  0.1674]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.3809e-03, 4.1406e-01, 8.8274e-05, 7.7019e-03, 2.5513e-01, 2.9810e-01,\n",
       "           1.9974e-02, 1.7185e-03, 2.0084e-03],\n",
       "          [3.4809e-05, 1.8021e-02, 1.2040e-05, 9.8038e-03, 6.2294e-03, 2.1072e-02,\n",
       "           3.2654e-02, 9.1016e-01, 1.9608e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   103.749023    0.000000  379.216431  420.316406    0.943691      0   \n",
       "  1   398.059753    0.000000  504.179077  194.316254    0.926784     56   \n",
       "  2   225.451752  197.781616  456.471588  396.360962    0.919575     53   \n",
       "  3     0.315758    0.007439  130.543243  176.028564    0.905620     56   \n",
       "  4   562.652710   19.753433  639.932739  197.814026    0.886193     56   \n",
       "  5   329.624420    0.000000  410.205780  124.275635    0.850790     56   \n",
       "  6   530.311707  353.032990  570.476868  422.483154    0.722084     44   \n",
       "  7   179.123108  195.808655  640.000000  420.376465    0.707685     60   \n",
       "  8   175.958954  367.605438  236.506073  423.496826    0.671153     56   \n",
       "  9   547.057800  380.937927  605.595886  423.512390    0.569628     41   \n",
       "  10  484.898743    0.000000  639.046204  151.515869    0.518736     60   \n",
       "  11  621.992554  381.384369  639.958862  422.498108    0.398531     41   \n",
       "  12  411.886719  337.530334  485.544922  390.043457    0.395704     42   \n",
       "  13  569.004822  307.414215  640.000000  393.836029    0.385907     40   \n",
       "  14  120.429108   15.092140  178.305634   78.694565    0.348429     56   \n",
       "  15  415.586670  337.382416  486.190613  388.718536    0.256340     43   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1          chair  \n",
       "  2          pizza  \n",
       "  3          chair  \n",
       "  4          chair  \n",
       "  5          chair  \n",
       "  6          spoon  \n",
       "  7   dining table  \n",
       "  8          chair  \n",
       "  9            cup  \n",
       "  10  dining table  \n",
       "  11           cup  \n",
       "  12          fork  \n",
       "  13    wine glass  \n",
       "  14         chair  \n",
       "  15         knife  ,\n",
       "  'caption': ['A table covered with a white table cloth.',\n",
       "   'A table severd with noodles and pepsi drink on it'],\n",
       "  'bbox_target': [327.77, 202.0, 312.23, 216.28]},\n",
       " 497: {'image_emb': tensor([[ 0.0549,  0.0107,  0.3333,  ...,  0.8081,  0.2710, -0.3999],\n",
       "          [ 0.0815,  0.0332,  0.0080,  ...,  1.1934,  0.0732, -0.2988],\n",
       "          [ 0.1609, -0.1218,  0.3284,  ...,  0.9141,  0.0712, -0.5176],\n",
       "          [ 0.1632, -0.0591,  0.1497,  ...,  0.2583,  0.2676, -0.3511]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3462, -0.5825, -0.2101,  ..., -0.1083,  0.1403, -0.3457],\n",
       "          [ 0.1232, -0.2450,  0.1315,  ...,  0.2029, -0.1423, -0.6904]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6572, 0.0761, 0.1974, 0.0693],\n",
       "          [0.7876, 0.1786, 0.0291, 0.0046]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  165.023560  181.992981  476.003418  364.459900    0.920503     18  sheep\n",
       "  1  126.939720  116.036499  319.008911  218.172607    0.883420     18  sheep\n",
       "  2  352.440765  163.781616  537.855103  331.668945    0.723938     18  sheep,\n",
       "  'caption': ['A black sheep with some White fur heading to go under the sign.',\n",
       "   'Sheep with white wool.'],\n",
       "  'bbox_target': [165.56, 182.84, 312.87, 181.96]},\n",
       " 498: {'image_emb': tensor([[-0.1746, -0.2930,  0.0132,  ...,  0.7217,  0.1884, -0.0858],\n",
       "          [ 0.0614, -0.2019,  0.0287,  ...,  0.6533,  0.1465, -0.0810],\n",
       "          [-0.3596, -0.2408,  0.1437,  ...,  0.4844,  0.1968,  0.0969]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0463, -0.1633, -0.5020,  ..., -0.5898, -0.1073,  0.1139],\n",
       "          [ 0.4341, -0.3743, -0.3457,  ..., -0.0006, -0.0458, -0.2053]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1576, 0.1576, 0.6846],\n",
       "          [0.3210, 0.3262, 0.3528]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0   65.843941  160.744720  259.187012  475.023682    0.943379     23  giraffe\n",
       "  1  222.930878  237.768005  336.776337  402.536133    0.923781     23  giraffe,\n",
       "  'caption': ['The giraffe in the back.', 'A giraffe next to the door.'],\n",
       "  'bbox_target': [227.6, 240.54, 111.1, 161.8]},\n",
       " 499: {'image_emb': tensor([[-0.1438,  0.2334, -0.1833,  ...,  0.8286, -0.0895,  0.0843],\n",
       "          [ 0.2524,  0.3281,  0.0815,  ...,  0.8398, -0.0894, -0.2659],\n",
       "          [ 0.3364, -0.4490, -0.0113,  ...,  0.1177, -0.0332, -0.2333],\n",
       "          ...,\n",
       "          [ 0.1614, -0.0239, -0.0779,  ...,  0.8008,  0.2961, -0.4004],\n",
       "          [ 0.0727, -0.0321, -0.1685,  ...,  0.5410,  0.2194, -0.4124],\n",
       "          [ 0.1014, -0.4187,  0.1279,  ...,  0.1854,  0.0737, -0.1119]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3181,  0.0138, -0.4709,  ...,  0.0731, -0.1031,  0.1256],\n",
       "          [ 0.4668, -0.3950, -0.2949,  ...,  0.4883,  0.1289,  0.1418]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.1921e-07, 1.7881e-06, 2.1924e-01, 5.0259e-04, 1.4923e-02, 1.7881e-07,\n",
       "           0.0000e+00, 2.3842e-07, 7.6514e-01],\n",
       "          [9.5367e-07, 1.5020e-04, 5.8398e-01, 1.3313e-02, 6.9763e-02, 2.9802e-06,\n",
       "           2.9802e-07, 1.4424e-05, 3.3276e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0   278.684357  374.065613  414.514130  478.579895    0.938929      0   person\n",
       "  1    72.554993  361.094147  202.177124  478.410889    0.919865      0   person\n",
       "  2    75.234894  185.344971  638.529785  475.800537    0.918122     23  giraffe\n",
       "  3   532.108826  271.167908  639.694763  475.909424    0.909809      0   person\n",
       "  4   366.082581   67.782349  640.000000  274.439789    0.867736     23  giraffe\n",
       "  5     0.166464  413.288940   59.978073  478.883179    0.794574      0   person\n",
       "  6    70.790863  381.231110  116.184204  438.918762    0.759672      0   person\n",
       "  7    89.238670  362.672516  133.754059  417.371674    0.748515      0   person\n",
       "  8    27.011555  375.652252   78.227142  447.820801    0.638266      0   person\n",
       "  9    48.591049  424.359192   73.638931  479.677673    0.442656      0   person\n",
       "  10   26.360964  376.348022   62.632721  414.446655    0.296411      0   person,\n",
       "  'caption': ['A giraffe leaning far through the fence, reaching towards people.',\n",
       "   \"A giraffe reaching it's neck through a gate to get to some food with people watching the giraffe.\"],\n",
       "  'bbox_target': [70.11, 182.11, 555.51, 297.89]},\n",
       " 500: {'image_emb': tensor([[-0.4851,  0.2524, -0.1923,  ...,  0.7417,  0.0661, -0.4224],\n",
       "          [-0.5747,  0.1503,  0.0862,  ...,  0.7622,  0.1058, -0.5093]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.5273,  0.0413, -0.2335,  ..., -0.0414,  0.2477, -0.1998],\n",
       "          [-0.5552, -0.0387,  0.1141,  ...,  0.3496,  0.0869, -0.4922]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.7744, 0.2255],\n",
       "          [0.3774, 0.6226]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  151.746536  239.529022  399.622375  405.367523    0.708714     51  carrot\n",
       "  1  237.805206  242.956268  402.211517  294.841888    0.605222     51  carrot\n",
       "  2  257.372711   99.822403  336.602203  140.349686    0.568374     51  carrot\n",
       "  3  189.486145  333.499817  352.427490  407.745667    0.514689     51  carrot\n",
       "  4  197.093399  179.749908  247.369034  221.166779    0.424317     51  carrot\n",
       "  5  141.837677  293.184143  298.925690  351.619446    0.412416     51  carrot\n",
       "  6  186.293488  245.387390  359.711884  320.859924    0.323293     51  carrot\n",
       "  7  272.470276  103.435928  378.478455  133.493332    0.317902     51  carrot\n",
       "  8  335.822327  218.567108  391.482178  246.942596    0.288508     51  carrot\n",
       "  9  334.564453  199.721222  382.016541  228.835114    0.281205     51  carrot,\n",
       "  'caption': ['The carrots toward the front of the picture.',\n",
       "   'Carrots in stand next to celery'],\n",
       "  'bbox_target': [142.39, 239.66, 262.82, 169.65]},\n",
       " 501: {'image_emb': tensor([[-0.1371,  0.3540, -0.2285,  ...,  1.0850,  0.1284, -0.1444],\n",
       "          [-0.0455,  0.1663, -0.2380,  ...,  0.9458,  0.3708,  0.0336],\n",
       "          [ 0.0921,  0.3140, -0.2422,  ...,  1.3086, -0.0762, -0.1967],\n",
       "          [-0.0802, -0.0905,  0.2603,  ...,  0.3936,  0.2275,  0.1355]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1368, -0.1335, -0.4346,  ...,  0.0707,  0.0587, -0.0330],\n",
       "          [-0.0073,  0.0390, -0.3350,  ..., -0.6250,  0.0554, -0.2625]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2839, 0.0013, 0.4263, 0.2883],\n",
       "          [0.4709, 0.0076, 0.0648, 0.4565]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  159.673782   72.252838  272.292542  453.889191    0.929805      0   \n",
       "  1    0.201435  411.455048   79.802048  541.850830    0.913618     16   \n",
       "  2  250.279495  119.529709  353.897583  456.422180    0.789988      0   \n",
       "  3  227.052597  132.406982  239.600784  168.295593    0.663293     27   \n",
       "  4  244.141617  186.079483  264.644226  219.796692    0.351452     40   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1         dog  \n",
       "  2      person  \n",
       "  3         tie  \n",
       "  4  wine glass  ,\n",
       "  'caption': ['The girl wearing the black dress.',\n",
       "   'The woman standing next to the man.'],\n",
       "  'bbox_target': [237.5, 117.52, 110.18, 339.11]},\n",
       " 502: {'image_emb': tensor([[ 0.3328,  0.2162,  0.2291,  ...,  0.0019, -0.2053, -0.1199],\n",
       "          [ 0.3765,  0.1603, -0.0139,  ...,  0.0185, -0.0504,  0.0478],\n",
       "          [ 0.6812, -0.0914, -0.1605,  ..., -0.0976, -0.2357,  0.1892],\n",
       "          [ 0.3425,  0.1659, -0.0514,  ...,  0.1398, -0.1471,  0.1283],\n",
       "          [ 0.3882, -0.0260, -0.0402,  ..., -0.2448, -0.1109, -0.1685]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2125,  0.0421, -0.2175,  ...,  0.1642,  0.0111,  0.1735],\n",
       "          [-0.2307, -0.4819,  0.1008,  ..., -0.8105, -0.1750, -0.8105]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.8145e-01, 9.3231e-03, 7.9803e-03, 6.3467e-04, 5.6028e-04],\n",
       "          [4.5135e-02, 2.7122e-03, 1.1129e-03, 6.7472e-04, 9.5020e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   334.920258   22.654949  562.669495  275.966888    0.942487      0  person\n",
       "  1    37.482624   25.162895  255.800980  257.848724    0.936838      0  person\n",
       "  2   336.113647  319.472260  567.269226  572.457092    0.935628      0  person\n",
       "  3    36.084507  324.104706  272.035675  572.558289    0.932043      0  person\n",
       "  4    12.798129  273.509216   47.582264  300.964996    0.593298     45    bowl\n",
       "  5   100.841293  274.477997  157.207809  301.168243    0.584712     41     cup\n",
       "  6   485.619751  273.025116  523.860229  300.796295    0.579600     45    bowl\n",
       "  7   489.689972  569.974854  526.425049  598.085266    0.578287     41     cup\n",
       "  8   188.030045  569.583435  224.391388  597.745544    0.532094     41     cup\n",
       "  9   183.362762  254.322418  218.007538  283.563965    0.498159     45    bowl\n",
       "  10   97.013672  564.179871  189.225281  596.977905    0.426046     53   pizza\n",
       "  11  489.566376  569.792114  526.429138  597.851257    0.384879     45    bowl\n",
       "  12  484.632599  272.726166  523.337708  300.839935    0.366421     41     cup\n",
       "  13  183.339157  254.700882  217.606430  283.915710    0.288096     41     cup\n",
       "  14  392.212158  566.549255  490.279388  597.661804    0.286518     53   pizza\n",
       "  15  187.106995  569.921082  224.019989  597.545593    0.279860     45    bowl,\n",
       "  'caption': ['Woman in denim shirt displaying a crooked smile and big eyes looking up to the righton her face.',\n",
       "   'top right photo'],\n",
       "  'bbox_target': [342.44, 30.26, 218.67, 243.42]},\n",
       " 503: {'image_emb': tensor([[ 0.7173,  0.5454,  0.1763,  ...,  0.7056,  0.3484, -0.2986],\n",
       "          [ 0.9731,  0.2130,  0.0824,  ...,  0.4377,  0.3320, -0.2668],\n",
       "          [ 0.6777,  0.4973,  0.1313,  ...,  0.9316,  0.2173, -0.2600],\n",
       "          [ 0.6465,  0.6440,  0.0793,  ...,  0.6865,  0.1411, -0.3997],\n",
       "          [ 0.5039,  0.2152,  0.1029,  ...,  0.3357,  0.0627, -0.0296]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2754,  0.2015, -0.0084,  ..., -0.4456,  0.4033, -0.5610],\n",
       "          [ 0.5674,  0.2834, -0.1025,  ..., -0.2405,  0.3281, -0.4250]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6001, 0.2278, 0.0123, 0.1238, 0.0360],\n",
       "          [0.1334, 0.3984, 0.0164, 0.0141, 0.4375]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  name\n",
       "  0    7.097149  260.559265  118.318695  442.335266    0.905903     14  bird\n",
       "  1  251.389236   64.033516  529.980469  257.942780    0.899321     14  bird\n",
       "  2  149.040039  326.002991  273.772827  422.664001    0.898861     14  bird\n",
       "  3  544.325562  415.167633  622.351318  530.332581    0.892721     14  bird,\n",
       "  'caption': ['a goose in mid flight', 'The bird is flying out of the water.'],\n",
       "  'bbox_target': [254.55, 60.1, 220.38, 233.35]},\n",
       " 504: {'image_emb': tensor([[-0.1404, -0.0108, -0.1111,  ...,  0.8115, -0.0052,  0.3201],\n",
       "          [-0.0328,  0.0012, -0.2485,  ...,  0.8599, -0.0586,  0.3359],\n",
       "          [-0.0917,  0.1476, -0.0551,  ...,  0.9116, -0.2053,  0.2225],\n",
       "          ...,\n",
       "          [ 0.0454,  0.3303, -0.4758,  ...,  0.7520,  0.2822,  0.0291],\n",
       "          [-0.1940,  0.0847, -0.0375,  ...,  1.0527, -0.1738, -0.0706],\n",
       "          [-0.1410, -0.1086, -0.2377,  ...,  0.7852, -0.0545,  0.1168]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0765,  0.0356, -0.0612,  ..., -0.0098,  0.2130,  0.2283],\n",
       "          [-0.0416,  0.0374, -0.4470,  ...,  0.1039,  0.0980,  0.3059]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.4951e-01, 7.3340e-01, 4.4289e-03, 3.5465e-05, 9.7156e-06, 4.3602e-03,\n",
       "           4.5700e-03, 7.0095e-04, 1.4381e-03, 1.3113e-06, 2.9385e-05, 1.2279e-05,\n",
       "           1.1740e-03, 1.4019e-04],\n",
       "          [3.1055e-01, 7.4890e-02, 2.9785e-02, 5.9605e-07, 3.9935e-06, 4.5700e-03,\n",
       "           5.8008e-01, 4.6968e-05, 1.8275e-04, 5.9605e-08, 4.1723e-07, 1.7881e-07,\n",
       "           9.7156e-06, 1.0747e-04]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   335.239441   98.949310  601.930237  398.641205    0.937602      0   \n",
       "  1   336.300415  205.058777  639.648254  476.770691    0.937218     56   \n",
       "  2     0.000000  275.704834  229.975433  475.917542    0.920249     56   \n",
       "  3   236.753082  174.827042  270.192749  220.449585    0.919960     41   \n",
       "  4   410.979706  170.364883  440.931366  208.558472    0.907146     41   \n",
       "  5   437.098114   77.863586  538.040833  176.408173    0.892201     56   \n",
       "  6     0.887573   38.036346  243.412979  408.864838    0.866891      0   \n",
       "  7   140.425903  127.839188  495.315186  447.907532    0.863071     60   \n",
       "  8   134.829224   62.429733  211.338440  161.759476    0.841636     56   \n",
       "  9   209.417862  134.644867  238.823044  166.467316    0.813591     41   \n",
       "  10  293.439056   72.120117  319.044647   96.219574    0.804385     47   \n",
       "  11  290.822998  178.556122  317.749695  207.107269    0.765838     47   \n",
       "  12  258.310760  291.033508  388.671600  391.119629    0.763907     16   \n",
       "  13  354.358887  219.808136  389.335571  249.617828    0.662623     42   \n",
       "  14  240.626556  152.771851  258.397705  176.170624    0.506963     47   \n",
       "  15  363.507111  198.513458  440.679901  237.372223    0.499448     45   \n",
       "  16  193.593399  209.624084  244.446213  234.403931    0.251146     42   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1          chair  \n",
       "  2          chair  \n",
       "  3            cup  \n",
       "  4            cup  \n",
       "  5          chair  \n",
       "  6         person  \n",
       "  7   dining table  \n",
       "  8          chair  \n",
       "  9            cup  \n",
       "  10         apple  \n",
       "  11         apple  \n",
       "  12           dog  \n",
       "  13          fork  \n",
       "  14         apple  \n",
       "  15          bowl  \n",
       "  16          fork  ,\n",
       "  'caption': ['A girl in pink with glasses.',\n",
       "   'A young girl with long dark hair wearing glasses, seated at a dinner table and looking back over her shoulder'],\n",
       "  'bbox_target': [334.71, 96.86, 269.06, 305.65]},\n",
       " 505: {'image_emb': tensor([[-0.3638,  0.1072, -0.2023,  ...,  0.8721, -0.1584,  0.1268],\n",
       "          [ 0.2434,  0.4780, -0.0178,  ...,  0.9917, -0.3938,  0.0334],\n",
       "          [-0.3403, -0.0509, -0.2008,  ...,  0.6855, -0.1274,  0.0953]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1083,  0.3037, -0.0914,  ...,  0.3784, -0.5625,  0.0159],\n",
       "          [-0.0617,  0.1129,  0.0229,  ...,  0.2125, -0.3394, -0.0386]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.8682, 0.0691, 0.0629],\n",
       "          [0.7817, 0.1469, 0.0716]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0    7.215683  221.598602  402.594543  476.388855    0.925098     13    bench\n",
       "  1   68.233299  129.570831  111.108299  175.960724    0.861124     13    bench\n",
       "  2  327.362640  172.733170  518.835449  385.601257    0.666267     26  handbag\n",
       "  3  325.831970   70.817627  546.149353  478.572144    0.605892      0   person\n",
       "  4   27.325577  128.905899   47.504295  154.990189    0.554084     13    bench\n",
       "  5    2.615398  125.091446   21.504946  146.495041    0.405899     13    bench,\n",
       "  'caption': ['Red heart shape bench on the beach.',\n",
       "   'Bench with lovely design on the beach'],\n",
       "  'bbox_target': [7.55, 227.42, 391.55, 248.09]},\n",
       " 506: {'image_emb': tensor([[-0.2439,  0.6792, -0.0063,  ...,  1.3535,  0.1376,  0.1263],\n",
       "          [-0.0535,  0.3743, -0.1813,  ...,  1.0889, -0.1267,  0.0645],\n",
       "          [ 0.2340,  0.4990,  0.0732,  ...,  1.2139,  0.1901, -0.0272],\n",
       "          [-0.1014,  0.5029,  0.0839,  ...,  1.0947,  0.3640, -0.2202],\n",
       "          [ 0.3545,  0.4368, -0.1763,  ...,  1.0322, -0.2910,  0.1559],\n",
       "          [-0.1472,  0.0018,  0.1669,  ...,  0.5337,  0.2756,  0.2842]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2878, -0.1777,  0.0154,  ...,  0.4751, -0.4116,  0.0898],\n",
       "          [-0.0359, -0.3162,  0.1949,  ...,  0.2988, -0.1215,  0.0247]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7881e-07, 4.7684e-07, 9.9902e-01, 1.1511e-03, 2.3544e-05, 1.1444e-05],\n",
       "          [1.3113e-06, 5.9605e-07, 9.9951e-01, 6.3610e-04, 5.3883e-05, 1.5199e-05]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   307.998169  107.822006  479.280823  390.896240    0.940381      0   \n",
       "  1   161.709503  209.889542  479.599731  398.657898    0.908116      3   \n",
       "  2     0.062569  162.594803  158.558319  274.170837    0.897905      2   \n",
       "  3    76.375366  140.428314  210.066437  198.774170    0.861989      2   \n",
       "  4   217.754883  119.922562  244.246216  183.413071    0.845258      0   \n",
       "  5    14.546463  264.858246   34.268295  285.172760    0.658940      2   \n",
       "  6     1.925888  273.814697   23.233536  292.129700    0.644615      2   \n",
       "  7   262.617920  143.878220  290.382812  175.227524    0.616735     58   \n",
       "  8     0.273819  276.245148   22.403961  355.244598    0.604060      2   \n",
       "  9    61.441010  591.273376  209.071564  639.671936    0.409969      0   \n",
       "  10  316.253082  119.261871  352.472137  148.916840    0.306009     58   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1     motorcycle  \n",
       "  2            car  \n",
       "  3            car  \n",
       "  4         person  \n",
       "  5            car  \n",
       "  6            car  \n",
       "  7   potted plant  \n",
       "  8            car  \n",
       "  9         person  \n",
       "  10  potted plant  ,\n",
       "  'caption': ['The back of a blue station wagon parked on the street.',\n",
       "   'Blue wagon with slanted back'],\n",
       "  'bbox_target': [0.0, 158.47, 158.88, 118.98]},\n",
       " 507: {'image_emb': tensor([[-0.2329,  0.4473, -0.1346,  ...,  1.0527, -0.0770, -0.0588],\n",
       "          [-0.0880,  0.5801, -0.0184,  ...,  1.0547, -0.1349, -0.2080],\n",
       "          [-0.2181,  0.2302,  0.0840,  ...,  0.6763, -0.1181,  0.0563]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0884,  0.3223,  0.1362,  ..., -0.1251, -0.3708, -0.5801],\n",
       "          [ 0.1216, -0.1674,  0.1060,  ...,  0.0705, -0.3516, -0.1384]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9658e-01, 1.3292e-04, 3.4828e-03],\n",
       "          [7.7393e-01, 1.1454e-03, 2.2510e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  489.163757  152.881592  639.369202  329.148560    0.846233     59       bed\n",
       "  1   96.791290  164.174377  203.308990  348.225464    0.832546     28  suitcase\n",
       "  2    8.532639  177.583710   97.943726  341.941681    0.674775     28  suitcase\n",
       "  3  249.078705  165.881409  555.428101  293.052917    0.603717     59       bed\n",
       "  4  143.097519  183.969360  240.978958  329.277405    0.588147     28  suitcase\n",
       "  5  318.896545  164.095886  497.434692  213.817749    0.582630     28  suitcase\n",
       "  6  240.926682  315.423828  458.755554  423.115173    0.509562     28  suitcase\n",
       "  7  240.283112  263.711548  637.004517  422.774780    0.416544     59       bed,\n",
       "  'caption': ['A bed with white and red bedding on it with a red open suit case on top of the bed.',\n",
       "   'bed with a red suitcase on it'],\n",
       "  'bbox_target': [247.42, 162.11, 328.8, 135.02]},\n",
       " 508: {'image_emb': tensor([[-0.7012, -0.1836, -0.2161,  ...,  0.2568, -0.3667, -0.0485],\n",
       "          [-0.6221, -0.1340, -0.4167,  ...,  1.0547, -0.0473, -0.1439],\n",
       "          [-0.0829,  0.2480, -0.4307,  ...,  1.2109, -0.0434, -0.3672],\n",
       "          ...,\n",
       "          [ 0.2415,  0.1628, -0.0692,  ...,  0.8042,  0.1111,  0.0681],\n",
       "          [-0.4915,  0.3567, -0.0853,  ...,  0.6528, -0.0522,  0.1091],\n",
       "          [-0.7358,  0.0716, -0.4033,  ...,  0.5210, -0.2507,  0.0371]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.4263, -0.1672, -0.2710,  ...,  0.3071, -0.0837, -0.2686],\n",
       "          [-0.2639, -0.2058, -0.2363,  ...,  0.3215, -0.0960, -0.5376]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.2398e-03, 5.7568e-01, 9.3603e-04, 7.3314e-05, 4.2114e-01, 2.9802e-07,\n",
       "           4.4203e-04, 2.5988e-04],\n",
       "          [5.4108e-02, 1.1633e-01, 2.1515e-02, 1.3893e-02, 7.3535e-01, 2.0332e-03,\n",
       "           3.6812e-03, 5.3253e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   11.596420   69.674652  452.654968  466.027954    0.925827      0   \n",
       "  1  416.495911  146.886307  636.853699  472.177795    0.918112      0   \n",
       "  2  322.561432   89.003281  448.720428  476.640869    0.896565      0   \n",
       "  3    0.170004  258.169983   53.693329  478.127502    0.895711     38   \n",
       "  4    9.322060  128.942810  212.956390  468.011230    0.882420      0   \n",
       "  5   73.450195  408.592987  101.369247  431.941437    0.800677     32   \n",
       "  6  557.055054  192.793274  621.144287  262.114197    0.720609      0   \n",
       "  7  462.626740  228.449829  639.757751  468.464233    0.675827     38   \n",
       "  8  525.201599  226.861084  640.000000  390.317017    0.650491     38   \n",
       "  9   15.547058  309.096985  209.703339  428.656494    0.554961     38   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3  tennis racket  \n",
       "  4         person  \n",
       "  5    sports ball  \n",
       "  6         person  \n",
       "  7  tennis racket  \n",
       "  8  tennis racket  \n",
       "  9  tennis racket  ,\n",
       "  'caption': ['tennis racket being held by a man in a white shirt with blue stripes',\n",
       "   'racket on right'],\n",
       "  'bbox_target': [516.41, 229.55, 123.59, 164.8]},\n",
       " 509: {'image_emb': tensor([[-0.2825,  0.3582, -0.2211,  ...,  1.3379, -0.0930, -0.3308],\n",
       "          [-0.3684,  0.3259, -0.2878,  ...,  1.2793, -0.0349, -0.5732],\n",
       "          [-0.1394,  0.1575, -0.1311,  ...,  0.6162,  0.1208, -0.2383],\n",
       "          [-0.8345,  0.0865, -0.0651,  ...,  0.4004, -0.1057, -0.3794],\n",
       "          [-0.5742,  0.1016, -0.0714,  ...,  0.6494, -0.1056, -0.4023]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0222, -0.1208, -0.7339,  ..., -0.4043,  0.0390, -0.0503],\n",
       "          [-0.0648,  0.1741, -0.5103,  ..., -0.0607, -0.1417, -0.2771]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1287, 0.0310, 0.0026, 0.0496, 0.7881],\n",
       "          [0.0483, 0.7104, 0.0011, 0.0022, 0.2379]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   48.371849   85.908051  224.739227  476.727417    0.926180      0   \n",
       "  1  338.751312  150.930542  443.599396  456.071289    0.862572      0   \n",
       "  2  329.044891  311.558563  363.007904  392.268463    0.849744     38   \n",
       "  3  179.853409   87.852814  272.360992  236.914185    0.755078     38   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2  tennis racket  \n",
       "  3  tennis racket  ,\n",
       "  'caption': ['man in background wearing red hat',\n",
       "   'The gentleman wearing a white shirt and white shorts, he is also wearing a red baseball cap and looking downward.'],\n",
       "  'bbox_target': [341.31, 141.5, 99.68, 321.43]},\n",
       " 510: {'image_emb': tensor([[ 4.8279e-02,  2.8906e-01,  1.4206e-02,  ...,  9.7217e-01,\n",
       "            1.3257e-01,  2.9755e-02],\n",
       "          [ 4.2627e-01,  1.1368e-02, -1.8616e-02,  ...,  6.5723e-01,\n",
       "            2.3352e-01,  1.6342e-02],\n",
       "          [ 1.7977e-03,  1.4148e-01, -2.3022e-01,  ...,  1.2012e+00,\n",
       "            2.5244e-01, -5.1025e-02],\n",
       "          [ 2.6226e-04,  3.0103e-01, -3.9160e-01,  ...,  1.4326e+00,\n",
       "            3.0859e-01, -3.1152e-01],\n",
       "          [ 2.1057e-01,  2.7759e-01,  9.1919e-02,  ...,  6.4844e-01,\n",
       "            6.8970e-02, -9.7595e-02],\n",
       "          [ 3.1055e-01,  9.7717e-02,  1.3013e-01,  ...,  5.7861e-01,\n",
       "            1.4819e-01,  5.0720e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 2.3950e-01,  1.3269e-01,  2.5635e-01,  ...,  2.2583e-01,\n",
       "           -1.0883e-01, -3.6108e-01],\n",
       "          [-7.7942e-02,  3.9101e-05,  3.2471e-01,  ...,  2.2705e-01,\n",
       "           -9.4543e-02, -5.2588e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.9550e-04, 6.7215e-03, 1.9760e-02, 4.7207e-04, 9.6680e-01, 5.1537e-03],\n",
       "          [3.4034e-05, 4.4067e-01, 2.7740e-02, 5.9605e-08, 2.2864e-01, 3.0298e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  243.807770   69.335548  349.928314  287.492645    0.913707      0  person\n",
       "  1    7.106262  232.452850  420.940002  621.913330    0.911373      8    boat\n",
       "  2    0.000000   26.153496  270.203766  257.330750    0.859392      8    boat\n",
       "  3  187.502350   87.947617  245.657928  216.199905    0.856490      0  person\n",
       "  4    0.094330  206.193634  264.768280  342.544708    0.793013      8    boat\n",
       "  5  374.905914  103.358948  426.295288  146.268021    0.660048      8    boat\n",
       "  6  356.172089  105.495880  367.737152  132.054245    0.628662      0  person\n",
       "  7  327.665161  130.299789  367.972168  145.324020    0.490971      8    boat,\n",
       "  'caption': ['larger brown boat', 'boat with fruits and vegetables'],\n",
       "  'bbox_target': [10.07, 225.8, 404.13, 395.5]},\n",
       " 511: {'image_emb': tensor([[ 0.1782,  0.3105, -0.1304,  ..., -0.0021,  0.1122,  0.0314],\n",
       "          [ 0.7812,  0.2261, -0.1002,  ..., -0.0150,  0.3684, -0.0375],\n",
       "          [ 0.4746,  0.2441,  0.2202,  ...,  0.1124,  0.2491, -0.2964],\n",
       "          [ 0.3120,  0.5410,  0.2148,  ..., -0.1559, -0.1217, -0.0123]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0374, -0.1699, -0.0179,  ..., -0.2219, -0.2092,  0.2551],\n",
       "          [-0.3257, -0.0685, -0.2766,  ..., -0.5000, -0.1104, -0.0437]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9463e-01, 4.9305e-04, 1.8911e-03, 2.7504e-03],\n",
       "          [8.9355e-01, 1.9658e-04, 1.0462e-03, 1.0510e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   30.514832   29.539062  273.065979  422.728516    0.941070      0  person\n",
       "  1  271.105316  118.547546  445.667999  421.179504    0.891734     14    bird\n",
       "  2  193.805374  176.343536  596.880615  421.611389    0.853069     13   bench\n",
       "  3  387.494324   77.564087  425.337830  100.593384    0.621696     14    bird\n",
       "  4  412.014740  107.450348  429.777679  140.888519    0.621064     14    bird\n",
       "  5   21.035675    4.133034   29.689117   16.090309    0.489910     14    bird\n",
       "  6    9.467697  123.261261  170.497543  353.212799    0.379787     13   bench\n",
       "  7  273.061829  129.919128  447.004211  418.751038    0.353601     13   bench,\n",
       "  'caption': ['a wooden bench is placed near a fence',\n",
       "   'the left area of the bench before the woman in the right hand picture'],\n",
       "  'bbox_target': [2.87, 122.26, 170.01, 232.1]},\n",
       " 512: {'image_emb': tensor([[ 0.0451,  0.3542, -0.1730,  ...,  1.2822,  0.3782, -0.0526],\n",
       "          [-0.0250,  0.5571, -0.3345,  ...,  1.1299,  0.3540,  0.0626],\n",
       "          [ 0.2441,  0.1750, -0.2111,  ...,  0.6890,  0.0989,  0.2280],\n",
       "          ...,\n",
       "          [ 0.2438,  0.3850, -0.3665,  ...,  0.6455,  0.0736,  0.4436],\n",
       "          [-0.1437,  0.2756, -0.3621,  ...,  0.9004,  0.5938,  0.0345],\n",
       "          [ 0.1047,  0.4392,  0.1438,  ...,  0.8101,  0.1832, -0.1843]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2263,  0.2869, -0.3374,  ...,  0.2944,  0.3467,  0.0921],\n",
       "          [ 0.2573,  0.4868, -0.3894,  ...,  0.3906,  0.3022, -0.0216]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.4661e-01, 7.9248e-01, 2.1160e-05, 1.1921e-07, 2.9802e-07, 8.4937e-05,\n",
       "           1.0133e-05, 6.1096e-02],\n",
       "          [3.2227e-01, 2.0801e-01, 2.4354e-04, 7.9274e-06, 2.5630e-06, 6.2180e-04,\n",
       "           2.2519e-04, 4.6875e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    76.808838  206.606934  186.494019  477.974243    0.938381      0  person\n",
       "  1   154.221710  255.232605  248.820831  479.959656    0.900898      0  person\n",
       "  2   542.800049  116.011139  618.464600  259.194305    0.868425     46  banana\n",
       "  3   521.970947  425.022491  612.460083  467.739319    0.846463     47   apple\n",
       "  4     8.236317  181.548767   61.407314  250.352966    0.839600     46  banana\n",
       "  5   324.848572  155.064529  386.620300  229.215027    0.839043     46  banana\n",
       "  6   362.501129  300.571350  449.911652  341.731873    0.776800     47   apple\n",
       "  7   247.117218  339.413208  315.212311  389.933228    0.442912     49  orange\n",
       "  8   554.119812  386.332886  571.158997  401.772095    0.342124     49  orange\n",
       "  9   450.993073  208.942017  488.227081  245.819580    0.316928     46  banana\n",
       "  10  544.020020  367.783386  563.622925  387.249207    0.313108     49  orange\n",
       "  11  247.633514  339.002808  316.149078  389.656128    0.255631     47   apple\n",
       "  12  268.635651  374.119629  286.302521  389.558655    0.255112     49  orange\n",
       "  13  571.920105  387.361023  597.094910  404.031250    0.251973     49  orange,\n",
       "  'caption': ['A woman in a yellow shirt at a produce stand.',\n",
       "   'A woman with a yellow shirt is standing next to a man in a fruit market'],\n",
       "  'bbox_target': [153.17, 256.39, 101.39, 215.73]},\n",
       " 513: {'image_emb': tensor([[ 0.0540, -0.2219, -0.0109,  ...,  0.8345,  0.0523,  0.1533],\n",
       "          [ 0.0689, -0.2742, -0.1488,  ...,  0.7266, -0.0375,  0.0577],\n",
       "          [ 0.1002, -0.2192, -0.1453,  ...,  0.4409,  0.0199,  0.1466],\n",
       "          [ 0.0767, -0.1448, -0.2047,  ...,  0.5410, -0.1281,  0.2673]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0707, -0.0664, -0.0918,  ...,  0.1123, -0.0753, -0.1171],\n",
       "          [ 0.0757, -0.0960, -0.0028,  ...,  0.2029, -0.0720, -0.1251]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1340, 0.1183, 0.1832, 0.5645],\n",
       "          [0.1567, 0.0893, 0.3477, 0.4065]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  421.989136  177.174561  639.476929  421.770996    0.944559     20  elephant\n",
       "  1  235.528824  155.130737  539.858582  418.109131    0.936449     20  elephant\n",
       "  2   31.538719  186.111206  215.249939  385.702637    0.935962     20  elephant,\n",
       "  'caption': ['The elephant in the middle of two other elephants.',\n",
       "   'elephant in the middle of two other elephants.'],\n",
       "  'bbox_target': [236.35, 156.39, 294.48, 256.36]},\n",
       " 514: {'image_emb': tensor([[ 0.1428,  0.2734, -0.3357,  ...,  0.0898,  0.1178,  0.1788],\n",
       "          [-0.0346, -0.0501, -0.3047,  ...,  0.8345,  0.1565,  0.0341],\n",
       "          [ 0.2281, -0.0052, -0.3701,  ...,  0.3394,  0.2373, -0.1249],\n",
       "          [ 0.1851,  0.2598, -0.3054,  ...,  0.6011,  0.0112, -0.0784],\n",
       "          [ 0.1935,  0.2615, -0.2067,  ...,  0.6777,  0.1395, -0.0933],\n",
       "          [-0.0090, -0.0045, -0.3438,  ...,  0.8960,  0.0659,  0.0459]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0747, -0.2734, -0.1996,  ..., -0.3037, -0.1132, -0.3962],\n",
       "          [ 0.1183, -0.3599,  0.1263,  ..., -0.1526, -0.1179, -0.2092]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.8147e-02, 5.1855e-01, 3.3975e-06, 2.1458e-06, 6.8545e-06, 4.4336e-01],\n",
       "          [2.6074e-01, 2.8198e-01, 1.4124e-03, 8.1730e-04, 4.2839e-03, 4.5068e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  126.099525  279.361725  438.836212  530.026123    0.894258     53   \n",
       "  1    4.419008    0.399945  611.571777  604.594543    0.861754      0   \n",
       "  2  517.838867  114.741821  611.756104  224.481216    0.789995     56   \n",
       "  3  496.287842   40.515850  584.414368  100.358841    0.740484     56   \n",
       "  4  476.629486    0.438488  604.722412   69.749207    0.727330      0   \n",
       "  5  459.392456  162.707840  528.655640  218.890442    0.644155     56   \n",
       "  6  497.220673   75.941826  611.899719  151.726746    0.615610     60   \n",
       "  7  577.931641   40.415833  612.000000   83.087929    0.379767     56   \n",
       "  8   84.665474  124.093422  179.927765  338.779663    0.275161      0   \n",
       "  \n",
       "             name  \n",
       "  0         pizza  \n",
       "  1        person  \n",
       "  2         chair  \n",
       "  3         chair  \n",
       "  4        person  \n",
       "  5         chair  \n",
       "  6  dining table  \n",
       "  7         chair  \n",
       "  8        person  ,\n",
       "  'caption': ['a kid eating a pizza', 'A yougn girl eating pizza.'],\n",
       "  'bbox_target': [4.77, 2.69, 607.23, 599.29]},\n",
       " 515: {'image_emb': tensor([[ 0.2389, -0.0427, -0.1857,  ...,  0.6079, -0.0682,  0.0417],\n",
       "          [ 0.0289,  0.2500, -0.2172,  ...,  0.9038, -0.3840,  0.2612],\n",
       "          [ 0.1354, -0.1129, -0.2708,  ...,  0.4875, -0.1131,  0.1766]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1688, -0.1097, -0.0648,  ...,  0.0892, -0.1224,  0.3538],\n",
       "          [-0.2285, -0.2396, -0.1289,  ...,  0.4946, -0.0446, -0.0853]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0048, 0.9800, 0.0151],\n",
       "          [0.0927, 0.1307, 0.7764]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  142.595810   42.894928  432.612976  445.012939    0.918631     23  giraffe\n",
       "  1   79.482574  346.132751  332.030914  476.224487    0.906458     22    zebra,\n",
       "  'caption': ['Zebra body.',\n",
       "   'Zebra standing in an open field in front of large giraffe.'],\n",
       "  'bbox_target': [138.55, 345.25, 193.01, 134.75]},\n",
       " 516: {'image_emb': tensor([[ 0.1100,  0.4741,  0.2458,  ...,  0.3640,  0.0948,  0.1215],\n",
       "          [ 0.3423,  0.2930, -0.3000,  ...,  1.0264,  0.2439, -0.0061],\n",
       "          [ 0.1620,  0.4768, -0.0606,  ...,  0.8984,  0.1455,  0.1011],\n",
       "          ...,\n",
       "          [ 0.4004,  0.2568, -0.1066,  ...,  0.8931,  0.5034,  0.0693],\n",
       "          [ 0.1558,  0.3845, -0.0323,  ...,  1.2061,  0.4204,  0.4897],\n",
       "          [ 0.2544,  0.1763, -0.0179,  ..., -0.1008,  0.1311,  0.0955]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0039, -0.0519,  0.0702,  ..., -0.5679,  0.2693, -0.6201],\n",
       "          [ 0.4216,  0.1942, -0.0187,  ...,  0.4106,  0.1627, -0.4192]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.2614e-02, 6.7139e-01, 2.1118e-01, 7.6477e-02, 1.3931e-02, 3.9291e-03,\n",
       "           6.3133e-04],\n",
       "          [2.5635e-01, 8.3694e-03, 1.8978e-03, 2.2278e-01, 1.5640e-02, 6.5565e-04,\n",
       "           4.9414e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  147.188675  306.652924  339.463440  476.905884    0.933609     37   \n",
       "  1  339.671997  232.407135  418.560486  451.698975    0.917689      0   \n",
       "  2  388.379608   67.627960  455.519562  166.003357    0.916432      0   \n",
       "  3  152.591766  241.748108  274.504242  471.169495    0.910860      0   \n",
       "  4  411.631927  136.534225  454.577301  181.569550    0.848159     37   \n",
       "  5  230.541000  277.618378  513.152954  337.160919    0.702516     37   \n",
       "  \n",
       "          name  \n",
       "  0  surfboard  \n",
       "  1     person  \n",
       "  2     person  \n",
       "  3     person  \n",
       "  4  surfboard  \n",
       "  5  surfboard  ,\n",
       "  'caption': ['a man carrying a white board',\n",
       "   'A surfer holding a surf board with two other surfers.'],\n",
       "  'bbox_target': [148.85, 308.49, 190.93, 163.96]},\n",
       " 517: {'image_emb': tensor([[-0.0413,  0.1620, -0.0767,  ...,  0.9829,  0.1747, -0.0316],\n",
       "          [-0.2335, -0.2329, -0.4482,  ...,  1.0430,  0.1282, -0.1438],\n",
       "          [ 0.3682,  0.1222,  0.0404,  ...,  0.9336, -0.2729, -0.0271],\n",
       "          ...,\n",
       "          [ 0.0657,  0.1404, -0.0922,  ...,  1.1318,  0.2384, -0.3594],\n",
       "          [ 0.0269, -0.1864, -0.1255,  ...,  1.0254,  0.2593, -0.5723],\n",
       "          [-0.4224, -0.0275,  0.1946,  ...,  1.2471,  0.1899,  0.1298]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0847, -0.2988, -0.2174,  ...,  0.1886, -0.0872, -0.0341],\n",
       "          [-0.0293,  0.0927, -0.1378,  ..., -0.2646, -0.1420, -0.0544]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.8661e-02, 5.1514e-02, 2.9816e-02, 5.4283e-03, 8.5373e-03, 4.5190e-01,\n",
       "           3.6955e-04, 2.3083e-01, 6.8237e-02, 1.9852e-02, 1.0796e-02, 2.5497e-02,\n",
       "           2.3346e-03, 7.6111e-02],\n",
       "          [1.3049e-01, 2.7344e-02, 8.1152e-01, 2.3026e-02, 3.5167e-05, 1.9817e-03,\n",
       "           1.7881e-07, 1.4267e-03, 1.7481e-03, 2.1095e-03, 5.5313e-05, 3.5167e-05,\n",
       "           9.0599e-06, 1.6868e-05]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   325.329071  122.031036  456.286835  279.835602    0.939446      0   \n",
       "  1   268.040375  102.364105  357.797211  225.348907    0.926260      0   \n",
       "  2   488.720032  148.951660  608.083069  331.870728    0.922998      0   \n",
       "  3   110.754807  141.442352  254.761734  373.731964    0.910293      0   \n",
       "  4     0.000000   20.567474   64.533997  251.258392    0.851152     58   \n",
       "  5   188.672409  188.903687  597.396667  421.007141    0.805379     60   \n",
       "  6   384.110260  317.455902  424.637482  350.737091    0.784406     44   \n",
       "  7    60.978424  328.811890  281.149719  426.492065    0.783809     56   \n",
       "  8   440.834656  309.771698  469.558472  362.389984    0.783684     41   \n",
       "  9    56.050526  114.345779  134.827805  290.758179    0.774097      0   \n",
       "  10  308.933136  240.083832  328.561005  285.529144    0.760916     41   \n",
       "  11  401.510773  258.022522  427.263397  305.612671    0.747434     41   \n",
       "  12  392.689392  245.324280  414.099548  296.789551    0.708893     41   \n",
       "  13   11.564323  245.659149  264.457214  420.458374    0.663155     56   \n",
       "  14  279.030029  209.682678  300.032715  263.910645    0.582676     41   \n",
       "  15  306.355988  324.392609  357.121979  344.858307    0.581470     43   \n",
       "  16    0.000000  208.659363   95.150917  423.276794    0.578977     56   \n",
       "  17  243.840622  194.447723  258.467438  217.003265    0.574140     41   \n",
       "  18  387.897247  355.474792  443.922760  394.730469    0.493354     43   \n",
       "  19  427.369415  279.019104  457.556671  305.206055    0.401891     43   \n",
       "  20  348.103516  258.362091  367.296875  275.254303    0.398260     45   \n",
       "  21  192.379913  178.907196  211.232697  204.909637    0.392134     41   \n",
       "  22  348.149384  258.558472  367.389740  276.158691    0.391184     41   \n",
       "  23  189.518982  177.048157  601.022034  427.000000    0.371457     13   \n",
       "  24  275.673279  203.396332  304.496704  231.776642    0.362886     41   \n",
       "  25  337.542694  151.654846  640.000000  421.748535    0.345816     57   \n",
       "  26  246.961090  275.318481  304.908051  297.353027    0.321538     42   \n",
       "  27  388.640137  355.310425  445.055908  395.025330    0.281562     42   \n",
       "  28  296.745178  232.899506  319.611572  266.787628    0.277597     41   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3         person  \n",
       "  4   potted plant  \n",
       "  5   dining table  \n",
       "  6          spoon  \n",
       "  7          chair  \n",
       "  8            cup  \n",
       "  9         person  \n",
       "  10           cup  \n",
       "  11           cup  \n",
       "  12           cup  \n",
       "  13         chair  \n",
       "  14           cup  \n",
       "  15         knife  \n",
       "  16         chair  \n",
       "  17           cup  \n",
       "  18         knife  \n",
       "  19         knife  \n",
       "  20          bowl  \n",
       "  21           cup  \n",
       "  22           cup  \n",
       "  23         bench  \n",
       "  24           cup  \n",
       "  25         couch  \n",
       "  26          fork  \n",
       "  27          fork  \n",
       "  28           cup  ,\n",
       "  'caption': ['chair that the lady with the blue paisley skirt and gray shirt is sitting in',\n",
       "   'a brown chair with a woman wearing a grey top sitting'],\n",
       "  'bbox_target': [20.74, 247.31, 245.46, 141.57]},\n",
       " 518: {'image_emb': tensor([[-0.5366,  0.2764,  0.1788,  ...,  0.2629,  0.2305, -0.1510],\n",
       "          [-0.2783,  0.4561,  0.1700,  ...,  0.8481,  0.1240, -0.1417],\n",
       "          [ 0.0258,  0.1709, -0.0735,  ...,  0.6870, -0.1570, -0.3552],\n",
       "          [-0.3093, -0.0400,  0.2563,  ...,  0.1541, -0.2288, -0.4426]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2150, -0.1210, -0.0866,  ..., -0.0278, -0.0967, -0.6299],\n",
       "          [ 0.3381,  0.4409, -0.2524,  ..., -0.6479, -0.4263, -0.7715]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0061, 0.6377, 0.1255, 0.2308],\n",
       "          [0.0192, 0.8408, 0.1289, 0.0111]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0    1.567352    0.794098  639.405273  170.417328    0.914313     66  keyboard\n",
       "  1  209.922607  122.186615  309.390930  222.387573    0.896267     55      cake\n",
       "  2  146.860123  204.679718  335.157166  391.873444    0.896155     55      cake,\n",
       "  'caption': ['A small napkin with a cupcake printed on it',\n",
       "   'A picture of a large cupcake with a cherry on top'],\n",
       "  'bbox_target': [149.93, 207.1, 188.77, 183.37]},\n",
       " 519: {'image_emb': tensor([[-0.3840,  0.0073, -0.1017,  ...,  0.7314, -0.1700,  0.1401],\n",
       "          [-0.3818,  0.3606, -0.1998,  ...,  0.8125, -0.1166,  0.1052],\n",
       "          [-0.1143, -0.0999, -0.0920,  ...,  0.5781, -0.2727,  0.0588]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-2.7905e-01, -3.8379e-01,  4.7546e-02, -1.5784e-01, -8.3862e-02,\n",
       "           -2.2607e-01, -2.3621e-01, -5.8643e-01,  7.1716e-02, -1.6968e-02,\n",
       "            2.5220e-01, -4.0234e-01, -5.1086e-02, -1.5918e-01,  8.7524e-02,\n",
       "            1.2939e-01, -4.0375e-02,  6.3210e-03, -3.8574e-01,  7.0215e-01,\n",
       "            3.4082e-01,  4.5093e-01,  1.3745e-01, -4.3408e-01,  7.0068e-02,\n",
       "           -5.7251e-02, -3.4253e-01,  3.7280e-01, -2.5586e-01,  1.5495e-02,\n",
       "           -1.8219e-02, -2.1378e-02, -3.0371e-01,  1.5149e-01, -2.1216e-01,\n",
       "           -3.0420e-01,  9.2468e-02,  3.4692e-01, -2.0764e-01, -1.3847e-02,\n",
       "           -3.3350e-01, -2.4951e-01,  4.8462e-01,  1.6516e-01, -3.6816e-01,\n",
       "            1.2955e-02,  9.7107e-02,  3.3618e-01,  2.8711e-01, -1.8030e-01,\n",
       "           -5.2856e-02, -3.0786e-01,  3.3417e-02, -1.9617e-01, -1.2634e-01,\n",
       "           -1.8860e-01,  1.0419e-01, -6.2988e-01,  1.0211e-01,  1.9177e-01,\n",
       "            1.3916e-01, -9.6375e-02, -1.7847e-01,  1.8018e-01,  2.0715e-01,\n",
       "            2.5464e-01, -3.1006e-02,  1.9445e-03,  6.2842e-01, -1.4941e-01,\n",
       "           -7.7454e-02,  3.6678e-03,  2.3206e-01,  2.3413e-01,  4.4531e-01,\n",
       "           -9.6484e-01, -1.4197e-01, -7.6050e-02, -2.2614e-02, -3.5303e-01,\n",
       "           -7.4341e-02,  4.2749e-01, -4.9878e-01,  3.0005e-01, -1.2720e-01,\n",
       "            2.9688e-01, -1.0278e-01, -1.5396e-02, -2.9053e-01, -8.9539e-02,\n",
       "            6.1859e-02, -1.2170e-01, -8.5840e-01,  4.3831e-03, -2.8027e-01,\n",
       "            3.2324e-01,  9.6252e-02, -5.1709e-01, -5.3253e-03, -1.8799e-01,\n",
       "           -2.5482e-02, -3.8525e-01, -2.7515e-01,  2.2534e-01,  2.2461e-01,\n",
       "           -3.6572e-01,  6.1188e-02,  4.4531e-01,  4.1479e-01, -3.0225e-01,\n",
       "           -5.9776e-03,  1.5027e-01, -1.7578e-01, -1.1090e-01,  1.7273e-01,\n",
       "            2.5830e-01, -5.0195e-01,  3.3356e-02,  7.4036e-02,  2.6855e-01,\n",
       "            7.1106e-02, -1.1104e+00, -1.4929e-01,  9.4238e-02,  2.2388e-01,\n",
       "            1.0413e-01,  8.7585e-02,  2.8052e-01,  1.1035e-01,  3.5278e-01,\n",
       "           -3.0615e-01, -1.2421e-01, -5.2826e-02,  2.9434e+00, -1.9678e-01,\n",
       "           -2.8833e-01, -3.4839e-01, -4.2383e-01, -2.2046e-01, -3.8354e-01,\n",
       "           -2.4780e-01,  1.2512e-01, -2.5879e-01,  2.7173e-01,  1.0742e-01,\n",
       "           -5.9357e-02,  3.1348e-01, -2.0837e-01, -1.0535e-01,  3.9948e-02,\n",
       "            1.6748e-01, -4.4312e-01,  3.8428e-01, -1.8677e-01,  4.3774e-01,\n",
       "           -4.2529e-01,  2.4255e-01,  4.5166e-02, -6.0608e-02,  5.8350e-02,\n",
       "            2.2205e-01, -3.6353e-01, -2.9980e-01, -5.0415e-02,  1.1572e-01,\n",
       "            2.5391e-01,  7.6355e-02, -2.4548e-01,  1.0278e-01,  1.0016e-01,\n",
       "           -3.1158e-02, -5.9784e-02,  2.2229e-01, -1.5784e-01, -1.1139e-02,\n",
       "            4.0308e-01,  2.1069e-01, -7.4341e-02, -6.4893e-01,  1.3660e-01,\n",
       "           -2.8149e-01, -1.8567e-01, -1.3626e-02,  4.1870e-01, -3.6304e-01,\n",
       "           -7.0007e-02, -3.4985e-01, -3.6523e-01, -1.8591e-01,  2.6001e-01,\n",
       "           -6.5308e-02,  2.6196e-01, -2.3718e-01,  4.1962e-02, -4.1943e-01,\n",
       "            6.7749e-02, -1.2939e-01,  3.8013e-01, -2.4146e-01, -2.1802e-01,\n",
       "           -8.9600e-02, -2.2021e-01,  1.8762e-01, -4.9927e-02,  6.7932e-02,\n",
       "            1.5198e-01,  1.5540e-01, -8.4229e-02,  3.7354e-01, -4.8486e-01,\n",
       "            1.4734e-01,  7.0898e-01,  3.3984e-01, -2.5848e-02,  3.4277e-01,\n",
       "           -4.6906e-02,  8.0078e-02,  8.6212e-03,  6.3086e-01,  1.0706e-01,\n",
       "            9.9792e-02, -2.5049e-01,  5.0720e-02,  3.7817e-01,  5.4169e-03,\n",
       "            3.2471e-02, -5.1514e-01, -1.1926e-01, -1.1023e-01, -3.0737e-01,\n",
       "            1.1664e-01, -4.7668e-02,  2.3645e-01,  3.1152e-01,  6.1890e-02,\n",
       "           -3.9673e-01,  6.4514e-02,  3.7872e-02, -4.8804e-01, -3.1616e-01,\n",
       "            2.8247e-01,  2.5562e-01, -2.3877e-01,  3.8159e-01, -1.5918e-01,\n",
       "           -3.1891e-02, -9.0790e-03, -4.9878e-01, -5.6091e-02,  7.1289e-02,\n",
       "           -4.6436e-01,  7.2607e-01, -8.6365e-02,  1.0413e-01, -7.4658e-01,\n",
       "           -3.8477e-01,  2.3413e-01, -8.1238e-02,  2.0422e-01,  6.1279e-01,\n",
       "           -1.4795e-01,  5.9967e-02,  2.1680e-01, -8.2397e-02,  3.7134e-01,\n",
       "            1.4673e-01,  2.0703e-01, -2.5195e-01,  2.2302e-01,  3.5431e-02,\n",
       "           -5.5127e-01, -4.3823e-02, -1.5125e-01,  6.6528e-03,  2.2864e-01,\n",
       "            1.0175e-01, -6.5491e-02,  1.5259e-01,  1.7297e-01,  5.4359e-03,\n",
       "            3.8599e-01,  4.3091e-01, -2.3834e-02, -4.5685e-02,  8.0811e-02,\n",
       "            1.2262e-01, -9.1797e-02,  1.2067e-01,  3.0273e-01, -1.9055e-01,\n",
       "           -6.8176e-02,  1.8585e-02,  3.0609e-02, -1.0767e-01, -2.0737e-02,\n",
       "            4.2676e-01,  1.4636e-01,  1.9678e-01,  1.0126e-01, -1.7944e-01,\n",
       "           -1.3171e-01, -8.8928e-02,  4.2456e-01,  6.7041e-01,  1.0645e-01,\n",
       "           -1.9922e-01,  4.9561e-02, -3.6353e-01,  3.7427e-01, -1.3135e-01,\n",
       "            6.3037e-01,  1.9116e-01,  2.9355e+00,  9.3536e-03,  8.6548e-02,\n",
       "            2.1265e-01,  1.3733e-01, -1.4124e-01,  6.2073e-02, -3.3960e-01,\n",
       "            3.9062e-02,  2.5195e-01,  3.6841e-01,  4.2944e-01,  2.8345e-01,\n",
       "            1.0974e-01, -2.2375e-01, -3.5889e-01,  4.8950e-01, -9.7705e-01,\n",
       "           -2.0981e-02,  4.2267e-02,  5.1605e-02, -1.9580e-01, -1.1139e-01,\n",
       "           -3.2910e-01, -4.9408e-02,  9.8267e-02,  5.5145e-02, -9.5886e-02,\n",
       "           -3.0078e-01, -2.5711e-02, -1.6272e-01, -4.0741e-02,  2.3193e-01,\n",
       "           -7.1289e-02,  2.5146e-01, -8.9417e-02, -3.0493e-01,  6.6467e-02,\n",
       "            1.4941e-01,  1.0797e-01, -3.9368e-02, -3.5065e-02, -7.2632e-03,\n",
       "           -3.2764e-01,  2.0679e-01, -3.0457e-02,  1.2024e-01,  3.5327e-01,\n",
       "            4.1211e-01,  1.2537e-01,  3.0121e-02, -2.0554e-02,  3.7231e-01,\n",
       "           -5.2783e-01, -1.1768e-01, -2.8247e-01,  4.5874e-01,  2.2693e-01,\n",
       "            3.7567e-02, -1.4746e-01,  7.7637e-02, -5.2582e-02,  3.0298e-01,\n",
       "           -1.3168e-02, -1.1530e-01,  1.4000e-02,  3.2104e-02, -2.8931e-02,\n",
       "            2.3941e-02, -8.8135e-02,  2.5610e-01, -8.5693e-02,  2.2205e-01,\n",
       "            3.2959e-01, -2.7561e-03,  1.5515e-01, -4.1577e-01,  1.2901e-02,\n",
       "           -6.4990e-01, -2.2791e-01, -7.0801e-01, -1.9250e-01,  1.4746e-01,\n",
       "           -4.7681e-01,  5.0195e-01,  3.7158e-01, -2.9037e-02,  7.5391e-01,\n",
       "           -1.2091e-01, -1.4856e-01, -2.7084e-02, -2.7267e-02, -2.7856e-01,\n",
       "            3.2544e-01, -6.4148e-02, -2.4255e-01,  7.2900e-01,  6.7017e-02,\n",
       "            1.1810e-01,  9.6191e-02,  1.4819e-01,  4.9023e-01, -1.3965e-01,\n",
       "            2.6538e-01,  6.0010e-01, -7.0374e-02,  9.0027e-02,  8.9722e-02,\n",
       "            6.5079e-03, -5.3809e-01, -2.4963e-01, -4.2139e-01,  4.0747e-01,\n",
       "            5.3662e-01, -1.8091e-01, -4.7437e-01,  6.9275e-02, -1.3293e-01,\n",
       "           -1.1206e-01,  6.8359e-02,  2.1866e-02,  2.3132e-01,  3.8159e-01,\n",
       "           -3.5645e-01, -2.8687e-01,  3.5065e-02, -4.0527e-01, -3.7012e-01,\n",
       "            2.0947e-01, -2.9370e-01,  1.4746e-01,  1.1682e-01,  4.0234e-01,\n",
       "            2.3499e-01,  6.7627e-02, -2.9370e-01,  2.4170e-01, -1.6077e-01,\n",
       "           -3.3887e-01,  5.2783e-01,  6.9885e-02, -2.7275e-03, -2.0459e-01,\n",
       "            1.5100e-01,  4.4189e-01, -3.8525e-01,  2.0288e-01,  2.9712e-01,\n",
       "           -2.6172e-01, -1.5344e-01, -2.0056e-01,  3.5004e-02, -5.7434e-02,\n",
       "           -8.7769e-02,  2.1155e-01,  1.2115e-01,  3.6401e-01, -1.8018e-01,\n",
       "            4.2456e-01,  3.4607e-02,  2.3242e-01,  1.0339e-01,  1.9580e-01,\n",
       "           -1.8652e-01,  2.6514e-01, -2.2668e-01, -3.7280e-01,  3.9258e-01,\n",
       "            6.6162e-02, -1.3550e-01, -1.9348e-01, -4.0680e-02, -1.9397e-01,\n",
       "           -7.1289e-02, -1.5369e-01, -2.6074e-01, -3.4155e-01,  1.9800e-01,\n",
       "           -1.0834e-01, -2.7908e-02, -2.8638e-01,  6.7383e-01,  5.5469e-01,\n",
       "            8.5632e-02,  3.6816e-01,  1.7188e-01, -1.3757e-01, -6.1914e-01,\n",
       "            1.2469e-01, -1.7505e-01,  2.1106e-01,  2.6562e-01,  2.3010e-01,\n",
       "           -1.9312e-01, -2.2375e-01,  1.4465e-01, -2.7893e-02,  4.0576e-01,\n",
       "           -1.1215e-02,  5.2539e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5581, 0.3230, 0.1188]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  100.430832  103.387436  470.781128  389.449097    0.915148     22  zebra\n",
       "  1  328.305450  141.228638  503.410980  387.103516    0.714621     22  zebra,\n",
       "  'caption': ['A zebra standing near shade flicking its tail.'],\n",
       "  'bbox_target': [98.6, 107.81, 359.73, 275.21]},\n",
       " 520: {'image_emb': tensor([[-0.7549,  0.5371, -0.2800,  ...,  1.0977, -0.1379, -0.1285],\n",
       "          [-0.7905,  0.2423, -0.2842,  ...,  0.8613, -0.0214, -0.1328],\n",
       "          [-0.7026,  0.2600, -0.3455,  ...,  1.0693,  0.0533,  0.0249],\n",
       "          ...,\n",
       "          [-0.9561,  0.4478, -0.0568,  ...,  1.1494, -0.1656, -0.0721],\n",
       "          [-0.0254, -0.0977, -0.2913,  ...,  1.3809, -0.0037, -0.2031],\n",
       "          [-1.0322, -0.0194, -0.2285,  ...,  0.2588, -0.0966,  0.2976]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1876, -0.2512, -0.0328,  ..., -0.2245,  0.2205,  0.3550],\n",
       "          [-0.3259, -0.0816, -0.1448,  ..., -0.1080,  0.0056, -0.1779]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.5930e-04, 1.1843e-04, 9.7510e-01, 1.2410e-04, 1.8143e-02, 3.7050e-04,\n",
       "           2.9325e-04, 5.0354e-03, 6.8140e-04],\n",
       "          [2.3670e-03, 5.7459e-05, 1.4648e-01, 2.4962e-04, 7.8888e-03, 1.2994e-05,\n",
       "           4.8399e-05, 5.9605e-08, 8.4277e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  503.762695  154.373291  639.732910  420.763062    0.947914      0   \n",
       "  1  330.261292  109.390900  485.076477  424.000000    0.943601      0   \n",
       "  2  211.383987  253.514465  312.439331  419.267151    0.927679     38   \n",
       "  3    1.643280  139.100571  201.618896  417.757874    0.922471      0   \n",
       "  4  195.946075  164.931854  326.046356  417.236023    0.917314      0   \n",
       "  5    8.273026  269.713257  190.531708  342.028198    0.881321     38   \n",
       "  6  342.221924  273.471008  484.915955  365.033447    0.875645     38   \n",
       "  7  501.383240  358.384857  532.070251  423.466431    0.757812     38   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2  tennis racket  \n",
       "  3         person  \n",
       "  4         person  \n",
       "  5  tennis racket  \n",
       "  6  tennis racket  \n",
       "  7  tennis racket  ,\n",
       "  'caption': ['Woman in pink.',\n",
       "   'An older lady wearing a pink shirt happily poses with friends for a picture to remember the day they had playing tennis together.'],\n",
       "  'bbox_target': [193.86, 165.14, 136.55, 253.07]},\n",
       " 521: {'image_emb': tensor([[-0.2864,  0.4707, -0.1586,  ...,  1.3643,  0.0526, -0.1556],\n",
       "          [-0.2374,  0.3274, -0.1277,  ...,  1.0674, -0.1228,  0.0511],\n",
       "          [-0.2568, -0.0710, -0.2180,  ...,  0.7568, -0.3059,  0.0580]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2729, -0.2061, -0.0069,  ...,  0.1360, -0.2478,  0.3879],\n",
       "          [-0.0830, -0.2646,  0.0341,  ..., -0.0713, -0.3413,  0.3542]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.8218, 0.0089, 0.1696],\n",
       "          [0.9888, 0.0040, 0.0072]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0   33.495270  194.633484  210.744888  302.758911    0.873373     76  scissors\n",
       "  1  313.440491  184.172546  581.798767  344.928894    0.801061     76  scissors,\n",
       "  'caption': ['The blue scissors in the blue bag that is on top of the purple notebook .',\n",
       "   'A pair of blue scissors.'],\n",
       "  'bbox_target': [34.32, 193.31, 196.35, 107.23]},\n",
       " 522: {'image_emb': tensor([[ 0.0056,  0.0972, -0.1234,  ...,  0.9370,  0.3562,  0.2522],\n",
       "          [ 0.2585,  0.1564, -0.3469,  ...,  1.1133,  0.2346,  0.2910],\n",
       "          [ 0.2966,  0.0859, -0.0823,  ...,  1.0537,  0.2434, -0.0751],\n",
       "          [-0.3823,  0.3340,  0.0497,  ...,  1.2432, -0.0907,  0.0541],\n",
       "          [ 0.1520,  0.2159, -0.3167,  ...,  1.4971,  0.2957,  0.3672],\n",
       "          [ 0.0801,  0.2314,  0.2275,  ...,  0.3298,  0.0248,  0.3198]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0052,  0.0949,  0.2168,  ...,  0.2129,  0.1884,  0.0416],\n",
       "          [-0.0678,  0.0515,  0.1543,  ...,  0.1344, -0.2542,  0.0151]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.4512e-01, 1.1921e-07, 3.5763e-07, 5.0664e-06, 2.3246e-06, 7.5488e-01],\n",
       "          [3.7671e-01, 5.5194e-05, 1.9884e-04, 1.1375e-02, 1.3876e-04, 6.1133e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   138.486755  176.994812  389.735962  419.150574    0.903824      7   truck\n",
       "  1   189.723434  237.756866  233.662125  292.144440    0.891566      0  person\n",
       "  2   154.474869  237.497406  191.584366  293.959137    0.879693      0  person\n",
       "  3   385.239746  328.582642  444.397095  392.054626    0.867077      2     car\n",
       "  4   240.964508  245.149933  285.441803  290.186249    0.704812      0  person\n",
       "  5   491.746948  268.629456  535.081299  379.257751    0.676243      7   truck\n",
       "  6   223.611816  239.972717  252.868713  279.732788    0.651027      0  person\n",
       "  7   438.467377  329.829193  490.179230  373.841949    0.609256      2     car\n",
       "  8   517.035889  263.773804  639.455566  412.974487    0.501910      7   truck\n",
       "  9   316.488373  252.380829  334.000641  285.595306    0.416578      0  person\n",
       "  10  516.640747  264.077240  638.240845  414.071991    0.334669      2     car,\n",
       "  'caption': ['an army truck driving down the road',\n",
       "   'a mility truck in the road'],\n",
       "  'bbox_target': [142.32, 177.49, 249.58, 248.54]},\n",
       " 523: {'image_emb': tensor([[ 0.0829,  0.3220, -0.0669,  ...,  1.2080, -0.2537,  0.2874],\n",
       "          [ 0.2727,  0.2981, -0.1243,  ...,  1.3760, -0.2021,  0.1975],\n",
       "          [ 0.1097,  0.1923, -0.2247,  ...,  0.7832, -0.2612,  0.3147],\n",
       "          [ 0.2961,  0.2372, -0.1101,  ...,  0.8828, -0.2231,  0.1991]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-2.0447e-02, -2.1057e-01, -2.8418e-01,  1.8909e-01,  2.4255e-01,\n",
       "            3.3984e-01, -5.1709e-01, -6.3330e-01, -8.5327e-02, -2.8540e-01,\n",
       "            5.1318e-01,  1.2622e-01,  1.1835e-01, -5.9845e-02,  1.9409e-01,\n",
       "            3.1250e-02, -6.7810e-02, -3.6084e-01, -2.8491e-01,  1.1810e-01,\n",
       "            7.5806e-02,  1.8665e-01,  2.8418e-01, -8.5999e-02,  2.2766e-01,\n",
       "            8.4900e-02, -5.0781e-01,  2.7490e-01,  4.3304e-02,  1.7407e-01,\n",
       "           -2.6001e-01,  1.6431e-01, -1.4294e-01, -5.3520e-03, -6.0059e-01,\n",
       "           -2.7173e-01,  1.8750e-01, -3.3643e-01,  4.8462e-01,  1.3220e-01,\n",
       "            4.2017e-01,  1.5320e-01, -3.9337e-02, -2.8711e-01, -1.5662e-01,\n",
       "            4.7394e-02,  1.0071e-02,  6.5613e-02, -2.1606e-01, -3.7036e-01,\n",
       "            1.0980e-01, -3.7134e-01, -3.3997e-02,  3.5187e-02, -4.5459e-01,\n",
       "           -4.8828e-02, -3.9795e-02, -4.7192e-01, -9.6680e-02, -1.2817e-01,\n",
       "           -1.4087e-01,  6.1188e-02,  1.4839e-02,  7.6111e-02,  1.8042e-01,\n",
       "           -1.3159e-01,  1.3306e-01,  2.2400e-01, -3.7500e-01,  2.6880e-01,\n",
       "           -1.6785e-01, -5.7373e-01,  1.0669e-01, -8.2214e-02,  4.3262e-01,\n",
       "           -1.3147e-01, -5.0690e-02, -4.5990e-02, -8.9050e-02,  1.3573e-02,\n",
       "           -3.5278e-01, -4.0063e-01, -1.7883e-01,  2.5928e-01, -1.6809e-01,\n",
       "           -2.1582e-01,  3.4692e-01, -4.5728e-01, -4.6216e-01, -4.3427e-02,\n",
       "            1.1206e-01,  3.9697e-01, -1.3359e+00,  5.6250e-01, -4.9561e-01,\n",
       "            1.9043e-01,  9.9976e-02,  8.9905e-02,  1.6772e-01,  2.1729e-01,\n",
       "            3.5840e-01,  2.5854e-01,  1.8115e-01, -4.8523e-02,  1.7487e-02,\n",
       "            1.7407e-01, -1.5051e-01, -1.2097e-01,  2.0068e-01,  1.4502e-01,\n",
       "           -7.9285e-02,  2.8101e-01,  1.7468e-01, -4.5850e-01,  2.5177e-02,\n",
       "           -3.3264e-02,  1.5161e-01,  1.1658e-01, -5.2832e-01,  1.5771e-01,\n",
       "           -2.1021e-01, -7.5830e-01, -4.7852e-02,  2.8076e-01,  4.6460e-01,\n",
       "           -8.7341e-02, -1.2024e-01, -2.0105e-01, -4.3365e-02,  2.3712e-02,\n",
       "            2.6150e-03,  8.0322e-02,  7.7271e-02,  5.2539e+00, -2.6825e-02,\n",
       "           -2.7368e-01, -3.1689e-01, -3.6401e-01, -3.6572e-01, -3.5596e-01,\n",
       "           -5.4871e-02,  1.1749e-01, -1.9080e-01,  1.6370e-01, -3.0249e-01,\n",
       "            1.6064e-01,  6.4758e-02, -4.0503e-01,  1.6907e-02,  2.1408e-02,\n",
       "           -4.1724e-01, -1.3269e-01,  1.0156e-01,  4.2358e-01, -1.6272e-01,\n",
       "           -2.8198e-01, -2.6245e-01, -2.4731e-01, -1.0760e-01,  2.9932e-01,\n",
       "            7.0190e-04,  1.6138e-01, -2.8076e-01,  3.7280e-01, -8.8013e-02,\n",
       "           -6.8588e-03,  4.3188e-01,  1.1340e-01,  2.6392e-01, -2.9160e-02,\n",
       "            3.6011e-02, -2.6025e-01,  1.3391e-01,  4.2664e-02, -1.8262e-01,\n",
       "            3.4961e-01,  2.4255e-01,  1.2250e-01,  6.0463e-03,  2.3584e-01,\n",
       "            1.2415e-01, -4.5630e-01, -4.5532e-01, -1.9580e-01, -3.0249e-01,\n",
       "           -1.1676e-01, -1.3477e-01, -5.3223e-01,  2.5366e-01,  8.4717e-02,\n",
       "            2.6514e-01,  9.1980e-02, -4.6814e-02,  2.4890e-01, -6.6833e-02,\n",
       "            7.0984e-02, -2.6489e-01,  5.1611e-01, -3.2300e-01, -1.3794e-01,\n",
       "            2.4280e-01, -7.2754e-02,  1.7957e-01,  4.6295e-02, -1.3123e-01,\n",
       "            2.0581e-01, -1.6418e-01, -7.2998e-02, -2.2473e-01, -3.1494e-01,\n",
       "            3.4595e-01,  2.3730e-01,  1.2299e-01,  2.4500e-01,  1.0437e-02,\n",
       "            2.7100e-01, -8.0505e-02, -2.7612e-01, -9.9121e-02, -2.9810e-01,\n",
       "           -5.4321e-02, -4.2407e-01, -3.8037e-01,  5.7098e-02, -1.8591e-01,\n",
       "           -2.7441e-01, -3.5229e-01,  2.0837e-01,  6.7749e-02,  9.6313e-02,\n",
       "           -1.4807e-01,  1.5198e-01,  3.2739e-01,  5.7922e-02,  1.8445e-01,\n",
       "            6.9275e-02,  4.0283e-01, -5.9998e-02,  1.9150e-02,  1.1377e-01,\n",
       "            1.1884e-01,  2.4170e-01, -1.8152e-01,  2.3743e-01, -1.7712e-01,\n",
       "            6.8542e-02, -7.6538e-02,  3.4912e-01,  2.1729e-01,  4.4800e-01,\n",
       "           -5.1953e-01, -6.0742e-01,  3.3813e-01, -4.6082e-02, -2.1899e-01,\n",
       "           -2.1204e-01, -6.4819e-02,  3.3813e-01,  8.4610e-03, -2.7832e-01,\n",
       "            2.3230e-01,  3.1226e-01,  2.8648e-03,  2.3059e-01,  1.8835e-01,\n",
       "            1.7261e-01,  2.2388e-01, -2.6382e-02,  2.4243e-01,  3.1616e-01,\n",
       "           -1.7090e-01, -1.5289e-02,  1.8872e-01, -1.2964e-01, -6.0400e-01,\n",
       "            4.9316e-02, -1.0553e-01,  2.0886e-01,  1.2561e-01,  2.5171e-01,\n",
       "            4.1113e-01, -7.5035e-03, -3.3594e-01,  2.0203e-01, -2.2363e-01,\n",
       "            1.5735e-01, -1.3977e-01, -2.7319e-01,  6.4697e-02,  1.2146e-02,\n",
       "            2.6611e-01,  9.1980e-02,  2.5391e-01, -9.4482e-02, -2.3779e-01,\n",
       "            2.2766e-01, -2.4792e-01, -5.7800e-02,  2.4390e-01, -2.1191e-01,\n",
       "           -2.1301e-01, -1.4258e-01, -3.2983e-01, -2.3352e-01, -3.4546e-02,\n",
       "           -1.0529e-01, -7.2205e-02, -7.1716e-02, -4.5068e-01, -2.7075e-01,\n",
       "           -4.4189e-01,  4.3774e-01,  5.2500e+00,  1.2646e-01, -3.2300e-01,\n",
       "           -1.2512e-01,  5.7922e-02, -1.5796e-01,  4.3018e-01,  3.4473e-01,\n",
       "            1.9946e-01,  2.9858e-01,  3.4839e-01, -2.2034e-01, -4.7803e-01,\n",
       "           -3.4326e-01,  3.1982e-01, -1.1719e-02,  2.7808e-01, -2.1797e+00,\n",
       "            1.0925e-01, -1.5161e-01,  2.3590e-02, -2.5732e-01,  1.2079e-01,\n",
       "            2.2998e-01,  2.2046e-01, -1.3049e-01, -4.4823e-03,  1.1536e-01,\n",
       "           -1.3074e-01, -1.7548e-02,  3.3887e-01, -1.8140e-01, -1.1243e-01,\n",
       "           -1.4331e-01,  7.4951e-02,  3.3325e-01, -2.5220e-01,  1.5210e-01,\n",
       "           -2.5195e-01, -2.0984e-01,  1.1658e-02,  1.0315e-01, -3.5278e-01,\n",
       "           -2.3145e-01, -4.8901e-01,  2.3880e-02,  1.0034e-01, -1.5289e-02,\n",
       "            1.5186e-01, -1.7310e-01, -2.0203e-01, -2.8711e-01, -1.3257e-01,\n",
       "            1.0620e-01,  3.1836e-01,  2.1448e-01,  3.6377e-02, -2.6660e-01,\n",
       "           -5.5328e-02,  3.1079e-01,  1.2256e-01,  5.2460e-02, -1.4001e-01,\n",
       "           -1.1865e-01,  1.0626e-01, -4.5117e-01,  2.6978e-02, -3.1982e-01,\n",
       "            1.2561e-01,  2.7417e-01, -5.5969e-02, -3.3301e-01, -2.9236e-02,\n",
       "           -3.8361e-02, -2.3865e-01, -3.3875e-02, -1.9629e-01, -8.4106e-02,\n",
       "           -4.8828e-01,  3.7793e-01, -3.2300e-01, -4.1748e-01,  2.3819e-02,\n",
       "            1.7065e-01,  1.0107e-01, -4.2725e-02, -3.7256e-01,  3.5986e-01,\n",
       "            1.1584e-01,  3.8391e-02,  2.0776e-01,  4.5728e-01, -8.9905e-02,\n",
       "            2.6535e-02, -1.1627e-01, -1.3892e-01, -1.4648e-01,  1.9128e-01,\n",
       "            1.6016e-01, -2.8595e-02,  1.4542e-02,  1.9775e-01,  2.6270e-01,\n",
       "           -5.0781e-01, -5.4131e-03, -4.1382e-01,  3.5107e-01, -5.9814e-02,\n",
       "           -2.2156e-01, -2.5073e-01,  2.0703e-01, -1.1200e-01,  1.0513e-02,\n",
       "           -1.6833e-01,  2.0667e-01,  2.4643e-02,  2.3132e-01,  1.7578e-01,\n",
       "           -2.7026e-01, -2.6270e-01, -2.2327e-01,  3.5156e-02, -4.2920e-01,\n",
       "           -1.4624e-01, -1.1871e-01, -4.6661e-02, -8.1116e-02, -1.5259e-01,\n",
       "            1.4319e-01, -9.7778e-02, -4.1724e-01, -2.3230e-01, -1.3916e-01,\n",
       "            1.1133e-01, -1.8408e-01, -1.1377e-01,  4.5654e-02,  2.5195e-01,\n",
       "           -5.3027e-01, -2.3938e-01, -1.0663e-01,  3.7811e-02, -1.3599e-01,\n",
       "           -2.3572e-01, -1.9434e-01, -7.6599e-02, -1.3757e-01, -2.4512e-01,\n",
       "           -1.5771e-01, -1.0016e-01,  1.3757e-01, -9.1797e-02,  5.5273e-01,\n",
       "            1.0114e-01, -2.5586e-01,  3.2227e-02,  5.0842e-02, -1.2390e-01,\n",
       "            7.4402e-02,  5.8014e-02,  2.4872e-02, -2.5732e-01,  4.6814e-02,\n",
       "            2.5220e-01,  5.4474e-03, -2.3108e-01, -1.3477e-01,  8.6609e-02,\n",
       "           -1.2299e-01, -5.9619e-01,  2.5269e-01, -3.9648e-01,  2.2046e-01,\n",
       "            2.0117e-01,  2.5781e-01,  9.9487e-02, -4.1064e-01,  4.8584e-02,\n",
       "            8.5876e-02, -1.4198e-02, -3.6084e-01,  7.6514e-01,  4.5264e-01,\n",
       "            8.5083e-02,  2.7490e-01,  2.6392e-01,  1.6919e-01,  1.6418e-01,\n",
       "           -3.7891e-01,  3.4332e-02,  1.7017e-01, -2.0920e-02,  4.8877e-01,\n",
       "           -1.1391e-02, -3.2129e-01, -4.1809e-02,  2.6871e-02,  1.6525e-02,\n",
       "           -3.8428e-01, -2.7100e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1282, 0.6509, 0.0457, 0.1752]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0   62.119217  324.391571  343.670044  557.147339    0.942111     28  suitcase\n",
       "  1   53.006989  239.278900  283.295135  415.889618    0.920955     28  suitcase\n",
       "  2   71.295807  420.918640  346.151520  639.284119    0.885178     28  suitcase\n",
       "  3  418.214111  158.862198  441.296936  203.321945    0.526749      0    person\n",
       "  4    1.051575    0.892853  480.000000  637.298828    0.489475      2       car\n",
       "  5    0.000000    0.031952  476.662964  640.000000    0.372820     28  suitcase\n",
       "  6  398.349396  156.786087  434.282898  222.771164    0.370897      0    person,\n",
       "  'caption': ['A suitcase below two others.'],\n",
       "  'bbox_target': [76.22, 415.64, 270.39, 224.36]},\n",
       " 524: {'image_emb': tensor([[-5.2643e-02, -1.1176e-01,  3.0420e-01,  ...,  6.8701e-01,\n",
       "            2.3645e-01,  1.3062e-02],\n",
       "          [ 7.7820e-04, -6.0852e-02,  2.5073e-01,  ...,  9.0918e-01,\n",
       "            3.2886e-01, -9.7656e-02],\n",
       "          [-5.1392e-02,  4.4556e-01, -3.3960e-01,  ...,  9.5215e-01,\n",
       "            2.0615e-02,  1.2207e-04],\n",
       "          [-9.1064e-02,  2.2632e-01, -2.3523e-01,  ...,  1.2207e+00,\n",
       "           -4.7424e-02,  1.6406e-01],\n",
       "          [ 3.9185e-02, -1.7761e-01,  3.2544e-01,  ...,  6.1377e-01,\n",
       "            6.1279e-01,  2.2009e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0591, -0.0061, -0.1864,  ...,  0.0033, -0.0266, -0.4492],\n",
       "          [-0.4832,  0.1569, -0.0733,  ..., -0.1359, -0.1538, -0.4038]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.6919e-01, 7.8223e-01, 8.5175e-05, 2.9802e-07, 4.8462e-02],\n",
       "          [2.0599e-02, 5.9277e-01, 3.7085e-01, 1.1200e-02, 4.5242e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  205.110077   99.978119  640.000000  477.094971    0.954246      0  person\n",
       "  1  131.987000  192.390381  261.005890  421.840698    0.940554     18   sheep\n",
       "  2  519.325073    0.971069  639.810059  243.133850    0.896504      0  person\n",
       "  3  425.177063    0.000000  468.374512   89.855438    0.753038      0  person\n",
       "  4  351.821564  281.373901  493.373688  477.089966    0.613191     18   sheep\n",
       "  5  417.244843    0.000000  441.411591   86.342575    0.429566      0  person,\n",
       "  'caption': ['A man in white t-shirt is touching a sheep.',\n",
       "   'A man with headphones on his neck.'],\n",
       "  'bbox_target': [203.01, 93.42, 436.81, 386.58]},\n",
       " 525: {'image_emb': tensor([[ 0.0543,  0.4912, -0.2839,  ...,  1.3320, -0.1127, -0.1366],\n",
       "          [ 0.1556,  0.0885, -0.2489,  ...,  1.0322, -0.1289, -0.0342],\n",
       "          [-0.1428,  0.1020, -0.2072,  ...,  1.2500,  0.1896, -0.1208],\n",
       "          [ 0.1906,  0.4211, -0.2444,  ...,  1.0068,  0.1320,  0.0935],\n",
       "          [-0.3738,  0.1887, -0.2230,  ...,  0.8594, -0.1969, -0.0115]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0905,  0.1233, -0.4668,  ..., -0.0475, -0.3289, -0.2607],\n",
       "          [-0.1411,  0.1245, -0.4170,  ...,  0.1158, -0.1748, -0.1885]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.9970e-03, 8.8477e-01, 6.8998e-04, 1.1249e-01, 2.9802e-06],\n",
       "          [1.2293e-03, 9.2676e-01, 1.0574e-04, 7.1472e-02, 6.4802e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  279.740295  112.666565  381.672607  417.280701    0.943352      0  person\n",
       "  1  465.874237  157.028992  566.228149  406.764709    0.933021      0  person\n",
       "  2  377.884857  119.314743  475.070099  405.933105    0.926657      0  person\n",
       "  3  178.941711  103.320480  261.840637  366.850525    0.913288      0  person,\n",
       "  'caption': ['A girl in white tees and shorts.',\n",
       "   'A girl in a white shirt with pigtails standing next to binoculars.'],\n",
       "  'bbox_target': [464.74, 156.55, 99.44, 250.97]},\n",
       " 526: {'image_emb': tensor([[ 0.0221,  0.2484, -0.2335,  ...,  1.0693, -0.0641, -0.0778],\n",
       "          [-0.4478, -0.1151, -0.2534,  ...,  1.1641,  0.2406,  0.1074],\n",
       "          [-0.0935,  0.2361, -0.3152,  ...,  0.8389, -0.1464,  0.1063],\n",
       "          [-0.0640,  0.1169,  0.0540,  ...,  0.7710,  0.0668,  0.0294],\n",
       "          [-0.0426,  0.2034, -0.3098,  ...,  0.9976, -0.1063,  0.0098],\n",
       "          [-0.2581, -0.1545, -0.2399,  ...,  0.6582,  0.1426,  0.1715]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0058,  0.0041, -0.0630,  ...,  0.2471, -0.0715, -0.3447],\n",
       "          [-0.2046,  0.2031, -0.1150,  ..., -0.2151, -0.2815, -0.2355]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.5973e-03, 8.9404e-01, 7.1228e-05, 4.0131e-03, 2.3594e-03, 9.5764e-02],\n",
       "          [1.4076e-03, 5.6787e-01, 2.2626e-04, 3.9648e-01, 1.8358e-03, 3.2043e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   502.372711  226.161407  581.958496  461.558472    0.922720      0  person\n",
       "  1   148.520050  210.607300  331.760956  326.875061    0.922479     17   horse\n",
       "  2   370.768066  211.103516  420.451904  350.750854    0.879303      0  person\n",
       "  3   410.696594  221.702698  565.610718  438.001160    0.811679     17   horse\n",
       "  4   236.937180  170.771362  265.295563  279.205383    0.776846      0  person\n",
       "  5    99.278900  214.620514  131.937988  310.622162    0.661144      0  person\n",
       "  6    72.558464  223.069397  103.047340  312.541809    0.591953      0  person\n",
       "  7   425.640991  233.127075  452.211853  290.568115    0.509269      0  person\n",
       "  8   351.267395  222.671021  373.500183  293.316345    0.502076      0  person\n",
       "  9   615.127075  225.464844  639.516479  312.509216    0.492097      0  person\n",
       "  10  569.185791  212.352997  599.808960  283.168121    0.441095      0  person\n",
       "  11   41.723911  221.583405   64.885818  251.037689    0.423416      0  person\n",
       "  12   56.490639  212.578247   78.031166  247.599731    0.415104      0  person\n",
       "  13    2.345732  216.520630   45.687294  310.449707    0.378721      0  person\n",
       "  14  187.297485  247.991058  209.505737  303.024200    0.300281      0  person\n",
       "  15  324.329620  230.211609  340.063568  292.302368    0.277403      0  person\n",
       "  16  449.302734  234.550049  465.636047  289.314270    0.268697      0  person\n",
       "  17   37.910698  207.454712   55.696075  233.582397    0.264159      0  person\n",
       "  18   98.453049  214.295074  130.633301  249.666656    0.257068      0  person,\n",
       "  'caption': ['A big white horse with a rider on the back',\n",
       "   'A horse on which a man is sitting'],\n",
       "  'bbox_target': [148.03, 210.84, 181.36, 115.71]},\n",
       " 527: {'image_emb': tensor([[-0.4270,  0.1254,  0.2556,  ...,  0.9512, -0.2174, -0.2180],\n",
       "          [-0.3181,  0.3748,  0.2732,  ...,  0.9541, -0.0220, -0.4668],\n",
       "          [-0.1411,  0.1825, -0.0878,  ...,  1.2959,  0.0525, -0.0755],\n",
       "          [-0.0593,  0.1309, -0.2502,  ...,  1.1846, -0.0203,  0.0947],\n",
       "          [-0.2600, -0.0471,  0.2384,  ...,  0.8799, -0.1427, -0.0558]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0095,  0.1368, -0.2114,  ..., -0.1783, -0.3191, -0.2207],\n",
       "          [-0.0945,  0.2140, -0.2529,  ...,  0.4468, -0.1592,  0.2025]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.2308e-02, 9.6338e-01, 6.5565e-07, 5.0068e-06, 1.4397e-02],\n",
       "          [2.8397e-02, 9.4043e-01, 2.8729e-04, 9.9301e-05, 3.0716e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class    name\n",
       "  0   46.844124  13.655079  497.597992  369.260010    0.878097     17   horse\n",
       "  1  295.068695  45.566021  498.837952  348.917450    0.871278     17   horse\n",
       "  2  397.952301   0.000000  475.945190   55.740738    0.787159      0  person\n",
       "  3    0.410253  85.619141   70.971199  225.680588    0.706796      0  person\n",
       "  4  102.959778   0.696754  244.218796  267.381714    0.669081      0  person\n",
       "  5  102.522812   0.000000  242.628616   32.095219    0.371478      0  person\n",
       "  6  474.795685   0.397247  492.699677   43.622841    0.304445      0  person,\n",
       "  'caption': ['a brown horse with its eyes closed.',\n",
       "   'The horse with a dark mane, not the light mane.'],\n",
       "  'bbox_target': [53.6, 21.52, 304.53, 249.31]},\n",
       " 528: {'image_emb': tensor([[ 0.0153,  0.1727, -0.2064,  ..., -0.2717,  0.1249,  0.1854],\n",
       "          [-0.3557,  0.3445,  0.1318,  ...,  0.4558, -0.0280,  0.0499],\n",
       "          [-0.1500,  0.8105,  0.1196,  ...,  0.9443,  0.1168, -0.2019],\n",
       "          [-0.0311, -0.0227, -0.1293,  ...,  0.9971,  0.0068, -0.5342],\n",
       "          [ 0.4321, -0.5000, -0.4805,  ...,  0.6885,  0.3271, -0.1343],\n",
       "          [-0.0630,  0.1298, -0.1981,  ..., -0.0902, -0.0914,  0.0996]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0101, -0.0372, -0.0973,  ..., -0.2012, -0.0603,  0.0113],\n",
       "          [ 0.1907, -0.2805,  0.0936,  ...,  0.0604, -0.3486,  0.0411]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.8271e-01, 1.3818e-01, 7.0286e-04, 7.6008e-04, 2.1577e-05, 7.7515e-02],\n",
       "          [6.8311e-01, 6.5552e-02, 1.9670e-06, 1.6868e-05, 4.1723e-07, 2.5122e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0   101.759384   93.325577  442.875000  418.702087    0.952485      5      bus\n",
       "  1     0.658932  210.024475  110.706116  367.173523    0.920382      5      bus\n",
       "  2   497.592896  299.764679  562.144165  353.947540    0.894231      2      car\n",
       "  3   439.973877  289.310608  464.533813  333.377075    0.826456      2      car\n",
       "  4   183.936478  153.091965  205.115158  174.252090    0.724475      0   person\n",
       "  5   467.720306  292.058105  490.784088  309.454590    0.655146      2      car\n",
       "  6   488.760834  287.774628  506.729523  304.462799    0.555364      2      car\n",
       "  7   506.652039  287.787415  526.371765  303.305908    0.422834      2      car\n",
       "  8    34.816799  293.487854   55.347530  317.998108    0.373134      0   person\n",
       "  9   610.172302  309.868652  637.427551  353.038635    0.302740      1  bicycle\n",
       "  10  629.741577  281.433838  639.937378  311.158203    0.281477      0   person\n",
       "  11  137.202576  148.986328  167.378601  176.781067    0.269954      0   person\n",
       "  12  509.615204  281.110565  531.906738  300.913849    0.251579      2      car,\n",
       "  'caption': ['The bus with 205 on it', 'a red double-decker bus'],\n",
       "  'bbox_target': [103.55, 97.08, 337.62, 316.04]},\n",
       " 529: {'image_emb': tensor([[-0.3770,  0.1500, -0.3159,  ...,  1.0957, -0.0774,  0.0204],\n",
       "          [ 0.1226,  0.4128, -0.2328,  ...,  0.8384, -0.3186,  0.3931],\n",
       "          [-0.2479,  0.5518, -0.1429,  ...,  1.1494,  0.2590, -0.0034],\n",
       "          [-0.1379,  0.2717, -0.1158,  ...,  0.8125, -0.2350,  0.2771]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3945,  0.1836,  0.0894,  ..., -0.2749, -0.0685, -0.1908],\n",
       "          [-0.2800,  0.2246,  0.1300,  ..., -0.3528, -0.0017, -0.1384]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1967, 0.0572, 0.6973, 0.0490],\n",
       "          [0.1533, 0.1462, 0.5610, 0.1396]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  413.733856    0.341165  499.983582  109.803925    0.822468     41   \n",
       "  1   28.383041   11.416531  268.603729  257.641663    0.803085     52   \n",
       "  2   88.340004  125.103737  166.082794  214.542221    0.728964     51   \n",
       "  3  300.335144  136.154053  454.858124  276.508209    0.664751     45   \n",
       "  4   15.663363  101.508354   55.999481  206.114700    0.654342     43   \n",
       "  5    0.159770    0.582105  108.409286  175.116089    0.529922     42   \n",
       "  6  153.541534  100.021935  207.418655  193.167542    0.488582     51   \n",
       "  7    2.411699    1.925921  498.617920  325.650391    0.372415     60   \n",
       "  \n",
       "             name  \n",
       "  0           cup  \n",
       "  1       hot dog  \n",
       "  2        carrot  \n",
       "  3          bowl  \n",
       "  4         knife  \n",
       "  5          fork  \n",
       "  6        carrot  \n",
       "  7  dining table  ,\n",
       "  'caption': ['A white cup in the plate', 'A white cup is in the plate.'],\n",
       "  'bbox_target': [299.19, 137.56, 156.47, 141.0]},\n",
       " 530: {'image_emb': tensor([[-0.4180,  0.0675,  0.0765,  ...,  0.7676,  0.0636,  0.0483],\n",
       "          [-0.3708,  0.0015,  0.0024,  ...,  0.9438,  0.1755, -0.2139],\n",
       "          [-0.2820, -0.0400, -0.1895,  ...,  0.7295, -0.1580, -0.2379],\n",
       "          [-0.1266,  0.0496,  0.2671,  ...,  0.6001,  0.0172,  0.2922]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1960, -0.4990,  0.0146,  ...,  0.4304, -0.1398, -0.6782],\n",
       "          [-0.2898, -0.4719,  0.1102,  ...,  0.3582, -0.4656, -0.5132]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.8477e-06, 5.0781e-01, 1.6248e-04, 4.9219e-01],\n",
       "          [5.2452e-06, 8.5498e-01, 1.2903e-01, 1.6159e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  471.191406  323.971619  632.785767  477.288330    0.915157     56   \n",
       "  1  194.399597  158.003159  415.617859  420.241333    0.912827     69   \n",
       "  2   79.444221  211.904999  227.355408  477.727783    0.846147     69   \n",
       "  3  406.267029  260.711853  638.943787  476.552185    0.680644     60   \n",
       "  \n",
       "             name  \n",
       "  0         chair  \n",
       "  1          oven  \n",
       "  2          oven  \n",
       "  3  dining table  ,\n",
       "  'caption': ['The white stove and propane tank.',\n",
       "   'white four burner stove with blue pot on top.'],\n",
       "  'bbox_target': [197.39, 143.46, 223.28, 282.61]},\n",
       " 531: {'image_emb': tensor([[-0.0765,  0.1714, -0.1477,  ...,  0.5103,  0.0369,  0.0995],\n",
       "          [ 0.1490,  0.3521, -0.2341,  ...,  1.1377,  0.2438,  0.1478],\n",
       "          [ 0.0411, -0.0153, -0.3630,  ...,  0.9375,  0.3008,  0.1235],\n",
       "          ...,\n",
       "          [-0.3018,  0.0942, -0.1278,  ...,  1.1201, -0.1175, -0.3696],\n",
       "          [ 0.0091, -0.0323, -0.1349,  ...,  0.8711,  0.0120, -0.0371],\n",
       "          [-0.0856, -0.0469, -0.3296,  ...,  0.7847,  0.3149,  0.0381]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0803,  0.2891,  0.0467,  ..., -0.1556,  0.0189,  0.0386],\n",
       "          [-0.2266, -0.1405, -0.1116,  ..., -0.2345, -0.1525, -0.0172]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.8811e-06, 7.3004e-04, 5.2595e-04, 9.9609e-01, 3.9279e-05, 6.1393e-06,\n",
       "           1.2808e-03, 1.1482e-03, 1.4007e-05, 4.9651e-05, 1.0431e-05],\n",
       "          [8.9705e-05, 1.9058e-02, 1.4389e-02, 9.6289e-01, 2.1517e-04, 5.9414e-04,\n",
       "           7.5102e-04, 5.0020e-04, 6.5231e-04, 6.7329e-04, 3.5477e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     0.735138  336.595306  342.696014  614.207153    0.909729     53   \n",
       "  1     0.000000   19.442741  188.615784  271.960602    0.901735      0   \n",
       "  2   253.203918  146.138840  291.080383  224.805191    0.881350     40   \n",
       "  3   144.242981   20.522255  291.735229  168.550842    0.853677      0   \n",
       "  4   222.803986  123.449005  259.690887  229.279388    0.850269     39   \n",
       "  5   131.809570  250.435654  345.706055  340.132904    0.846272     53   \n",
       "  6   206.911774   82.749641  237.465668  222.535431    0.831504     39   \n",
       "  7   326.637329   58.552635  359.471741  221.310669    0.820551      0   \n",
       "  8    83.126984  247.601410  128.958923  312.521881    0.773751     43   \n",
       "  9   181.715485  111.382301  208.673309  231.344910    0.726026     39   \n",
       "  10  159.798523  164.152710  200.678894  247.428894    0.624838     41   \n",
       "  11    0.732529  122.304871  357.375610  636.948059    0.602788     60   \n",
       "  12  303.947510  260.111237  359.837463  274.388092    0.397210     43   \n",
       "  13   69.402969  251.617691  111.198990  311.152283    0.366563     43   \n",
       "  \n",
       "              name  \n",
       "  0          pizza  \n",
       "  1         person  \n",
       "  2     wine glass  \n",
       "  3         person  \n",
       "  4         bottle  \n",
       "  5          pizza  \n",
       "  6         bottle  \n",
       "  7         person  \n",
       "  8          knife  \n",
       "  9         bottle  \n",
       "  10           cup  \n",
       "  11  dining table  \n",
       "  12         knife  \n",
       "  13         knife  ,\n",
       "  'caption': ['A woman wearing glasses.', 'a women sitting the table'],\n",
       "  'bbox_target': [146.15, 20.47, 148.37, 130.21]},\n",
       " 532: {'image_emb': tensor([[-3.4253e-01, -1.5205e-02,  6.7902e-04,  ...,  1.2578e+00,\n",
       "            1.2433e-01,  1.8262e-01],\n",
       "          [-3.7915e-01,  2.9932e-01, -3.7891e-01,  ...,  1.0361e+00,\n",
       "            1.3660e-01,  8.3374e-02],\n",
       "          [-2.3083e-01,  3.3374e-01, -5.8691e-01,  ...,  6.6113e-01,\n",
       "            2.4854e-01, -1.6211e-01],\n",
       "          [-2.0361e-01,  2.8345e-01, -5.9082e-01,  ...,  6.6699e-01,\n",
       "            2.3047e-01, -1.6028e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1146, -0.1119, -0.1948,  ...,  0.4104, -0.3442,  0.0552],\n",
       "          [ 0.1689,  0.0822, -0.1216,  ...,  0.1722,  0.0255, -0.1143]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.6684e-05, 9.7217e-01, 1.3443e-02, 1.4305e-02],\n",
       "          [1.4377e-04, 9.2139e-01, 3.4637e-02, 4.3762e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  573.434998   95.467422  639.379333  253.817810    0.852350     43   \n",
       "  1  233.546783  247.582062  458.941193  374.293915    0.816205     50   \n",
       "  2    0.000000    0.000000  640.000000  477.464233    0.715049     60   \n",
       "  3  608.246826  103.880692  639.926147  177.839325    0.687115     44   \n",
       "  4   87.903206   94.144043  236.010284  271.090027    0.651857     52   \n",
       "  5  140.931030  106.637009  298.524780  279.026550    0.316460     52   \n",
       "  \n",
       "             name  \n",
       "  0         knife  \n",
       "  1      broccoli  \n",
       "  2  dining table  \n",
       "  3         spoon  \n",
       "  4       hot dog  \n",
       "  5       hot dog  ,\n",
       "  'caption': ['GREEN BROCCOLI FRIED IN THE WHITE TABLE IN FIRST POSITION WITH ANOTHER ONE',\n",
       "   'Two pieces of broccoli facing opposite directions next to each other.'],\n",
       "  'bbox_target': [225.35, 263.77, 212.96, 114.93]},\n",
       " 533: {'image_emb': tensor([[-0.2153,  0.1121, -0.3108,  ...,  1.1074, -0.2522, -0.2367],\n",
       "          [ 0.1814,  0.3225,  0.1362,  ...,  0.7886,  0.1746,  0.1390],\n",
       "          [ 0.1819,  0.2874, -0.1024,  ...,  0.8774,  0.0743, -0.0288],\n",
       "          [-0.1997,  0.2959, -0.0822,  ...,  1.1963,  0.3494, -0.0556],\n",
       "          [ 0.1533,  0.3296, -0.0762,  ...,  0.8115,  0.2458,  0.0929]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.4546, -0.4106, -0.0049,  ...,  0.2432, -0.1356, -0.0527],\n",
       "          [-0.1559, -0.4607, -0.1331,  ...,  0.3188, -0.3870, -0.0463]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0270e-04, 7.1287e-04, 3.2959e-03, 9.8779e-01, 7.9041e-03],\n",
       "          [1.0891e-03, 1.2732e-03, 2.8248e-03, 9.9023e-01, 4.8065e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  295.248352    0.000000  366.336182  188.757202    0.919130     39  bottle\n",
       "  1   64.796692  231.671448  521.354553  378.393494    0.861659     53   pizza\n",
       "  2  308.033752   72.117462  640.000000  473.627808    0.800633      0  person\n",
       "  3    0.340141  304.273621  193.307800  473.120117    0.713135     69    oven\n",
       "  4  346.907898   93.004440  396.866821  171.881516    0.688169     39  bottle\n",
       "  5  375.838470    0.000000  428.222260  101.075089    0.498971     39  bottle\n",
       "  6  443.555817  191.910034  639.667114  472.785278    0.321171     69    oven,\n",
       "  'caption': ['The burners of the stove under the pan', 'left stove burners'],\n",
       "  'bbox_target': [0.0, 284.38, 201.25, 190.0]},\n",
       " 534: {'image_emb': tensor([[-0.0680,  0.7490, -0.0745,  ...,  1.1523,  0.2417, -0.3562],\n",
       "          [-0.1074,  0.2910, -0.2607,  ...,  0.0367,  0.5127, -0.6978]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 2.5650e-02, -8.2947e-02, -8.2031e-02, -2.3975e-01, -2.0178e-01,\n",
       "           -1.1816e-01, -1.2878e-01, -9.8926e-01, -3.0688e-01,  3.4973e-02,\n",
       "            2.8027e-01, -7.3303e-02,  1.4331e-01, -1.4294e-01, -1.9116e-01,\n",
       "            1.2061e-01,  8.7891e-02, -1.2917e-02,  6.8665e-02,  9.8999e-02,\n",
       "            2.0837e-01,  2.0044e-01,  1.7792e-02,  7.1472e-02, -3.4790e-01,\n",
       "            1.6394e-01, -1.2561e-01, -1.6797e-01,  5.5908e-02,  1.2244e-01,\n",
       "            1.6187e-01, -2.7002e-01,  3.5986e-01, -1.9272e-02, -4.0698e-01,\n",
       "            3.7720e-01,  1.1499e-01,  1.1768e-01,  1.7761e-01,  1.0681e-01,\n",
       "            2.7612e-01, -5.5298e-02, -3.8147e-02,  3.0688e-01, -1.1426e-01,\n",
       "            1.2183e-01, -1.4612e-01,  4.5410e-01,  3.9825e-02, -3.9307e-01,\n",
       "           -4.3518e-02, -3.2617e-01,  4.0430e-01, -2.5293e-01, -5.4657e-02,\n",
       "            5.1483e-02,  5.0879e-01, -1.1810e-02,  4.8755e-01, -1.2927e-01,\n",
       "            1.9714e-01,  3.1519e-01,  1.8762e-01, -3.4497e-01,  2.1680e-01,\n",
       "           -2.8955e-01,  3.8086e-01, -1.2109e-01, -2.7783e-01,  6.7139e-02,\n",
       "            2.8101e-01, -1.2256e-01, -1.2805e-01,  5.2734e-02,  5.7959e-01,\n",
       "           -1.1249e-01, -2.3523e-01,  1.3257e-01, -1.1267e-01, -2.3682e-02,\n",
       "           -1.8066e-02, -1.3647e-01, -2.4792e-01,  2.3193e-01, -5.2528e-03,\n",
       "           -1.7822e-01, -7.9285e-02,  2.8638e-01,  3.1445e-01, -9.7351e-03,\n",
       "            3.1677e-02, -1.9226e-01, -1.4443e+00,  2.4561e-01, -1.7444e-01,\n",
       "            1.7456e-01, -2.5610e-01, -3.0786e-01, -4.8535e-01,  3.0615e-01,\n",
       "            4.5074e-02, -1.5198e-01,  2.0654e-01, -8.5083e-02,  4.7028e-02,\n",
       "           -1.2878e-01, -1.6675e-01,  9.0271e-02, -9.8495e-03, -1.3293e-01,\n",
       "           -1.8994e-01,  1.8384e-01,  1.5234e-01, -2.7515e-01, -1.2854e-01,\n",
       "           -3.4302e-01,  2.1191e-01,  2.3303e-01, -2.1985e-01,  1.9067e-01,\n",
       "           -1.3847e-02, -1.2585e-01,  5.5511e-02,  3.0615e-01, -5.1849e-02,\n",
       "            1.9861e-01,  2.8027e-01,  1.4026e-01,  2.2693e-01,  3.6041e-02,\n",
       "            4.4006e-02,  1.5613e-01,  5.2344e-01,  5.5781e+00, -1.4172e-01,\n",
       "            7.0251e-02, -3.2202e-01, -3.1372e-01, -2.6294e-01,  2.8412e-02,\n",
       "           -4.3530e-01, -2.6416e-01, -1.0138e-01,  1.7163e-01, -1.4612e-01,\n",
       "           -3.7891e-01, -1.4343e-02,  2.8906e-01,  5.9723e-02, -1.8042e-01,\n",
       "           -2.8777e-04, -4.8218e-01,  1.6821e-01,  2.5293e-01,  4.0375e-02,\n",
       "            4.2334e-01,  2.7881e-01, -4.3854e-02, -5.4504e-02, -3.0469e-01,\n",
       "            2.9321e-01,  3.4204e-01, -9.2651e-02,  2.1399e-01,  3.6407e-02,\n",
       "           -1.2988e-01, -1.5979e-01,  1.6174e-01,  1.0809e-01,  3.8727e-02,\n",
       "            1.2915e-01, -6.2744e-02,  7.8659e-03, -5.1178e-02, -1.8750e-01,\n",
       "            1.5479e-01, -1.6858e-01, -1.8372e-01,  1.5417e-01, -3.0640e-01,\n",
       "            2.2253e-01,  7.3181e-02,  2.7173e-01,  9.3933e-02,  4.5288e-02,\n",
       "           -3.6328e-01,  8.4412e-02,  4.1772e-01, -1.7737e-01,  3.3276e-01,\n",
       "           -3.1299e-01,  3.4009e-01, -3.1812e-01, -3.7201e-02, -2.3828e-01,\n",
       "            6.4514e-02, -5.7080e-01,  2.9688e-01,  8.1329e-03,  1.5442e-01,\n",
       "           -1.1847e-01,  9.5291e-03,  2.9468e-01,  5.4596e-02, -1.5906e-01,\n",
       "           -6.1676e-02,  1.1469e-01,  1.1543e-02, -4.4952e-02, -4.3408e-01,\n",
       "           -1.2720e-01, -3.7880e-03, -1.4343e-01,  2.6147e-01,  1.3924e-02,\n",
       "           -1.2732e-01,  1.6516e-01, -5.6445e-01, -2.0874e-01, -9.5520e-02,\n",
       "           -9.2041e-02, -8.2886e-02, -8.8562e-02,  3.1714e-01,  1.8201e-01,\n",
       "            1.0608e-01, -1.4941e-01, -4.5227e-02,  1.9867e-02, -2.1143e-01,\n",
       "           -3.1030e-01, -2.5195e-01,  3.8501e-01,  3.4082e-01,  2.2156e-01,\n",
       "           -1.2636e-03, -4.8065e-02, -2.6685e-01,  1.3867e-01, -1.0022e-01,\n",
       "           -5.4169e-02, -2.6733e-01,  4.1107e-02, -1.1658e-01, -3.3545e-01,\n",
       "            3.8788e-02, -2.4060e-01,  2.7539e-01,  4.4006e-02,  6.7627e-02,\n",
       "            3.6353e-01,  2.4475e-01,  1.9202e-01,  2.4976e-01,  1.8402e-02,\n",
       "            1.6724e-01,  1.3220e-01, -1.7896e-01,  3.2007e-01,  2.5366e-01,\n",
       "           -1.4795e-01, -8.0017e-02, -3.5034e-01,  2.9039e-04,  2.6880e-01,\n",
       "            1.1591e-01, -5.0110e-02, -6.5308e-03, -2.6276e-02, -1.4209e-01,\n",
       "            1.8420e-01,  5.4297e-01, -5.5939e-02, -2.6074e-01, -2.8760e-01,\n",
       "           -3.1665e-01, -3.8062e-01, -4.9683e-02,  1.3321e-02,  3.3081e-01,\n",
       "            2.6074e-01,  1.4124e-01, -8.8379e-01,  3.5083e-01, -9.9182e-02,\n",
       "           -1.5602e-02,  1.8652e-01,  1.9556e-01,  2.0081e-01, -1.4221e-02,\n",
       "            1.9800e-01, -1.4001e-01, -2.3132e-01,  1.1749e-01,  2.4182e-01,\n",
       "            2.5903e-01, -2.4744e-01, -5.1544e-02,  1.4185e-01, -1.0614e-01,\n",
       "           -3.3032e-01, -1.5356e-01,  2.7856e-01,  3.0420e-01, -1.2329e-01,\n",
       "            1.9116e-01, -1.6296e-01,  5.4053e-01, -2.4307e-02, -3.6346e-02,\n",
       "            1.4795e-01, -6.4812e-03,  5.5664e+00, -1.1755e-01,  6.7383e-02,\n",
       "           -1.1920e-01,  3.4180e-02, -5.9619e-01,  2.8027e-01,  3.2324e-01,\n",
       "            4.2822e-01,  3.4839e-01,  1.9556e-01,  1.7493e-01, -6.0059e-01,\n",
       "           -4.7089e-02,  2.0752e-01,  2.0471e-01, -1.5540e-01, -2.4688e+00,\n",
       "           -9.9854e-02, -3.0005e-01, -2.6154e-02, -2.7374e-02, -1.6138e-01,\n",
       "           -4.3994e-01,  1.8823e-01, -4.3286e-01,  8.9600e-02, -1.3635e-01,\n",
       "            4.6112e-02, -1.2671e-01,  3.3936e-01, -5.9766e-01, -1.3574e-01,\n",
       "            4.0967e-01, -9.3750e-02, -2.2644e-01, -4.2651e-01, -3.7158e-01,\n",
       "           -6.0107e-01, -2.2559e-01,  6.7773e-01, -1.2988e-01, -2.0007e-01,\n",
       "            9.5062e-03, -2.7539e-01,  2.8052e-01, -1.0718e-01,  6.7261e-02,\n",
       "            2.6929e-01,  1.4087e-01,  5.5420e-02, -2.2070e-01, -1.0535e-01,\n",
       "           -3.3989e-03,  4.0558e-02,  3.7305e-01, -3.9429e-01,  4.2944e-01,\n",
       "           -1.8176e-01,  2.3596e-01,  3.7048e-02, -2.0828e-02,  7.4097e-02,\n",
       "           -2.3901e-01, -1.9153e-01,  2.5098e-01,  4.6936e-02,  3.4698e-02,\n",
       "           -2.0569e-01,  4.6631e-01,  5.0049e-01,  3.1055e-01, -2.9224e-01,\n",
       "            2.8247e-01, -2.8763e-02, -2.8394e-01,  1.1847e-01, -2.6489e-01,\n",
       "           -1.3809e-03, -1.0065e-01,  1.9312e-03,  2.3230e-01, -1.0669e-01,\n",
       "            2.0105e-01,  9.3506e-02, -4.4342e-02,  3.7720e-01, -2.0288e-01,\n",
       "           -1.6284e-01, -5.0140e-02, -2.6782e-01,  2.8030e-02, -3.8647e-01,\n",
       "           -7.4890e-02, -2.6855e-01, -6.5247e-02,  4.0210e-01, -3.1226e-01,\n",
       "            3.5449e-01, -3.4576e-02, -2.6489e-02,  1.2726e-02, -3.9453e-01,\n",
       "           -7.7148e-02, -4.4434e-02,  1.0014e-03, -7.2365e-03,  5.1605e-02,\n",
       "           -1.9180e-02,  1.0693e-01,  1.3464e-01, -5.3613e-01,  2.9129e-02,\n",
       "            1.9791e-02,  1.6089e-01,  2.3972e-02,  4.1504e-01, -1.2347e-01,\n",
       "           -3.2349e-01, -2.9980e-01, -6.4160e-01,  1.1871e-01, -2.1277e-01,\n",
       "           -5.3711e-02, -9.5093e-02, -6.5552e-02,  1.1328e-01, -4.2676e-01,\n",
       "            1.6309e-01, -1.0748e-01, -3.0566e-01,  2.0032e-01,  3.4973e-02,\n",
       "            2.8687e-01,  3.5858e-02, -6.4331e-02,  3.3447e-02,  1.9946e-01,\n",
       "            5.5420e-01, -1.8408e-01, -7.8369e-02,  4.2139e-01,  2.4146e-01,\n",
       "            6.2164e-02, -1.8677e-01, -1.7798e-01,  1.1029e-01, -9.7900e-02,\n",
       "            3.6926e-02, -8.7036e-02, -1.7163e-01, -9.2957e-02,  1.5976e-02,\n",
       "            1.3733e-01,  2.8760e-01,  2.9956e-01,  1.3306e-01,  3.3521e-01,\n",
       "           -1.9885e-01,  3.8647e-01, -2.2864e-01,  1.6785e-01,  2.3389e-04,\n",
       "            2.3804e-01, -2.1802e-01,  1.1456e-01, -2.1582e-01,  7.1411e-02,\n",
       "           -2.1790e-02, -4.8169e-01,  1.0217e-01,  9.6558e-02, -4.4128e-02,\n",
       "            1.1951e-01,  1.8335e-01,  1.7761e-01, -2.9907e-01, -9.3628e-02,\n",
       "           -5.8319e-02,  3.8892e-01, -7.1143e-01,  5.1318e-01, -1.8982e-01,\n",
       "           -1.5930e-01, -9.1614e-02, -8.5510e-02,  3.4332e-02,  1.6956e-01,\n",
       "           -3.1665e-01, -2.5537e-01,  2.6709e-01, -3.3569e-02,  6.4795e-01,\n",
       "           -2.0459e-01,  7.2266e-02, -1.5100e-01,  5.1910e-02,  2.5269e-01,\n",
       "            4.6997e-01, -4.8193e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.4534, 0.5469]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  149.283630    1.159865  261.733276  272.373474    0.805967      9   \n",
       "  1   14.006257    2.300829  145.714386  276.290955    0.681096      9   \n",
       "  2  449.094574  256.057251  479.059692  307.353546    0.599852      9   \n",
       "  \n",
       "              name  \n",
       "  0  traffic light  \n",
       "  1  traffic light  \n",
       "  2  traffic light  ,\n",
       "  'caption': ['Opposing traffic signal.'],\n",
       "  'bbox_target': [14.06, 0.0, 172.87, 291.15]},\n",
       " 535: {'image_emb': tensor([[-0.4417, -0.0206, -0.1448,  ...,  1.0742, -0.2323, -0.3420],\n",
       "          [-0.2937,  0.2937, -0.2644,  ...,  0.8350, -0.0162, -0.1907],\n",
       "          [-0.7476, -0.0256, -0.2600,  ...,  0.7773,  0.0020, -0.2959],\n",
       "          [-0.1083,  0.2070, -0.2269,  ...,  0.5879,  0.0109,  0.1434],\n",
       "          [-0.7744, -0.2568, -0.1235,  ...,  0.5776,  0.1663, -0.0380]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3276, -0.2360, -0.2771,  ...,  0.4124, -0.1104, -0.1477],\n",
       "          [-0.3735, -0.0196,  0.0193,  ..., -0.1932, -0.1771,  0.0916]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.6738e-01, 4.4670e-03, 1.3077e-04, 5.9605e-08, 4.2822e-01],\n",
       "          [2.2437e-01, 1.2268e-02, 6.3916e-01, 9.2041e-02, 3.2318e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  183.122803  114.831558  307.689270  479.280396    0.938936      0   \n",
       "  1  547.384766  268.051727  639.828613  477.644897    0.933794      0   \n",
       "  2  171.872620  324.394653  233.328552  444.140442    0.895722     38   \n",
       "  3  542.483887  396.789612  569.192627  479.788391    0.791863     38   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2  tennis racket  \n",
       "  3  tennis racket  ,\n",
       "  'caption': ['An adult man in a black shirt and blue shorts playing tennis with a little girl.',\n",
       "   'a man was playing tennes'],\n",
       "  'bbox_target': [186.61, 116.31, 122.96, 358.12]},\n",
       " 536: {'image_emb': tensor([[ 0.1150,  0.3525,  0.0189,  ...,  0.7598,  0.3389, -0.1569],\n",
       "          [-0.0045,  0.3391, -0.0898,  ...,  0.7808,  0.2341, -0.1886]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1415, -0.1296,  0.1702,  ..., -0.3184, -0.0852, -0.1017],\n",
       "          [-0.1122,  0.2874,  0.2024,  ...,  0.0514,  0.1207, -0.3682]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3999, 0.6001],\n",
       "          [0.9072, 0.0927]], dtype=torch.float16),\n",
       "  'df_boxes':         xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  12.760742  101.501923  637.147339  417.791870    0.883320     53   \n",
       "  1   6.676178    0.451706  636.087036  419.091980    0.629911     60   \n",
       "  2   0.084785   30.458191   80.064003  137.690216    0.624183     53   \n",
       "  \n",
       "             name  \n",
       "  0         pizza  \n",
       "  1  dining table  \n",
       "  2         pizza  ,\n",
       "  'caption': ['The pizza on the plate.', 'A pizza with onions.'],\n",
       "  'bbox_target': [23.14, 98.33, 616.86, 326.81]},\n",
       " 537: {'image_emb': tensor([[-0.2241,  0.2954, -0.2185,  ...,  0.7764,  0.3630,  0.0663],\n",
       "          [-0.2280,  0.1187, -0.3284,  ...,  1.2451,  0.1511,  0.0439],\n",
       "          [-0.0304,  0.1244, -0.3130,  ...,  1.0352,  0.1774,  0.1541],\n",
       "          ...,\n",
       "          [ 0.0898, -0.1656, -0.2620,  ...,  0.9009,  0.2095, -0.2241],\n",
       "          [ 0.0456,  0.1339, -0.2861,  ...,  1.0957, -0.0075, -0.0754],\n",
       "          [-0.0065,  0.3188, -0.1165,  ...,  0.6616,  0.3333, -0.1183]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1753,  0.0648,  0.0276,  ...,  0.1145, -0.3394,  0.2177],\n",
       "          [ 0.0315, -0.1398, -0.0386,  ..., -0.1816, -0.2532,  0.0580]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.3798e-03, 7.6628e-04, 2.2650e-04, 1.1914e-01, 5.2887e-02, 6.6455e-01,\n",
       "           1.5784e-01, 1.2054e-03],\n",
       "          [1.2695e-02, 2.3479e-03, 4.3182e-03, 4.6899e-01, 3.5400e-01, 6.6910e-03,\n",
       "           1.4990e-01, 1.1444e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  275.364807   95.961426  481.547363  421.068420    0.953505      0  person\n",
       "  1  172.579407   89.739594  325.946686  422.643921    0.935415      0  person\n",
       "  2  122.108040  121.558395  156.988571  158.928223    0.845094     74   clock\n",
       "  3   77.849762  302.083679   97.853027  330.306396    0.793348     41     cup\n",
       "  4  106.312553  271.228149  193.754517  389.881470    0.785664     56   chair\n",
       "  5  268.644470  188.489166  308.779297  209.949249    0.732085     65  remote\n",
       "  6  192.855362  231.286407  215.631119  263.313019    0.717349     65  remote\n",
       "  7    0.725929  322.261749  148.658112  425.183777    0.672427      0  person\n",
       "  8    0.967003  261.902527  142.017822  405.606628    0.583889     56   chair\n",
       "  9  578.591003  361.472748  639.574402  425.445801    0.313508     57   couch,\n",
       "  'caption': ['The arm of a red chair with a blue blanket on the back of it.',\n",
       "   'A red chair is being sat on.'],\n",
       "  'bbox_target': [28.87, 269.33, 108.21, 139.72]},\n",
       " 538: {'image_emb': tensor([[-0.2169,  0.1709,  0.1958,  ...,  0.0932,  0.7432, -0.0096],\n",
       "          [-0.1533,  0.2185,  0.2230,  ...,  0.3782,  0.6450, -0.1364],\n",
       "          [-0.3420,  0.1736,  0.3357,  ...,  0.3140,  0.7002, -0.1472]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1306, -0.2292,  0.1558,  ...,  0.4316,  0.1401, -0.1519],\n",
       "          [ 0.0199, -0.0847, -0.0060,  ...,  0.0522,  0.2078, -0.1011]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.8169, 0.1766, 0.0064],\n",
       "          [0.4592, 0.3691, 0.1716]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  157.250610  153.558151  479.820496  638.007324    0.892633     31   \n",
       "  1   13.809525   18.568390  202.479858  639.531860    0.865208     30   \n",
       "  \n",
       "          name  \n",
       "  0  snowboard  \n",
       "  1       skis  ,\n",
       "  'caption': ['the snowboard with the red letters on it', 'The black skiis.'],\n",
       "  'bbox_target': [162.25, 155.74, 317.0, 473.24]},\n",
       " 539: {'image_emb': tensor([[-0.1512,  0.4534,  0.2474,  ...,  0.9370, -0.2922,  0.0168],\n",
       "          [-0.1388,  0.4084,  0.2395,  ...,  1.1719, -0.2952, -0.0886],\n",
       "          [-0.3491,  0.2192,  0.0654,  ...,  1.0723,  0.0098, -0.2352],\n",
       "          [-0.0167,  0.5073, -0.4082,  ...,  1.1426,  0.2262, -0.4033],\n",
       "          [-0.0200,  0.3679, -0.2151,  ...,  1.2070,  0.2632, -0.5327],\n",
       "          [-0.0843,  0.5537,  0.0524,  ...,  0.6909, -0.2561,  0.1378]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0646,  0.0064, -0.2421,  ..., -0.2603, -0.3811,  0.0610],\n",
       "          [-0.0955,  0.0756, -0.3103,  ..., -0.1456, -0.3679,  0.1311]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.5781e-01, 1.3586e-01, 9.1980e-02, 6.2287e-05, 1.2100e-05, 1.4099e-02],\n",
       "          [3.2983e-01, 1.2512e-01, 5.1855e-01, 5.2786e-04, 5.3048e-05, 2.5833e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  272.939026  427.187439  455.773926  639.454773    0.932680     56   \n",
       "  1  107.259445  466.241516  295.317993  639.394226    0.897953     56   \n",
       "  2   44.176361  421.169739  145.867096  639.444763    0.851404     56   \n",
       "  3  164.995605  349.449402  207.274323  441.144531    0.812328     75   \n",
       "  4  212.790375  326.306824  253.325592  429.239075    0.735144     75   \n",
       "  5  386.716278  375.084015  421.129730  393.588470    0.680863     45   \n",
       "  6  210.260315  272.655884  285.803040  429.500000    0.562183     58   \n",
       "  7   62.152084  427.309143  397.594482  639.385559    0.485204     60   \n",
       "  8  290.058716  429.609924  318.634888  448.967834    0.409970     41   \n",
       "  9  248.038269  442.246948  274.331604  452.412415    0.344905     45   \n",
       "  \n",
       "             name  \n",
       "  0         chair  \n",
       "  1         chair  \n",
       "  2         chair  \n",
       "  3          vase  \n",
       "  4          vase  \n",
       "  5          bowl  \n",
       "  6  potted plant  \n",
       "  7  dining table  \n",
       "  8           cup  \n",
       "  9          bowl  ,\n",
       "  'caption': ['A chair with the front facing the window.',\n",
       "   'The chair facing the window.'],\n",
       "  'bbox_target': [108.27, 472.94, 189.12, 157.6]},\n",
       " 540: {'image_emb': tensor([[-0.3438,  0.3152, -0.0210,  ...,  0.8516,  0.3728,  0.1823],\n",
       "          [ 0.2988,  0.3567, -0.3586,  ...,  0.8452, -0.0202, -0.0857],\n",
       "          [ 0.3276,  0.1525, -0.7017,  ...,  0.9131,  0.0927,  0.1954],\n",
       "          [ 0.5840,  0.0936, -0.2426,  ...,  0.5576,  0.1097,  0.2239]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0424, -0.1074,  0.1223,  ...,  0.3320,  0.2421, -0.3987],\n",
       "          [-0.0115, -0.1588, -0.0178,  ..., -0.0562,  0.1443,  0.0548]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0044, 0.0529, 0.0347, 0.9082],\n",
       "          [0.0310, 0.5493, 0.0756, 0.3440]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  392.153320    0.176281  484.121521   97.059669    0.915008     40   \n",
       "  1  123.622238  135.527008  460.016571  295.409332    0.877100     53   \n",
       "  2   72.948875   62.810738  291.719391  229.018661    0.854270     53   \n",
       "  3  448.579559  145.023819  499.997192  167.889832    0.628667     43   \n",
       "  4   68.434891    0.000000  163.373764   56.377060    0.589899     41   \n",
       "  \n",
       "           name  \n",
       "  0  wine glass  \n",
       "  1       pizza  \n",
       "  2       pizza  \n",
       "  3       knife  \n",
       "  4         cup  ,\n",
       "  'caption': ['A slice of pizza that is close to a wine bottle.',\n",
       "   'The slice of pizza which is closest to the black object in the back ground.'],\n",
       "  'bbox_target': [74.46, 63.37, 205.88, 168.34]},\n",
       " 541: {'image_emb': tensor([[ 0.0087,  0.3579, -0.1346,  ...,  0.8540,  0.2089, -0.5234],\n",
       "          [-0.0992,  0.0623, -0.3867,  ...,  1.3398, -0.1283, -0.1935],\n",
       "          [-0.2129,  0.1713, -0.2961,  ...,  1.0625,  0.2438, -0.0275],\n",
       "          [ 0.0435,  0.2220, -0.2986,  ...,  0.5063,  0.2827, -0.2120]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2278,  0.4060, -0.2474,  ..., -0.0543,  0.2018, -0.5371],\n",
       "          [-0.2207,  0.1237, -0.3179,  ...,  0.0110,  0.2795, -0.2776]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.8806e-02, 9.9540e-06, 1.1504e-05, 9.1113e-01],\n",
       "          [8.8806e-02, 1.2946e-04, 5.7399e-05, 9.1113e-01]], dtype=torch.float16),\n",
       "  'df_boxes':         xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  12.548737   77.556854  410.389771  638.743286    0.924787      0  person\n",
       "  1   0.000000  442.990723   45.170273  559.766846    0.824988      0  person\n",
       "  2   0.000000  499.782318  424.126404  638.246338    0.725560     17   horse,\n",
       "  'caption': ['A man with a hat riding on a horse with his daughter.',\n",
       "   'A man holding a little girl on a horse.'],\n",
       "  'bbox_target': [10.46, 76.23, 401.3, 563.77]},\n",
       " 542: {'image_emb': tensor([[ 0.2649,  0.4739, -0.1442,  ...,  0.9131, -0.4236, -0.2217],\n",
       "          [-0.2330,  0.2291, -0.0226,  ...,  0.7456,  0.2812, -0.5581],\n",
       "          [ 0.3088,  0.2769, -0.0219,  ...,  1.0898, -0.1864, -0.1877],\n",
       "          ...,\n",
       "          [-0.1606,  0.4570,  0.1920,  ...,  1.1885,  0.0425, -0.1516],\n",
       "          [ 0.2148,  0.0797, -0.1306,  ...,  1.3301,  0.1307, -0.4626],\n",
       "          [ 0.3494,  0.3818,  0.2358,  ...,  0.6621, -0.1366, -0.2598]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1066,  0.3921, -0.3044,  ...,  0.1160, -0.1729, -0.1614],\n",
       "          [-0.2216,  0.0141, -0.0246,  ...,  0.0644,  0.1277, -0.2910]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.0400e-01, 0.0000e+00, 1.3506e-04, 3.6359e-05, 3.0859e-01, 1.7285e-06,\n",
       "           8.7036e-02],\n",
       "          [1.7200e-01, 5.4240e-06, 4.2629e-04, 1.0490e-02, 2.0740e-01, 4.9353e-05,\n",
       "           6.0986e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   35.413734  148.104492  172.278442  426.772827    0.939008      0   \n",
       "  1  185.703934  266.871948  338.018555  334.504089    0.919429     13   \n",
       "  2  431.804993  210.627319  475.164246  276.065186    0.844002      0   \n",
       "  3  358.074280  325.690796  637.541870  420.242554    0.817766     13   \n",
       "  4   38.327278  104.867157  109.315468  214.707001    0.768484     34   \n",
       "  5  567.581787  303.344604  628.164307  326.554626    0.745019     24   \n",
       "  6  201.728302  364.787933  210.339630  373.995758    0.667581     32   \n",
       "  7    0.178799  285.169250  122.164841  416.461548    0.499642     13   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1         bench  \n",
       "  2        person  \n",
       "  3         bench  \n",
       "  4  baseball bat  \n",
       "  5      backpack  \n",
       "  6   sports ball  \n",
       "  7         bench  ,\n",
       "  'caption': ['A boy in navy shorts holding a baseball bat.',\n",
       "   'a children plaing the base ball'],\n",
       "  'bbox_target': [34.52, 147.85, 139.14, 277.21]},\n",
       " 543: {'image_emb': tensor([[ 0.4302,  0.2783, -0.0654,  ...,  1.0811,  0.0564, -0.2515],\n",
       "          [-0.2267,  0.5332, -0.1699,  ...,  0.9585,  0.0022, -0.2341],\n",
       "          [-0.0512,  0.2856,  0.0110,  ...,  1.1270,  0.1447,  0.1399],\n",
       "          [-0.0598,  0.3250, -0.1346,  ...,  0.8613,  0.0978, -0.0471]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2866, -0.2045, -0.4053,  ..., -0.2089,  0.0270, -0.4026],\n",
       "          [-0.2249,  0.2412,  0.0333,  ..., -0.5474, -0.0238, -0.3855]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.4971e-01, 3.9279e-05, 5.0323e-02, 6.3717e-05],\n",
       "          [1.0000e+00, 0.0000e+00, 4.5419e-05, 0.0000e+00]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  194.346558   21.484154  361.056396  210.403412    0.869242      0    person\n",
       "  1  338.389191  240.287994  482.531982  430.261932    0.837946     50  broccoli\n",
       "  2  393.264954    0.856865  482.274109  189.503021    0.761948      0    person,\n",
       "  'caption': ['Man in white shirt, back to camera, top center of picture.',\n",
       "   'A man wearing a white jacket with his back turned'],\n",
       "  'bbox_target': [201.35, 31.4, 155.32, 174.02]},\n",
       " 544: {'image_emb': tensor([[-7.8049e-03,  3.2275e-01, -2.4866e-01,  ...,  1.1094e+00,\n",
       "           -2.0370e-02,  1.5063e-01],\n",
       "          [-1.2952e-01,  1.3269e-01, -2.9004e-01,  ...,  8.2617e-01,\n",
       "            7.6477e-02, -7.3290e-04],\n",
       "          [-5.7031e-01,  2.2363e-01, -5.7434e-02,  ...,  6.9775e-01,\n",
       "            5.2155e-02,  2.8174e-01],\n",
       "          ...,\n",
       "          [-5.9296e-02,  1.3379e-01, -1.7932e-01,  ...,  1.0537e+00,\n",
       "           -1.5100e-01,  2.5000e-01],\n",
       "          [-1.5527e-01,  1.6309e-01, -1.9128e-01,  ...,  1.1660e+00,\n",
       "           -9.9060e-02,  2.1057e-02],\n",
       "          [-6.3843e-02,  1.5112e-01, -1.1151e-01,  ...,  4.8901e-01,\n",
       "            1.7834e-01,  3.3716e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2627, -0.0573, -0.1855,  ...,  0.2754, -0.1514,  0.0457],\n",
       "          [-0.2328,  0.0537, -0.5420,  ..., -0.3801, -0.1072, -0.1998]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.9729e-05, 9.9609e-01, 4.1723e-07, 3.5024e-04, 5.2547e-04, 3.1590e-05,\n",
       "           3.0251e-03],\n",
       "          [1.8895e-05, 9.9854e-01, 1.3113e-06, 8.1635e-04, 3.3200e-05, 2.8658e-04,\n",
       "           1.7929e-04]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   105.679108  200.964462  282.220490  638.003174    0.922756      0   \n",
       "  1   266.989655  127.700264  362.502838  279.342377    0.917991      0   \n",
       "  2   380.162750  351.958405  435.141602  433.337738    0.909658     41   \n",
       "  3   401.741974  168.558197  479.622253  298.396057    0.903047      0   \n",
       "  4     0.000000  117.759338   96.784790  571.798828    0.886709      0   \n",
       "  5   241.525726  488.236145  322.219635  639.457947    0.832643     56   \n",
       "  6    71.369080  218.576584  104.389923  266.716736    0.688502     79   \n",
       "  7     1.148468  414.800293  118.382812  640.000000    0.614079     56   \n",
       "  8   277.189941  341.694427  388.181519  413.062958    0.510165     71   \n",
       "  9   239.726868  310.592560  292.590515  327.087982    0.442469     79   \n",
       "  10   89.815475  309.934723  397.966858  414.158600    0.359881     71   \n",
       "  11  424.814392  288.556000  452.391785  405.437225    0.304823     39   \n",
       "  \n",
       "            name  \n",
       "  0       person  \n",
       "  1       person  \n",
       "  2          cup  \n",
       "  3       person  \n",
       "  4       person  \n",
       "  5        chair  \n",
       "  6   toothbrush  \n",
       "  7        chair  \n",
       "  8         sink  \n",
       "  9   toothbrush  \n",
       "  10        sink  \n",
       "  11      bottle  ,\n",
       "  'caption': ['A boy in a blue bathrobe standing on a blue chair holding an electric toothbrush.',\n",
       "   'A boy holding a brush.'],\n",
       "  'bbox_target': [0.0, 119.37, 94.26, 453.06]},\n",
       " 545: {'image_emb': tensor([[-0.0975,  0.2448, -0.2191,  ...,  1.3525,  0.3115, -0.1832],\n",
       "          [ 0.0877,  0.5791, -0.3528,  ...,  0.9355, -0.1174,  0.0257],\n",
       "          [ 0.3198,  0.2754,  0.0251,  ...,  0.9600,  0.1667,  0.3818],\n",
       "          [-0.1087,  0.1433,  0.3354,  ...,  0.9966,  0.0448, -0.1573]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0743, -0.2505, -0.5903,  ...,  0.2311, -0.3145,  0.0517],\n",
       "          [ 0.0882, -0.0269, -0.3330,  ..., -0.2207, -0.2321,  0.1202]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.4072e-01, 3.3569e-02, 2.5034e-06, 2.2583e-01],\n",
       "          [9.9121e-01, 6.5765e-03, 3.1412e-05, 2.1019e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  204.701111  210.472595  433.574585  390.272583    0.900895     57   \n",
       "  1  215.649918  175.347504  351.038269  233.124115    0.866798     57   \n",
       "  2  539.359741  174.468445  577.025513  227.866638    0.794065     62   \n",
       "  3  439.236511  180.805573  530.721497  247.305145    0.687145     56   \n",
       "  4  495.271362  154.419312  536.723267  190.179504    0.677791     58   \n",
       "  5   17.707340  288.824890   73.751221  317.824402    0.601687     73   \n",
       "  6    0.484415  208.092712   34.325096  230.051941    0.552925     45   \n",
       "  \n",
       "             name  \n",
       "  0         couch  \n",
       "  1         couch  \n",
       "  2            tv  \n",
       "  3         chair  \n",
       "  4  potted plant  \n",
       "  5          book  \n",
       "  6          bowl  ,\n",
       "  'caption': ['A couch facing a fireplace.', 'The back of a sofa.'],\n",
       "  'bbox_target': [202.66, 212.63, 235.07, 178.22]},\n",
       " 546: {'image_emb': tensor([[ 0.0709, -0.1935, -0.1927,  ...,  0.5825,  0.2490,  0.0594],\n",
       "          [ 0.0342, -0.1650, -0.1141,  ...,  0.6611, -0.0385,  0.0042],\n",
       "          [ 0.0282, -0.1265, -0.1599,  ...,  0.5566,  0.1650,  0.0933]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1908, -0.3015, -0.3694,  ...,  0.1096,  0.3103,  0.1976],\n",
       "          [-0.2194,  0.0209, -0.0206,  ...,  0.3003,  0.0767,  0.2517]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0812, 0.2703, 0.6484],\n",
       "          [0.0268, 0.1144, 0.8589]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0    0.435135    0.000000  241.781937  550.976440    0.935266     20  elephant\n",
       "  1  111.422592  134.515396  440.034485  473.220947    0.796733     20  elephant\n",
       "  2  292.044495  171.050735  472.636414  432.690613    0.554906     20  elephant,\n",
       "  'caption': ['The elephant walking beside the white line.',\n",
       "   'The elephant closest to the white line on the road.'],\n",
       "  'bbox_target': [125.97, 172.56, 345.77, 266.08]},\n",
       " 547: {'image_emb': tensor([[-0.1141,  0.1490, -0.0655,  ...,  0.3645, -0.1250,  0.1193],\n",
       "          [ 0.3218, -0.0448, -0.3560,  ...,  0.5239, -0.1638,  0.1506],\n",
       "          [ 0.0602, -0.0260, -0.3369,  ...,  0.6938, -0.0714, -0.0857],\n",
       "          [ 0.2460, -0.2517, -0.5020,  ...,  0.2355, -0.4187,  0.1362]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3132, -0.4624,  0.4358,  ...,  0.0728,  0.0436, -0.2520],\n",
       "          [ 0.1492, -0.2603,  0.1938,  ...,  0.1285,  0.0909,  0.0173]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9756e-01, 1.7095e-04, 6.3181e-06, 2.1152e-03],\n",
       "          [7.9004e-01, 1.8469e-01, 2.2769e-05, 2.5391e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  236.384705  240.611328  456.100830  427.807739    0.938913      5     bus\n",
       "  1   14.686363  223.114349  246.326019  448.687286    0.916025      5     bus\n",
       "  2  451.247131  270.687988  480.000000  409.804382    0.814697      5     bus\n",
       "  3    0.041023  265.836792   22.333672  344.541321    0.654101      5     bus\n",
       "  4  131.614166  277.139435  180.754608  329.306305    0.546194      0  person\n",
       "  5   60.212082  299.792999   91.915909  320.925995    0.444039      0  person,\n",
       "  'caption': ['White and orange bus that says Piccadilly on it.',\n",
       "   'Bus 94 to piccadilly'],\n",
       "  'bbox_target': [240.17, 240.23, 215.07, 193.2]},\n",
       " 548: {'image_emb': tensor([[-0.1024,  0.2372, -0.1704,  ...,  0.9766,  0.1329,  0.2008],\n",
       "          [-0.4294, -0.0763, -0.1105,  ...,  0.5215,  0.1371, -0.1132],\n",
       "          [-0.4197, -0.0152, -0.0556,  ...,  0.5752,  0.1366,  0.0591]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1071,  0.0011,  0.0722,  ..., -0.2964,  0.1511,  0.4854],\n",
       "          [-0.3394, -0.1989, -0.2871,  ...,  0.2729,  0.0069,  0.3308]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2419, 0.3464, 0.4116],\n",
       "          [0.7617, 0.1726, 0.0656]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class   name\n",
       "  0    0.437630  79.429230  152.976654  422.236450    0.942419     22  zebra\n",
       "  1  136.474136  17.904633  413.619324  423.311768    0.937297     22  zebra\n",
       "  2  258.294006  15.060432  469.510925  268.016235    0.469873     22  zebra,\n",
       "  'caption': ['A zebra in the wild', 'zebra facing camera'],\n",
       "  'bbox_target': [134.02, 50.98, 275.71, 375.02]},\n",
       " 549: {'image_emb': tensor([[ 0.0355,  0.5190,  0.2429,  ...,  0.8398,  0.2883, -0.0383],\n",
       "          [ 0.2153,  0.3337, -0.2529,  ...,  1.2656, -0.0377, -0.2659],\n",
       "          [ 0.1157,  0.1118, -0.4746,  ...,  1.0117, -0.2734, -0.0948],\n",
       "          ...,\n",
       "          [ 0.3572,  0.5479, -0.0905,  ...,  1.1299,  0.0267,  0.1021],\n",
       "          [-0.0734,  0.0710, -0.1759,  ...,  0.4231,  0.1610,  0.0226],\n",
       "          [ 0.1624,  0.0583, -0.1910,  ...,  0.9902,  0.0807,  0.1210]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0628, -0.1322,  0.0913,  ...,  0.1158, -0.1938,  0.0199],\n",
       "          [-0.3215, -0.2668, -0.1061,  ..., -0.2512, -0.1643, -0.4524]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.8667, 0.0011, 0.0042, 0.0035, 0.0012, 0.0845, 0.0130, 0.0258],\n",
       "          [0.9053, 0.0013, 0.0018, 0.0050, 0.0024, 0.0607, 0.0081, 0.0153]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   423.250427  223.062225  638.940247  395.072113    0.932622      2   \n",
       "  1   363.386017  228.808197  421.366241  386.504120    0.908070      0   \n",
       "  2   196.281158  249.418274  258.770752  377.999023    0.890071      0   \n",
       "  3   332.425232  236.954956  374.239319  384.241638    0.880728      0   \n",
       "  4   257.325256  247.976654  300.056946  381.175018    0.826147      0   \n",
       "  5     0.852150  208.475952  147.539520  413.515198    0.818704      2   \n",
       "  6   228.889465    0.000000  256.814606   28.190125    0.786922      9   \n",
       "  7   286.340973  250.344604  331.487823  386.119324    0.660836      0   \n",
       "  8   489.200958  234.135071  521.468445  282.271240    0.654177      0   \n",
       "  9   156.510132  231.356995  206.527740  337.402527    0.614187      0   \n",
       "  10  219.389679  270.673248  255.558350  322.883698    0.523930     26   \n",
       "  11  298.032684  275.338531  326.306671  329.329132    0.477304     26   \n",
       "  12  408.439941  287.331848  434.532471  330.218140    0.432761      1   \n",
       "  13  257.766205  269.987152  273.537506  314.932343    0.297388     26   \n",
       "  14  173.804733  310.966888  206.147995  353.992401    0.288775      0   \n",
       "  \n",
       "               name  \n",
       "  0             car  \n",
       "  1          person  \n",
       "  2          person  \n",
       "  3          person  \n",
       "  4          person  \n",
       "  5             car  \n",
       "  6   traffic light  \n",
       "  7          person  \n",
       "  8          person  \n",
       "  9          person  \n",
       "  10        handbag  \n",
       "  11        handbag  \n",
       "  12        bicycle  \n",
       "  13        handbag  \n",
       "  14         person  ,\n",
       "  'caption': ['A silver car, stopped.',\n",
       "   'white color car in the right side of the image'],\n",
       "  'bbox_target': [426.0, 227.2, 213.53, 167.24]},\n",
       " 550: {'image_emb': tensor([[ 0.0610,  0.0695, -0.3284,  ...,  0.4133,  0.2302,  0.2217],\n",
       "          [-0.0095,  0.2554, -0.1891,  ...,  1.4180,  0.0277,  0.1418],\n",
       "          [ 0.0546,  0.0435,  0.0264,  ...,  1.1855,  0.0908, -0.3125],\n",
       "          ...,\n",
       "          [ 0.1033,  0.2407, -0.1644,  ...,  1.1777,  0.0385,  0.2781],\n",
       "          [-0.5210, -0.1277, -0.2759,  ...,  0.8306,  0.2559, -0.0502],\n",
       "          [ 0.0652, -0.2365, -0.1454,  ...,  1.0088,  0.1152,  0.0394]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0267,  0.0176,  0.2018,  ...,  0.0284, -0.0327, -0.0190],\n",
       "          [ 0.1774, -0.0567, -0.2271,  ..., -0.0562, -0.2766, -0.2651]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.1646e-03, 5.5237e-03, 9.7363e-01, 5.0316e-03, 2.8229e-03, 1.6510e-05,\n",
       "           3.0041e-03, 3.6240e-03],\n",
       "          [1.5345e-03, 6.4583e-03, 9.5850e-01, 7.2060e-03, 1.1879e-02, 2.3305e-05,\n",
       "           5.5237e-03, 8.8272e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    59.862175  111.179382  204.900635  331.962769    0.923037      0   \n",
       "  1   317.595459  268.315521  639.044922  414.591553    0.895798      0   \n",
       "  2   172.890656  180.077545  382.255646  415.253357    0.843592     60   \n",
       "  3     0.686764  142.391251   66.891418  411.521912    0.797800     57   \n",
       "  4   239.527802  123.106415  347.672028  203.699493    0.788195     56   \n",
       "  5   350.817200  124.200027  392.577942  228.612244    0.742908      0   \n",
       "  6   425.299896  172.130920  599.687012  277.283569    0.742792     60   \n",
       "  7   127.929337   97.007217  200.021408  198.355835    0.689857      0   \n",
       "  8   414.419037  160.688446  475.158966  240.709747    0.661223     56   \n",
       "  9   406.012756  121.013107  436.705933  206.718689    0.656898     56   \n",
       "  10  165.123444  101.632782  195.873077  128.928925    0.522967     63   \n",
       "  11    0.684822  115.592468   93.145042  218.490295    0.522308     56   \n",
       "  12  184.672958   64.404938  229.523575  121.707977    0.499301     58   \n",
       "  13    0.052330  115.380249   95.307304  219.141968    0.450839     57   \n",
       "  14  334.764618  205.829346  387.307770  256.964478    0.368037     16   \n",
       "  15  207.268066  106.289581  216.150543  118.033447    0.279999     75   \n",
       "  16  186.994507  170.574707  199.853760  180.913391    0.273247     65   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2   dining table  \n",
       "  3          couch  \n",
       "  4          chair  \n",
       "  5         person  \n",
       "  6   dining table  \n",
       "  7         person  \n",
       "  8          chair  \n",
       "  9          chair  \n",
       "  10        laptop  \n",
       "  11         chair  \n",
       "  12  potted plant  \n",
       "  13         couch  \n",
       "  14           dog  \n",
       "  15          vase  \n",
       "  16        remote  ,\n",
       "  'caption': ['A wooden table.', 'coffee table'],\n",
       "  'bbox_target': [167.81, 179.69, 219.72, 238.38]},\n",
       " 551: {'image_emb': tensor([[-0.5845,  0.0911,  0.1957,  ...,  0.5776,  0.1118, -0.2876],\n",
       "          [-0.2520,  0.1577, -0.4658,  ...,  1.1025, -0.0135, -0.1288],\n",
       "          [-0.4150, -0.0826,  0.1399,  ...,  0.5161,  0.1754, -0.1722],\n",
       "          ...,\n",
       "          [ 0.0488, -0.3110, -0.1936,  ...,  0.7466, -0.0196, -0.4087],\n",
       "          [ 0.1302, -0.2803, -0.1403,  ...,  0.5991,  0.0046, -0.4260],\n",
       "          [-0.0358, -0.2949, -0.0829,  ...,  0.1741, -0.0732, -0.0513]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2756,  0.1348,  0.0127,  ...,  0.0081, -0.4092, -0.3755],\n",
       "          [ 0.2010,  0.0741, -0.1949,  ...,  0.1722,  0.0097, -0.2395]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.9590e-01, 4.3068e-03, 1.2863e-02, 5.4474e-03, 7.3280e-03, 3.5725e-03,\n",
       "           2.8534e-02, 4.6326e-02, 9.2621e-03, 8.6548e-02],\n",
       "          [5.4395e-01, 3.0756e-05, 1.5344e-01, 5.3644e-04, 1.2350e-04, 2.2829e-05,\n",
       "           1.2153e-04, 1.0662e-03, 4.2415e-04, 3.0029e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   355.738770  148.676285  590.738037  332.958313    0.948211     17   horse\n",
       "  1   461.067108   99.780273  516.826538  257.883423    0.920796      0  person\n",
       "  2    37.008934  160.470276  231.491516  347.114197    0.919394     17   horse\n",
       "  3   142.964890  125.669891  195.799240  272.730438    0.911503      0  person\n",
       "  4   374.568481   72.962341  400.582397  110.477051    0.883123     18   sheep\n",
       "  5   298.211823   75.088898  337.320282  107.950928    0.875232     18   sheep\n",
       "  6    82.041245  116.253830  128.089355  145.128586    0.836373     18   sheep\n",
       "  7   111.028450   48.144135  145.474609   67.706482    0.771524     18   sheep\n",
       "  8   246.658432  104.849167  277.623413  123.055099    0.744926     18   sheep\n",
       "  9   371.190918    7.957130  388.644531   16.985329    0.679239     18   sheep\n",
       "  10  344.281281    2.740219  359.498016   15.293121    0.644436     18   sheep,\n",
       "  'caption': ['The horse with white legs and a large white spot on its side',\n",
       "   'A brown, black, and white horse running outside'],\n",
       "  'bbox_target': [354.33, 146.27, 240.0, 189.52]},\n",
       " 552: {'image_emb': tensor([[-0.0295,  0.4814, -0.1934,  ...,  1.4443, -0.1769,  0.1801],\n",
       "          [-0.3879,  0.3347, -0.0505,  ...,  0.7090,  0.0830, -0.1558],\n",
       "          [ 0.0352,  0.5801, -0.2920,  ...,  0.6836,  0.0099,  0.3330],\n",
       "          ...,\n",
       "          [-0.1459, -0.0804, -0.5913,  ...,  0.8618,  0.3801, -0.3291],\n",
       "          [-0.1191,  0.2996, -0.0656,  ...,  0.9185, -0.1287,  0.1174],\n",
       "          [-0.9150,  0.2438,  0.0665,  ...,  0.0779, -0.0975,  0.1104]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0640, -0.0243, -0.3555,  ..., -0.4419,  0.2114, -0.0424],\n",
       "          [ 0.1213,  0.2159, -0.3821,  ..., -0.3896, -0.0997, -0.0828]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.4983e-02, 6.6872e-03, 1.9849e-01, 5.4588e-03, 1.9543e-01, 9.9373e-04,\n",
       "           5.4785e-01],\n",
       "          [1.0818e-02, 0.0000e+00, 9.8926e-01, 0.0000e+00, 1.2696e-05, 0.0000e+00,\n",
       "           7.0632e-05]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.222298  120.036713  139.786438  475.573975    0.956883      0   \n",
       "  1  437.576874  140.911270  628.906677  477.913818    0.941174     72   \n",
       "  2  285.742065  137.057709  411.661255  460.810913    0.910812      0   \n",
       "  3  227.018509  311.348419  282.171570  382.019135    0.871978     26   \n",
       "  4   88.778290  362.560181  466.829498  475.604492    0.857742     59   \n",
       "  5  137.124664  286.420410  252.900513  363.699219    0.843111     68   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1  refrigerator  \n",
       "  2        person  \n",
       "  3       handbag  \n",
       "  4           bed  \n",
       "  5     microwave  ,\n",
       "  'caption': ['person in red on right',\n",
       "   'A woman in a red t-shirt with her hand by her face'],\n",
       "  'bbox_target': [284.44, 138.73, 125.84, 324.29]},\n",
       " 553: {'image_emb': tensor([[ 1.3989e-01,  2.1301e-01, -9.2834e-02,  ...,  6.9629e-01,\n",
       "           -7.4585e-02,  2.7561e-04],\n",
       "          [-3.7098e-03, -6.7993e-02, -2.1094e-01,  ...,  5.2734e-01,\n",
       "           -3.9258e-01, -5.7465e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3184,  0.2178,  0.3315,  ...,  0.5508, -0.1907, -0.1323],\n",
       "          [ 0.4722,  0.0643,  0.4932,  ...,  0.1688, -0.0145, -0.0968]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0876, 0.9126],\n",
       "          [0.0070, 0.9932]], dtype=torch.float16),\n",
       "  'df_boxes':         xmin       ymin        xmax        ymax  confidence  class      name\n",
       "  0  10.486969  72.636932  636.180786  294.876129    0.923919      4  airplane\n",
       "  1   1.929672   0.149094  213.412888   22.946213    0.441569      4  airplane,\n",
       "  'caption': ['a delta airplane on runway landing',\n",
       "   'A Delta flight touching down.'],\n",
       "  'bbox_target': [12.47, 73.89, 622.75, 210.14]},\n",
       " 554: {'image_emb': tensor([[ 0.1205,  0.2581, -0.1959,  ...,  0.4573,  0.1340, -0.1522],\n",
       "          [-0.1975,  0.5845, -0.1824,  ...,  0.8047, -0.0746,  0.2291],\n",
       "          [-0.1968,  0.2120, -0.2360,  ...,  0.8140,  0.3179,  0.2666],\n",
       "          ...,\n",
       "          [ 0.0082,  0.4705, -0.2539,  ...,  0.9771,  0.0678,  0.1765],\n",
       "          [ 0.1036, -0.1182, -0.5327,  ...,  0.9224, -0.1093, -0.0176],\n",
       "          [-0.3589,  0.4824, -0.5317,  ...,  0.4922,  0.2360, -0.0270]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1090, -0.1516, -0.4592,  ...,  0.3472, -0.5210, -0.1365],\n",
       "          [-0.2305, -0.0211, -0.0728,  ...,  0.0703, -0.6606, -0.0551]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.8053e-03, 1.1802e-05, 5.9424e-01, 1.0828e-01, 3.3402e-04, 7.3910e-06,\n",
       "           2.8516e-01, 2.4974e-05, 1.0133e-06, 9.9106e-03],\n",
       "          [6.2347e-02, 2.4319e-04, 4.3286e-01, 8.3923e-02, 1.9538e-04, 7.3314e-06,\n",
       "           3.5889e-01, 2.5082e-04, 1.2100e-05, 6.1401e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   278.556976    0.513016  494.128448  266.556335    0.939784      0   \n",
       "  1    10.851151  287.860413  129.490738  362.537476    0.930333     41   \n",
       "  2     0.488419    0.294434  306.765564  319.013306    0.925857      0   \n",
       "  3   277.038757  296.269775  426.067017  414.494629    0.878992     55   \n",
       "  4   264.841431  252.957214  336.502258  338.798157    0.818882     43   \n",
       "  5   182.306534   68.511139  245.366806  115.954010    0.781839     26   \n",
       "  6     3.036163  176.744812  637.996948  473.207031    0.776512     60   \n",
       "  7   442.008545  199.830872  488.602173  239.086060    0.759582     41   \n",
       "  8   275.791473   37.339645  290.129547   85.364792    0.716047     39   \n",
       "  9   465.092072  270.821075  542.584229  309.420746    0.688641     42   \n",
       "  10  553.745605  123.499344  639.210327  199.336548    0.559137     56   \n",
       "  11  324.087128   71.531830  355.201813   89.543945    0.529856     46   \n",
       "  12  416.487610  270.896790  530.386475  317.937622    0.464358     42   \n",
       "  13  611.998718  193.955994  639.878723  212.421509    0.443542     45   \n",
       "  14  304.568146   88.078171  341.046356  104.260452    0.442759     45   \n",
       "  15  519.968079   48.966751  530.743591   83.596329    0.442361     39   \n",
       "  16  274.324463    0.421509  349.597961   80.221344    0.423152     58   \n",
       "  17  554.113098  121.979950  591.820374  192.022461    0.327496     56   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1            cup  \n",
       "  2         person  \n",
       "  3           cake  \n",
       "  4          knife  \n",
       "  5        handbag  \n",
       "  6   dining table  \n",
       "  7            cup  \n",
       "  8         bottle  \n",
       "  9           fork  \n",
       "  10         chair  \n",
       "  11        banana  \n",
       "  12          fork  \n",
       "  13          bowl  \n",
       "  14          bowl  \n",
       "  15        bottle  \n",
       "  16  potted plant  \n",
       "  17         chair  ,\n",
       "  'caption': ['An asian women cutting a chocolate cake',\n",
       "   'a sitting woman cuts into a dessert'],\n",
       "  'bbox_target': [0.0, 2.97, 308.49, 310.92]},\n",
       " 555: {'image_emb': tensor([[ 0.1904,  0.3481,  0.0172,  ...,  0.6035,  0.0320, -0.2004],\n",
       "          [ 0.2607,  0.0840,  0.0456,  ...,  0.3337,  0.3958,  0.2480],\n",
       "          [ 0.3098,  0.2566,  0.0994,  ...,  0.0659, -0.0192, -0.0874]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1094,  0.2803, -0.2712,  ...,  0.1350, -0.3489, -0.4954],\n",
       "          [-0.1318,  0.3271, -0.1927,  ...,  0.0060, -0.1248, -0.3643]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0644, 0.0040, 0.9316],\n",
       "          [0.2103, 0.0083, 0.7812]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class name\n",
       "  0   95.330597  156.914307  601.918579  422.757202    0.897832     15  cat\n",
       "  1  283.831543  148.656631  383.159729  276.160400    0.818707     15  cat,\n",
       "  'caption': ['a large cat standing close to a  sitting cat in the bathtub',\n",
       "   'A big cat in a tub.'],\n",
       "  'bbox_target': [93.29, 171.67, 502.01, 251.0]},\n",
       " 556: {'image_emb': tensor([[ 0.0751, -0.1807,  0.1812,  ...,  0.9756,  0.2117, -0.5366],\n",
       "          [ 0.1932, -0.0635,  0.2727,  ...,  0.9189, -0.1257, -0.4648],\n",
       "          [-0.0703,  0.1455,  0.3391,  ...,  0.9717, -0.1687, -0.3091],\n",
       "          [-0.0371, -0.4812, -0.1287,  ...,  0.9546,  0.0436, -0.1157],\n",
       "          [-0.2463, -0.5840,  0.3181,  ...,  0.5630, -0.0110, -0.1364]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1018, -0.3672, -0.3403,  ...,  0.3040, -0.2627,  0.0978],\n",
       "          [ 0.2076, -0.1848, -0.0260,  ..., -0.0431, -0.0966, -0.5889]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.2385e-02, 1.4534e-03, 8.1848e-02, 8.9404e-01, 2.9993e-04],\n",
       "          [8.7988e-01, 4.0527e-02, 6.6833e-02, 3.2234e-03, 9.4757e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  200.989258  261.627747  369.299438  401.507385    0.915755     18   sheep\n",
       "  1  165.706573  231.504486  342.800385  364.301971    0.893089     18   sheep\n",
       "  2  439.859314  259.337494  579.875305  386.628448    0.812831     18   sheep\n",
       "  3  438.639984   66.404327  639.355103  385.415558    0.732037     17   horse\n",
       "  4    0.596512   81.350235  322.072571  341.505432    0.564232     18   sheep\n",
       "  5  622.507202    7.285660  639.845703   87.264847    0.424055      0  person\n",
       "  6    5.566956   68.197510  304.634613  341.225159    0.401333     19     cow\n",
       "  7  476.950195  220.598999  531.464783  270.443970    0.322257     18   sheep\n",
       "  8  436.716248   68.986374  637.412048  396.483582    0.275420     18   sheep,\n",
       "  'caption': ['A baby brown lamb looking through a green fence.',\n",
       "   'The little brown sheep.'],\n",
       "  'bbox_target': [201.7, 260.45, 173.43, 141.53]},\n",
       " 557: {'image_emb': tensor([[ 0.1963,  0.5669, -0.2627,  ...,  0.5488,  0.3665, -0.1229],\n",
       "          [-0.1481,  0.3218, -0.1048,  ...,  0.5513,  0.2903, -0.2485],\n",
       "          [-0.4829,  0.5771,  0.0840,  ...,  0.4231,  0.4429, -0.0477],\n",
       "          [-0.5791,  0.5244,  0.0723,  ...,  0.4983,  0.4131, -0.1765],\n",
       "          [-0.2568,  0.3831,  0.1250,  ..., -0.3301,  0.0820, -0.0942]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-2.8027e-01, -1.9678e-01, -3.1519e-01,  1.2646e-01, -8.1543e-02,\n",
       "            1.3672e-01, -1.5417e-01, -1.1641e+00, -2.9248e-01,  4.2603e-01,\n",
       "           -6.0089e-02, -3.5913e-01,  3.6279e-01,  2.2754e-01,  2.4646e-01,\n",
       "            2.0065e-02,  1.7908e-01, -2.1069e-01, -3.7793e-01,  1.7624e-02,\n",
       "            8.9941e-01,  2.6440e-01, -1.1346e-01,  2.5223e-02, -1.4014e-01,\n",
       "            2.7679e-02, -4.3701e-01,  3.6182e-01, -8.4229e-02,  1.1505e-01,\n",
       "           -1.1188e-01,  3.4698e-02, -1.6357e-02,  2.6270e-01, -2.3865e-01,\n",
       "            1.0852e-01, -4.9591e-02,  1.5332e-01,  1.1255e-01,  1.4685e-01,\n",
       "           -1.4099e-01,  3.6914e-01,  7.2449e-02,  1.4685e-01,  3.3057e-01,\n",
       "            2.8613e-01,  2.0154e-01,  9.1064e-02,  1.4990e-01, -1.7981e-01,\n",
       "            1.7493e-01,  3.3600e-02, -9.0332e-02,  3.9966e-01, -2.2693e-01,\n",
       "           -2.3083e-01, -4.3549e-02, -1.5039e-01, -1.4160e-01, -2.1362e-02,\n",
       "            1.8677e-01, -1.1920e-01,  3.4814e-01, -6.2683e-02, -2.2125e-02,\n",
       "           -9.8389e-02,  1.4417e-01,  9.8755e-02,  2.2827e-01,  3.3691e-01,\n",
       "           -8.3252e-02, -1.1084e-01,  1.8689e-01,  1.0541e-01,  1.2396e-01,\n",
       "           -6.3672e-01,  8.5526e-03,  4.2310e-01, -2.6367e-01, -2.3315e-01,\n",
       "            8.0200e-02,  9.8816e-02, -8.9294e-02,  2.2522e-01,  1.7712e-01,\n",
       "           -3.8501e-01,  1.0919e-01,  1.1090e-01, -1.9958e-01,  9.6191e-02,\n",
       "           -9.5886e-02,  1.0254e-01, -1.5400e+00,  3.9502e-01, -1.5845e-01,\n",
       "           -9.8389e-02,  2.2815e-01, -2.6660e-01, -5.3558e-02,  3.8422e-02,\n",
       "           -3.0029e-01, -3.4155e-01,  2.9053e-01, -1.0046e-01, -2.0398e-01,\n",
       "           -2.0703e-01, -7.1144e-03,  2.6025e-01, -1.8726e-01,  3.5083e-01,\n",
       "           -1.5549e-02, -2.4182e-01, -3.8281e-01,  3.1982e-01, -1.7505e-01,\n",
       "            1.7639e-02,  1.8860e-01, -1.1731e-01,  2.1716e-01, -7.1655e-02,\n",
       "           -8.5876e-02, -8.7744e-01, -8.1238e-02, -3.2739e-01,  2.6001e-01,\n",
       "           -9.3994e-02, -2.9150e-01, -2.1033e-01,  7.5562e-02,  2.9443e-01,\n",
       "           -1.5454e-01, -5.9326e-01, -1.4343e-01,  5.9805e+00, -3.0029e-01,\n",
       "            7.4097e-02, -2.8491e-01, -3.9233e-01, -1.2207e-01, -3.6670e-01,\n",
       "           -3.1519e-01,  1.7090e-01, -2.8174e-01, -7.7667e-03, -6.4087e-02,\n",
       "           -1.5796e-01,  8.2764e-02, -1.7432e-01, -5.0879e-01, -3.0930e-02,\n",
       "           -4.1008e-04, -4.6704e-01,  7.4768e-02,  1.6797e-01, -8.2932e-03,\n",
       "           -4.6948e-01,  4.3091e-01, -1.6882e-01, -9.3628e-02,  1.7883e-01,\n",
       "            6.9397e-02,  9.7046e-02,  3.7018e-02, -6.5369e-02,  4.4250e-02,\n",
       "            3.2300e-01,  9.4177e-02, -3.7378e-01,  2.3645e-01, -1.2952e-01,\n",
       "           -4.0747e-01, -3.4058e-01,  2.7734e-01,  2.5330e-02, -4.5386e-01,\n",
       "            3.4253e-01, -3.7231e-01,  2.2778e-01,  6.3660e-02,  5.0684e-01,\n",
       "           -3.1494e-01, -1.5100e-01,  5.0781e-02, -1.1749e-01,  2.9541e-01,\n",
       "           -1.8250e-01, -1.5900e-02, -6.2286e-02, -4.0479e-01, -2.5928e-01,\n",
       "           -3.6401e-01,  2.2021e-01,  3.4253e-01, -1.2535e-02,  7.5134e-02,\n",
       "           -5.4016e-03, -2.0776e-01,  4.2017e-01, -8.2825e-02, -7.5867e-02,\n",
       "           -1.4282e-01, -3.0624e-02,  7.4890e-02,  3.2886e-01,  1.8506e-01,\n",
       "           -6.6833e-02, -3.2654e-02, -1.4124e-01, -9.4604e-02, -2.7930e-01,\n",
       "           -1.1169e-02, -2.6947e-02,  2.4023e-01,  6.1572e-01,  1.9751e-01,\n",
       "            1.5764e-03,  5.9448e-02, -1.4697e-01,  2.1069e-01, -2.0569e-01,\n",
       "            3.2990e-02, -3.5919e-02,  1.9580e-01, -2.3840e-01, -5.4718e-02,\n",
       "           -3.6255e-01,  2.0227e-01, -8.0444e-02, -2.5806e-01,  7.0190e-02,\n",
       "            9.9716e-03,  1.9336e-01, -5.5054e-02,  1.8286e-01,  1.8604e-01,\n",
       "            1.2469e-01,  4.2700e-01, -2.1033e-01, -5.3906e-01,  1.0498e-01,\n",
       "            3.1616e-01, -7.3914e-02,  2.4429e-02,  1.3477e-01,  5.0964e-03,\n",
       "            5.3040e-02,  1.7419e-01, -3.3691e-01,  1.6260e-01, -2.3529e-02,\n",
       "            1.8677e-01, -5.7190e-02, -3.4363e-02,  3.9136e-01, -3.4888e-01,\n",
       "           -2.4329e-01, -8.1238e-02,  7.2205e-02,  1.4771e-01,  3.0273e-01,\n",
       "           -9.2041e-02,  8.8806e-02,  4.3457e-02, -1.8494e-01,  7.2083e-02,\n",
       "            1.6943e-01,  1.5125e-01, -3.6450e-01, -4.0991e-01,  5.2734e-02,\n",
       "           -1.7224e-01,  2.8244e-02, -1.3599e-01,  5.4108e-02, -4.4159e-02,\n",
       "           -4.2343e-03, -1.2079e-01,  3.5986e-01, -2.9602e-02,  1.7725e-01,\n",
       "           -1.9763e-01,  9.6375e-02,  2.3792e-01,  3.1934e-01,  6.6284e-02,\n",
       "            2.3096e-01,  2.3633e-01,  1.8091e-01,  2.0618e-01, -3.8281e-01,\n",
       "            1.6370e-01,  2.5171e-01, -2.2449e-01,  3.2928e-02, -3.0444e-01,\n",
       "            1.3049e-01, -1.2891e-01,  7.2098e-03, -1.3721e-01, -2.2046e-01,\n",
       "            2.0020e-01, -2.0813e-01,  1.4990e-01,  9.9304e-02, -9.3628e-02,\n",
       "           -3.5034e-01, -2.1948e-01,  2.9541e-02,  1.6028e-01, -2.8320e-01,\n",
       "            6.2134e-02,  2.2607e-01,  5.9688e+00,  2.3251e-03,  2.0483e-01,\n",
       "           -1.0483e-02,  3.2812e-01, -1.6199e-01,  3.3936e-01,  4.5068e-01,\n",
       "            1.3562e-01,  4.4281e-02,  5.1172e-01, -1.5732e-02, -1.3000e-01,\n",
       "            3.0151e-01, -1.8982e-01, -3.4424e-01,  4.3726e-01, -2.4629e+00,\n",
       "            4.6600e-02, -3.3569e-02,  5.6824e-02,  1.0089e-01,  9.7656e-02,\n",
       "            1.3504e-02, -9.8694e-02,  9.4681e-03,  1.4075e-01, -2.3743e-01,\n",
       "           -1.5613e-01,  1.9165e-01,  2.5903e-01, -1.4978e-01, -6.2675e-03,\n",
       "           -1.4197e-01,  3.7183e-01,  1.6296e-01, -5.4932e-01,  2.0859e-02,\n",
       "            2.9272e-01,  1.9336e-01, -2.0288e-01,  1.0083e-01,  2.6733e-01,\n",
       "            1.9885e-01, -7.5928e-02, -9.7229e-02,  3.9795e-01,  1.1597e-02,\n",
       "            2.2852e-01, -2.0386e-01, -1.2688e-02,  2.1814e-01,  2.6245e-01,\n",
       "           -2.5421e-02, -6.3110e-02, -3.0469e-01,  1.1090e-01,  2.6025e-01,\n",
       "            1.2952e-01, -1.5796e-01,  1.2720e-01,  1.6556e-02, -4.6948e-01,\n",
       "            1.5271e-01,  2.0557e-01, -5.4779e-02, -3.9014e-01, -8.9111e-02,\n",
       "            2.7539e-01, -2.9938e-02,  3.9795e-01,  1.9348e-01,  6.3354e-02,\n",
       "            4.6570e-02, -6.3591e-03,  9.8389e-02, -3.9282e-01, -7.6538e-02,\n",
       "           -6.0596e-01, -5.1300e-02, -9.4788e-02,  8.4290e-02,  3.9612e-02,\n",
       "           -2.4438e-01, -1.3940e-01,  9.8267e-03, -3.3276e-01,  5.4785e-01,\n",
       "            4.0894e-02, -1.1041e-01,  1.2006e-01,  4.9500e-02, -5.2490e-02,\n",
       "           -2.2302e-01,  2.0523e-02, -1.6537e-03,  3.5449e-01, -2.1533e-01,\n",
       "            1.6870e-01, -1.3452e-01,  1.3901e-02,  1.0297e-01,  7.6538e-02,\n",
       "           -1.5149e-01, -1.5335e-02, -2.4500e-01, -1.3634e-02, -6.3232e-01,\n",
       "            7.2327e-02, -3.6572e-01,  2.4719e-01, -1.9812e-01,  2.1301e-01,\n",
       "            3.7170e-02, -7.9712e-02,  1.7932e-01,  4.6094e-01, -1.1298e-01,\n",
       "            2.0538e-02,  8.4229e-02, -2.5903e-01,  1.7838e-02,  3.6890e-01,\n",
       "            2.1286e-03,  2.0764e-01, -2.7661e-01, -2.2510e-01,  2.0172e-02,\n",
       "           -1.6809e-01, -2.5009e-02, -9.6069e-02, -9.8083e-02,  3.2324e-01,\n",
       "           -2.4445e-02,  2.5122e-01, -1.7798e-01,  3.3936e-01,  2.9724e-02,\n",
       "           -4.3488e-02,  8.5526e-03,  1.8726e-01, -2.4683e-01,  2.4231e-01,\n",
       "            7.4707e-02,  7.2876e-02,  7.9041e-02, -1.9946e-01, -2.6074e-01,\n",
       "            1.5479e-01, -1.9641e-01,  1.1377e-01,  2.6245e-01,  2.8735e-01,\n",
       "            1.7322e-01, -1.8506e-01,  1.9775e-01,  4.7882e-02,  1.8845e-02,\n",
       "           -6.6948e-04,  8.0933e-02,  3.3643e-01, -2.7490e-01,  3.7506e-02,\n",
       "            9.0576e-02,  6.0913e-02,  3.2593e-01,  1.6769e-02,  2.6611e-01,\n",
       "           -2.2156e-01, -6.9678e-01, -1.9121e-03, -4.2786e-02,  6.8665e-04,\n",
       "           -5.0684e-01, -1.9116e-01, -3.5669e-01, -1.4172e-01, -2.7148e-01,\n",
       "           -1.3611e-01, -1.0992e-01, -1.1035e-01,  7.6270e-01,  1.8417e-02,\n",
       "           -2.7539e-01,  3.1348e-01,  1.7566e-01, -1.1420e-01,  2.5806e-01,\n",
       "            5.9631e-02, -1.2805e-01, -1.7871e-01,  2.4915e-01, -1.4389e-02,\n",
       "            4.7705e-01, -7.5684e-02,  3.9526e-01,  4.6570e-02, -2.4329e-01,\n",
       "           -9.5673e-03, -1.1456e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1010, 0.1470, 0.6484, 0.0812, 0.0225]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  192.432434  161.627075  299.886292  419.708008    0.842555     17   horse\n",
       "  1  391.453979  234.414764  521.715027  411.471619    0.799683     17   horse\n",
       "  2  175.167969  114.940033  318.554108  317.390656    0.783433      0  person\n",
       "  3  392.007660  115.499191  499.862823  291.043701    0.736880      0  person,\n",
       "  'caption': ['the horse facing the camera'],\n",
       "  'bbox_target': [174.88, 154.63, 116.9, 262.32]},\n",
       " 558: {'image_emb': tensor([[ 0.1127,  0.2289,  0.0891,  ...,  0.9033, -0.0413,  0.2771],\n",
       "          [-0.2458,  0.2673, -0.0485,  ...,  0.7002,  0.0024,  0.0357],\n",
       "          [-0.1825,  0.2832, -0.2361,  ...,  1.0156,  0.0131, -0.2258],\n",
       "          [-0.0247,  0.2964, -0.0325,  ...,  0.6626, -0.1183, -0.2020],\n",
       "          [ 0.1051, -0.0153,  0.0109,  ...,  0.5884,  0.0286,  0.1416]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0556, -0.0963,  0.3972,  ...,  0.1594, -0.0714, -0.0593],\n",
       "          [ 0.1702, -0.2460,  0.3955,  ...,  0.2634,  0.1057,  0.1742]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0030, 0.0789, 0.0334, 0.6704, 0.2144],\n",
       "          [0.0007, 0.4983, 0.2466, 0.2245, 0.0299]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   56.391472  214.957642  187.989502  476.806458    0.930394      0   \n",
       "  1  313.114258  113.244354  448.953735  456.211060    0.877839     72   \n",
       "  2  428.727173  103.656128  578.543884  448.513916    0.831504     72   \n",
       "  3  198.156326  144.755798  323.131165  456.371094    0.773097     72   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1  refrigerator  \n",
       "  2  refrigerator  \n",
       "  3  refrigerator  ,\n",
       "  'caption': ['A red and blue striped refrigerator.',\n",
       "   'A purple and blue striped fridge'],\n",
       "  'bbox_target': [203.41, 161.83, 110.27, 291.39]},\n",
       " 559: {'image_emb': tensor([[ 0.0151,  0.7549,  0.2123,  ...,  0.6421, -0.1993,  0.1362],\n",
       "          [ 0.1071,  0.4944, -0.1603,  ...,  0.8071, -0.3772, -0.2234],\n",
       "          [ 0.0053,  0.5664,  0.1213,  ...,  0.7261, -0.3025, -0.1842],\n",
       "          [ 0.0692, -0.0250, -0.1388,  ...,  0.8965,  0.0691, -0.2168],\n",
       "          [ 0.1354, -0.0400, -0.5107,  ...,  0.8745, -0.1810, -0.0154],\n",
       "          [-0.0340,  0.3569,  0.6353,  ...,  0.6162, -0.1964,  0.1976]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1552, -0.0160,  0.1044,  ..., -0.1337, -0.2028,  0.1284],\n",
       "          [-0.3018,  0.1632,  0.2059,  ...,  0.3298, -0.4089, -0.3123]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.0953e-03, 3.5840e-01, 3.7549e-01, 7.8392e-04, 8.1360e-05, 2.5806e-01],\n",
       "          [1.5545e-04, 9.9658e-01, 2.5101e-03, 1.5616e-05, 1.2517e-06, 5.0974e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  295.953949   75.808868  464.219147  252.665314    0.950672      0   \n",
       "  1   52.062065   86.321121  150.828827  254.844116    0.942212      0   \n",
       "  2  134.027863  100.600800  262.613678  235.820404    0.931427      0   \n",
       "  3  240.631607   97.902863  263.408173  133.908020    0.796730     35   \n",
       "  4  483.966827   75.560120  501.177826   85.486176    0.763754     32   \n",
       "  \n",
       "               name  \n",
       "  0          person  \n",
       "  1          person  \n",
       "  2          person  \n",
       "  3  baseball glove  \n",
       "  4     sports ball  ,\n",
       "  'caption': ['an empire crouching down behind a catcher',\n",
       "   'An umpire standing on a baseball field.'],\n",
       "  'bbox_target': [49.05, 88.48, 100.98, 167.34]},\n",
       " 560: {'image_emb': tensor([[-0.0311,  0.0909,  0.2561,  ...,  0.8159,  0.4075,  0.1979],\n",
       "          [-0.0440, -0.0799,  0.0501,  ...,  1.0488,  0.4263,  0.5093],\n",
       "          [-0.0900,  0.0670,  0.1572,  ...,  0.6201,  0.3923,  0.1940],\n",
       "          ...,\n",
       "          [-0.1635,  0.0455, -0.1681,  ...,  0.8613,  0.1926, -0.4031],\n",
       "          [-0.0457,  0.0065, -0.1443,  ...,  0.9570,  0.2400, -0.2352],\n",
       "          [ 0.0912,  0.1584,  0.0514,  ...,  0.5215,  0.4353,  0.2966]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0049,  0.0616, -0.3042,  ..., -0.3372,  0.0529,  0.4490],\n",
       "          [-0.0594,  0.0690, -0.4773,  ..., -0.4209,  0.3635,  0.0644]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.4570e-01, 3.6438e-02, 1.1358e-03, 5.6982e-01, 1.7862e-03, 4.0054e-05,\n",
       "           5.9843e-04, 2.5344e-04, 4.3945e-02],\n",
       "          [9.3213e-01, 1.7075e-02, 9.3365e-04, 4.1595e-02, 3.4034e-05, 1.0133e-06,\n",
       "           3.9160e-05, 9.5367e-05, 8.0643e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    44.412155  181.008911  212.329208  333.416321    0.937294      0   \n",
       "  1   540.210632  200.347260  639.705139  423.927734    0.928687      0   \n",
       "  2   274.439697  145.790314  368.151917  274.278778    0.912150      0   \n",
       "  3   276.673157  312.720062  426.101135  424.813965    0.868889      0   \n",
       "  4    73.338249  314.878876  218.486633  364.917511    0.821769     71   \n",
       "  5   427.633209  324.493958  546.691345  384.772278    0.815797     71   \n",
       "  6   253.004761  275.357086  276.186829  322.154022    0.734280     39   \n",
       "  7   277.703827  290.636505  297.796722  368.907135    0.718062     39   \n",
       "  8   404.898804  332.517273  435.605469  386.900391    0.671232     41   \n",
       "  9   445.294434  375.589722  512.962036  406.295654    0.609908     79   \n",
       "  10  169.725311  399.440765  286.000610  425.569275    0.575791     71   \n",
       "  11  284.769318  264.511627  308.585785  310.543793    0.476661     39   \n",
       "  12  301.586426  274.331329  395.602417  302.751862    0.432600     71   \n",
       "  13  565.516785  254.953491  589.244568  296.506226    0.325711     79   \n",
       "  14  565.308411  254.067871  588.206360  295.318115    0.251363     41   \n",
       "  \n",
       "            name  \n",
       "  0       person  \n",
       "  1       person  \n",
       "  2       person  \n",
       "  3       person  \n",
       "  4         sink  \n",
       "  5         sink  \n",
       "  6       bottle  \n",
       "  7       bottle  \n",
       "  8          cup  \n",
       "  9   toothbrush  \n",
       "  10        sink  \n",
       "  11      bottle  \n",
       "  12        sink  \n",
       "  13  toothbrush  \n",
       "  14         cup  ,\n",
       "  'caption': ['The reflection of a little girl in the mirror.',\n",
       "   \"The little girl's reflection of her sticking her tongue out.\"],\n",
       "  'bbox_target': [44.1, 181.67, 170.37, 155.32]},\n",
       " 561: {'image_emb': tensor([[-0.1003,  0.1921, -0.2700,  ...,  1.0654, -0.0817,  0.1199],\n",
       "          [-0.0565,  0.1401, -0.1814,  ...,  0.8794,  0.0786,  0.2330],\n",
       "          [ 0.0894,  0.1602, -0.1188,  ...,  0.9565,  0.3135, -0.0276],\n",
       "          [ 0.1293, -0.2139, -0.0578,  ...,  0.5127,  0.2288,  0.4805]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2976,  0.5430, -0.1829,  ..., -0.3230, -0.3789, -0.1101],\n",
       "          [-0.1812,  0.0622, -0.4880,  ...,  0.1141,  0.3438,  0.2817]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9121e-01, 8.5754e-03, 7.1526e-07, 6.4492e-05],\n",
       "          [9.4678e-01, 9.5749e-03, 4.9973e-04, 4.2938e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    0.553356    0.000000  107.268814  419.825073    0.921789      0  person\n",
       "  1  382.007568  191.411255  456.613464  406.914551    0.905659      0  person\n",
       "  2  268.295319  242.193359  299.526398  278.676758    0.834501     65  remote\n",
       "  3  170.339996  137.615128  289.842468  420.693970    0.633304      0  person\n",
       "  4  307.286652  275.909271  424.732574  384.664764    0.510876     56   chair\n",
       "  5  415.101013  298.709839  435.384949  327.857361    0.492399     65  remote\n",
       "  6  467.967438  248.913849  483.096649  262.893707    0.362882     74   clock\n",
       "  7  143.434586  191.274597  291.005249  348.501648    0.272860     56   chair,\n",
       "  'caption': ['a person wearing a brown blue and grey shirt',\n",
       "   'man standing with striped shirt'],\n",
       "  'bbox_target': [0.96, 0.0, 103.16, 422.32]},\n",
       " 562: {'image_emb': tensor([[-0.1923, -0.1119, -0.1471,  ...,  0.4973,  0.0484, -0.4456],\n",
       "          [-0.3105,  0.0083,  0.1222,  ...,  0.6230, -0.0937, -0.3362],\n",
       "          [-0.2094, -0.1251, -0.0511,  ...,  0.4148, -0.0641, -0.3655]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1074,  0.2798, -0.0960,  ..., -0.0172, -0.3960, -0.3499],\n",
       "          [-0.0933,  0.2595, -0.1599,  ...,  0.2122, -0.3503, -0.0262]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0552, 0.9331, 0.0117],\n",
       "          [0.1063, 0.8228, 0.0708]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0    6.105682   47.133911  420.686523  639.048035    0.811979     17  horse\n",
       "  1  166.080383  103.872360  426.011292  502.937134    0.758202     17  horse,\n",
       "  'caption': ['A brown and white horse with its eyes closed',\n",
       "   'Brown and White horse tied to a fence'],\n",
       "  'bbox_target': [171.25, 104.49, 255.75, 409.25]},\n",
       " 563: {'image_emb': tensor([[-0.2781,  0.0279,  0.3928,  ...,  0.0518, -0.1947,  0.4001],\n",
       "          [-0.3083, -0.0383,  0.0817,  ...,  0.4839,  0.4404,  0.1782],\n",
       "          [ 0.4822,  0.0782, -0.0513,  ...,  0.3474,  0.2122,  0.1031],\n",
       "          ...,\n",
       "          [ 0.0066, -0.1207, -0.2717,  ...,  0.4907,  0.1814, -0.1835],\n",
       "          [-0.2766, -0.0504, -0.1626,  ...,  0.9116, -0.4583,  0.2949],\n",
       "          [-0.2102, -0.2025,  0.5586,  ...,  0.1858,  0.1694,  0.3574]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.4148, -0.0242, -0.6025,  ..., -0.0979, -0.2080, -0.5542],\n",
       "          [-0.1616, -0.1921, -0.2991,  ..., -0.0847, -0.1798, -0.5020]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7035e-04, 9.7900e-01, 2.1338e-05, 5.9605e-08, 3.5501e-04, 1.0880e-02,\n",
       "           2.6798e-04, 9.3002e-03],\n",
       "          [3.0398e-06, 9.5361e-01, 1.6594e-04, 2.6226e-06, 1.1051e-04, 4.3213e-02,\n",
       "           8.4257e-04, 2.2545e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  409.720520   31.797073  515.885559  147.586594    0.916098     77   \n",
       "  1  134.653076   89.197113  378.029541  419.812531    0.897217      0   \n",
       "  2  378.506683  173.531982  527.120850  419.063354    0.886297      0   \n",
       "  3  421.214661  318.472412  494.675659  394.391418    0.870937     56   \n",
       "  4  558.386658  322.372528  614.605652  398.820953    0.809310     77   \n",
       "  5  518.672729  284.690277  555.550293  328.191010    0.763359     77   \n",
       "  6  328.083008  341.053650  387.769958  399.671753    0.747448     77   \n",
       "  7   91.975143  182.854828  149.960556  249.786041    0.614676      0   \n",
       "  8  371.987671  231.684479  445.110291  286.449493    0.271924     77   \n",
       "  \n",
       "           name  \n",
       "  0  teddy bear  \n",
       "  1      person  \n",
       "  2      person  \n",
       "  3       chair  \n",
       "  4  teddy bear  \n",
       "  5  teddy bear  \n",
       "  6  teddy bear  \n",
       "  7      person  \n",
       "  8  teddy bear  ,\n",
       "  'caption': ['A young girl in a white dress pulling on a plush toy',\n",
       "   'A girl wearing white gown'],\n",
       "  'bbox_target': [131.89, 90.81, 243.25, 327.57]},\n",
       " 564: {'image_emb': tensor([[-5.4346e-01,  2.7783e-01, -4.8920e-02,  ...,  8.9209e-01,\n",
       "            1.6187e-01, -5.8317e-04],\n",
       "          [ 4.2529e-01,  6.5137e-01, -1.5515e-01,  ...,  6.8213e-01,\n",
       "           -2.0862e-01,  1.5796e-01],\n",
       "          [-4.4922e-01,  6.4355e-01,  3.0640e-01,  ...,  1.2080e+00,\n",
       "           -9.9976e-02, -1.2152e-01],\n",
       "          ...,\n",
       "          [-1.0101e-01,  9.1797e-01, -2.8735e-01,  ...,  4.6948e-01,\n",
       "           -8.2825e-02,  4.1687e-02],\n",
       "          [ 3.1445e-01,  4.9292e-01, -1.3771e-02,  ...,  7.1680e-01,\n",
       "           -2.0325e-01,  9.2529e-02],\n",
       "          [ 3.1372e-01,  4.1235e-01, -1.1023e-01,  ...,  8.1689e-01,\n",
       "           -8.7402e-02,  9.7595e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3789,  0.4368,  0.1726,  ...,  0.1088,  0.1324, -0.3872],\n",
       "          [-0.2211,  0.0066, -0.1786,  ...,  0.1244, -0.0445, -0.2629]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.4234e-03, 5.9032e-04, 9.7217e-01, 6.1531e-03, 5.9624e-03, 4.2305e-03,\n",
       "           1.2693e-03, 2.2984e-03],\n",
       "          [3.3936e-02, 2.3441e-03, 9.3115e-01, 1.1425e-03, 3.5954e-04, 1.1009e-02,\n",
       "           6.2752e-03, 1.3702e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   81.325272    1.092140  342.250061  165.488342    0.919262      0   \n",
       "  1  242.928680  178.275391  420.458038  363.263855    0.914221     54   \n",
       "  2  521.242371  311.548798  639.585876  477.382446    0.910031     45   \n",
       "  3  470.421967  178.955292  639.847717  327.130280    0.896625     45   \n",
       "  4    0.454090  121.170898   74.619354  257.157227    0.866724     45   \n",
       "  5  165.071625  160.078888  273.011841  390.473969    0.828125     44   \n",
       "  6    1.047974    6.448364  634.205200  473.442017    0.719425     60   \n",
       "  7  164.622284  166.166550  210.026703  305.881653    0.364763     44   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1         donut  \n",
       "  2          bowl  \n",
       "  3          bowl  \n",
       "  4          bowl  \n",
       "  5         spoon  \n",
       "  6  dining table  \n",
       "  7         spoon  ,\n",
       "  'caption': ['Bowl with orange liquid in it.',\n",
       "   'A cup of orange dipping sauce.'],\n",
       "  'bbox_target': [519.56, 311.53, 120.44, 168.47]},\n",
       " 565: {'image_emb': tensor([[-0.1843, -0.0969, -0.4211,  ...,  0.2822,  0.0685, -0.1345],\n",
       "          [-0.1975, -0.0403, -0.2546,  ...,  0.7549, -0.1270, -0.0846],\n",
       "          [ 0.1594, -0.2668, -0.2096,  ...,  0.3818, -0.2003, -0.2217]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1370, -0.0046, -0.2321,  ..., -0.1221,  0.2205, -0.1832],\n",
       "          [ 0.1656, -0.1624, -0.1221,  ..., -0.0471,  0.3904, -0.0063]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.9102, 0.0066, 0.0833],\n",
       "          [0.4319, 0.2620, 0.3062]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  253.727173  258.730438  619.147217  422.178650    0.899319      6  train\n",
       "  1  117.255394  219.982117  308.450928  288.368530    0.817261      6  train\n",
       "  2  217.023712  196.661499  371.827606  264.206238    0.690612      6  train\n",
       "  3  471.272491  223.021912  508.243866  278.408325    0.663406      6  train,\n",
       "  'caption': ['The train with the red part on top.',\n",
       "   'The black and red train.'],\n",
       "  'bbox_target': [246.03, 261.34, 376.22, 158.92]},\n",
       " 566: {'image_emb': tensor([[-0.1647,  0.4958, -0.0394,  ...,  1.1191, -0.0914, -0.1536],\n",
       "          [-0.0552,  0.0811, -0.0663,  ...,  1.3613,  0.0260, -0.1962],\n",
       "          [ 0.1172,  0.1493, -0.0432,  ...,  1.2148, -0.0905, -0.1225],\n",
       "          ...,\n",
       "          [-0.0411,  0.6030, -0.3416,  ...,  1.1436,  0.0836, -0.3823],\n",
       "          [-0.0170,  0.2224, -0.1790,  ...,  1.0596, -0.0580, -0.2849],\n",
       "          [-0.0579,  0.1964, -0.1461,  ...,  0.9199, -0.2080,  0.0233]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3711, -0.2271, -0.1326,  ..., -0.2268, -0.2053, -0.0868],\n",
       "          [-0.0511,  0.1296, -0.0574,  ...,  0.2727, -0.1158, -0.2128]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.0625e-01, 7.6721e-02, 4.4670e-03, 1.3628e-03, 1.9634e-04, 1.0719e-02,\n",
       "           2.5201e-04],\n",
       "          [6.4893e-01, 3.2617e-01, 1.0818e-02, 1.2207e-04, 3.0398e-06, 1.3893e-02,\n",
       "           2.0444e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  346.171997  411.603760  480.000000  638.665039    0.932159     56   \n",
       "  1  279.012604  362.837158  445.072388  502.738159    0.864565     56   \n",
       "  2  243.762146  320.125000  461.601257  427.485718    0.857370     57   \n",
       "  3  274.196075  215.260071  349.679291  335.886780    0.854965     58   \n",
       "  4  383.835693  287.886902  411.298950  330.204651    0.803889     75   \n",
       "  5   32.406975  306.282104  173.908020  372.254028    0.736926     57   \n",
       "  6  142.011368  323.438995  229.510803  413.128632    0.591896     56   \n",
       "  7  198.074615  355.601105  239.142975  392.620636    0.412387     75   \n",
       "  \n",
       "             name  \n",
       "  0         chair  \n",
       "  1         chair  \n",
       "  2         couch  \n",
       "  3  potted plant  \n",
       "  4          vase  \n",
       "  5         couch  \n",
       "  6         chair  \n",
       "  7          vase  ,\n",
       "  'caption': ['chair lower right hand corner.',\n",
       "   'arm chair with wooden arm rests and legs'],\n",
       "  'bbox_target': [345.1, 412.41, 134.9, 227.59]},\n",
       " 567: {'image_emb': tensor([[-0.0012,  0.0244,  0.3628,  ...,  1.0098, -0.3601, -0.3499],\n",
       "          [ 0.0225,  0.1370,  0.3623,  ...,  0.8569, -0.0895, -0.4336],\n",
       "          [-0.0538,  0.2549, -0.2257,  ...,  1.2129,  0.0520, -0.2494],\n",
       "          [ 0.0101,  0.1036,  0.1349,  ...,  0.4199, -0.2434, -0.2333]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2737,  0.0405,  0.1456,  ...,  0.3423, -0.3931,  0.0082],\n",
       "          [ 0.0880,  0.1449, -0.0618,  ...,  0.1028, -0.4736, -0.2451]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.6763e-02, 8.8818e-01, 1.5211e-04, 5.5023e-02],\n",
       "          [2.0142e-01, 7.7197e-01, 2.1225e-02, 5.4512e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  112.966667  282.008240  359.454376  465.483826    0.926000     17  horse\n",
       "  1  425.358856  283.578186  624.084473  430.357849    0.908183     17  horse\n",
       "  2  414.788574  283.229340  463.030457  404.115204    0.749525     17  horse,\n",
       "  'caption': ['A dark brown horse with a short mane grazing on grass in front of a lake.',\n",
       "   'A chocolate-colored horse feeding to the right of another horse'],\n",
       "  'bbox_target': [430.76, 288.23, 193.21, 138.31]},\n",
       " 568: {'image_emb': tensor([[-0.2478,  0.2710, -0.1394,  ...,  1.0537,  0.0680,  0.2111],\n",
       "          [ 0.1011, -0.0224, -0.0451,  ...,  0.7983,  0.2744,  0.0251],\n",
       "          [-0.2223,  0.2263, -0.1188,  ...,  1.1338, -0.0715, -0.3357],\n",
       "          ...,\n",
       "          [-0.1973,  0.4648, -0.0974,  ...,  1.0127,  0.2947, -0.0788],\n",
       "          [-0.1725,  0.5601, -0.4028,  ...,  1.0322, -0.3037, -0.1201],\n",
       "          [ 0.0110,  0.5273, -0.3811,  ...,  0.7129, -0.0526,  0.0177]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2196,  0.0900, -0.3296,  ..., -0.0190,  0.1903, -0.0898],\n",
       "          [-0.3833,  0.3262,  0.0098,  ..., -0.0939, -0.0956, -0.0181]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.4636e-02, 6.0081e-03, 3.4790e-03, 9.1125e-02, 7.5098e-01, 7.0984e-02,\n",
       "           1.2726e-02],\n",
       "          [6.0516e-02, 7.5006e-04, 1.9043e-02, 2.7124e-01, 3.8232e-01, 2.6294e-01,\n",
       "           3.2578e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  237.342255  126.314659  435.817810  290.642120    0.905208     56   \n",
       "  1    0.256706    8.674759  108.391266  159.622849    0.889406      0   \n",
       "  2  424.631805  116.768425  480.000000  157.586487    0.870785     56   \n",
       "  3  137.520447  248.076157  299.098511  418.110718    0.861325     75   \n",
       "  4  226.769318  388.407806  313.413055  449.893829    0.796176     44   \n",
       "  5    4.904556  281.204224  477.726440  628.003174    0.731050     60   \n",
       "  6  431.844330  153.723846  479.998657  288.494843    0.461592     60   \n",
       "  7  261.671356  378.562744  383.078766  510.645386    0.409026     45   \n",
       "  8  262.115082  379.335846  383.054474  513.391663    0.371094     41   \n",
       "  9  130.957764  401.797852  267.277954  536.130493    0.357757     41   \n",
       "  \n",
       "             name  \n",
       "  0         chair  \n",
       "  1        person  \n",
       "  2         chair  \n",
       "  3          vase  \n",
       "  4         spoon  \n",
       "  5  dining table  \n",
       "  6  dining table  \n",
       "  7          bowl  \n",
       "  8           cup  \n",
       "  9           cup  ,\n",
       "  'caption': ['silver container on left',\n",
       "   'The metal bowl with brown grounds in it.'],\n",
       "  'bbox_target': [134.12, 404.07, 132.71, 132.14]},\n",
       " 569: {'image_emb': tensor([[ 0.0453,  0.2294, -0.0806,  ...,  0.6045,  0.2067,  0.1365],\n",
       "          [-0.7031,  0.3486, -0.1732,  ...,  0.3667,  0.3323,  0.1086],\n",
       "          [ 0.0236,  0.6904,  0.1934,  ...,  0.6982,  0.1814,  0.0153],\n",
       "          [-0.1765,  0.3303,  0.0575,  ...,  0.4553,  0.1437, -0.3345]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1726, -0.1788, -0.0982,  ..., -0.2018,  0.3777,  0.0597],\n",
       "          [-0.0081,  0.0532, -0.1114,  ..., -0.2316, -0.1130, -0.1098]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9756e-01, 1.7805e-03, 2.6822e-06, 7.1955e-04],\n",
       "          [4.1675e-01, 2.9077e-01, 1.4938e-02, 2.7759e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.593531    0.906169  164.128891  369.143188    0.946622      0   \n",
       "  1  267.894531    1.685965  499.524414  364.696686    0.945888      0   \n",
       "  2  150.168137  257.256287  319.348633  365.234558    0.739634     55   \n",
       "  3  296.455078  283.433655  383.791138  310.597504    0.694960     43   \n",
       "  4  100.195763  225.552368  414.945221  373.075623    0.693131     60   \n",
       "  5  149.268570  294.861206  190.704880  330.376129    0.604075     43   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2          cake  \n",
       "  3         knife  \n",
       "  4  dining table  \n",
       "  5         knife  ,\n",
       "  'caption': ['This woman is wearing a turtleneck sweater',\n",
       "   'The woman has a knife'],\n",
       "  'bbox_target': [0.0, 1.48, 169.3, 368.23]},\n",
       " 570: {'image_emb': tensor([[-0.0327,  0.2891,  0.0852,  ...,  0.4397, -0.2373, -0.3042],\n",
       "          [-0.1156,  0.0662,  0.0492,  ...,  0.7363, -0.2820, -0.1246],\n",
       "          [ 0.0443,  0.0325, -0.0807,  ...,  0.4917, -0.1464, -0.1436],\n",
       "          [ 0.0858, -0.0456, -0.0644,  ...,  0.5522, -0.3293, -0.3303]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2627,  0.2307, -0.4048,  ..., -0.2076,  0.0516, -0.0861],\n",
       "          [ 0.3372,  0.2783, -0.4312,  ...,  0.4028, -0.2173, -0.0390]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3665, 0.1992, 0.2087, 0.2257],\n",
       "          [0.0862, 0.2964, 0.4590, 0.1586]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   220.753769  201.591919  327.940460  335.254852    0.859372     56   \n",
       "  1    95.170250  254.330536  204.649872  337.797455    0.853683     56   \n",
       "  2    77.342102   28.209007  435.289368  342.119659    0.846761     25   \n",
       "  3     0.000000  269.141052   46.766186  288.033051    0.643077     25   \n",
       "  4   399.072632  279.386719  450.047668  313.543030    0.638640      0   \n",
       "  5   174.101837  260.743469  212.367889  302.153717    0.616963     56   \n",
       "  6   362.070618  230.196121  448.690216  284.616821    0.550830     25   \n",
       "  7     0.238812  296.402863   57.510746  336.919464    0.532401     56   \n",
       "  8   361.896942  229.946136  448.307861  252.557709    0.458160     25   \n",
       "  9   215.558212  256.131836  253.137085  292.797089    0.318258     56   \n",
       "  10   26.519445  293.684784   41.574673  309.349274    0.309213     56   \n",
       "  11   62.790577  294.339905  117.966225  348.050659    0.259863     24   \n",
       "  \n",
       "          name  \n",
       "  0      chair  \n",
       "  1      chair  \n",
       "  2   umbrella  \n",
       "  3   umbrella  \n",
       "  4     person  \n",
       "  5      chair  \n",
       "  6   umbrella  \n",
       "  7      chair  \n",
       "  8   umbrella  \n",
       "  9      chair  \n",
       "  10     chair  \n",
       "  11  backpack  ,\n",
       "  'caption': ['big green umbrella, in foreground', 'green beach umbrella'],\n",
       "  'bbox_target': [73.68, 30.25, 360.57, 306.99]},\n",
       " 571: {'image_emb': tensor([[ 0.0942,  0.0948, -0.1638,  ...,  0.9067,  0.2847,  0.1761],\n",
       "          [ 0.0764,  0.1389, -0.2275,  ...,  0.8994,  0.2094,  0.1556],\n",
       "          [ 0.1931,  0.1478, -0.0981,  ...,  0.8462,  0.0338, -0.1175],\n",
       "          ...,\n",
       "          [-0.1353,  0.0157,  0.1661,  ...,  1.2041, -0.0194,  0.1395],\n",
       "          [-0.0329,  0.0550, -0.1165,  ...,  0.9521,  0.0936,  0.0288],\n",
       "          [ 0.0365, -0.2683, -0.1753,  ...,  0.4744,  0.1754,  0.1683]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0256,  0.1721, -0.2500,  ..., -0.2135, -0.1302, -0.1066],\n",
       "          [ 0.0032, -0.0184, -0.5229,  ..., -0.3726, -0.0962, -0.1931]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1738, 0.6260, 0.0012, 0.0007, 0.0376, 0.0024, 0.1583],\n",
       "          [0.0764, 0.0314, 0.0013, 0.0019, 0.4263, 0.0091, 0.4536]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  410.467804   98.711288  639.366699  331.558655    0.946458      0   \n",
       "  1    0.903976   17.353867  250.511978  290.841370    0.940275      0   \n",
       "  2   28.819077  269.245758  332.588379  414.185883    0.909944     53   \n",
       "  3  364.865845  283.449951  639.779297  459.711609    0.899189     53   \n",
       "  4    0.674713   86.312592  629.086792  300.658844    0.857766     13   \n",
       "  5    0.450897  276.576935  638.967651  471.902710    0.819166     60   \n",
       "  6   44.109039    0.000000  634.159424  152.692688    0.275067      5   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2         pizza  \n",
       "  3         pizza  \n",
       "  4         bench  \n",
       "  5  dining table  \n",
       "  6           bus  ,\n",
       "  'caption': ['The older of the two girls', 'girl on left.'],\n",
       "  'bbox_target': [4.33, 16.22, 248.73, 269.28]},\n",
       " 572: {'image_emb': tensor([[-1.3135e-01,  1.3477e-01, -2.0215e-01,  5.0879e-01,  1.3159e-01,\n",
       "            4.2847e-01,  6.7993e-02,  3.1152e-01, -3.4277e-01,  2.5684e-01,\n",
       "           -1.8945e-01, -5.0293e-01, -2.2791e-01,  6.2561e-02,  1.0730e-01,\n",
       "            1.6528e-01, -6.8359e-02,  2.9028e-01,  4.8267e-01, -2.8589e-01,\n",
       "            4.4385e-01, -7.5989e-02,  4.7705e-01, -5.8789e-01,  7.6660e-02,\n",
       "            4.3628e-01,  1.4795e-01, -2.8101e-01, -1.0938e-01, -2.7051e-01,\n",
       "            1.2634e-01,  3.4985e-01,  9.8572e-02, -1.4404e-01, -5.7031e-01,\n",
       "            3.8550e-01,  1.5051e-01, -1.0205e-01,  3.1665e-01,  9.7412e-01,\n",
       "            3.8599e-01,  9.9854e-02, -4.0479e-01,  5.5313e-03,  8.5022e-02,\n",
       "           -1.1855e+00,  1.9226e-01,  1.1261e-01,  3.1616e-02, -1.6028e-01,\n",
       "            1.5442e-01, -3.2153e-01, -2.4841e-01, -2.8101e-01,  1.6431e-01,\n",
       "            1.5186e-01,  2.1130e-01,  2.5269e-01, -1.7385e-03,  2.7295e-01,\n",
       "           -4.3701e-01, -1.8542e-01, -1.2036e-01, -2.1082e-01, -1.3135e-01,\n",
       "            4.5752e-01,  9.9304e-02,  4.4629e-01,  3.0591e-01, -2.1851e-01,\n",
       "           -2.6221e-01,  5.7910e-01,  2.5854e-01, -2.3413e-01,  1.4368e-01,\n",
       "           -4.5776e-02,  1.0486e-01, -4.2938e-02,  2.8427e-02, -6.2134e-02,\n",
       "           -2.0752e-02, -1.9653e-01, -1.9318e-02, -4.1235e-01,  1.4673e-01,\n",
       "            1.6504e-01,  5.1003e-03, -1.6760e-01, -1.0156e-01, -1.7371e-01,\n",
       "            1.2756e-01,  5.8472e-02, -7.8359e+00,  2.1912e-01,  1.3232e-01,\n",
       "            1.5869e-01, -1.7090e-02, -9.6130e-02,  2.6270e-01,  8.6475e-01,\n",
       "            1.3672e-01,  2.5928e-01, -3.3179e-01, -9.0454e-02,  8.6426e-01,\n",
       "           -2.3022e-01, -5.3418e-01, -8.1116e-02,  4.6356e-02,  1.4429e-01,\n",
       "           -2.4817e-01, -3.7744e-01,  1.5515e-01, -3.4058e-01,  9.9182e-03,\n",
       "           -1.9714e-01, -1.6687e-01,  4.0063e-01,  5.4639e-01,  2.4597e-01,\n",
       "            2.1057e-01, -1.4482e+00, -1.8970e-01,  2.8247e-01, -2.9770e-02,\n",
       "           -3.3521e-01, -4.6661e-02,  2.1912e-01, -1.4661e-01,  7.5012e-02,\n",
       "            3.0566e-01, -1.8018e-01, -2.0947e-01,  1.0156e+00, -6.4014e-01,\n",
       "            6.8604e-02, -2.3840e-01, -1.0840e-01,  4.1229e-02,  1.5234e-01,\n",
       "            5.0830e-01, -6.3049e-02, -3.0298e-01, -3.6259e-03, -3.7659e-02,\n",
       "           -3.1104e-01, -7.8003e-02,  5.3497e-02, -9.5093e-02,  3.0664e-01,\n",
       "            2.4707e-01, -1.7566e-01,  3.8232e-01, -2.0422e-01, -1.5808e-01,\n",
       "           -1.7957e-01,  1.7346e-01, -1.7200e-01,  2.8613e-01, -1.7310e-01,\n",
       "           -1.0254e-01, -5.0000e-01,  1.4441e-01,  2.5952e-01, -3.5303e-01,\n",
       "            1.0608e-01,  2.3999e-01,  3.0078e-01,  4.9927e-01, -2.2412e-01,\n",
       "           -3.0688e-01, -1.1279e-01,  1.5186e-01, -2.3083e-01, -4.4019e-01,\n",
       "            2.4280e-01,  1.2881e+00,  6.9824e-01, -2.4185e-02, -1.5613e-01,\n",
       "            2.2815e-01, -1.5955e-01,  2.1744e-02,  4.0430e-01, -2.1826e-01,\n",
       "            5.2344e-01,  4.1382e-01,  1.7151e-01,  1.6553e-01, -3.9703e-02,\n",
       "            4.8714e-03,  2.6025e-01,  4.3335e-01,  2.0874e-01, -2.0325e-02,\n",
       "           -3.1616e-01,  4.0088e-01, -8.0139e-02, -9.5459e-01, -6.6772e-02,\n",
       "            5.3528e-02,  1.9775e-01, -4.7339e-01,  5.8685e-02, -1.3016e-02,\n",
       "            2.4506e-02,  2.2705e-01, -1.3403e-01,  3.6407e-02,  5.3253e-02,\n",
       "           -6.1426e-01,  2.3413e-01,  1.5015e-01, -1.9507e-01,  2.4548e-01,\n",
       "           -1.9128e-01,  2.0581e-01, -1.2408e-01,  5.6689e-01, -1.4091e-02,\n",
       "           -2.2449e-01,  5.5615e-01, -1.3989e-01,  5.3528e-02, -8.8562e-02,\n",
       "            3.8727e-02,  2.8748e-02,  1.8250e-02, -1.0828e-01,  6.5979e-02,\n",
       "            3.5370e-02,  2.0325e-01,  1.3171e-01,  6.3525e-01, -2.0654e-01,\n",
       "            3.7354e-02, -1.8173e-02, -1.3110e-01, -1.5601e-01,  2.5659e-01,\n",
       "           -1.2610e-01,  3.2983e-01, -1.6968e-01,  2.8061e-02, -5.8350e-01,\n",
       "            5.7275e-01,  9.4849e-02, -1.5710e-01,  4.5728e-01,  4.7021e-01,\n",
       "            1.4343e-01,  1.9971e-01, -1.7102e-01, -3.2562e-02, -1.6467e-01,\n",
       "            6.2042e-02, -2.6709e-01,  2.5269e-01, -7.3633e-01,  1.1029e-01,\n",
       "            2.4792e-01,  2.5269e-01, -1.7749e-01,  1.2927e-01,  4.0723e-01,\n",
       "           -3.7793e-01, -5.4395e-01, -4.6234e-02,  6.6528e-02,  1.1304e-01,\n",
       "           -3.9160e-01,  1.3721e-01, -1.6602e-01,  1.6797e-01, -1.6785e-01,\n",
       "           -4.5135e-02, -1.3037e-01,  1.5381e-01, -2.1033e-01,  1.3550e-01,\n",
       "            2.1399e-01,  9.1431e-02, -2.5757e-01,  3.6938e-01,  4.7363e-02,\n",
       "            2.8369e-01, -7.8613e-01, -2.6685e-01, -2.6636e-01, -4.1479e-01,\n",
       "            1.2817e-01,  1.2561e-01,  4.9658e-01,  3.2690e-01, -1.2494e-01,\n",
       "            2.6562e-01, -4.8218e-01,  2.2107e-01,  1.8396e-01,  3.1104e-01,\n",
       "            3.5205e-01, -4.2065e-01, -2.7069e-02, -2.7313e-02,  1.6443e-01,\n",
       "           -2.1545e-02, -2.5589e-02, -7.7209e-02,  2.2925e-01, -2.0837e-01,\n",
       "           -7.7490e-01,  3.4741e-01,  1.0146e+00,  5.9863e-01,  9.4681e-03,\n",
       "            4.3262e-01,  2.9126e-01,  4.5923e-01,  4.9652e-02, -7.4219e-01,\n",
       "            3.8403e-01,  1.6641e+00,  2.2644e-01, -1.5588e-01,  1.0504e-01,\n",
       "           -3.0350e-02, -1.0791e-01,  3.6682e-02, -1.5266e-02,  3.5254e-01,\n",
       "           -3.9856e-02, -3.7915e-01, -3.2135e-02, -2.4561e-01,  1.3513e-01,\n",
       "            6.2225e-02,  7.1411e-02,  3.6304e-01,  3.8940e-01,  8.8074e-02,\n",
       "           -6.1676e-02, -3.4937e-01, -1.4246e-01,  1.9238e-01,  6.1462e-02,\n",
       "           -6.7578e-01, -4.2725e-01,  3.1860e-01, -2.0447e-01, -3.1952e-02,\n",
       "            2.6221e-01, -4.9170e-01,  2.5684e-01, -3.2373e-01, -6.8787e-02,\n",
       "           -5.5957e-01, -2.0264e-01,  4.5508e-01,  2.4402e-01,  3.1470e-01,\n",
       "            8.5449e-02,  4.0649e-01, -1.1035e-01, -6.8665e-02, -2.0203e-01,\n",
       "            1.7419e-01, -1.0461e-01, -3.6743e-01, -2.2925e-01,  2.6709e-01,\n",
       "           -4.3549e-02,  2.1973e-01,  1.2073e-01, -1.4539e-01,  4.2407e-01,\n",
       "            7.7576e-02,  1.3779e+00, -1.9922e-01, -3.7915e-01, -4.6234e-02,\n",
       "            2.7905e-01, -2.7612e-01, -1.6602e-01,  2.9712e-01,  8.5754e-02,\n",
       "           -1.3818e-01, -5.5878e-02,  1.2817e-01,  6.6467e-02, -1.1578e-03,\n",
       "           -2.5342e-01, -4.7900e-01, -8.8684e-02,  3.5547e-01,  2.3865e-01,\n",
       "           -2.3987e-02, -1.6113e-01, -2.4805e-01, -2.0007e-01, -2.8778e-02,\n",
       "           -3.4619e-01,  9.9304e-02,  1.0977e+00,  3.7012e-01, -9.5032e-02,\n",
       "           -3.9551e-02, -5.0781e-02,  8.1848e-02,  6.2164e-02, -7.8674e-02,\n",
       "            6.0081e-03, -8.2703e-02, -6.1218e-02,  2.1164e-02, -1.4246e-01,\n",
       "           -2.6929e-01, -2.8369e-01, -2.9443e-01, -3.7573e-01, -7.1680e-01,\n",
       "           -6.2451e-01, -2.3389e-01,  1.2268e-01,  6.4551e-01, -2.9907e-01,\n",
       "           -1.8640e-01, -3.2031e-01,  8.6060e-02,  6.8506e-01, -8.2031e-02,\n",
       "            1.8677e-01,  5.8398e-01,  9.5508e-01, -3.7671e-01,  1.0242e-01,\n",
       "            3.9746e-01,  7.3669e-02, -2.2900e-01, -1.0658e-02, -9.7534e-02,\n",
       "           -1.7920e-01,  2.1680e-01,  3.0493e-01, -2.2510e-01, -1.9608e-02,\n",
       "            5.5969e-02,  2.5586e-01,  2.3303e-01,  8.8562e-02,  1.9470e-01,\n",
       "           -1.3514e-03, -6.7822e-01, -1.4844e-01, -6.4636e-02, -9.2239e-03,\n",
       "            1.7102e-01,  2.7930e-01,  4.0039e-01, -2.0203e-01, -2.1497e-01,\n",
       "           -1.5295e-01, -3.1763e-01, -8.2302e-04, -2.3779e-01,  4.3652e-01,\n",
       "           -2.5659e-01, -5.2979e-01,  3.9787e-03, -1.7786e-01,  3.4326e-01,\n",
       "           -1.7731e-02, -4.7607e-02, -1.7273e-01, -3.8745e-01, -8.0261e-02,\n",
       "           -4.1895e-01, -2.0630e-01, -4.0356e-01,  2.3572e-01, -2.0374e-01,\n",
       "           -2.9663e-01,  2.9810e-01, -1.3464e-01,  3.1958e-01,  1.0706e-01,\n",
       "           -3.5217e-02,  1.5735e-01,  4.5776e-01, -3.0518e-01, -2.3987e-01,\n",
       "           -3.1689e-01,  1.2000e-01,  5.3662e-01, -6.2793e-01,  1.5894e-01,\n",
       "           -3.5010e-01, -4.7089e-02,  2.6538e-01, -1.1670e-01,  2.8369e-01,\n",
       "           -2.5562e-01,  2.4219e-01, -2.8516e-01,  1.1047e-01,  7.2754e-01,\n",
       "            2.2253e-01,  1.3293e-01, -1.0571e-01, -1.4233e-01,  7.0508e-01,\n",
       "            3.8879e-02, -1.0596e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 4.3732e-02,  2.3206e-01, -3.5571e-01, -2.4109e-01, -1.7334e-01,\n",
       "           -2.2058e-01, -1.2695e-01, -9.6240e-01, -1.3416e-01,  4.4739e-02,\n",
       "           -2.4182e-01,  7.2144e-02,  4.3848e-01,  1.0675e-01, -1.2238e-01,\n",
       "            1.4026e-01, -3.8086e-01, -9.1736e-02, -3.3325e-01,  5.6006e-01,\n",
       "            5.4901e-02, -8.0688e-02, -1.2299e-01, -4.2999e-02,  6.8115e-02,\n",
       "            2.5317e-01, -2.5269e-01,  3.0005e-01,  8.3008e-02,  2.3438e-01,\n",
       "           -4.6851e-01,  5.0163e-03,  1.1261e-01, -2.2778e-01, -1.3954e-02,\n",
       "            1.8542e-01,  2.9565e-01, -2.4402e-01, -2.5171e-01,  3.4644e-01,\n",
       "            2.2192e-01, -8.5205e-02, -2.4933e-02, -5.4102e-01,  1.7065e-01,\n",
       "           -5.4413e-02, -1.0461e-01, -7.9803e-03,  4.1357e-01, -2.0142e-01,\n",
       "           -1.5906e-01, -4.0674e-01, -1.6980e-01,  3.2251e-01, -1.8933e-01,\n",
       "            1.8701e-01, -2.7756e-02, -2.4182e-01, -2.6685e-01,  7.2205e-02,\n",
       "            8.3130e-02, -1.2830e-01, -2.2461e-01,  1.6260e-01, -2.0239e-01,\n",
       "            5.6915e-02,  4.8071e-01, -2.5952e-01,  1.3855e-01, -5.9784e-02,\n",
       "            4.9268e-01,  2.8320e-02,  4.6631e-02, -6.9214e-02,  3.1274e-01,\n",
       "            9.2239e-03,  6.4062e-01, -2.2049e-02,  2.5439e-01,  2.0911e-01,\n",
       "           -3.1836e-01, -1.9440e-02,  1.9824e-01, -1.2292e-01,  7.4036e-02,\n",
       "            3.1958e-01, -8.2581e-02, -1.1566e-01, -4.4165e-01,  1.3391e-01,\n",
       "            3.2227e-01,  4.7882e-02, -1.4043e+00,  2.8540e-01, -9.1370e-02,\n",
       "            7.1220e-03, -3.7476e-02, -8.8379e-02,  9.6558e-02,  2.8589e-01,\n",
       "            1.9690e-01,  2.5854e-01,  3.9139e-03, -1.5198e-01,  9.6863e-02,\n",
       "            1.5234e-01,  4.5105e-02, -6.1859e-02,  1.8896e-01, -5.1514e-01,\n",
       "           -7.2876e-02,  4.1504e-01, -8.8806e-02,  1.9946e-01,  1.3696e-01,\n",
       "           -1.6565e-01,  7.7133e-03,  2.2986e-01,  2.4927e-01,  4.1675e-01,\n",
       "           -4.2786e-02, -1.0977e+00,  1.5344e-01,  5.6201e-01,  3.8135e-01,\n",
       "           -2.5909e-02, -8.0627e-02,  8.3984e-02,  1.5515e-01,  3.8330e-01,\n",
       "            1.6953e-02,  3.6035e-01, -6.6956e-02,  4.9648e+00, -4.9210e-03,\n",
       "           -2.9248e-01, -2.7661e-01,  9.8495e-03,  3.0975e-02, -5.2500e-04,\n",
       "           -4.1040e-01,  1.9006e-01, -1.0803e-01,  7.2144e-02, -1.5881e-01,\n",
       "           -4.2114e-01, -1.1621e-01, -5.7556e-02, -3.3960e-01,  4.9121e-01,\n",
       "           -1.8542e-01,  2.1057e-01,  9.6191e-02,  7.3471e-03, -2.0996e-02,\n",
       "           -1.6638e-01,  1.0687e-01, -1.7847e-01,  3.0127e-01,  4.0332e-01,\n",
       "            4.5624e-03, -1.3220e-01,  2.5146e-01,  1.3953e-01, -5.1155e-03,\n",
       "           -2.6886e-02,  3.6072e-02,  2.2424e-01,  2.8369e-01,  2.0227e-01,\n",
       "           -1.6342e-02,  2.3743e-01,  1.5247e-01,  2.0233e-02, -4.4531e-01,\n",
       "           -4.6631e-01,  1.4343e-01,  3.3936e-01, -3.0103e-01, -6.2256e-02,\n",
       "           -2.6514e-01, -9.0759e-02,  2.1802e-01, -6.3281e-01,  7.2754e-02,\n",
       "            2.2351e-01,  1.3501e-01, -4.1772e-01, -1.9324e-01, -3.9856e-02,\n",
       "            4.0649e-01,  1.1517e-01,  8.4595e-02, -6.5491e-02,  2.4805e-01,\n",
       "           -3.6548e-01, -1.1360e-02, -1.3342e-01, -6.0303e-01, -1.8848e-01,\n",
       "           -5.2783e-01, -2.6962e-02, -4.7168e-01,  1.6980e-01, -2.2293e-02,\n",
       "            4.0698e-01, -1.1224e-01,  7.7576e-02,  2.6172e-01,  1.6083e-02,\n",
       "           -1.1157e-01,  2.8906e-01, -3.5864e-01, -2.3425e-01, -2.4377e-01,\n",
       "           -2.2614e-02,  1.7688e-01, -8.4595e-02,  4.2969e-01,  8.5083e-02,\n",
       "            2.9712e-01,  1.0901e-01, -6.3354e-02,  9.4482e-02, -1.1047e-01,\n",
       "            1.3672e-01, -1.1481e-01,  2.3962e-01, -8.3557e-02, -8.0176e-01,\n",
       "           -1.5222e-01,  1.9897e-01, -1.0046e-01,  9.3445e-02, -5.0812e-02,\n",
       "            9.2834e-02, -9.9304e-02,  3.5278e-02, -3.1982e-01, -2.9443e-01,\n",
       "           -3.6621e-02, -2.5171e-01, -3.7939e-01,  3.5370e-02, -2.0532e-01,\n",
       "            2.5562e-01, -7.1106e-02, -3.1470e-01, -2.4438e-01,  1.5417e-01,\n",
       "            1.1084e-01,  1.3513e-01,  5.2002e-01,  2.2507e-03,  6.9946e-02,\n",
       "            8.9050e-02, -1.5601e-01, -1.2932e-02, -1.5723e-01,  3.7354e-01,\n",
       "           -3.9648e-01, -9.3384e-02,  1.6064e-01,  7.5195e-02, -1.0425e-01,\n",
       "           -1.0223e-01, -3.5645e-01, -4.3579e-02, -1.8872e-01, -1.2250e-01,\n",
       "           -8.8318e-02, -7.6111e-02, -2.3560e-01,  1.4514e-01, -4.4775e-01,\n",
       "           -4.4952e-02,  2.4670e-01,  8.9172e-02,  3.3081e-02,  2.2049e-02,\n",
       "            3.8513e-02, -3.9282e-01,  1.3062e-01,  5.0195e-01,  1.6406e-01,\n",
       "            6.6162e-01, -3.3350e-01,  2.4948e-02,  5.8398e-01,  8.5754e-02,\n",
       "            2.9590e-01, -1.8799e-01,  1.5881e-01, -1.0323e-02,  2.8613e-01,\n",
       "           -1.6125e-01, -5.5664e-02,  2.7612e-01, -5.2393e-01, -1.9867e-02,\n",
       "            8.1055e-02, -4.0039e-01,  8.1177e-02, -8.2458e-02,  4.1235e-01,\n",
       "           -2.9816e-02, -4.0576e-01, -1.8542e-01, -9.6359e-03, -3.8208e-01,\n",
       "            6.8359e-02,  5.8691e-01,  4.9570e+00, -3.6896e-02, -1.2402e-01,\n",
       "            1.4258e-01,  1.9543e-01, -1.2244e-01,  2.4133e-01, -1.7807e-02,\n",
       "            1.3977e-01,  3.5352e-01, -1.5247e-01, -3.3691e-02, -2.5195e-01,\n",
       "            1.6809e-01,  1.6844e-04,  5.4047e-02,  7.5195e-02, -2.3652e+00,\n",
       "            3.2324e-01, -5.8746e-02,  1.0229e-01, -1.2622e-01,  1.4374e-02,\n",
       "           -5.3101e-02, -2.9736e-01, -3.4473e-01,  2.6440e-01,  3.4253e-01,\n",
       "            1.0773e-01, -1.5100e-01,  2.5360e-02, -1.9885e-01, -6.6040e-02,\n",
       "            2.0166e-01,  3.2227e-01,  4.2749e-01, -5.1123e-01, -5.2490e-02,\n",
       "            4.3530e-01, -3.8025e-02,  7.2083e-02, -1.5234e-01, -2.8125e-01,\n",
       "           -1.5356e-01, -1.7847e-01,  3.1494e-02,  3.0737e-01,  2.3230e-01,\n",
       "           -6.8115e-02, -3.3386e-02, -8.7952e-02,  9.3506e-02, -1.7432e-01,\n",
       "           -2.5977e-01, -1.5405e-01, -2.6099e-01, -4.5190e-01,  3.6774e-02,\n",
       "           -6.0669e-02, -1.5222e-01, -3.1689e-01,  3.3887e-01,  4.0497e-02,\n",
       "           -8.5388e-02, -2.1411e-01,  1.5430e-01,  1.9958e-01,  1.6211e-01,\n",
       "            1.1365e-01, -6.2256e-03,  1.0223e-01, -8.7891e-02,  2.9565e-01,\n",
       "            4.6021e-01,  3.2666e-01,  2.6782e-01, -9.9121e-02, -1.5210e-01,\n",
       "           -3.6768e-01,  3.8770e-01, -1.1536e-01, -2.4561e-01,  1.6748e-01,\n",
       "           -1.6235e-01, -6.0822e-02, -2.0538e-02, -4.3359e-01,  1.3098e-01,\n",
       "            5.6689e-01, -2.2217e-01,  3.6694e-01, -3.9941e-01, -1.9604e-01,\n",
       "            1.7810e-01,  4.3872e-01,  4.2065e-01, -2.3987e-01, -1.2537e-01,\n",
       "           -1.0364e-01, -1.4465e-01,  5.0934e-02,  6.5039e-01,  6.6223e-02,\n",
       "            1.1920e-01, -2.1460e-01,  1.0498e-01,  2.6465e-01, -3.1708e-02,\n",
       "            3.0933e-01, -3.2397e-01,  1.2256e-01, -2.6929e-01, -1.1490e-02,\n",
       "            1.9446e-01, -8.1238e-02, -5.1788e-02,  2.6221e-01, -1.0236e-01,\n",
       "            1.1856e-02,  1.3687e-02, -1.4460e-04,  3.5217e-02, -5.0781e-02,\n",
       "           -2.0764e-01,  2.8467e-01, -1.7505e-01, -3.5962e-01,  3.0786e-01,\n",
       "           -2.9565e-01, -7.1106e-02, -6.3818e-01,  1.4563e-01,  1.4685e-01,\n",
       "            1.5125e-01, -3.6694e-01, -2.1790e-02,  2.1179e-01,  1.3513e-01,\n",
       "           -2.8931e-01,  1.3489e-01,  9.0393e-02, -1.2164e-01,  1.4856e-01,\n",
       "           -2.0157e-02, -1.3953e-01,  5.0488e-01, -1.8970e-01,  3.3374e-01,\n",
       "            2.5928e-01,  1.5100e-01,  2.9434e-02,  3.0518e-02, -2.5952e-01,\n",
       "            5.9265e-02, -3.8681e-03, -1.6467e-01,  2.4854e-01,  3.7109e-02,\n",
       "           -3.0212e-02,  2.3209e-02,  3.9136e-01,  6.9629e-01, -2.4597e-02,\n",
       "            4.2847e-02,  1.0498e-01, -4.0222e-02, -1.4197e-01,  7.5439e-02,\n",
       "            2.2852e-01, -4.7827e-01, -2.0374e-01, -3.0319e-02,  4.7852e-01,\n",
       "            2.6733e-01, -3.2251e-01, -1.9722e-03,  3.4332e-02,  2.5488e-01,\n",
       "            3.6572e-01,  9.2041e-02,  5.2948e-02,  5.8398e-01,  6.7688e-02,\n",
       "            5.1544e-02,  1.5540e-01, -5.6793e-02, -3.1403e-02, -1.3550e-01,\n",
       "           -2.2070e-01,  1.0968e-01,  1.9760e-02,  1.3135e-01,  4.6265e-01,\n",
       "            6.0364e-02, -2.3962e-01,  5.0293e-01,  6.8115e-02, -3.7750e-02,\n",
       "           -7.7942e-02, -2.2473e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax       ymax  confidence  class name\n",
       "  0    0.000000  311.593658  345.838837  640.00000    0.627495     59  bed\n",
       "  1  207.229401  274.694153  403.199554  559.86792    0.360763     59  bed,\n",
       "  'caption': ['The bed end has a shawl hanging on it.'],\n",
       "  'bbox_target': [0.35, 320.28, 326.88, 319.72]},\n",
       " 573: {'image_emb': tensor([[ 0.2832,  0.3347,  0.1467,  ...,  1.0977,  0.0695, -0.0867],\n",
       "          [ 0.3127,  0.6323,  0.2360,  ...,  1.0537, -0.0567,  0.0043],\n",
       "          [ 0.3125,  0.0822, -0.0300,  ...,  0.8198, -0.1357, -0.1431]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1030, -0.0814, -0.1688,  ...,  0.0464,  0.0636, -0.3140],\n",
       "          [ 0.1846,  0.0457, -0.1140,  ...,  0.5034,  0.2937, -0.5537]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1016, 0.7056, 0.1929],\n",
       "          [0.2656, 0.1073, 0.6270]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  437.591858   92.122955  568.989380  187.964081    0.919427      0   \n",
       "  1   58.309654  135.232025  246.349121  275.578461    0.825104      0   \n",
       "  2   63.986420  145.141296  279.654602  282.551025    0.624091     37   \n",
       "  3  226.584824  144.787689  281.167358  178.216583    0.283714     37   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2  surfboard  \n",
       "  3  surfboard  ,\n",
       "  'caption': ['The board of the man with no shirt',\n",
       "   'Surfboard with ying yang.'],\n",
       "  'bbox_target': [65.01, 146.52, 214.66, 136.73]},\n",
       " 574: {'image_emb': tensor([[-0.0431,  0.5991,  0.1572,  ...,  0.8320, -0.2075, -0.0898],\n",
       "          [-0.4866,  0.4214, -0.0155,  ...,  1.0186, -0.2778,  0.0177],\n",
       "          [-0.2903,  0.3792, -0.1176,  ...,  0.9365, -0.1390,  0.0054],\n",
       "          [ 0.1080,  0.4712, -0.0230,  ...,  1.2695, -0.4221,  0.3459],\n",
       "          [-0.2015,  0.5986,  0.2612,  ...,  0.5625, -0.1632, -0.0638]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1648, -0.2942, -0.2708,  ..., -0.0798, -0.1954, -0.1321],\n",
       "          [ 0.0720, -0.0307,  0.1604,  ...,  0.1891, -0.2700,  0.0500]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.6028e-06, 3.2422e-01, 1.5330e-04, 6.7578e-01, 2.2471e-05],\n",
       "          [1.9372e-05, 9.9365e-01, 1.1816e-03, 5.0545e-03, 1.7560e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0   70.937683   73.986526  636.985718  472.876709    0.917836     59       bed\n",
       "  1   54.469498   46.588951  242.245773  239.853485    0.897732     56     chair\n",
       "  2  377.173950  320.477509  639.285278  473.996643    0.851139     59       bed\n",
       "  3  245.398743   46.582115  441.141174  192.592194    0.813620     56     chair\n",
       "  4  303.347107   58.996216  394.954285  151.465057    0.599836     24  backpack,\n",
       "  'caption': ['An empty red chair next to a chair with a backpack.',\n",
       "   'Red recliner next to door.'],\n",
       "  'bbox_target': [61.48, 55.42, 179.81, 178.29]},\n",
       " 575: {'image_emb': tensor([[-0.2385,  0.4854, -0.2783,  ...,  1.0840,  0.1375, -0.3982],\n",
       "          [-0.3523,  0.2805, -0.1350,  ...,  0.6250,  0.4304, -0.2068],\n",
       "          [-0.0311,  0.0489, -0.1890,  ...,  0.7979,  0.1371, -0.2832],\n",
       "          [-0.3774,  0.1121,  0.1804,  ...,  0.4548,  0.2986, -0.0208]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2019,  0.0532, -0.0327,  ..., -0.0243,  0.1541, -0.2406],\n",
       "          [-0.1995, -0.2539, -0.1824,  ...,  0.1283,  0.4304, -0.2179]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.2451e-02, 7.2754e-02, 2.4438e-05, 9.1455e-01],\n",
       "          [8.5022e-02, 9.4849e-02, 7.8297e-04, 8.1934e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  480.302979   35.502167  639.461182  348.838104    0.944798      0   \n",
       "  1  186.054825  148.254059  346.539673  407.177368    0.889389      0   \n",
       "  2  104.537750  383.193970  432.377045  416.978271    0.773927     31   \n",
       "  3  167.452148   43.683487  336.987000  314.929749    0.346118      0   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2  snowboard  \n",
       "  3     person  ,\n",
       "  'caption': ['A man teaching a boy to ski',\n",
       "   'Man showing his kid how to skateboard'],\n",
       "  'bbox_target': [166.0, 43.67, 168.0, 355.0]},\n",
       " 576: {'image_emb': tensor([[ 0.2333,  0.2561, -0.3335,  ...,  1.3828,  0.0070, -0.1058],\n",
       "          [ 0.4749,  0.5151,  0.2089,  ...,  0.8467, -0.0368, -0.2583]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1170,  0.3643,  0.2455,  ..., -0.0062,  0.0869, -0.2659],\n",
       "          [ 0.3538,  0.3652,  0.1655,  ...,  0.0174, -0.1439, -0.2173]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1113, 0.8887],\n",
       "          [0.0102, 0.9897]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  288.912476  221.952209  337.670837  270.450134    0.813955     62   \n",
       "  1    0.155670  289.996582  210.386765  476.013794    0.696916     56   \n",
       "  2  163.351593  271.174683  325.401276  406.216187    0.693997     56   \n",
       "  3  596.182922  315.041107  639.971741  441.247559    0.672857     57   \n",
       "  4  437.171814  354.870056  637.714050  478.091125    0.298595     60   \n",
       "  \n",
       "             name  \n",
       "  0            tv  \n",
       "  1         chair  \n",
       "  2         chair  \n",
       "  3         couch  \n",
       "  4  dining table  ,\n",
       "  'caption': ['A white armchair directly in front of a panel of stained glass.',\n",
       "   'A white arm chair nearest to a peacock stained glass.'],\n",
       "  'bbox_target': [162.08, 266.81, 163.32, 140.88]},\n",
       " 577: {'image_emb': tensor([[-0.3762,  0.0341,  0.1849,  ...,  0.9268, -0.1406, -0.1940],\n",
       "          [-0.1003,  0.2382,  0.2759,  ...,  0.5410, -0.3479, -0.5298],\n",
       "          [-0.0970, -0.3923,  0.3389,  ...,  0.2986,  0.1700,  0.0363],\n",
       "          [-0.0246,  0.1172,  0.0445,  ...,  0.7842, -0.1287, -0.5874],\n",
       "          [ 0.0459, -0.3030,  0.4517,  ...,  0.0338,  0.2456,  0.0232]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0759,  0.1196, -0.4771,  ..., -0.3801,  0.1816, -0.3271],\n",
       "          [ 0.0682, -0.4043, -0.1952,  ...,  0.4526,  0.0698, -0.2725]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.0610e-03, 2.1534e-03, 3.9771e-01, 1.4267e-02, 5.7861e-01],\n",
       "          [1.9670e-06, 1.1802e-05, 5.2185e-03, 2.1875e-04, 9.9463e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  430.176971  254.910767  638.986877  628.581665    0.933253     17   horse\n",
       "  1  499.555054  207.599762  639.948242  383.829437    0.907070     17   horse\n",
       "  2    0.635361    0.000000  467.849487  636.267273    0.845424      0  person\n",
       "  3  517.140503  279.655090  618.959473  384.839722    0.760922     17   horse\n",
       "  4    0.608971  121.017349  182.940735  502.611816    0.635530      0  person,\n",
       "  'caption': ['a man holding a baby',\n",
       "   'Man with glasses holding baby, looking at farm animals'],\n",
       "  'bbox_target': [0.0, 5.75, 467.42, 627.06]},\n",
       " 578: {'image_emb': tensor([[-0.1272,  0.5566, -0.0364,  ...,  0.8335,  0.1541,  0.4570],\n",
       "          [-0.0601,  0.2656,  0.0999,  ...,  0.4121,  0.2920,  0.0984],\n",
       "          [ 0.0131,  0.5479,  0.3313,  ...,  0.7744,  0.2085,  0.2107],\n",
       "          [ 0.2705,  0.1001, -0.1406,  ...,  1.2900,  0.2891, -0.0721],\n",
       "          [-0.2529,  0.2003,  0.0239,  ...,  0.9951,  0.3188, -0.0124],\n",
       "          [-0.5649,  0.6006,  0.1671,  ...,  0.6509,  0.2096,  0.0221]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0967,  0.0280, -0.0131,  ...,  0.4712,  0.2405, -0.3848],\n",
       "          [ 0.2388,  0.0249, -0.0728,  ...,  0.4309,  0.3237, -0.3757]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.0654e-01, 8.7036e-02, 1.7041e-01, 1.2226e-03, 4.0092e-03, 3.1036e-02],\n",
       "          [7.9639e-01, 2.9007e-02, 1.6187e-01, 6.1512e-05, 1.2751e-03, 1.1360e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  151.654266   20.615311  268.925537  195.510864    0.936663      0   \n",
       "  1  469.417786   46.173340  565.456726  128.652466    0.929450     31   \n",
       "  2  490.943298   25.851562  556.414734   95.899658    0.869218      0   \n",
       "  3  111.840942  122.427307  164.602661  164.313049    0.849230      0   \n",
       "  4  116.258163  156.645508  239.605270  209.512817    0.752020     31   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1  snowboard  \n",
       "  2     person  \n",
       "  3     person  \n",
       "  4  snowboard  ,\n",
       "  'caption': ['A man snowboarding with his arms out.',\n",
       "   'A person riding a snowboard landing after going over a jump'],\n",
       "  'bbox_target': [154.54, 24.85, 110.87, 163.24]},\n",
       " 579: {'image_emb': tensor([[ 0.3262,  0.1500, -0.1111,  ...,  0.9810, -0.0153,  0.1562],\n",
       "          [-0.3223,  0.0554,  0.0639,  ...,  1.4053, -0.2803,  0.0225],\n",
       "          [ 0.0769,  0.5435,  0.1548,  ...,  1.2031,  0.0871, -0.3289],\n",
       "          [ 0.2137,  0.3633, -0.0533,  ...,  0.8467, -0.0439,  0.0786]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1774,  0.2302, -0.6455,  ..., -0.0418, -0.0066,  0.6904],\n",
       "          [ 0.0387, -0.2225, -0.5396,  ..., -0.1506, -0.0573,  0.4048]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1125, 0.0575, 0.0727, 0.7573],\n",
       "          [0.0013, 0.0016, 0.9868, 0.0103]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0   97.552475  191.210632  308.727783  541.985413    0.947473     16      dog\n",
       "  1   58.369629   82.914017  165.271393  275.469604    0.895804     16      dog\n",
       "  2  228.406067    1.714569  418.707336  326.631256    0.782355      0   person\n",
       "  3    0.185097    1.490326   74.575470  289.331573    0.676960      0   person\n",
       "  4  353.364716    8.269928  426.098755  636.193115    0.607032      0   person\n",
       "  5  235.696899  123.888245  389.378784  171.192535    0.419438     29  frisbee,\n",
       "  'caption': ['The jeans of the person that is mostly not visible behind the dog.',\n",
       "   'The legs of the person in the background in blue pants with red stripe.'],\n",
       "  'bbox_target': [222.81, 3.16, 192.79, 330.27]},\n",
       " 580: {'image_emb': tensor([[-3.1592e-01,  2.8320e-01, -1.5137e-01,  ...,  1.3174e+00,\n",
       "            6.7383e-02, -9.3628e-02],\n",
       "          [ 3.7567e-02, -6.4850e-05, -2.2009e-01,  ...,  1.1592e+00,\n",
       "            1.4563e-01, -2.9028e-01],\n",
       "          [ 4.5630e-01,  2.1838e-01, -1.7993e-01,  ...,  9.9512e-01,\n",
       "           -4.9225e-02,  2.2791e-01],\n",
       "          [ 4.3506e-01,  2.7344e-01, -1.6467e-01,  ...,  8.3545e-01,\n",
       "            2.8717e-02,  1.8164e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2349, -0.3320, -0.0955,  ..., -0.0717, -0.0216, -0.2505],\n",
       "          [-0.0735, -0.0715,  0.3643,  ..., -0.2949, -0.1741, -0.1843]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0206, 0.0707, 0.4331, 0.4756],\n",
       "          [0.0008, 0.0027, 0.4866, 0.5098]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.276207    0.963493   57.762032  142.113525    0.864974     41   \n",
       "  1  305.005432   13.958527  420.396667   64.853714    0.828414     53   \n",
       "  2    7.452576   10.642471  639.392944  464.510742    0.750194     60   \n",
       "  3  309.794617  136.900909  571.541321  334.677399    0.544385     53   \n",
       "  4  172.834595  221.575165  430.829834  430.478668    0.511141     53   \n",
       "  5  166.613922    4.794449  317.485138   56.750748    0.449941     42   \n",
       "  6   27.806564   96.794098  302.946350  380.211273    0.359883     53   \n",
       "  7   10.243256   14.035294  636.047241  476.306763    0.293338     53   \n",
       "  8  166.521820    7.566620  368.612823   72.454712    0.268093     43   \n",
       "  \n",
       "             name  \n",
       "  0           cup  \n",
       "  1         pizza  \n",
       "  2  dining table  \n",
       "  3         pizza  \n",
       "  4         pizza  \n",
       "  5          fork  \n",
       "  6         pizza  \n",
       "  7         pizza  \n",
       "  8         knife  ,\n",
       "  'caption': ['The piece of pizza nearest to the camera',\n",
       "   'The closest piece of pizza.'],\n",
       "  'bbox_target': [104.66, 222.69, 327.66, 204.36]},\n",
       " 581: {'image_emb': tensor([[-0.4788, -0.2117, -0.0903,  ...,  0.1998,  0.3093,  0.0290],\n",
       "          [-0.0684,  0.1008, -0.2257,  ...,  0.5615,  0.1179,  0.0190],\n",
       "          [-0.2861, -0.3789, -0.0214,  ...,  0.2627,  0.1234, -0.2404]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2693, -0.0603,  0.0091,  ...,  0.4758,  0.3918,  0.3469],\n",
       "          [-0.4585, -0.1270,  0.0679,  ...,  0.3044,  0.2954,  0.5835]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0079, 0.0670, 0.9253],\n",
       "          [0.0063, 0.1251, 0.8687]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  188.964844   84.948105  386.950439  527.572571    0.943875     22  zebra\n",
       "  1   88.190079  178.532318  208.347107  523.687744    0.848449     22  zebra,\n",
       "  'caption': ['The baby zebra stands close to his mom.', 'a baby zebra.'],\n",
       "  'bbox_target': [91.53, 181.31, 118.58, 338.22]},\n",
       " 582: {'image_emb': tensor([[ 9.4421e-02,  5.3192e-02,  6.5552e-02,  ...,  3.9868e-01,\n",
       "            2.4536e-01, -8.6487e-02],\n",
       "          [-1.1963e-01, -5.5008e-03,  9.5581e-02,  ...,  1.1211e+00,\n",
       "            9.9754e-04, -2.7344e-01],\n",
       "          [-2.0056e-01, -2.2293e-02, -6.7139e-04,  ...,  1.9531e-01,\n",
       "            1.8726e-01, -7.4646e-02],\n",
       "          [-7.3181e-02,  9.3933e-02, -2.5732e-01,  ...,  6.8994e-01,\n",
       "           -2.5854e-01,  5.2643e-03],\n",
       "          [-1.7847e-01, -9.8419e-03, -5.0964e-02,  ...,  4.3774e-01,\n",
       "           -6.9519e-02,  3.0563e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1752, -0.1914, -0.2095,  ..., -0.1315, -0.1741,  0.1377],\n",
       "          [-0.1448, -0.0258, -0.1962,  ..., -0.8003, -0.1107, -0.2112]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.0967e-01, 2.3132e-02, 4.9744e-02, 3.7014e-05, 1.7456e-02],\n",
       "          [3.4106e-01, 2.9343e-02, 5.8936e-01, 9.9792e-03, 3.0273e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  214.204285    0.536577  405.339478  579.423035    0.931799      0   \n",
       "  1    4.836083    0.000000  285.135254  580.795532    0.895251     72   \n",
       "  2  207.737976  300.852783  342.285156  598.468506    0.866000      0   \n",
       "  3  564.989502  159.226959  612.000000  593.888855    0.703725     69   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1  refrigerator  \n",
       "  2        person  \n",
       "  3          oven  ,\n",
       "  'caption': ['Baby boy wearing a gray jumper.', 'a young baby'],\n",
       "  'bbox_target': [215.92, 305.31, 126.52, 292.94]},\n",
       " 583: {'image_emb': tensor([[-0.0270,  0.2595, -0.5679,  ...,  0.6133,  0.0302, -0.0400],\n",
       "          [-0.2355,  0.3413, -0.4041,  ...,  1.3711,  0.2335, -0.2002],\n",
       "          [ 0.1603,  0.5044, -0.5869,  ...,  0.7432,  0.0947, -0.1863]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1573, -0.2764, -0.3008,  ..., -0.4219, -0.1246, -0.0392],\n",
       "          [-0.0997, -0.1365, -0.3025,  ..., -0.7100, -0.1246, -0.3704]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.7720, 0.0066, 0.2212],\n",
       "          [0.9072, 0.0016, 0.0912]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   66.483887    0.712341  639.487793  640.000000    0.927436      0   \n",
       "  1  357.296570    0.665848  590.902588  144.905228    0.765022      0   \n",
       "  2   86.916641  374.207031  127.081619  405.825562    0.690086     67   \n",
       "  3    0.948242   14.603729  166.703369  640.000000    0.484242      0   \n",
       "  4   66.901520  165.138779  392.350037  638.890381    0.437516      0   \n",
       "  5  564.802856    0.356464  639.208008  109.379852    0.253567      0   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2  cell phone  \n",
       "  3      person  \n",
       "  4      person  \n",
       "  5      person  ,\n",
       "  'caption': ['The woman kneeling over the child.', 'A woman helping a child'],\n",
       "  'bbox_target': [64.72, 1.44, 575.28, 631.37]},\n",
       " 584: {'image_emb': tensor([[-0.3948,  0.3103,  0.3059,  ..., -0.0151,  0.0262,  0.0345],\n",
       "          [-0.2913,  0.1653, -0.0045,  ...,  0.8022,  0.3333, -0.2252],\n",
       "          [-0.1843,  0.2764,  0.0190,  ...,  0.9150,  0.3318,  0.0793],\n",
       "          [ 0.2255, -0.2332,  0.1022,  ...,  0.5596, -0.1326, -0.0635],\n",
       "          [-0.0641, -0.1990,  0.0556,  ...,  0.4746, -0.1058, -0.2866]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0565, -0.0644, -0.2842,  ..., -0.0875,  0.4668, -0.4072],\n",
       "          [ 0.1536,  0.1827, -0.1691,  ...,  0.5083,  0.5781, -0.1823]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6313, 0.0213, 0.3433, 0.0015, 0.0029],\n",
       "          [0.5156, 0.1387, 0.3379, 0.0009, 0.0070]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  333.415710  160.116516  612.134277  423.176819    0.953300      0   \n",
       "  1   69.528465   46.852661  276.394897  421.945190    0.938377      0   \n",
       "  2  267.022400   49.059799  442.547058  251.094604    0.928958      0   \n",
       "  3   70.902145  160.074829  527.160461  424.628906    0.917909     31   \n",
       "  4  319.061493  189.420441  337.609344  215.642853    0.601013     41   \n",
       "  5   43.776356  253.307007   53.043606  282.205688    0.595812     39   \n",
       "  6   12.310873  349.782227   30.573759  392.401917    0.552158     39   \n",
       "  7   27.608990  354.215210   46.713642  393.179626    0.462651     39   \n",
       "  8    0.000000   78.837509   28.963707  261.701477    0.368774      0   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2     person  \n",
       "  3  snowboard  \n",
       "  4        cup  \n",
       "  5     bottle  \n",
       "  6     bottle  \n",
       "  7     bottle  \n",
       "  8     person  ,\n",
       "  'caption': ['A lady in yellow dress holding a snow board.',\n",
       "   'A woman in a yellow jacket holding one end of a snowboard.'],\n",
       "  'bbox_target': [333.11, 162.47, 287.19, 264.47]},\n",
       " 585: {'image_emb': tensor([[ 0.0598,  0.6201,  0.1740,  ...,  0.9995, -0.1995,  0.3003],\n",
       "          [-0.2048,  0.3699, -0.0990,  ...,  1.0605,  0.1650,  0.1837],\n",
       "          [-0.0995,  0.5225, -0.0293,  ...,  1.2461,  0.1658,  0.2294],\n",
       "          ...,\n",
       "          [ 0.1865,  0.3740, -0.3721,  ...,  1.5371,  0.0293, -0.1068],\n",
       "          [ 0.0957,  0.3889, -0.1284,  ...,  1.5059,  0.1469, -0.0070],\n",
       "          [ 0.2311,  0.5010,  0.0119,  ...,  1.2383,  0.0236,  0.2068]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1731, -0.4199, -0.3203,  ...,  0.5508, -0.0716,  0.1570],\n",
       "          [ 0.0695, -0.3853, -0.1221,  ...,  0.4893,  0.1136, -0.1166]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.3952e-03, 3.8910e-02, 1.1871e-02, 9.7839e-02, 1.7920e-03, 2.1500e-02,\n",
       "           3.1281e-02, 3.2593e-01, 4.5947e-01, 9.9945e-03],\n",
       "          [7.8918e-02, 5.8770e-05, 8.8074e-02, 3.8867e-01, 1.8382e-04, 5.9664e-05,\n",
       "           7.6942e-03, 2.5635e-02, 3.8867e-01, 2.2263e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   179.155579   98.987732  315.431396  203.833344    0.932012     63   \n",
       "  1   327.229462   15.636242  440.847870  146.900848    0.919851     56   \n",
       "  2   221.764786   16.311407  342.851990  146.178314    0.918242     56   \n",
       "  3     0.000000   82.479805  350.846802  328.963623    0.906108     60   \n",
       "  4   246.260315  201.102829  499.574188  329.761902    0.869752     56   \n",
       "  5    64.304367   17.933781  171.522278   93.890747    0.849080     56   \n",
       "  6   268.369720   54.894619  499.351440  297.181305    0.837579     56   \n",
       "  7    53.332798  123.368385  182.769485  201.664902    0.830452     26   \n",
       "  8   329.232208  142.651459  419.082489  246.856735    0.702717     26   \n",
       "  9   289.365784  163.119537  333.152679  180.615585    0.578475     67   \n",
       "  10  321.538055  117.064354  339.442261  165.271790    0.540373     67   \n",
       "  11    0.119088   86.614983   24.918949  112.878059    0.380229     26   \n",
       "  12  335.392548    5.144543  379.829865   69.534439    0.279565     58   \n",
       "  \n",
       "              name  \n",
       "  0         laptop  \n",
       "  1          chair  \n",
       "  2          chair  \n",
       "  3   dining table  \n",
       "  4          chair  \n",
       "  5          chair  \n",
       "  6          chair  \n",
       "  7        handbag  \n",
       "  8        handbag  \n",
       "  9     cell phone  \n",
       "  10    cell phone  \n",
       "  11       handbag  \n",
       "  12  potted plant  ,\n",
       "  'caption': ['Black purse sitting on a wooden chair.',\n",
       "   'briefcase sitting on chair next to laptop'],\n",
       "  'bbox_target': [329.87, 141.29, 87.31, 104.43]},\n",
       " 586: {'image_emb': tensor([[ 0.3652, -0.4319,  0.2876,  ..., -0.0330,  0.2100, -0.2568],\n",
       "          [-0.1642, -0.4463,  0.0927,  ...,  0.3958,  0.2908, -0.0400],\n",
       "          [-0.1632,  0.2781, -0.3967,  ...,  1.1094,  0.1102,  0.4731],\n",
       "          ...,\n",
       "          [-0.1101, -0.3618, -0.1049,  ...,  0.3562, -0.3528, -0.0571],\n",
       "          [ 0.3716,  0.0880, -0.3623,  ...,  1.1641,  0.0581,  0.1422],\n",
       "          [ 0.0787, -0.6108,  0.1140,  ..., -0.2491, -0.0290, -0.0554]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0300,  0.1124, -0.2181,  ..., -0.8794,  0.1360, -0.2708],\n",
       "          [-0.0073, -0.1700, -0.0305,  ...,  0.1501, -0.0459, -0.1565]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.6744e-05, 3.0994e-03, 7.1240e-01, 2.8345e-01, 6.5994e-04, 4.4656e-04,\n",
       "           4.2200e-05],\n",
       "          [4.2000e-03, 4.5868e-02, 4.9316e-01, 1.4807e-01, 1.7859e-01, 1.2469e-01,\n",
       "           5.3940e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   46.922974  171.043152  513.016846  523.740417    0.951100     18   sheep\n",
       "  1  395.146942   21.827286  626.296753  487.420776    0.928069      0  person\n",
       "  2    0.137486    8.239868   64.938934  342.991943    0.868063      0  person\n",
       "  3  157.238678    6.064651  368.292694  239.246155    0.836024      0  person\n",
       "  4    2.668045    2.289352  325.805481  305.928528    0.782045      7   truck\n",
       "  5  237.628647  118.186447  262.869293  146.157700    0.701146     27     tie,\n",
       "  'caption': ['A man facing away from the camera holding a paper in his right hand.',\n",
       "   'man facing truck'],\n",
       "  'bbox_target': [0.0, 0.84, 66.87, 336.39]},\n",
       " 587: {'image_emb': tensor([[-0.0617,  0.6714,  0.1165,  ...,  1.1777,  0.2539,  0.2595],\n",
       "          [ 0.3076,  0.5249,  0.1024,  ...,  0.6621,  0.2339, -0.0885],\n",
       "          [-0.5991,  0.3904,  0.1467,  ...,  0.7949, -0.3005, -0.2001],\n",
       "          ...,\n",
       "          [-0.4014,  0.4851,  0.0165,  ...,  0.8164,  0.0060, -0.4309],\n",
       "          [-0.1143,  0.2581, -0.2795,  ...,  1.2910, -0.0416,  0.2056],\n",
       "          [ 0.1708,  0.4202, -0.0208,  ...,  0.7554,  0.0443,  0.1283]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2312,  0.0580, -0.3589,  ...,  0.2358, -0.1614,  0.1576],\n",
       "          [-0.2615, -0.0869, -0.2524,  ..., -0.0397, -0.1127,  0.2472]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0133e-06, 6.5565e-06, 4.0802e-02, 5.6744e-05, 4.7684e-07, 3.6061e-05,\n",
       "           1.8609e-04, 1.2279e-05, 3.4392e-05, 9.5801e-01, 3.2842e-05, 5.9128e-04,\n",
       "           2.3246e-06],\n",
       "          [7.2122e-06, 5.9426e-05, 2.1362e-01, 5.4646e-04, 6.7353e-05, 5.4646e-04,\n",
       "           5.7268e-04, 3.0339e-05, 4.8518e-05, 7.8125e-01, 6.5899e-04, 2.3727e-03,\n",
       "           2.0206e-05]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   456.181946  250.840057  618.633240  371.822052    0.939067     41   \n",
       "  1     3.122253   79.579285  640.000000  473.291382    0.918220     60   \n",
       "  2   121.171799  303.658722  187.876175  475.914368    0.910376     42   \n",
       "  3   532.827942  348.367828  640.000000  396.777069    0.896667     44   \n",
       "  4   193.517975  104.324844  447.289215  256.913269    0.887768     53   \n",
       "  5   452.030640  187.134125  501.527283  271.149139    0.884889     39   \n",
       "  6   496.994415    1.624878  639.670105  240.225494    0.883094     56   \n",
       "  7    62.013317   48.580399  148.640808  122.416641    0.874375     41   \n",
       "  8   508.628693  165.688263  594.924194  236.514923    0.852769     45   \n",
       "  9     2.132957  253.352600  150.917145  365.406128    0.834411     42   \n",
       "  10  157.065552  305.278992  220.187469  476.476013    0.782541     43   \n",
       "  11  124.547096   90.826263  156.947372  155.745392    0.752849     44   \n",
       "  12  156.684723  304.178925  220.418121  475.054016    0.658336     42   \n",
       "  13  179.258148    0.000000  413.189178   80.153961    0.646647     56   \n",
       "  14    0.072839  351.770691   19.493378  452.681885    0.286761     45   \n",
       "  \n",
       "              name  \n",
       "  0            cup  \n",
       "  1   dining table  \n",
       "  2           fork  \n",
       "  3          spoon  \n",
       "  4          pizza  \n",
       "  5         bottle  \n",
       "  6          chair  \n",
       "  7            cup  \n",
       "  8           bowl  \n",
       "  9           fork  \n",
       "  10         knife  \n",
       "  11         spoon  \n",
       "  12          fork  \n",
       "  13         chair  \n",
       "  14          bowl  ,\n",
       "  'caption': ['A fork lying on a butter knife on the table.',\n",
       "   'The fork that is kept on the knife'],\n",
       "  'bbox_target': [9.16, 255.41, 140.07, 112.22]},\n",
       " 588: {'image_emb': tensor([[ 0.1384,  0.4070, -0.0069,  ...,  0.9951, -0.3711, -0.5361],\n",
       "          [-0.0601,  0.1455, -0.0754,  ...,  0.8618, -0.0229, -0.3687],\n",
       "          [ 0.0061,  0.3391, -0.0767,  ...,  0.6006, -0.2468, -0.1672],\n",
       "          ...,\n",
       "          [-0.2379, -0.3066, -0.5317,  ...,  0.5869,  0.0785,  0.1578],\n",
       "          [-0.2664,  0.1311, -0.5049,  ...,  1.1631, -0.1655,  0.2103],\n",
       "          [-0.2612,  0.5044, -0.1969,  ...,  0.3813, -0.1484, -0.4150]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0497,  0.3157,  0.0790,  ..., -0.2983, -0.1949,  0.1302],\n",
       "          [ 0.3213,  0.2571, -0.1945,  ..., -0.0558, -0.0480, -0.6670]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.6298e-03, 3.0762e-02, 8.0566e-01, 1.1206e-03, 1.0796e-02, 3.0460e-03,\n",
       "           6.8367e-05, 1.4453e-01, 2.2984e-03],\n",
       "          [1.9638e-02, 9.2224e-02, 8.7500e-01, 8.4102e-05, 5.9891e-03, 2.6569e-03,\n",
       "           1.1921e-07, 2.7847e-03, 1.8549e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  241.584229  142.760315  526.694153  288.238220    0.909480     25   \n",
       "  1   78.229630  257.912292  224.091873  412.922180    0.909468      0   \n",
       "  2  256.275238  250.993134  440.661652  413.278564    0.908738      0   \n",
       "  3    0.000000  187.904846  245.496735  297.360413    0.876945     25   \n",
       "  4  130.135925  298.272095  227.851868  414.190796    0.832332     26   \n",
       "  5    0.867645    4.519501  634.547241  415.804932    0.827842      5   \n",
       "  6  567.372681  205.315735  595.777954  234.689880    0.767490      0   \n",
       "  7  371.756226  344.475098  442.497986  415.007568    0.763763     26   \n",
       "  8  592.766968    0.035934  640.000000   73.096176    0.458721      9   \n",
       "  \n",
       "              name  \n",
       "  0       umbrella  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3       umbrella  \n",
       "  4        handbag  \n",
       "  5            bus  \n",
       "  6         person  \n",
       "  7        handbag  \n",
       "  8  traffic light  ,\n",
       "  'caption': ['A woman wearing a blue jacket.',\n",
       "   'A woman with an umbrella and a brown bag'],\n",
       "  'bbox_target': [255.0, 242.06, 187.06, 167.52]},\n",
       " 589: {'image_emb': tensor([[ 0.0378, -0.1219, -0.0933,  ...,  0.4905, -0.0393,  0.0489],\n",
       "          [-0.0626, -0.0388,  0.0283,  ...,  1.1572, -0.0813, -0.0080],\n",
       "          [-0.0781, -0.3313,  0.0497,  ...,  0.4490, -0.0344, -0.1426]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3894,  0.0500, -0.2188,  ...,  0.2148,  0.0765, -0.1465],\n",
       "          [ 0.0975, -0.1323, -0.1464,  ...,  0.4041,  0.0493, -0.0053]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.4951, 0.0681, 0.4368],\n",
       "          [0.1453, 0.1620, 0.6929]], dtype=torch.float16),\n",
       "  'df_boxes':         xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  65.317017   50.867889  640.000000  249.051727     0.92252     20  elephant\n",
       "  1   0.575493  182.966553  251.971222  266.029602     0.86688     20  elephant,\n",
       "  'caption': ['The submerged elephant whose face we cannot se',\n",
       "   'Hump of an elephant swimming beside another elephant raising its trunk.'],\n",
       "  'bbox_target': [2.87, 183.37, 241.62, 80.23]},\n",
       " 590: {'image_emb': tensor([[-1.8286e-01,  3.3618e-01,  2.0520e-01, -1.2396e-01,  2.0520e-01,\n",
       "           -2.0398e-01,  1.9543e-01, -1.6882e-01,  2.5928e-01,  2.9395e-01,\n",
       "            4.4897e-01, -3.0884e-01,  3.9453e-01, -1.4636e-01,  1.6101e-01,\n",
       "           -1.3725e-02,  1.9482e-01, -7.3975e-02,  3.9648e-01, -3.9331e-01,\n",
       "            2.8174e-01, -3.8647e-01,  3.2935e-01, -1.4148e-01,  9.8267e-03,\n",
       "            9.0527e-01, -7.1777e-02,  3.6255e-01, -4.1919e-01,  2.6001e-01,\n",
       "            1.5662e-01,  3.6499e-01,  2.0386e-01,  1.0864e-01, -2.1033e-01,\n",
       "           -1.4061e-02,  6.0822e-02, -8.6426e-02,  2.3041e-02,  1.2520e+00,\n",
       "            6.1230e-01, -1.0742e-01,  1.1487e-01, -8.6823e-03,  4.9512e-01,\n",
       "           -2.3279e-01,  4.0137e-01,  4.9976e-01, -5.7764e-01,  6.9763e-02,\n",
       "            9.3628e-02, -2.2192e-01, -6.1096e-02, -8.3594e-01, -1.9067e-01,\n",
       "            9.1260e-01, -4.4019e-01,  3.8696e-02,  3.8940e-01,  1.2195e-01,\n",
       "           -1.0479e+00,  3.6523e-01,  5.0476e-02, -1.6638e-01, -2.6646e-03,\n",
       "            6.7383e-02,  9.8328e-02,  9.6729e-01,  4.7778e-01, -4.6094e-01,\n",
       "           -4.2456e-01, -1.0345e-01, -2.5293e-01, -5.4688e-01,  1.2199e-02,\n",
       "           -2.9712e-01, -2.6001e-01,  9.7961e-02, -4.0869e-01, -1.2764e-02,\n",
       "           -1.2622e-01, -1.5671e-02, -1.5698e-01, -6.7188e-01,  4.2944e-01,\n",
       "            3.1177e-01, -8.1689e-01,  3.5620e-01,  2.8589e-01, -3.8013e-01,\n",
       "           -1.0083e-01, -9.4238e-02, -5.2461e+00, -2.4307e-02,  3.1714e-01,\n",
       "           -1.8652e-01, -3.3472e-01, -6.0352e-01,  2.9883e-01, -1.2695e+00,\n",
       "            4.1431e-01,  1.6223e-01, -3.6743e-01, -1.5564e-01,  2.1069e-01,\n",
       "           -1.8677e-01, -6.6357e-01, -8.3496e-02,  1.3501e-01, -3.6316e-02,\n",
       "           -6.7261e-02, -8.7305e-01, -6.8604e-02, -1.9263e-01,  7.5317e-02,\n",
       "           -2.5342e-01, -2.2437e-01, -6.3782e-03,  1.7542e-01,  3.5083e-01,\n",
       "           -5.9784e-02,  6.1073e-03, -3.1616e-02, -1.7993e-01, -9.0271e-02,\n",
       "            5.2795e-02,  1.2500e-01, -1.2213e-01, -1.2840e-02, -5.1562e-01,\n",
       "            5.1416e-01,  3.1982e-02, -2.1704e-01,  8.0078e-01, -8.9478e-02,\n",
       "            1.2854e-01,  4.4385e-01,  3.5791e-01, -1.5674e-01,  1.0284e-01,\n",
       "            7.0984e-02, -2.3267e-01, -3.7427e-01, -5.9547e-03, -3.7231e-01,\n",
       "           -3.8135e-01, -5.6152e-02, -1.8829e-02, -1.7380e-02, -4.8065e-03,\n",
       "            1.5857e-01, -9.2407e-02, -1.1304e-01,  4.2090e-01,  3.1152e-01,\n",
       "           -9.3359e-01,  2.1448e-01,  1.7444e-01,  2.2046e-01,  6.7101e-03,\n",
       "           -4.0601e-01, -3.2471e-01,  3.7964e-01,  3.6475e-01,  1.6296e-01,\n",
       "           -2.5659e-01,  1.6064e+00,  2.9663e-01,  1.8286e-01,  1.6980e-01,\n",
       "            1.6571e-02,  1.7920e-01, -3.4847e-03, -7.3120e-02, -1.6663e-01,\n",
       "           -2.3877e-01, -1.0342e+00,  2.7515e-01, -1.4771e-01, -2.6831e-01,\n",
       "            2.8809e-01,  2.2003e-02,  2.9248e-01,  1.0223e-01,  6.9580e-03,\n",
       "            1.7102e-01,  2.6343e-01, -1.0785e-01, -6.7261e-02, -6.3232e-02,\n",
       "           -3.3350e-01,  6.9458e-02, -2.6001e-01,  3.7231e-01, -1.5125e-01,\n",
       "           -1.8176e-01,  3.2202e-01,  1.8359e-01, -3.7646e-01, -1.2512e-01,\n",
       "           -8.5876e-02, -3.9291e-03, -5.1575e-02,  1.0962e-01,  2.6270e-01,\n",
       "            3.6523e-01, -5.3906e-01,  3.2642e-01,  6.2207e-01,  1.5125e-01,\n",
       "           -7.1680e-01,  3.7451e-01,  3.1226e-01,  4.9146e-01, -2.4817e-01,\n",
       "            1.6162e-01,  2.0569e-01,  3.1982e-02,  3.8501e-01, -3.8745e-01,\n",
       "            7.8369e-02,  5.4541e-01, -3.0078e-01,  2.1667e-01, -1.3843e-01,\n",
       "            1.6809e-01,  1.0889e-01, -7.4425e-03,  1.4392e-01, -7.0801e-02,\n",
       "           -7.9163e-02, -5.9631e-02,  6.1572e-01, -3.3569e-01, -5.3619e-02,\n",
       "           -2.0977e+00, -6.9873e-01, -7.8247e-02, -4.0855e-03,  4.3677e-01,\n",
       "           -5.7556e-02,  9.3323e-02,  3.1787e-01,  4.6045e-01, -4.8315e-01,\n",
       "           -8.4473e-01,  6.7627e-01,  3.0200e-01, -3.3325e-02, -2.5366e-01,\n",
       "           -3.6255e-01, -3.7842e-01,  1.5381e-01, -6.4941e-02,  2.3718e-01,\n",
       "            1.6687e-01,  1.2915e-01,  9.1919e-02,  2.9980e+00,  5.6885e-01,\n",
       "            4.5361e-01,  1.3504e-02,  4.8120e-01,  5.4297e-01, -3.9642e-02,\n",
       "           -5.1666e-02,  1.3245e-01, -6.6711e-02, -2.1643e-01, -3.3618e-01,\n",
       "            1.1646e-01, -2.0370e-02, -2.0966e-02,  3.3936e-02, -5.3418e-01,\n",
       "           -3.1201e-01, -3.5156e-01, -2.6660e-01, -2.9785e-02,  3.0957e-01,\n",
       "            2.9907e-01, -5.3809e-01, -4.9072e-01,  2.5513e-01, -1.6736e-01,\n",
       "            3.5718e-01, -7.5879e-01, -3.3765e-01,  5.9448e-02, -3.4937e-01,\n",
       "            6.4735e-03,  1.9690e-01, -2.0398e-01,  1.3390e-02, -2.8003e-01,\n",
       "           -2.6538e-01,  3.7305e-01, -2.1497e-01, -2.2815e-01,  4.1138e-01,\n",
       "           -4.0398e-03, -7.4658e-01,  5.8197e-02,  4.3259e-03,  5.0488e-01,\n",
       "           -1.7188e-01, -3.2642e-01,  3.6133e-01,  1.8335e-01,  5.4492e-01,\n",
       "            1.6675e-01,  3.0493e-01,  7.9932e-01,  3.2928e-02, -7.9651e-02,\n",
       "            5.6299e-01,  4.1113e-01, -3.3051e-02, -2.7246e-01, -6.5479e-01,\n",
       "           -1.5820e-01,  1.0049e+00, -1.9495e-01,  1.1877e-01,  3.9307e-02,\n",
       "            5.5115e-02, -6.6795e-03, -1.2405e-02, -1.9434e-01, -2.6807e-01,\n",
       "           -2.2290e-01, -2.0642e-01,  3.6743e-02, -3.3838e-01, -4.4287e-01,\n",
       "            2.3596e-01, -4.1650e-01,  1.8542e-01,  5.0146e-01,  5.1025e-01,\n",
       "           -8.2520e-02, -1.3977e-01, -9.1431e-02, -4.6924e-01, -4.1357e-01,\n",
       "           -4.9438e-01, -1.7627e-01,  2.2180e-01, -7.5989e-02,  2.1216e-01,\n",
       "            4.4507e-01,  7.2632e-02, -9.2285e-02, -4.9512e-01, -5.4199e-01,\n",
       "            2.3035e-01, -1.4087e-01, -3.5791e-01,  2.2461e-01,  2.6587e-01,\n",
       "           -2.6758e-01,  8.0750e-02, -1.4954e-01,  1.8213e-01, -4.9170e-01,\n",
       "            2.1216e-01,  1.4429e-01,  1.8784e-02, -1.4600e-01, -3.0014e-02,\n",
       "           -3.5205e-01, -8.8257e-02,  1.7383e-01, -3.7036e-01,  3.7402e-01,\n",
       "           -3.3911e-01, -9.8511e-02,  3.3667e-01, -3.0347e-01,  4.3750e-01,\n",
       "            5.7465e-02, -4.3121e-02, -2.3376e-01, -1.3708e-01,  1.8726e-01,\n",
       "            9.8206e-02,  7.8247e-02,  1.2749e-02, -1.9128e-01,  5.0586e-01,\n",
       "           -7.7832e-01, -2.6855e-01, -1.9531e-03,  4.2896e-01, -8.9905e-02,\n",
       "            2.0410e-01, -5.9277e-01, -4.9414e-01, -8.1348e-01,  9.7473e-02,\n",
       "            2.9724e-02, -4.3286e-01,  4.2267e-02,  4.5386e-01, -2.6929e-01,\n",
       "           -1.8518e-01, -9.7839e-02, -2.4207e-01,  2.6270e-01, -3.7329e-01,\n",
       "            4.5581e-01, -3.3905e-02, -5.0488e-01, -2.0142e-01, -3.5156e-01,\n",
       "            2.1252e-01,  3.8940e-02, -1.1688e-01,  2.5488e-01,  6.4502e-01,\n",
       "           -4.3640e-02, -1.7810e-01,  1.5796e-01, -3.7727e-03, -2.1912e-01,\n",
       "           -3.8513e-02, -3.6652e-02,  4.0985e-02,  2.2888e-01,  5.9753e-02,\n",
       "            1.6565e-01,  5.9998e-02,  3.0566e-01, -2.4246e-02, -5.1953e-01,\n",
       "            2.8000e-02,  2.9272e-01,  2.9004e-01, -1.0211e-01,  1.6980e-01,\n",
       "            2.9150e-01, -2.7124e-01, -1.9458e-01,  2.8076e-01, -7.6965e-02,\n",
       "            3.0542e-01, -3.2227e-01, -1.7273e-01, -1.5442e-01, -3.4619e-01,\n",
       "           -3.9966e-01,  5.1666e-02, -4.7607e-02, -4.8047e-01, -2.8979e-01,\n",
       "            2.4811e-02,  2.3718e-01, -1.1169e-01,  1.5759e-01, -2.6245e-01,\n",
       "           -3.7506e-02, -3.6133e-02,  1.1005e-01,  2.0129e-01,  4.4238e-01,\n",
       "            2.0401e-02,  1.1090e-01, -2.7563e-01, -6.2354e-01, -2.2986e-01,\n",
       "           -9.0576e-02, -3.2593e-01,  2.5415e-01, -1.5112e-01, -1.8091e-01,\n",
       "            1.0150e-01,  1.0474e-01,  2.7197e-01, -2.8052e-01,  1.3281e-01,\n",
       "            1.2622e-01, -5.2930e-01, -5.4840e-02, -4.1870e-01, -1.8036e-02,\n",
       "            5.1941e-02, -2.8101e-01, -3.6377e-01, -2.4023e-01, -4.9365e-01,\n",
       "           -2.0422e-01,  7.1289e-02,  2.4231e-02, -6.8994e-01, -4.4006e-02,\n",
       "            5.7650e-04,  9.9365e-02, -5.4297e-01, -1.1163e-01, -1.8274e-01,\n",
       "           -7.1338e-01,  1.0107e-01,  1.3501e-01,  7.2876e-02, -7.8369e-01,\n",
       "            3.0640e-01, -5.2734e-02, -2.4365e-01,  1.0486e-01,  7.3291e-01,\n",
       "           -3.5181e-01, -2.8320e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 2.8900e-02,  1.0345e-01,  2.3590e-02, -3.0981e-01, -3.5767e-02,\n",
       "           -3.0273e-01, -4.7333e-02, -8.8037e-01, -2.6807e-01, -2.4268e-01,\n",
       "            9.8145e-02,  9.1362e-04,  6.3843e-02,  1.8640e-01,  1.8494e-01,\n",
       "           -2.4933e-02,  1.1584e-01,  9.6008e-02, -2.4390e-01,  1.8042e-01,\n",
       "            5.8643e-01,  1.7639e-01,  1.8652e-01,  1.9421e-01,  6.4230e-04,\n",
       "            3.6530e-02, -8.7891e-02,  3.1763e-01, -2.9053e-01,  7.1729e-01,\n",
       "           -9.8328e-02, -7.7576e-02,  2.6688e-02,  6.5491e-02, -4.1235e-01,\n",
       "           -1.6040e-01,  6.1554e-02,  7.8308e-02,  4.3144e-03,  2.8540e-01,\n",
       "           -1.8884e-01,  1.1603e-01, -6.6284e-02,  1.5234e-01,  3.0737e-01,\n",
       "           -3.0664e-01,  2.2998e-01, -3.2471e-01, -3.5474e-01, -1.2329e-02,\n",
       "            5.6854e-02, -1.5088e-01, -2.8076e-01, -4.8999e-01, -1.2079e-01,\n",
       "            2.3438e-01, -6.6040e-02, -8.4229e-02, -1.9275e-01, -2.9980e-01,\n",
       "           -1.1658e-01, -1.7102e-01, -2.6294e-01, -1.9165e-01, -1.3953e-01,\n",
       "           -5.9424e-01,  1.4600e-01,  2.3120e-01, -3.2422e-01, -4.1138e-02,\n",
       "           -1.5637e-01,  6.6650e-02,  1.7822e-01, -3.7671e-01, -1.5564e-01,\n",
       "           -3.7573e-01,  5.2197e-01,  2.8320e-01, -5.1221e-01, -1.4001e-01,\n",
       "            3.6804e-02,  2.8198e-01,  2.8976e-02, -1.4563e-01, -3.1958e-01,\n",
       "           -1.9666e-01, -1.6406e-01, -1.0840e-01, -4.1406e-01,  5.5939e-02,\n",
       "            3.0640e-01,  1.3660e-01, -1.0322e+00,  6.2939e-01,  1.2341e-01,\n",
       "           -8.8318e-02,  1.3135e-01,  9.2468e-02, -5.8777e-02, -4.6631e-01,\n",
       "            2.5482e-02,  2.2449e-01,  9.3140e-02, -2.3010e-01,  8.4045e-02,\n",
       "           -1.0657e-01, -1.5771e-01,  2.8418e-01,  1.8387e-02,  1.3977e-01,\n",
       "            1.1420e-01, -1.7371e-01, -2.3514e-02, -1.2335e-01, -2.1741e-01,\n",
       "            1.2323e-01, -2.2693e-01,  1.5479e-01,  1.5601e-01,  8.0078e-02,\n",
       "            2.1741e-01, -5.9180e-01,  2.2437e-01, -3.9771e-01,  5.2100e-01,\n",
       "            5.5634e-02, -3.9917e-01, -2.2995e-02, -2.4597e-02,  2.9395e-01,\n",
       "           -1.8579e-01,  2.1875e-01, -3.8257e-01,  5.1211e+00, -1.5112e-01,\n",
       "            9.9060e-02,  8.8257e-02, -8.9844e-02,  1.5160e-02, -2.4109e-01,\n",
       "           -2.8687e-02,  6.6710e-04, -2.2778e-01, -1.1246e-02,  1.5247e-01,\n",
       "           -1.2781e-01,  1.1890e-01, -2.0374e-01, -3.3984e-01, -7.4829e-02,\n",
       "           -1.7883e-01, -1.8701e-01, -3.2532e-02,  4.9365e-01,  6.8359e-02,\n",
       "           -4.4702e-01,  1.9458e-01, -1.9934e-01, -1.0352e-01,  1.7126e-01,\n",
       "           -1.0687e-01,  1.6150e-01, -1.8298e-01, -3.7781e-02,  1.3330e-01,\n",
       "            5.8685e-02,  3.4058e-01, -1.8652e-01, -2.1936e-01,  1.2939e-01,\n",
       "            2.6709e-01, -1.6321e-01, -1.2048e-01,  3.4082e-01, -1.9031e-01,\n",
       "           -6.8359e-02,  6.9275e-02,  6.6414e-03, -1.7676e-01, -2.7661e-01,\n",
       "            3.7085e-01,  1.5234e-01,  1.2445e-01,  1.1975e-01, -3.0029e-01,\n",
       "            7.2205e-02,  3.0914e-02, -5.7800e-02, -1.8359e-01,  2.0789e-01,\n",
       "            1.2708e-01,  5.5518e-01, -1.8921e-01, -5.4541e-01,  9.3307e-03,\n",
       "            1.8445e-01,  7.5500e-02,  5.1514e-02,  4.2603e-02, -3.6072e-02,\n",
       "           -6.3538e-02, -1.3538e-01,  1.6919e-01,  2.5439e-01,  1.5152e-02,\n",
       "            7.5745e-02,  1.5930e-01,  1.2465e-03,  7.8247e-02, -1.9861e-01,\n",
       "            4.2163e-01, -2.6978e-01,  3.2135e-02,  2.9175e-01,  3.8794e-01,\n",
       "           -1.3391e-01, -2.4891e-03, -1.0236e-01,  1.4465e-01, -1.0443e-01,\n",
       "            5.4834e-01,  2.8381e-03, -5.3467e-02,  2.1179e-01, -4.7272e-02,\n",
       "            1.3843e-01,  1.9067e-01, -1.0370e-01, -2.5928e-01, -2.3389e-01,\n",
       "            9.8022e-02, -1.5466e-01, -1.4795e-01,  1.1530e-01,  2.7441e-01,\n",
       "            2.6782e-01, -2.4377e-01,  6.9946e-02, -2.7222e-01,  1.0748e-01,\n",
       "           -1.1969e-01,  4.5380e-02, -1.5186e-01,  1.1606e-03,  7.5928e-02,\n",
       "            5.2881e-01,  8.2947e-02,  3.4790e-01, -3.3008e-01, -4.0845e-01,\n",
       "           -2.9761e-01, -1.3550e-01,  2.9846e-02,  2.5098e-01,  1.8457e-01,\n",
       "            2.4841e-01,  3.5864e-01,  6.3965e-02,  3.8721e-01,  3.8208e-01,\n",
       "            2.2241e-01, -1.2878e-01,  1.3542e-02,  9.3811e-02,  7.8735e-03,\n",
       "           -5.2673e-02,  2.3206e-01, -3.5034e-02, -7.4829e-02, -2.4948e-02,\n",
       "           -1.8774e-01,  1.1884e-01,  1.3257e-01,  2.8540e-01, -2.3376e-01,\n",
       "           -7.7820e-02, -3.4277e-01,  5.2460e-02,  1.9043e-01,  1.4963e-03,\n",
       "            2.1191e-01,  9.0698e-02, -4.4043e-01,  4.2065e-01, -4.2871e-01,\n",
       "            6.1133e-01, -1.4267e-02,  4.9316e-02,  5.7324e-01,  2.4529e-03,\n",
       "           -6.3110e-02,  4.0619e-02,  1.3428e-01, -6.0211e-02, -3.3179e-01,\n",
       "           -1.1481e-01, -2.8516e-01,  1.2927e-01,  1.4816e-02, -2.3047e-01,\n",
       "            2.7466e-01, -1.0480e-01, -5.2460e-02,  9.1064e-02,  2.6587e-01,\n",
       "           -1.8445e-01, -1.5588e-01,  3.1152e-01, -9.0637e-02, -3.5620e-01,\n",
       "           -3.1323e-01, -2.0837e-01,  5.1133e+00, -3.7476e-02, -8.3252e-02,\n",
       "           -1.1475e-01, -1.0669e-01, -3.8037e-01, -1.1426e-01, -2.8839e-02,\n",
       "            9.9792e-02,  4.6533e-01,  1.9922e-01,  3.9124e-02, -9.5154e-02,\n",
       "           -1.1426e-01, -1.4868e-01, -9.3079e-02,  2.7856e-01, -1.5781e+00,\n",
       "           -8.7952e-02, -8.5266e-02,  6.2469e-02, -3.5498e-01,  1.7426e-02,\n",
       "            1.2103e-01, -2.5049e-01,  5.9845e-02,  4.2920e-01,  4.6777e-01,\n",
       "           -2.1021e-01, -3.9795e-02,  3.4473e-01, -3.1860e-01,  3.7988e-01,\n",
       "           -2.8979e-01,  2.9175e-01,  2.3669e-01,  2.2461e-01, -4.2065e-01,\n",
       "            1.6077e-01, -2.8442e-01,  7.1045e-02,  1.7456e-01, -7.0703e-01,\n",
       "            4.6425e-03, -4.4043e-01,  1.4868e-01,  2.4353e-01,  3.0981e-01,\n",
       "           -2.1057e-01, -5.7324e-01, -5.5420e-02,  5.9033e-01, -1.2878e-01,\n",
       "            2.0007e-01, -1.2634e-01,  1.2305e-01, -4.9731e-01, -4.3457e-01,\n",
       "            1.5942e-01, -2.7319e-01,  1.9394e-02,  1.4905e-01,  1.9531e-01,\n",
       "            5.5029e-01, -1.3184e-01, -1.8143e-02, -6.3171e-02,  4.8279e-02,\n",
       "           -4.1779e-02,  2.5391e-01,  2.5171e-01, -1.3965e-01,  8.7036e-02,\n",
       "            6.9084e-03, -2.5684e-01,  1.2939e-01,  4.8248e-02, -1.0706e-01,\n",
       "           -1.7578e-01,  2.1252e-01,  4.3549e-02,  2.8687e-01, -1.8396e-01,\n",
       "            1.5833e-01, -5.3589e-02,  4.7583e-01, -2.4939e-01,  6.5820e-01,\n",
       "           -1.0852e-01,  1.6077e-01, -2.7637e-01, -5.9967e-02, -1.8140e-01,\n",
       "            1.2274e-01, -2.7051e-01,  5.0903e-02,  1.5295e-01,  7.6111e-02,\n",
       "            2.5708e-01, -5.5615e-01, -2.7344e-01,  4.4604e-01,  5.4492e-01,\n",
       "            4.1235e-01, -2.4805e-01, -2.9037e-02,  3.5791e-01,  2.8101e-01,\n",
       "            3.4326e-01, -3.5913e-01, -1.1566e-01, -8.1396e-01, -1.9058e-02,\n",
       "            6.4148e-02,  2.9370e-01, -1.8127e-01,  2.3340e-01,  6.6605e-03,\n",
       "            2.5000e-01, -4.5679e-01, -2.9648e-02,  2.1896e-02, -3.5693e-01,\n",
       "           -2.0923e-01,  1.0541e-01,  2.2742e-01,  9.2468e-02,  3.4912e-02,\n",
       "            3.1952e-02, -1.3403e-01, -3.6621e-01, -3.0457e-02,  1.7517e-01,\n",
       "            2.1057e-01,  1.1145e-01, -5.3857e-01, -1.1224e-01,  4.7461e-01,\n",
       "           -3.7183e-01,  2.5848e-02, -2.0544e-01,  1.4233e-01,  4.1479e-01,\n",
       "           -3.3594e-01, -8.6670e-02,  2.6703e-02,  1.1774e-01,  7.9956e-02,\n",
       "           -2.6807e-01, -1.6809e-01,  6.7871e-02,  9.8267e-02,  2.2388e-01,\n",
       "            1.3268e-02,  1.5759e-01, -5.9998e-02,  1.9135e-02,  1.7163e-01,\n",
       "           -1.1856e-02, -4.8706e-01,  2.5000e-01, -1.6492e-01,  1.6943e-01,\n",
       "            2.0703e-01, -4.3042e-01,  3.2837e-01, -5.2032e-03,  1.6504e-01,\n",
       "           -1.0364e-01, -3.3081e-01,  2.2900e-01,  1.5771e-01,  2.5830e-01,\n",
       "           -1.5869e-01,  1.4758e-01, -2.2461e-01,  1.7273e-01, -3.6914e-01,\n",
       "            2.4646e-01, -1.5759e-01,  2.3828e-01,  1.0498e+00,  2.5659e-01,\n",
       "            4.9561e-02,  2.9907e-01, -1.3721e-01,  2.1765e-01, -1.4343e-01,\n",
       "            9.1003e-02, -4.0698e-01, -2.5586e-01,  3.5889e-01,  3.9551e-01,\n",
       "            3.5767e-01,  2.2046e-01, -2.8137e-02,  2.0154e-01,  2.7954e-02,\n",
       "           -2.0251e-01, -4.6436e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  name\n",
       "  0   34.749596  317.433441  221.353638  348.409760    0.683574      8  boat\n",
       "  1  503.090088  251.962067  638.946655  418.850464    0.568820      8  boat\n",
       "  2  166.354401  374.297821  367.619537  402.824066    0.435850      8  boat,\n",
       "  'caption': ['The small building on the dock.'],\n",
       "  'bbox_target': [503.76, 250.44, 136.24, 171.76]},\n",
       " 591: {'image_emb': tensor([[-0.6484,  0.1409,  0.2053,  ...,  0.6626,  0.4417, -0.1693],\n",
       "          [-0.3647,  0.3074,  0.0384,  ...,  0.6836,  0.2086,  0.0850],\n",
       "          [ 0.0334,  0.4395,  0.2747,  ...,  0.5513,  0.2788,  0.2749],\n",
       "          [-0.0846,  0.3044,  0.3618,  ...,  0.5449,  0.2612,  0.3032],\n",
       "          [-0.4038,  0.2072,  0.1183,  ...,  0.3792,  0.4138,  0.1344]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2039, -0.1442, -0.3416,  ...,  0.4744,  0.2805, -0.0966],\n",
       "          [-0.0028, -0.1741, -0.4565,  ..., -0.2520, -0.0654, -0.4138]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.3816e-04, 1.2062e-02, 5.4901e-02, 4.6967e-02, 8.8574e-01],\n",
       "          [9.8267e-02, 5.8655e-02, 2.0801e-01, 6.9641e-02, 5.6543e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   78.590973  356.475708  455.678741  523.085144    0.904648     37   \n",
       "  1  111.206230  234.493530  346.804413  545.592834    0.886430      0   \n",
       "  2  278.401337  179.579712  558.260498  369.120178    0.833125     37   \n",
       "  3  282.881897  128.742813  484.848511  424.868347    0.816491      0   \n",
       "  \n",
       "          name  \n",
       "  0  surfboard  \n",
       "  1     person  \n",
       "  2  surfboard  \n",
       "  3     person  ,\n",
       "  'caption': ['Surfer girl on the left.', 'The female to the left.'],\n",
       "  'bbox_target': [113.34, 235.76, 243.05, 323.91]},\n",
       " 592: {'image_emb': tensor([[-8.7646e-02,  2.1777e-01, -4.1577e-01,  ...,  2.6831e-01,\n",
       "           -4.2603e-01,  2.7075e-01],\n",
       "          [ 1.3940e-01,  2.2461e-01, -3.3862e-01,  ...,  1.3604e+00,\n",
       "            1.6919e-01,  6.7902e-04],\n",
       "          [ 4.0283e-02,  5.9265e-02, -3.4180e-01,  ...,  1.3892e-01,\n",
       "           -5.4395e-01,  1.5332e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.4387, -0.3206, -0.1954,  ...,  0.3247, -0.1045,  0.0455],\n",
       "          [-0.0887, -0.6201,  0.0956,  ...,  0.2308, -0.1241,  0.3086]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.8242e-01, 3.9380e-01, 1.2390e-01],\n",
       "          [3.4070e-04, 9.9951e-01, 8.3447e-05]], dtype=torch.float16),\n",
       "  'df_boxes':         xmin        ymin        xmax        ymax  confidence  class name\n",
       "  0  64.952850   38.785614  600.512817  428.528839    0.955350      5  bus\n",
       "  1   0.629852  136.917053  103.082016  389.233704    0.926717      5  bus,\n",
       "  'caption': ['A blue and black bus next to a green and black bus.',\n",
       "   'Small piece of a light blue bus.'],\n",
       "  'bbox_target': [0.0, 135.17, 92.56, 256.15]},\n",
       " 593: {'image_emb': tensor([[-0.3203,  0.1384, -0.4402,  ...,  1.0391,  0.1610,  0.5283],\n",
       "          [ 0.0154,  0.5376, -0.2573,  ...,  1.0859,  0.3655, -0.0804],\n",
       "          [ 0.2017,  0.4385, -0.5029,  ...,  1.3242,  0.1273,  0.2288],\n",
       "          ...,\n",
       "          [-0.0186,  0.4456, -0.1847,  ...,  1.0996,  0.1041, -0.1284],\n",
       "          [-0.2603,  0.0678, -0.2676,  ...,  1.5566, -0.0649, -0.1281],\n",
       "          [-0.3164,  0.3101, -0.2981,  ...,  0.9688,  0.2080,  0.4043]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0956,  0.2280,  0.2749,  ...,  0.4185,  0.1454, -0.5112],\n",
       "          [-0.1616, -0.1907,  0.2778,  ...,  0.2284,  0.0614, -0.3201]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.9472e-06, 5.5850e-05, 2.3842e-07, 2.0862e-06, 5.9605e-08, 8.3203e-01,\n",
       "           1.5393e-01, 7.1526e-07, 1.4091e-02],\n",
       "          [7.2765e-04, 2.3007e-05, 1.2255e-04, 3.0457e-02, 3.1948e-05, 8.3237e-03,\n",
       "           4.2041e-01, 3.1948e-05, 5.4004e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    56.679947    9.133011  480.000000  513.719055    0.950929      0   \n",
       "  1   379.632111  482.829803  475.524780  639.023193    0.930861     41   \n",
       "  2    56.413361  125.743416  185.997467  263.448914    0.855503      0   \n",
       "  3     0.382584  265.440887  102.650986  379.736115    0.854637     60   \n",
       "  4     0.561424  125.673233   72.790253  267.410339    0.834938      0   \n",
       "  5   300.556976  474.386017  388.185944  570.868591    0.805003     53   \n",
       "  6    88.765381  475.875977  480.000000  638.736206    0.793734     60   \n",
       "  7    79.704819  218.925507  138.515335  267.190460    0.712993     56   \n",
       "  8    40.594376  125.280609   87.727524  204.629700    0.684319      0   \n",
       "  9     2.520584  402.396729  134.331100  552.564575    0.671884     56   \n",
       "  10   83.929993  125.563110  115.430878  174.221436    0.654359      0   \n",
       "  11  261.661133  493.274658  345.919739  538.751953    0.647968     43   \n",
       "  12  161.890762  116.768890  212.272949  196.127960    0.620792      0   \n",
       "  13  153.155563  124.253876  184.152283  179.514313    0.581938      0   \n",
       "  14    0.725510  548.787781   81.403717  639.532776    0.338629     60   \n",
       "  15    0.000000  102.832367   22.508820  219.128204    0.310573      0   \n",
       "  16  189.470184  138.205658  225.826508  194.993347    0.289418      0   \n",
       "  17    0.000000  549.028137   83.528839  639.307678    0.268021     53   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1            cup  \n",
       "  2         person  \n",
       "  3   dining table  \n",
       "  4         person  \n",
       "  5          pizza  \n",
       "  6   dining table  \n",
       "  7          chair  \n",
       "  8         person  \n",
       "  9          chair  \n",
       "  10        person  \n",
       "  11         knife  \n",
       "  12        person  \n",
       "  13        person  \n",
       "  14  dining table  \n",
       "  15        person  \n",
       "  16        person  \n",
       "  17         pizza  ,\n",
       "  'caption': ['A plate of chicago style deep dish pizza with parmasean cheese and a drink.',\n",
       "   'A wooden table that a man is eating pizza on.'],\n",
       "  'bbox_target': [91.02, 466.43, 387.17, 165.99]},\n",
       " 594: {'image_emb': tensor([[ 0.0590,  0.2954, -0.2466,  ...,  1.3516, -0.0718,  0.1664],\n",
       "          [ 0.2749,  0.5278, -0.0832,  ...,  1.2451, -0.0363, -0.2037],\n",
       "          [ 0.1078,  0.2827, -0.0848,  ...,  1.1152,  0.0090, -0.0286],\n",
       "          [-0.0476,  0.4680,  0.0456,  ...,  0.5972,  0.0917, -0.6626]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0398, -0.1252,  0.1213,  ..., -0.0865,  0.2534, -0.2737],\n",
       "          [ 0.0140,  0.2284, -0.0587,  ..., -0.1902,  0.0927,  0.0464]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.4008e-03, 1.1084e-01, 2.5153e-05, 8.8574e-01],\n",
       "          [4.6021e-02, 9.5361e-01, 4.0650e-05, 2.3770e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   79.223328  253.723495  183.643753  499.256805    0.926464      0   \n",
       "  1  171.297241  221.175842  304.613129  499.428650    0.919062      0   \n",
       "  2    0.419325  246.268845   63.338036  291.515167    0.791594      2   \n",
       "  3  203.250839  184.747894  213.920929  206.898636    0.593574      9   \n",
       "  4  200.472427  267.453278  206.416580  286.804688    0.568452     27   \n",
       "  5   89.893692    0.185831  128.150589   20.402395    0.333047      9   \n",
       "  6  308.868378  198.466721  317.369446  212.810165    0.332151      9   \n",
       "  7  296.146118  226.611786  306.223236  238.658737    0.273373      9   \n",
       "  8  136.801620    0.087196  187.135025   20.423141    0.270728      9   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2            car  \n",
       "  3  traffic light  \n",
       "  4            tie  \n",
       "  5  traffic light  \n",
       "  6  traffic light  \n",
       "  7  traffic light  \n",
       "  8  traffic light  ,\n",
       "  'caption': ['Man dressed in black standing under a street sign',\n",
       "   'A man in a black outfit'],\n",
       "  'bbox_target': [171.68, 222.34, 132.59, 277.66]},\n",
       " 595: {'image_emb': tensor([[-0.1984,  0.1371,  0.0437,  ...,  0.7153,  0.0699,  0.2327],\n",
       "          [ 0.0986,  0.4580,  0.1951,  ...,  0.6118,  0.1078,  0.1672],\n",
       "          [ 0.2264,  0.4609, -0.1722,  ...,  0.3662,  0.0734,  0.0954]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0566,  0.3301, -0.1489,  ...,  0.1021, -0.2737, -0.2925],\n",
       "          [-0.0393,  0.2029, -0.4451,  ...,  0.2188,  0.1787, -0.3198]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.0828e-04, 1.2250e-01, 8.7744e-01],\n",
       "          [2.4300e-03, 3.8403e-01, 6.1377e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0   49.513191    0.294246  516.178101  294.635437    0.839538      0    person\n",
       "  1    3.064418    4.818574  397.988556  593.849060    0.807604      0    person\n",
       "  2  180.763870  153.268646  388.079834  337.351654    0.546481     63    laptop\n",
       "  3  269.156281  371.171539  471.758362  532.175964    0.460850     24  backpack\n",
       "  4    0.473551  279.868988  215.693054  609.778931    0.375821      0    person\n",
       "  5    1.220825  275.352417   66.267128  381.804382    0.354212     63    laptop,\n",
       "  'caption': ['A young man with a tablet and a skateboard, sitting between two other people.',\n",
       "   'A young boy holding and looking at a tablet that is resting on his skateboard.'],\n",
       "  'bbox_target': [8.25, 1.38, 363.08, 532.23]},\n",
       " 596: {'image_emb': tensor([[ 0.0698, -0.0468,  0.0125,  ..., -0.1081,  0.0854, -0.2759],\n",
       "          [-0.0437,  0.3196, -0.2329,  ...,  0.8003,  0.1626, -0.0110],\n",
       "          [ 0.1829, -0.0341, -0.2078,  ...,  0.3464, -0.0071, -0.6543],\n",
       "          [ 0.0400, -0.2256,  0.1481,  ..., -0.3682,  0.3328, -0.3931]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0732, -0.0406, -0.2250,  ...,  0.2822, -0.1504, -0.2732],\n",
       "          [-0.0699,  0.2944, -0.2316,  ..., -0.2808, -0.3313, -0.1049]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.4048, 0.0098, 0.5796, 0.0056],\n",
       "          [0.0696, 0.0117, 0.9175, 0.0011]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  187.610809   25.128403  420.184052  323.011841    0.944558      0   \n",
       "  1  230.053085  311.256195  277.961151  356.758026    0.920838     32   \n",
       "  2  138.420044   63.136078  269.738007  344.485565    0.890178      0   \n",
       "  \n",
       "            name  \n",
       "  0       person  \n",
       "  1  sports ball  \n",
       "  2       person  ,\n",
       "  'caption': ['The woman playing wearing the black and white jersey playing soccer.',\n",
       "   'The player in the darker uniform'],\n",
       "  'bbox_target': [137.2, 62.89, 146.74, 278.22]},\n",
       " 597: {'image_emb': tensor([[-4.6463e-03,  1.2891e-01, -8.4656e-02,  7.1777e-01, -2.1033e-01,\n",
       "           -3.6426e-01,  6.7993e-02, -5.2441e-01,  2.7124e-01,  1.2158e-01,\n",
       "            3.9307e-02, -3.1543e-01,  3.9258e-01, -1.6614e-01,  1.2732e-01,\n",
       "            3.0176e-01,  1.5898e+00, -1.3818e-01,  2.6685e-01, -5.5469e-01,\n",
       "           -5.9033e-01, -7.9834e-02,  1.3107e-02, -2.1277e-01, -4.2969e-01,\n",
       "            7.0618e-02,  4.2938e-02, -2.4500e-01,  7.3242e-02, -2.0248e-02,\n",
       "            2.7466e-01,  3.8062e-01, -2.9712e-01,  3.2739e-01,  1.3220e-01,\n",
       "           -2.6221e-01, -9.1492e-02,  1.7957e-01, -3.0869e-02,  1.2324e+00,\n",
       "           -1.4001e-01, -5.3101e-02, -3.8281e-01, -2.6270e-01,  5.3516e-01,\n",
       "           -2.8457e+00, -4.3701e-01, -1.5271e-01, -6.7041e-01, -1.0211e-01,\n",
       "            2.1802e-01,  9.4299e-02,  3.7134e-01, -1.0368e-02, -4.1821e-01,\n",
       "           -3.0811e-01, -2.8296e-01,  8.4229e-02,  3.8013e-01, -8.3923e-02,\n",
       "           -8.3618e-02, -7.3364e-02,  1.9760e-02,  2.0569e-01,  1.8787e-01,\n",
       "           -2.3792e-01,  4.1748e-01,  3.4546e-01, -2.5366e-01,  2.8394e-01,\n",
       "            2.9617e-02, -2.4622e-01, -6.8703e-03, -2.3730e-01, -1.8311e-01,\n",
       "           -9.3079e-02, -5.1514e-02, -1.5161e-01,  8.2031e-02, -1.4966e-01,\n",
       "            2.4231e-01,  3.8721e-01,  2.2302e-01, -2.1631e-01,  3.2324e-01,\n",
       "           -3.5919e-02,  6.7334e-01, -7.6332e-03,  6.4355e-01, -4.0967e-01,\n",
       "           -1.5247e-01, -9.6130e-02, -5.5977e+00,  2.8516e-01, -2.5208e-02,\n",
       "            3.2562e-02,  7.5500e-02,  5.2094e-02, -4.6240e-01,  4.9194e-01,\n",
       "            6.1371e-02,  3.8452e-01,  3.3643e-01,  2.0691e-01,  8.2520e-02,\n",
       "            2.3389e-01, -1.5322e+00, -4.2847e-02, -7.4036e-02, -4.5654e-01,\n",
       "            4.0430e-01,  3.1189e-02, -5.5322e-01,  3.1592e-01, -3.9771e-01,\n",
       "           -1.9956e-04, -2.9221e-02, -4.8657e-01,  1.3135e-01,  3.3105e-01,\n",
       "            7.5035e-03, -2.9956e-01, -5.1611e-01,  1.4697e-01, -7.2937e-02,\n",
       "           -5.2441e-01,  1.9604e-01,  4.4189e-01,  3.5034e-01,  8.0261e-02,\n",
       "           -2.6685e-01, -2.3694e-01,  1.7126e-01,  6.7822e-01,  9.1943e-01,\n",
       "            1.2054e-01,  4.4287e-01, -1.4648e-01, -8.1665e-02, -2.2693e-01,\n",
       "            1.4490e-01, -1.1072e-01, -7.3389e-01, -2.0190e-01,  3.6865e-02,\n",
       "           -7.9932e-01,  3.2202e-01, -8.3466e-03, -4.4092e-01, -1.3696e-01,\n",
       "           -2.5830e-01, -2.1521e-01, -5.4199e-01, -5.9180e-01, -1.2671e-01,\n",
       "           -2.9761e-01, -7.4234e-03, -3.2568e-01, -1.0535e-01,  2.1533e-01,\n",
       "           -1.5186e-01, -3.9478e-01,  5.9723e-02, -1.1639e-01,  6.5918e-01,\n",
       "           -4.5044e-01, -1.2646e-01,  2.6489e-01,  2.3889e-01, -9.0515e-02,\n",
       "            3.8989e-01, -1.8921e-01, -9.7107e-02, -1.0382e-01,  4.7699e-02,\n",
       "           -5.3760e-01,  6.3281e-01, -2.3584e-01, -1.4075e-01,  9.4910e-02,\n",
       "            1.0004e-01, -3.5791e-01,  4.0479e-01,  8.4106e-02, -8.0505e-02,\n",
       "            2.2107e-01, -2.5806e-01, -3.7537e-02, -2.0947e-01, -1.6687e-01,\n",
       "           -7.0129e-02,  1.3257e-01,  4.9072e-01, -2.4500e-01, -2.8784e-01,\n",
       "           -1.7227e-02,  5.5713e-01,  3.0444e-01,  3.3301e-01,  9.1187e-02,\n",
       "            1.3000e-01, -5.3662e-01, -2.8638e-01,  3.7549e-01, -3.2373e-01,\n",
       "           -3.7994e-02, -2.8101e-01, -2.4512e-01,  2.4585e-01, -7.7942e-02,\n",
       "           -1.0626e-01, -8.7402e-02, -2.4048e-01, -2.7051e-01, -5.0146e-01,\n",
       "           -5.7617e-01, -1.0162e-01,  1.2463e-01, -4.6897e-04, -1.1145e-01,\n",
       "           -1.5222e-01, -7.8613e-01,  1.3245e-01,  1.6541e-01, -3.0670e-02,\n",
       "           -8.4076e-03, -2.8857e-01, -3.4644e-01,  2.4707e-01, -1.5015e-01,\n",
       "           -1.1133e-01, -4.2969e-01,  3.4692e-01,  5.5127e-01, -4.1382e-02,\n",
       "           -5.2393e-01, -3.9429e-01,  2.1411e-01, -2.0374e-01, -4.3872e-01,\n",
       "           -1.3391e-01,  1.8591e-01, -1.8005e-01, -1.1484e+00,  5.5957e-01,\n",
       "            4.6631e-01, -2.0593e-01, -6.1230e-01,  2.9102e-01,  2.1204e-01,\n",
       "            3.9215e-02,  2.0959e-01,  3.4839e-01, -1.0052e-01, -3.9331e-01,\n",
       "            3.9612e-02,  1.4392e-01, -1.4252e-02, -1.0156e-01,  1.0400e-01,\n",
       "            2.4139e-02,  5.0476e-02, -2.0947e-01, -2.5635e-01,  2.7563e-01,\n",
       "            4.5068e-01, -2.4109e-01,  3.0151e-01,  2.0715e-01, -3.2642e-01,\n",
       "           -1.9153e-01, -1.9501e-02,  6.5552e-02, -1.7761e-02,  7.1680e-01,\n",
       "            3.1055e-01,  9.7290e-02,  3.3936e-01,  2.9321e-01,  2.2571e-01,\n",
       "            5.9619e-01,  1.0974e-01, -2.0129e-01, -1.7810e-01,  9.4971e-02,\n",
       "           -3.6182e-01,  3.0664e+00, -4.4336e-01, -3.0493e-01, -6.3428e-01,\n",
       "            3.8025e-02,  8.4473e-02,  1.5063e-01,  6.0303e-02, -7.2070e-01,\n",
       "            2.2107e-01, -4.4751e-01, -3.3545e-01,  9.7900e-02,  5.4346e-01,\n",
       "            1.8982e-01, -9.6497e-02, -1.6907e-01, -1.3110e-01,  3.9673e-02,\n",
       "           -2.9980e-01, -5.6201e-01,  6.3867e-01,  1.4343e-01, -6.3867e-01,\n",
       "           -1.2793e-01,  3.5034e-01,  6.7578e-01,  1.9861e-01, -9.6741e-02,\n",
       "            1.3245e-01, -3.2568e-01, -1.4221e-02, -5.7312e-02, -3.3960e-01,\n",
       "           -9.9487e-02, -1.7868e-02, -3.3203e-01, -5.5371e-01,  2.5586e-01,\n",
       "            2.3914e-01, -9.2834e-02, -4.2871e-01,  3.1616e-01, -6.0742e-01,\n",
       "            6.2549e-01,  7.6447e-03,  4.9487e-01,  6.8408e-01, -2.5513e-01,\n",
       "            4.8291e-01, -1.5442e-01,  2.5708e-01,  1.1975e-01, -7.4463e-02,\n",
       "           -5.3467e-01, -1.4771e-01, -2.4854e-01, -3.8477e-01,  4.9463e-01,\n",
       "           -3.1763e-01, -2.9468e-01,  4.0356e-01,  5.6592e-01,  2.4939e-01,\n",
       "           -3.4058e-01, -1.9202e-01,  4.7485e-01,  1.3037e-01, -1.1652e-01,\n",
       "           -7.8186e-02, -4.1553e-01,  3.1079e-01,  2.0966e-02, -7.8979e-02,\n",
       "            3.1567e-01,  4.9219e-01, -3.5229e-01, -6.7188e-01,  5.5176e-01,\n",
       "           -4.9622e-02, -2.1103e-02, -1.0879e+00, -2.4829e-01, -3.9160e-01,\n",
       "            1.8524e-02, -1.4844e-01, -4.0100e-02, -2.7441e-01,  3.0197e-02,\n",
       "            1.2488e-01,  3.9209e-01,  1.3379e-01, -4.3640e-02, -2.5879e-01,\n",
       "            1.1395e-01,  4.8975e-01, -3.7746e-03,  1.1652e-01,  4.1797e-01,\n",
       "            3.9258e-01,  1.3281e-01,  2.3730e-01,  7.7759e-02, -8.0420e-01,\n",
       "            5.6738e-01,  8.8013e-02,  3.2642e-01, -3.4741e-01, -7.9834e-01,\n",
       "            1.7334e-02,  5.2686e-01, -1.8176e-01,  2.7661e-01, -6.9946e-02,\n",
       "            1.3329e-02, -3.4741e-01,  1.7832e+00,  2.8931e-01, -1.1633e-01,\n",
       "           -2.2046e-01,  1.8860e-01, -9.4604e-02,  2.5122e-01, -7.0605e-01,\n",
       "           -4.7754e-01,  3.5522e-01,  9.7351e-02, -2.4567e-02,  1.7261e-01,\n",
       "           -1.2134e-01, -1.7249e-01, -7.8613e-01,  3.8867e-01,  3.0762e-01,\n",
       "            1.6260e-01,  9.7107e-02,  9.3567e-02,  2.4976e-01, -1.8713e-01,\n",
       "           -1.5167e-02,  1.3733e-01,  2.1326e-01, -2.6953e-01, -4.3286e-01,\n",
       "           -2.1875e-01,  1.5222e-01,  9.0027e-02, -1.0834e-01,  2.4634e-01,\n",
       "           -1.3501e-01, -3.8379e-01,  8.1543e-01,  1.0461e-01,  3.7769e-01,\n",
       "            2.0105e-01,  7.5317e-02,  6.2598e-01, -4.9438e-01, -1.5419e-02,\n",
       "           -4.9530e-02,  9.1003e-02, -4.3237e-01, -1.4417e-01, -5.9473e-01,\n",
       "           -1.2695e-01, -2.7954e-01,  1.9861e-01, -1.0559e-01, -3.9502e-01,\n",
       "            2.6318e-01,  3.2422e-01,  6.3782e-02, -4.7150e-02,  1.3550e-01,\n",
       "           -1.8921e-01,  2.2430e-02, -2.6514e-01, -4.0015e-01,  3.9703e-02,\n",
       "            1.1176e-01,  6.0791e-01,  2.0142e-01, -6.8213e-01,  2.9688e-01,\n",
       "           -2.3596e-01, -7.0374e-02,  4.8242e-01,  5.2246e-02,  4.0161e-02,\n",
       "           -3.9032e-02,  5.3516e-01,  3.2745e-02,  2.1960e-01,  2.0935e-01,\n",
       "           -6.1768e-01,  2.5098e-01,  3.9642e-02,  3.2544e-01, -6.1426e-01,\n",
       "            1.9373e-01, -1.3074e-01, -1.5332e-01,  3.6426e-01,  1.6455e-01,\n",
       "           -6.0638e-02, -6.6162e-02,  2.4451e-01, -3.2861e-01, -1.5076e-01,\n",
       "           -3.9282e-01,  1.9885e-01,  6.9031e-02,  2.4341e-01, -2.2192e-01,\n",
       "            4.0161e-01, -8.4045e-02, -1.4404e-01, -1.7126e-01,  1.1383e-01,\n",
       "            1.3367e-01, -4.0234e-01, -3.6523e-01,  4.8889e-02,  6.9775e-01,\n",
       "            4.1724e-01,  1.9638e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3354, -0.0919,  0.1578,  ..., -0.2029,  0.1962, -0.5132],\n",
       "          [-0.0255, -0.1901,  0.0533,  ..., -0.1848, -0.1661, -0.0606]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.],\n",
       "          [1.]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class     name\n",
       "  0   34.975464  98.245117  478.000000  638.185608    0.568723      1  bicycle\n",
       "  1  303.029938   1.037689  477.367981  527.663818    0.530214      0   person\n",
       "  2  308.204926   0.000000  477.927734  193.135010    0.528353      0   person,\n",
       "  'caption': ['Bicycle on a stand receiving adjustments and work by a man.',\n",
       "   'A bike being serviced'],\n",
       "  'bbox_target': [1.65, 137.42, 468.01, 436.59]},\n",
       " 598: {'image_emb': tensor([[ 0.0022,  0.5444,  0.1301,  ...,  1.0049, -0.1306,  0.0642],\n",
       "          [ 0.1054,  0.6309, -0.2659,  ...,  1.1709,  0.1204, -0.1478],\n",
       "          [ 0.1218,  0.5176,  0.0552,  ...,  1.0859, -0.0864, -0.4136],\n",
       "          [ 0.0979,  0.1254, -0.2113,  ...,  0.7173, -0.1477, -0.0509]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0224, -0.1863, -0.1520,  ..., -0.2083, -0.3259,  0.0443],\n",
       "          [ 0.3120, -0.1323,  0.0161,  ...,  0.0923, -0.6602, -0.1144]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.4385e-01, 7.5531e-03, 8.4229e-03, 4.0192e-02],\n",
       "          [9.9805e-01, 2.6488e-04, 4.9770e-05, 1.5478e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   43.170074  342.015381  335.522675  476.393799    0.922186     15   \n",
       "  1  495.090851  166.056763  640.000000  396.557434    0.907325     58   \n",
       "  2  378.236969    0.000000  549.209351  381.841187    0.873629     58   \n",
       "  3  132.710175   28.494339  314.178833  129.494003    0.648468     15   \n",
       "  4  369.742188  194.176544  436.486206  368.602631    0.385799     58   \n",
       "  \n",
       "             name  \n",
       "  0           cat  \n",
       "  1  potted plant  \n",
       "  2  potted plant  \n",
       "  3           cat  \n",
       "  4  potted plant  ,\n",
       "  'caption': ['the red cat', 'a brown cat'],\n",
       "  'bbox_target': [42.64, 341.74, 293.44, 136.63]},\n",
       " 599: {'image_emb': tensor([[-0.0493,  0.2871,  0.1294,  ...,  1.2803,  0.0301, -0.0824],\n",
       "          [ 0.0054,  0.2091, -0.0091,  ...,  1.3301,  0.2488,  0.0444],\n",
       "          [-0.2952,  0.1127,  0.2040,  ...,  0.8252, -0.0869, -0.1693],\n",
       "          [-0.2964, -0.0257,  0.2812,  ...,  0.5054, -0.0466, -0.1077],\n",
       "          [-0.2620, -0.0206,  0.2646,  ...,  0.7280, -0.0746, -0.0704]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2974,  0.2549, -0.3682,  ..., -0.4341,  0.0725, -0.3218],\n",
       "          [ 0.0435,  0.4624, -0.0703,  ..., -0.1742, -0.0700, -0.3667]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2397, 0.4341, 0.1957, 0.1168, 0.0137],\n",
       "          [0.1901, 0.1083, 0.5762, 0.1100, 0.0156]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.387085  374.697144   86.407288  639.068787    0.894435     58   \n",
       "  1  347.482819  373.258484  424.576660  616.089661    0.843689     58   \n",
       "  2  130.337799  155.641098  348.266876  576.617920    0.835549     58   \n",
       "  3   71.262375  345.677979  345.005066  638.813599    0.723282     58   \n",
       "  4  384.835754  595.535034  424.526917  639.793823    0.253788     58   \n",
       "  \n",
       "             name  \n",
       "  0  potted plant  \n",
       "  1  potted plant  \n",
       "  2  potted plant  \n",
       "  3  potted plant  \n",
       "  4  potted plant  ,\n",
       "  'caption': ['A cup to the left of three other cups',\n",
       "   'The tub extreem left.'],\n",
       "  'bbox_target': [0.0, 371.39, 86.59, 264.92]},\n",
       " 600: {'image_emb': tensor([[ 0.1891,  0.3250, -0.5020,  ...,  1.0479, -0.2109, -0.0751],\n",
       "          [ 0.0222,  0.3811, -0.4077,  ...,  0.9956, -0.2065, -0.1782],\n",
       "          [-0.1335,  0.2808, -0.2788,  ...,  0.8867, -0.3032, -0.2686],\n",
       "          [-0.4949,  0.1755, -0.0271,  ...,  0.9385, -0.3552, -0.0392]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1031,  0.2515, -0.5503,  ...,  0.0799, -0.2032, -0.0474],\n",
       "          [ 0.7622,  0.1027, -0.4578,  ...,  0.1329,  0.1846,  0.1238]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.8755, 0.0967, 0.0256, 0.0025],\n",
       "          [0.8320, 0.0408, 0.1256, 0.0013]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   58.695267   54.402863  206.064346  453.366364    0.925897      0   \n",
       "  1  193.459244   80.399994  361.777649  388.141693    0.896299      0   \n",
       "  2   63.394199  126.598618  406.016266  381.619659    0.839899      2   \n",
       "  3  227.558716  131.909653  243.186523  149.724655    0.626788     67   \n",
       "  4   28.323725   76.282349   91.177391  240.514023    0.444946      0   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2         car  \n",
       "  3  cell phone  \n",
       "  4      person  ,\n",
       "  'caption': ['the woman wearing reflective vest smiling',\n",
       "   'woman in vote for pedro shirt'],\n",
       "  'bbox_target': [55.06, 49.51, 147.19, 407.87]},\n",
       " 601: {'image_emb': tensor([[ 0.5903,  0.3997, -0.0461,  ...,  0.3037,  0.1312, -0.3020],\n",
       "          [-0.2114,  0.0688, -0.1334,  ...,  1.3008,  0.1523, -0.1179],\n",
       "          [ 0.3081,  0.1877, -0.2263,  ...,  0.4150, -0.1580, -0.3110]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1808,  0.2123, -0.1512,  ...,  0.0245, -0.1473, -0.4490],\n",
       "          [ 0.0804,  0.2172, -0.1482,  ...,  0.4419, -0.2922, -0.3191]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.5049e-01, 2.3518e-03, 2.4744e-01],\n",
       "          [6.7188e-01, 6.5231e-04, 3.2739e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  name\n",
       "  0  139.755707  125.309219  328.097595  365.327148    0.923990     14  bird\n",
       "  1  522.269104  129.829956  639.912903  328.563660    0.895729     14  bird,\n",
       "  'caption': ['A bird stands on the beach with its head turned to the right',\n",
       "   'bird with ruffled feathers on the sand'],\n",
       "  'bbox_target': [141.72, 127.47, 185.83, 237.93]},\n",
       " 602: {'image_emb': tensor([[-0.2952, -0.0662,  0.0184,  ...,  0.3672,  0.3274,  0.0323],\n",
       "          [-0.3577, -0.0947,  0.0752,  ...,  0.5728,  0.3523, -0.1039],\n",
       "          [ 0.0450, -0.4331, -0.6885,  ...,  1.3662, -0.0498, -0.1068],\n",
       "          ...,\n",
       "          [-0.8320,  0.0376, -0.1024,  ...,  0.4905,  0.0506, -0.2502],\n",
       "          [ 0.1096, -0.4492, -0.5942,  ...,  1.0986, -0.1355, -0.2886],\n",
       "          [-0.0682, -0.4536,  0.1238,  ...,  0.3179,  0.2554, -0.4136]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.4434, -0.0975, -0.1581,  ...,  0.0079,  0.0454, -0.3086],\n",
       "          [-0.2397, -0.2253, -0.4626,  ...,  0.4182, -0.1677, -0.4636]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1874, 0.1248, 0.0019, 0.0781, 0.1482, 0.0032, 0.4565],\n",
       "          [0.2344, 0.2534, 0.0005, 0.1335, 0.0420, 0.0006, 0.3357]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  409.568329   60.725159  623.947144  272.033813    0.948258      0   \n",
       "  1   91.941925   59.940903  308.502991  272.368530    0.947784      0   \n",
       "  2  356.525604  174.658691  370.000214  186.615906    0.836410     32   \n",
       "  3  335.185730  157.867798  420.133057  205.382324    0.835624     38   \n",
       "  4   17.054153  157.628540  104.914322  205.888550    0.823534     38   \n",
       "  5   40.900139  173.813385   52.018433  186.624115    0.762866     32   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2    sports ball  \n",
       "  3  tennis racket  \n",
       "  4  tennis racket  \n",
       "  5    sports ball  ,\n",
       "  'caption': ['the first picture of a man hitting a tennis ball',\n",
       "   'Tennis player on the left.'],\n",
       "  'bbox_target': [92.43, 60.56, 215.55, 210.07]},\n",
       " 603: {'image_emb': tensor([[-0.3445,  0.5864, -0.1172,  ...,  1.0186, -0.0626, -0.4639],\n",
       "          [ 0.3735,  0.3223,  0.3774,  ...,  0.7861, -0.2690,  0.0483],\n",
       "          [ 0.3320,  0.3225, -0.5591,  ...,  0.8794,  0.0166, -0.0670],\n",
       "          ...,\n",
       "          [-0.1132,  0.2391,  0.2793,  ...,  0.7017,  0.0555,  0.0625],\n",
       "          [ 0.2913,  0.0723,  0.6357,  ...,  0.6670, -0.1746,  0.2336],\n",
       "          [ 0.1644,  0.0620,  0.0086,  ...,  0.6558, -0.2964,  0.0217]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2360, -0.1520,  0.0139,  ...,  0.2102, -0.7495, -0.3499],\n",
       "          [-0.0660, -0.3159,  0.0622,  ...,  0.2043, -0.6167, -0.2418]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.1723e-06, 2.4109e-01, 1.5411e-02, 7.3096e-01, 3.1319e-03, 7.4387e-04,\n",
       "           8.1253e-03, 2.9588e-04],\n",
       "          [1.3447e-04, 6.9641e-02, 4.0723e-01, 4.4019e-01, 7.8918e-02, 3.5782e-03,\n",
       "           2.8014e-04, 6.9737e-05]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   71.784012  195.296509  354.094788  449.215088    0.920249     45   \n",
       "  1  452.494568  191.032715  639.820740  404.540771    0.836825     48   \n",
       "  2  335.251587    0.000000  518.987366  135.741119    0.826426     45   \n",
       "  3   53.286522   30.217346  213.731262  193.680847    0.802413     48   \n",
       "  4  278.799438    6.769798  394.570312  145.982025    0.790166     48   \n",
       "  5  204.948929   17.666023  308.130432  151.264862    0.740534     48   \n",
       "  6  329.038818  204.891052  634.579529  476.022400    0.739403     48   \n",
       "  7  436.383453   77.052002  633.085327  202.734283    0.645534     48   \n",
       "  8    0.467697  125.340698  191.055145  327.198486    0.585396     48   \n",
       "  9    0.000000    0.000000  634.520264  477.452087    0.481617     60   \n",
       "  \n",
       "             name  \n",
       "  0          bowl  \n",
       "  1      sandwich  \n",
       "  2          bowl  \n",
       "  3      sandwich  \n",
       "  4      sandwich  \n",
       "  5      sandwich  \n",
       "  6      sandwich  \n",
       "  7      sandwich  \n",
       "  8      sandwich  \n",
       "  9  dining table  ,\n",
       "  'caption': ['Chocolate cake to right of sandwich.',\n",
       "   'a small piece of chocolate cake'],\n",
       "  'bbox_target': [456.39, 187.95, 183.24, 222.58]},\n",
       " 604: {'image_emb': tensor([[-3.2043e-02,  4.3286e-01,  2.0142e-01,  ...,  1.1318e+00,\n",
       "            1.0193e-01, -3.6011e-01],\n",
       "          [ 1.0510e-01,  6.1182e-01,  1.2952e-01,  ...,  7.9492e-01,\n",
       "           -1.0571e-01, -1.5308e-01],\n",
       "          [ 7.6485e-04,  3.4961e-01,  3.5706e-02,  ...,  1.0420e+00,\n",
       "            2.2552e-02, -3.3667e-01],\n",
       "          [ 4.5776e-01,  3.2422e-01,  1.2634e-01,  ...,  7.6514e-01,\n",
       "           -3.1958e-01, -2.3389e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2046,  0.0656, -0.3979,  ...,  0.2063,  0.1554, -0.0367],\n",
       "          [ 0.3059, -0.0934, -0.1357,  ...,  0.2119, -0.3325, -0.1075]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.6942e-03, 1.4186e-05, 9.9219e-01, 4.7982e-05],\n",
       "          [2.6627e-02, 6.2609e-04, 9.5361e-01, 1.9180e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  263.682465  184.557495  417.845978  342.330017    0.942598     37   \n",
       "  1  201.644241  146.430222  322.920898  404.567749    0.928657      0   \n",
       "  2  313.252686  185.285736  453.792908  418.238434    0.910779      0   \n",
       "  \n",
       "          name  \n",
       "  0  surfboard  \n",
       "  1     person  \n",
       "  2     person  ,\n",
       "  'caption': ['A woman wearing swimming suit with beautiful tatto and long hair',\n",
       "   'A women in a blue bikini swimsuit.'],\n",
       "  'bbox_target': [303.57, 183.93, 152.84, 235.06]},\n",
       " 605: {'image_emb': tensor([[-0.1969,  0.4712, -0.0419,  ...,  1.2441, -0.2035, -0.1022],\n",
       "          [-0.2942,  0.3120,  0.0033,  ...,  0.9663,  0.2007, -0.0347],\n",
       "          [ 0.0718,  0.6719, -0.0073,  ...,  0.1755, -0.3516, -0.1428]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1110, -0.0336, -0.0136,  ..., -0.1589, -0.2172,  0.0438],\n",
       "          [ 0.0907,  0.0865, -0.2632,  ...,  0.1165, -0.2742,  0.1113]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.8096e-01, 3.1400e-04, 1.8539e-02],\n",
       "          [1.0000e+00, 1.6212e-05, 1.3769e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.870842   41.890831  190.835632  193.281006    0.839914      2   \n",
       "  1  112.455292  367.716248  194.127106  414.590820    0.774747     67   \n",
       "  2    6.452942   83.257935  424.529358  638.233032    0.583152      0   \n",
       "  3  393.656067  567.336121  426.110657  639.670349    0.458763     28   \n",
       "  \n",
       "           name  \n",
       "  0         car  \n",
       "  1  cell phone  \n",
       "  2      person  \n",
       "  3    suitcase  ,\n",
       "  'caption': ['A red car.',\n",
       "   'red car visible through small back window of a car'],\n",
       "  'bbox_target': [39.06, 49.73, 148.73, 138.21]},\n",
       " 606: {'image_emb': tensor([[ 0.0589,  0.3123,  0.0497,  ...,  0.7666, -0.0190, -0.0828],\n",
       "          [-0.0614,  0.2688,  0.1989,  ...,  0.8726, -0.0299, -0.1794],\n",
       "          [-0.3533,  0.3174,  0.1261,  ...,  0.4390, -0.5503,  0.2927]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-4.4312e-02,  8.6548e-02, -1.1279e-01,  2.6562e-01,  3.1982e-01,\n",
       "           -1.5979e-01,  9.2773e-02, -3.7378e-01,  3.5254e-01,  2.8857e-01,\n",
       "            2.1143e-01, -1.9336e-01,  5.3223e-01, -1.8335e-01, -1.8542e-01,\n",
       "            3.3032e-01,  6.0498e-01,  1.3269e-01, -2.0279e-02,  1.1908e-01,\n",
       "            3.7744e-01,  3.1543e-01,  1.1786e-01, -3.9429e-01,  1.0138e-01,\n",
       "            1.8967e-02, -7.5562e-02, -2.1420e-03, -2.1899e-01, -1.8591e-01,\n",
       "           -3.1079e-01,  1.3257e-01,  9.6741e-02,  3.6914e-01,  2.3389e-01,\n",
       "           -1.4478e-01, -1.5857e-01, -2.9614e-01, -3.2617e-01,  3.9185e-01,\n",
       "            1.6345e-01,  2.4643e-02,  2.8915e-02,  8.7463e-02,  5.4541e-01,\n",
       "            4.0479e-01, -2.8638e-01, -3.1113e-02, -4.6387e-02, -7.5830e-01,\n",
       "           -1.2878e-01,  1.7532e-02, -4.9500e-02, -4.4971e-01, -2.8271e-01,\n",
       "           -1.0016e-01,  3.7061e-01, -2.0496e-01,  2.8613e-01,  7.1106e-02,\n",
       "            6.3965e-01, -2.3865e-01, -1.3318e-01,  1.2708e-01,  1.9516e-02,\n",
       "           -9.6802e-02, -1.6895e-01,  4.5898e-01,  1.3855e-01, -3.5645e-01,\n",
       "            1.1969e-01,  1.0510e-01, -4.1199e-02,  3.3374e-01,  1.8762e-01,\n",
       "            2.4438e-01,  3.9722e-01, -2.4133e-01, -1.9666e-01, -4.5074e-02,\n",
       "           -1.0887e-02,  1.3977e-01, -2.4280e-01,  9.5276e-02, -1.0742e-02,\n",
       "            1.5149e-01, -1.6815e-02, -1.6870e-01,  2.9834e-01,  1.4819e-01,\n",
       "            3.8696e-01, -4.6899e-01, -8.7549e-01, -5.8057e-01,  1.8445e-01,\n",
       "           -9.8511e-02,  6.3660e-02, -3.1494e-01,  4.3433e-01,  2.4561e-01,\n",
       "            2.0044e-01,  3.8110e-01,  2.2815e-01, -2.5684e-01,  1.5833e-01,\n",
       "           -3.0859e-01, -9.4910e-02,  9.0869e-01,  5.8685e-02, -7.8125e-02,\n",
       "           -1.9324e-01, -4.3604e-01,  2.1439e-02, -7.0610e-03, -2.6709e-01,\n",
       "            9.6512e-03, -1.8445e-01,  3.1787e-01, -1.8335e-01,  2.5073e-01,\n",
       "           -1.2988e-01, -2.6685e-01, -7.5745e-02, -3.4033e-01,  1.5112e-01,\n",
       "           -8.4473e-02,  1.1566e-02, -1.0358e-01,  1.4709e-01,  4.3140e-01,\n",
       "            2.7417e-01,  5.2441e-01, -2.7222e-01,  3.5742e+00, -1.5735e-01,\n",
       "           -1.8738e-01, -1.7346e-01, -4.3335e-01, -1.5625e-01, -1.0376e-03,\n",
       "           -1.5479e-01,  1.9128e-01, -3.5327e-01,  6.4331e-02,  3.8025e-02,\n",
       "           -4.8071e-01,  2.3059e-01, -5.5566e-01, -1.2347e-01, -2.4377e-01,\n",
       "            1.9263e-01,  7.6416e-02,  6.9922e-01, -2.4231e-02,  4.1699e-01,\n",
       "            1.5161e-01, -1.7749e-01, -1.2561e-01,  1.3049e-01, -2.1582e-01,\n",
       "           -2.9443e-01,  2.6416e-01, -9.0088e-02,  3.3008e-01, -7.3730e-02,\n",
       "           -3.4912e-01,  4.9780e-01,  1.3220e-01, -3.3838e-01, -2.4927e-01,\n",
       "            2.3462e-01, -7.4036e-02, -8.9233e-02,  1.6248e-01,  1.6899e-03,\n",
       "            1.7676e-01,  2.6270e-01, -4.6777e-01, -3.0640e-02, -2.4158e-01,\n",
       "            3.7781e-02,  2.2974e-01,  3.3374e-01, -4.5557e-01, -1.9531e-01,\n",
       "           -1.1969e-01,  2.0569e-01, -5.3320e-01,  4.9835e-02,  4.7144e-01,\n",
       "            1.5356e-01, -6.0516e-02,  1.6248e-01, -5.0201e-02,  1.5747e-01,\n",
       "           -4.4141e-01, -4.6631e-01,  4.2920e-01,  2.7734e-01,  1.6403e-02,\n",
       "           -7.3730e-02, -1.2695e-01,  1.4819e-01,  6.9763e-02, -9.2346e-02,\n",
       "            7.9773e-02,  7.5760e-03, -2.8540e-01,  3.1152e-01,  2.7954e-01,\n",
       "            1.3879e-01,  3.1787e-01,  6.6284e-02,  1.9263e-01,  1.9958e-02,\n",
       "           -1.0345e-01,  1.6504e-01,  2.2913e-01, -6.0913e-02, -3.7354e-01,\n",
       "           -7.4158e-02, -3.8574e-01, -1.3748e-02, -4.9194e-02, -5.8301e-01,\n",
       "           -1.5762e-02,  8.4717e-02,  3.8843e-01,  4.0863e-02, -2.1021e-01,\n",
       "            8.4717e-02, -3.3325e-02,  1.2585e-01, -6.3904e-02,  8.5876e-02,\n",
       "            8.5205e-02,  2.5439e-01,  1.7932e-01, -2.1631e-01, -3.1006e-01,\n",
       "            8.7402e-02,  2.0032e-01,  2.9297e-01,  1.3367e-01, -5.1221e-01,\n",
       "            1.2683e-01, -3.6774e-02, -3.1567e-01, -2.5781e-01,  3.1689e-01,\n",
       "           -3.3765e-01, -3.3905e-02, -1.9324e-01, -6.6528e-02, -4.9561e-01,\n",
       "            2.6566e-02,  6.1096e-02, -2.3975e-01,  1.5588e-01,  8.1787e-02,\n",
       "            1.2384e-01, -3.5693e-01,  2.8149e-01, -5.1465e-01,  2.9468e-01,\n",
       "           -1.5732e-02,  1.9055e-01, -2.9785e-01,  2.5146e-01,  2.5610e-01,\n",
       "           -7.2876e-02,  2.8613e-01,  3.7573e-01, -2.4988e-01,  9.7351e-02,\n",
       "            1.5686e-01, -3.6108e-01,  3.3600e-02, -7.7698e-02, -1.3220e-01,\n",
       "            1.7200e-01,  5.5084e-02,  6.7444e-02,  1.4355e-01,  5.8228e-02,\n",
       "            2.6929e-01, -2.9150e-01, -3.1055e-01,  3.7241e-04,  3.9948e-02,\n",
       "            4.5728e-01,  1.0907e-01, -1.7609e-02,  3.2446e-01, -7.5317e-02,\n",
       "           -2.1802e-01, -1.1603e-01,  1.0358e-01,  3.2837e-01, -3.4473e-01,\n",
       "            3.5010e-01,  6.7139e-02,  2.9419e-01,  1.5039e-01, -2.2620e-01,\n",
       "           -7.3682e-01,  3.5352e-01,  2.2363e-01,  2.8418e-01,  2.3819e-02,\n",
       "           -5.0598e-02,  3.6230e-01,  3.5703e+00,  5.2539e-01,  2.2388e-01,\n",
       "            3.7402e-01,  4.1016e-01, -4.8096e-02,  2.6733e-01,  3.8849e-02,\n",
       "           -1.6931e-01, -1.9409e-01,  1.1530e-01,  3.0441e-02, -1.3916e-01,\n",
       "            2.6025e-01, -2.2742e-01, -1.4740e-02, -1.5588e-01, -7.5293e-01,\n",
       "           -1.4648e-01,  2.5830e-01,  1.4917e-01, -7.1338e-01, -1.3318e-01,\n",
       "            3.3887e-01, -5.0293e-01,  3.4497e-01, -1.6260e-01,  4.8364e-01,\n",
       "           -1.7981e-01, -4.0186e-01,  2.4255e-01,  1.0236e-01,  2.4976e-01,\n",
       "           -5.2539e-01,  3.2562e-02,  3.1763e-01,  1.9482e-01,  8.2764e-02,\n",
       "            6.0107e-01,  6.8420e-02,  2.6489e-01,  4.8779e-01,  4.6875e-02,\n",
       "           -3.6157e-01, -1.1664e-01, -4.3188e-01, -2.1277e-01, -3.4668e-02,\n",
       "            3.6670e-01, -4.6680e-01,  4.9243e-01,  5.4102e-01,  4.0894e-01,\n",
       "           -1.0071e-01, -1.8030e-01, -1.7883e-01,  1.9543e-01,  5.1611e-01,\n",
       "           -3.0640e-01, -9.7046e-02,  8.1482e-02,  2.5879e-01, -9.3750e-02,\n",
       "            1.1554e-01, -1.6312e-02, -2.0471e-01, -7.3828e-01,  5.1611e-01,\n",
       "           -2.4304e-01, -9.1125e-02, -2.1521e-01, -2.9395e-01, -2.0312e-01,\n",
       "           -2.4719e-01, -3.9062e-02,  1.1444e-01, -4.7791e-02,  4.8560e-01,\n",
       "           -6.0333e-02,  9.0122e-04, -3.0151e-01,  3.2007e-01, -1.8030e-01,\n",
       "           -7.0679e-02,  1.1279e-01,  2.3792e-01,  3.6591e-02,  1.7126e-01,\n",
       "           -2.7930e-01, -3.1055e-01,  2.2864e-01,  2.4084e-01,  5.0903e-02,\n",
       "           -9.1125e-02,  4.5197e-02, -1.9995e-01,  1.3672e-01, -2.5317e-01,\n",
       "           -1.4343e-01,  3.8623e-01,  2.2595e-01, -4.3506e-01,  3.3423e-01,\n",
       "           -2.9004e-01, -7.0496e-02, -1.0114e-01,  1.3196e-01, -2.8198e-01,\n",
       "            1.3550e-01, -8.5876e-02, -2.0630e-01, -3.1372e-01,  3.0933e-01,\n",
       "           -3.3496e-01,  2.0325e-01, -1.3013e-01,  1.9910e-01,  1.8103e-01,\n",
       "            4.2261e-01,  1.3049e-01,  5.3802e-02, -2.0508e-01, -1.0229e-01,\n",
       "           -3.5938e-01, -1.7163e-01, -1.9958e-01, -2.8711e-01,  6.9824e-02,\n",
       "            9.8999e-02,  6.1401e-02, -2.9633e-02, -1.6370e-01,  3.5522e-01,\n",
       "           -9.7351e-02, -3.2129e-01,  4.1229e-02,  2.0981e-02,  4.0039e-02,\n",
       "           -5.8203e-01, -2.1021e-01, -1.6541e-01,  4.8599e-03, -6.5552e-02,\n",
       "           -7.0435e-02, -4.2297e-02,  1.2323e-01, -1.4502e-01, -4.2041e-01,\n",
       "            1.8811e-01, -4.2358e-01,  1.2891e-01,  4.4653e-01,  3.5132e-01,\n",
       "           -2.0093e-01,  1.5491e-01, -3.7671e-01, -4.1821e-01,  1.4441e-01,\n",
       "           -4.1943e-01, -6.6711e-02,  5.1465e-01,  1.2128e-01,  2.3157e-01,\n",
       "           -1.3452e-01,  4.8364e-01, -2.7124e-01,  2.8931e-01,  3.1372e-02,\n",
       "           -1.5234e-01, -6.1768e-01,  7.4097e-02,  2.0911e-01, -1.7883e-01,\n",
       "           -2.4951e-01,  3.6060e-01,  1.2306e-02,  3.3325e-01,  1.3770e-01,\n",
       "            7.8247e-02,  3.8037e-01,  1.0498e-02,  1.2383e+00,  6.9922e-01,\n",
       "           -4.3823e-02,  3.2715e-01,  1.8661e-02, -2.7368e-01,  4.2261e-01,\n",
       "            3.2886e-01, -9.1492e-02, -2.0859e-02,  6.0059e-01,  9.0381e-01,\n",
       "           -3.1342e-02, -2.4170e-01, -1.6309e-01,  2.1179e-01, -2.7197e-01,\n",
       "            1.2866e-01, -6.2370e-03]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5884, 0.2695, 0.1420]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  385.793060  218.228271  480.714874  356.967651    0.877066      0    person\n",
       "  1  461.548401  291.435303  562.817200  374.629883    0.835301     13     bench\n",
       "  2  407.214325  338.951965  456.693146  367.883545    0.612569     24  backpack,\n",
       "  'caption': ['A man wearing black color dress.'],\n",
       "  'bbox_target': [408.29, 212.76, 122.05, 131.01]},\n",
       " 607: {'image_emb': tensor([[-0.0765,  0.2815, -0.4436,  ...,  1.1875,  0.1709,  0.1033],\n",
       "          [-0.0142,  0.4895, -0.2130,  ...,  1.1836,  0.6313,  0.0289],\n",
       "          [-0.0130,  0.3086,  0.0319,  ...,  0.9771,  0.2168,  0.4780],\n",
       "          ...,\n",
       "          [ 0.1577,  0.3926, -0.0974,  ...,  0.8818,  0.2009,  0.1261],\n",
       "          [-0.2776,  0.2625, -0.4282,  ...,  1.1416, -0.2491, -0.0799],\n",
       "          [-0.1063,  0.2966, -0.2551,  ...,  0.9419,  0.3242,  0.1375]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2163,  0.2485, -0.0332,  ...,  0.0093,  0.1467, -0.3877],\n",
       "          [ 0.2019,  0.2104,  0.0290,  ...,  0.2749,  0.0168,  0.0073]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.8610e-02, 7.8564e-01, 9.0942e-02, 9.3811e-02, 3.9935e-06, 1.8108e-04,\n",
       "           7.7486e-04],\n",
       "          [1.4893e-02, 9.2139e-01, 2.7390e-02, 3.5736e-02, 3.5763e-07, 2.6584e-05,\n",
       "           4.4274e-04]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     0.108391   46.706451   87.529610  272.275604    0.912557     41   \n",
       "  1   335.876221    0.404366  430.347595  164.598053    0.910120     41   \n",
       "  2    12.489883    7.914360  137.592117  206.417297    0.890707     41   \n",
       "  3   441.658051    1.010368  553.354553  139.715210    0.881822     41   \n",
       "  4   117.268585  164.493195  292.246948  301.032745    0.877968     48   \n",
       "  5   166.601669   20.160080  241.264725   65.469040    0.774341     43   \n",
       "  6   150.451340   39.201248  342.825439  128.650864    0.662951     45   \n",
       "  7   268.518066  158.731049  358.324890  211.269318    0.660384     42   \n",
       "  8     0.000000   14.154083  637.825684  421.075317    0.656435     60   \n",
       "  9   245.462753    0.381950  273.534271   40.353195    0.511145     42   \n",
       "  10  569.694885  380.045776  640.000000  425.658813    0.451905     67   \n",
       "  11  539.763184    0.613144  639.834473   86.357162    0.412398      0   \n",
       "  12  119.630035  258.395386  420.335907  422.521667    0.285879     48   \n",
       "  \n",
       "              name  \n",
       "  0            cup  \n",
       "  1            cup  \n",
       "  2            cup  \n",
       "  3            cup  \n",
       "  4       sandwich  \n",
       "  5          knife  \n",
       "  6           bowl  \n",
       "  7           fork  \n",
       "  8   dining table  \n",
       "  9           fork  \n",
       "  10    cell phone  \n",
       "  11        person  \n",
       "  12      sandwich  ,\n",
       "  'caption': ['The cup with the most beer.', 'Fullest glass of beer'],\n",
       "  'bbox_target': [336.44, 0.6, 96.02, 167.59]},\n",
       " 608: {'image_emb': tensor([[-0.1825,  0.3071,  0.4036,  ...,  0.6460,  0.1396, -0.3374],\n",
       "          [ 0.0573,  0.7095, -0.1797,  ...,  0.8599,  0.2231,  0.0788],\n",
       "          [ 0.1842,  0.5464, -0.0387,  ...,  1.0918, -0.0756, -0.3679],\n",
       "          [-0.4272,  0.4753, -0.0172,  ...,  0.4690, -0.3013, -0.4399]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0435,  0.4868,  0.0261,  ..., -0.3135,  0.1442, -0.3157],\n",
       "          [ 0.1846,  0.3584, -0.0739,  ..., -0.3411,  0.3252, -0.1285]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.4108e-02, 4.3716e-03, 9.1504e-01, 2.6367e-02],\n",
       "          [1.5778e-02, 6.0558e-05, 9.7607e-01, 8.1863e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  276.651062   85.310181  469.443909  474.966309    0.932876     14   bird\n",
       "  1  337.913361  293.283569  488.013031  443.331116    0.878861     47  apple\n",
       "  2   20.690224   79.618713  177.418732  335.868896    0.851699     14   bird,\n",
       "  'caption': [\"A blurry bird that isn't eating an apple.\",\n",
       "   'An out of focus bird behind another bird.'],\n",
       "  'bbox_target': [28.68, 83.53, 138.38, 244.37]},\n",
       " 609: {'image_emb': tensor([[ 0.0198,  0.1039, -0.4568,  ...,  0.9707,  0.1305, -0.1764],\n",
       "          [-0.1100,  0.0969,  0.0109,  ..., -0.0357,  0.4573,  0.0280],\n",
       "          [ 0.1752,  0.1865, -0.2812,  ...,  0.3613,  0.3777,  0.5273],\n",
       "          [-0.2115,  0.3743,  0.0166,  ...,  0.2888,  0.4592,  0.0824],\n",
       "          [-0.3562,  0.3677, -0.1989,  ...,  0.2747,  0.3447,  0.1730]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0072,  0.1537,  0.2917,  ..., -0.4785, -0.0430, -0.4678],\n",
       "          [ 0.1399,  0.0076,  0.4875,  ...,  0.0895,  0.0743, -0.0601]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2439, 0.0109, 0.2764, 0.0792, 0.3896],\n",
       "          [0.0028, 0.0019, 0.0126, 0.9707, 0.0118]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   223.298340    0.880081  479.671265  312.162170    0.934334      0   \n",
       "  1   217.002350  313.879456  480.000000  626.467896    0.929446      0   \n",
       "  2     1.953270  179.648651  233.419647  581.734863    0.919679      0   \n",
       "  3   139.713409  193.430374  461.026733  413.785461    0.826243     72   \n",
       "  4    22.003426  150.698624  124.278961  255.071243    0.595720     24   \n",
       "  5   177.046112  160.472809  194.939667  176.721161    0.570874     39   \n",
       "  6   242.812531   86.689888  263.083466  109.496117    0.520869     39   \n",
       "  7   200.393280  159.798401  216.531219  174.377930    0.511492     39   \n",
       "  8   266.834930   86.824722  286.815338  109.958115    0.481262     39   \n",
       "  9   215.658813  160.065308  228.381470  177.231781    0.470082     39   \n",
       "  10  241.357208  159.750107  259.300873  177.593369    0.427960     39   \n",
       "  11  264.445801  159.161591  282.121033  177.771637    0.419640     39   \n",
       "  12  190.369049   87.042465  213.968658  112.022720    0.405466     39   \n",
       "  13  218.213684   84.682579  240.791443  110.512794    0.397465     39   \n",
       "  14  169.105515   88.023804  192.780945  123.756271    0.375358     39   \n",
       "  15  154.686920  159.972977  172.443481  176.916885    0.369544     39   \n",
       "  16  191.927032  160.236328  202.566010  174.363068    0.294474     39   \n",
       "  17  255.554443  159.604736  267.811707  177.358948    0.278155     39   \n",
       "  18  226.447937  160.363937  241.369873  177.541550    0.272000     39   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3   refrigerator  \n",
       "  4       backpack  \n",
       "  5         bottle  \n",
       "  6         bottle  \n",
       "  7         bottle  \n",
       "  8         bottle  \n",
       "  9         bottle  \n",
       "  10        bottle  \n",
       "  11        bottle  \n",
       "  12        bottle  \n",
       "  13        bottle  \n",
       "  14        bottle  \n",
       "  15        bottle  \n",
       "  16        bottle  \n",
       "  17        bottle  \n",
       "  18        bottle  ,\n",
       "  'caption': ['The person is wearing white boots.',\n",
       "   'a man put his leg on fridge'],\n",
       "  'bbox_target': [230.8, 0.0, 249.2, 312.47]},\n",
       " 610: {'image_emb': tensor([[-0.0644,  0.2378,  0.0618,  ...,  0.7139,  0.3203, -0.0489],\n",
       "          [-0.3911,  0.0138, -0.3784,  ...,  0.2490,  0.1610,  0.0520]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2834, -0.0585,  0.0237,  ...,  0.3396,  0.3743,  0.2683],\n",
       "          [ 0.3572, -0.1425, -0.1108,  ...,  0.1655,  0.0085, -0.0141]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1943, 0.8057],\n",
       "          [0.0045, 0.9956]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin  ymin        xmax        ymax  confidence  class    name\n",
       "  0    1.990173   0.0  636.259399  208.865845    0.888767     46  banana\n",
       "  1  134.901566   0.0  451.461182   36.080917    0.359364     46  banana,\n",
       "  'caption': ['A banana that has black spots on it.',\n",
       "   'A very ripe banana proping up 3 chocolates.'],\n",
       "  'bbox_target': [0.0, 0.85, 640.0, 207.66]},\n",
       " 611: {'image_emb': tensor([[-0.1572,  0.4592, -0.3306,  ...,  1.2773,  0.0444, -0.1359],\n",
       "          [ 0.2271,  0.1327, -0.2217,  ...,  0.4973,  0.3994,  0.2571],\n",
       "          [ 0.1344,  0.1324, -0.2220,  ...,  0.5088,  0.2820, -0.0518],\n",
       "          [-0.1884,  0.3357, -0.3660,  ...,  0.6421,  0.3345, -0.0687],\n",
       "          [ 0.3542,  0.2397, -0.1541,  ...,  0.6030,  0.2661, -0.0582]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1626,  0.0422,  0.1628,  ...,  0.1134,  0.1836, -0.3469],\n",
       "          [-0.4727,  0.3835,  0.2009,  ...,  0.2793,  0.2305,  0.0534]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.3842e-06, 7.1631e-01, 2.4887e-02, 7.7881e-02, 1.8103e-01],\n",
       "          [1.2159e-05, 1.8152e-01, 6.4758e-02, 2.9004e-01, 4.6362e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.069790    0.227066   90.096619   77.346146    0.854433     41   \n",
       "  1  333.980499    0.000000  630.895630  156.643372    0.851210     53   \n",
       "  2    2.272400  136.714752  631.354065  476.785767    0.813006     53   \n",
       "  3   55.256119    0.105667  363.329102  158.658249    0.745349     53   \n",
       "  4    0.450653    0.000000  634.579102  476.664612    0.602864     60   \n",
       "  \n",
       "             name  \n",
       "  0           cup  \n",
       "  1         pizza  \n",
       "  2         pizza  \n",
       "  3         pizza  \n",
       "  4  dining table  ,\n",
       "  'caption': ['A small slice of a pizza with a large piece of ham on it.',\n",
       "   'The smallest portion of a pitza with pink sauce'],\n",
       "  'bbox_target': [335.46, 0.0, 298.79, 156.4]},\n",
       " 612: {'image_emb': tensor([[ 0.1454,  0.2354, -0.0899,  ...,  0.1726,  0.0560,  0.1389],\n",
       "          [ 0.1249,  0.0300, -0.1354,  ...,  0.4207, -0.0319,  0.0463],\n",
       "          [ 0.1733,  0.0533, -0.0104,  ...,  0.6729, -0.0721,  0.0784],\n",
       "          ...,\n",
       "          [-0.0618,  0.2854, -0.2302,  ...,  0.8945,  0.1447,  0.0627],\n",
       "          [-0.0531,  0.3718, -0.2463,  ...,  0.9365,  0.0943,  0.0522],\n",
       "          [-0.1635, -0.2234,  0.1512,  ...,  0.4324, -0.2700,  0.0181]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-3.9868e-01, -3.0640e-01, -1.1255e-01,  ..., -6.3623e-01,\n",
       "           -3.1921e-02, -3.7646e-01],\n",
       "          [ 1.4355e-01, -3.9331e-01,  1.6968e-02,  ..., -5.0163e-04,\n",
       "           -1.6235e-01,  1.1926e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0105, 0.0384, 0.0137, 0.0054, 0.1722, 0.2144, 0.2710, 0.2546, 0.0199],\n",
       "          [0.0089, 0.0089, 0.0019, 0.0014, 0.3408, 0.1661, 0.3357, 0.1335, 0.0025]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    41.485680   73.578590  151.271606  265.144073    0.934896      0  person\n",
       "  1   369.800476   71.201378  482.287964  264.884216    0.927605      0  person\n",
       "  2    49.157536  347.124573  161.899155  542.934265    0.927431      0  person\n",
       "  3   362.168762  348.159760  471.153931  539.618958    0.921803      0  person\n",
       "  4   561.379517  341.868927  631.379883  414.955414    0.870919     62      tv\n",
       "  5   241.712982   66.227173  309.734161  141.565643    0.869440     62      tv\n",
       "  6   254.928726  339.300476  320.338257  414.789612    0.850838     62      tv\n",
       "  7   575.630188   64.830292  639.551208  136.138611    0.721639     62      tv\n",
       "  8   321.251343  358.117188  365.345642  463.102417    0.653283     56   chair\n",
       "  9   330.819580   83.185623  374.005493  185.585159    0.549130     56   chair\n",
       "  10    0.741436   83.252975   45.515770  192.933884    0.478492     56   chair\n",
       "  11   10.397367  358.273621   53.442238  463.854919    0.382526     56   chair\n",
       "  12    0.748062   83.249115   45.545280  197.748260    0.357889     57   couch\n",
       "  13  432.066315  144.162567  471.735870  158.871155    0.304922     65  remote\n",
       "  14   10.459721  358.275146   53.572781  463.014954    0.295719     57   couch,\n",
       "  'caption': ['women on the lower right corner image',\n",
       "   'Woman watching tv who is the below and too the right of the other women.'],\n",
       "  'bbox_target': [361.85, 347.5, 106.06, 188.41]},\n",
       " 613: {'image_emb': tensor([[-0.0722,  0.1404,  0.2705,  ...,  0.5967,  0.2394, -0.2769],\n",
       "          [ 0.0632,  0.1718, -0.0323,  ...,  0.8384,  0.0917, -0.3591],\n",
       "          [ 0.1919,  0.0725, -0.0330,  ...,  0.3418, -0.2686, -0.3872]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2966,  0.0920, -0.2749,  ...,  0.2522,  0.0441, -0.6211],\n",
       "          [ 0.0557,  0.0720, -0.3037,  ..., -0.3499,  0.0898, -0.7134]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0087, 0.1014, 0.8896],\n",
       "          [0.2988, 0.0780, 0.6230]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  name\n",
       "  0  358.367340  240.889008  555.760742  402.932281    0.864798     14  bird\n",
       "  1  336.821869  168.290955  494.236786  274.123718    0.863924     14  bird,\n",
       "  'caption': ['Two black birds sitting on a rock.',\n",
       "   'Two bird heads and almost the complete body of the one on the top of the picture.'],\n",
       "  'bbox_target': [339.02, 170.31, 158.49, 107.91]},\n",
       " 614: {'image_emb': tensor([[ 0.0481,  0.3020, -0.1685,  ...,  0.7930, -0.1753, -0.1324],\n",
       "          [-0.0767,  0.6489, -0.3130,  ...,  1.1260,  0.1624, -0.2159],\n",
       "          [ 0.1383,  0.7090,  0.0862,  ...,  0.5874,  0.2108,  0.2194],\n",
       "          [ 0.0511,  0.5811, -0.2847,  ...,  1.1562,  0.0419,  0.0300],\n",
       "          [-0.1721,  0.4758, -0.2125,  ...,  1.2822,  0.1791, -0.2452],\n",
       "          [-0.0026,  0.2264, -0.0778,  ...,  0.9609,  0.4822,  0.1022]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0673,  0.4856, -0.0060,  ..., -0.4175, -0.0622, -0.3215],\n",
       "          [-0.2238,  0.1004, -0.2749,  ..., -0.3867, -0.1194, -0.2649]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.7684e-07, 9.9951e-01, 5.2452e-06, 4.5109e-04, 2.0862e-06, 1.2577e-05],\n",
       "          [2.4438e-06, 9.9951e-01, 1.7667e-04, 3.4046e-04, 1.4198e-04, 7.0274e-05]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  439.419922    1.129211  639.834839  476.107666    0.938544      0  person\n",
       "  1    0.057228    0.000000  172.838257  475.044067    0.922399      0  person\n",
       "  2  312.257507  307.922363  465.708557  479.226990    0.914872      0  person\n",
       "  3  204.253769   66.833191  371.596130  476.544800    0.904212      0  person\n",
       "  4    2.227631  267.954224   64.058006  326.268921    0.761609     39  bottle\n",
       "  5  201.853989  343.602966  371.596130  477.321655    0.298584     57   couch,\n",
       "  'caption': ['A person wearing a blue jacket, holding a water bottle.',\n",
       "   'a man wearing all blue holding an empty water bottle'],\n",
       "  'bbox_target': [0.0, 0.01, 164.3, 475.7]},\n",
       " 615: {'image_emb': tensor([[ 0.1342,  0.2712, -0.2498,  ...,  0.7319, -0.1877,  0.0493],\n",
       "          [-0.1833, -0.1776,  0.0578,  ...,  1.5605,  0.2325, -0.1810],\n",
       "          [ 0.0338,  0.2250, -0.2451,  ...,  1.3145,  0.1039,  0.1053],\n",
       "          [-0.2285, -0.0275,  0.0223,  ...,  0.3235,  0.0071, -0.0696]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0531, -0.1404, -0.2705,  ..., -0.0942, -0.1114, -0.1915],\n",
       "          [ 0.1494, -0.1237, -0.0854,  ...,  0.0295,  0.0764, -0.3140]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3289, 0.1653, 0.0274, 0.4785],\n",
       "          [0.3340, 0.5771, 0.0056, 0.0831]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0   64.246414  119.906639  277.046143  293.461639    0.858570      7     truck\n",
       "  1  431.376831  124.655602  499.674805  290.393616    0.851310      7     truck\n",
       "  2  225.578720  248.289917  260.277496  315.118347    0.753537      0    person\n",
       "  3  367.279877  252.896362  398.192169  286.004974    0.576150      1   bicycle\n",
       "  4    0.071430  229.192444   10.188950  331.202179    0.547053      0    person\n",
       "  5  366.045929  177.306610  439.771942  235.694550    0.541621      7     truck\n",
       "  6  329.023895  198.425003  345.591370  216.564270    0.501593      0    person\n",
       "  7   42.151588  203.393372   55.087341  239.448212    0.492251      0    person\n",
       "  8  313.757355  202.696091  330.790070  217.547745    0.322464      0    person\n",
       "  9   45.528828  186.999649   73.874268  199.985321    0.317030     25  umbrella,\n",
       "  'caption': ['car next to sign facing forward',\n",
       "   'Front of white van that says Tobinco.'],\n",
       "  'bbox_target': [63.87, 120.5, 202.77, 173.61]},\n",
       " 616: {'image_emb': tensor([[-2.1899e-01, -7.8369e-02,  3.1372e-01,  ...,  6.4795e-01,\n",
       "            1.1761e-01,  8.5022e-02],\n",
       "          [-1.0217e-01,  2.1252e-01, -9.8145e-02,  ...,  1.1113e+00,\n",
       "            2.2369e-02,  6.5625e-01],\n",
       "          [-2.9861e-02,  2.5391e-01, -9.0698e-02,  ...,  7.7393e-01,\n",
       "            2.0032e-01,  2.1619e-01],\n",
       "          [ 3.5449e-01,  3.1323e-01, -3.4521e-01,  ...,  1.0234e+00,\n",
       "            3.2104e-01,  2.8320e-01],\n",
       "          [-1.8408e-01,  5.3314e-02,  1.8091e-01,  ...,  2.3340e-01,\n",
       "            2.8418e-01, -2.0409e-04]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3704,  0.2847, -0.3428,  ..., -0.0705,  0.1890, -0.1874],\n",
       "          [-0.1127,  0.4802, -0.2371,  ..., -0.0342,  0.1830, -0.4128]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.3076e-01, 7.2622e-04, 1.4532e-04, 1.2231e-04, 4.6826e-01],\n",
       "          [3.2642e-01, 1.4420e-03, 3.7781e-02, 5.2460e-02, 5.8203e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class    name\n",
       "  0  147.725204  24.325567  323.231293  294.107391    0.930657     17   horse\n",
       "  1  288.344147  77.756645  372.569122  207.199783    0.902259      2     car\n",
       "  2   58.785988  26.804291  124.400009  144.614471    0.901935      0  person\n",
       "  3  112.510361  22.185038  169.230515   93.562202    0.885545      0  person\n",
       "  4  346.807770  90.606781  372.631866  119.680511    0.250740      0  person,\n",
       "  'caption': ['A man riding the horse vehicle and holding the rope of the horse',\n",
       "   'A men wearing coat with cap and stick is riding the horse.'],\n",
       "  'bbox_target': [60.61, 28.62, 63.97, 116.5]},\n",
       " 617: {'image_emb': tensor([[-0.1846, -0.0366,  0.4841,  ...,  0.8008,  0.0467, -0.2991],\n",
       "          [-0.1041,  0.0625,  0.3201,  ...,  0.8447, -0.0047,  0.0023],\n",
       "          [-0.2170,  0.1580,  0.2267,  ...,  0.4734, -0.2162, -0.3547]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0547, -0.1187, -0.1003,  ...,  0.2198, -0.1130, -0.1809],\n",
       "          [ 0.1537,  0.2068, -0.0481,  ...,  0.1862, -0.1537, -0.1227]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1586, 0.6675, 0.1741],\n",
       "          [0.4834, 0.4988, 0.0179]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  328.586731  102.949860  638.602417  354.180023    0.941332     17  horse\n",
       "  1   90.042511  140.872375  436.749664  354.923767    0.879969     17  horse,\n",
       "  'caption': ['The horse with the red halter.',\n",
       "   'A horse with a short and curly mane.'],\n",
       "  'bbox_target': [84.15, 128.93, 310.36, 225.44]},\n",
       " 618: {'image_emb': tensor([[-1.9485e-02,  2.8638e-01, -2.8198e-01,  ...,  5.8887e-01,\n",
       "            1.1060e-01,  4.3066e-01],\n",
       "          [-1.2524e-01,  3.6792e-01, -3.3154e-01,  ...,  7.0020e-01,\n",
       "           -5.3101e-02,  1.1267e-01],\n",
       "          [-2.7954e-01, -1.3184e-01, -2.0947e-01,  ...,  7.2412e-01,\n",
       "            7.0007e-02,  1.5100e-01],\n",
       "          ...,\n",
       "          [ 4.5654e-02,  1.5283e-01, -1.8176e-01,  ...,  7.5977e-01,\n",
       "           -1.6342e-02, -2.8418e-01],\n",
       "          [ 7.9632e-04,  1.8997e-02, -3.1201e-01,  ...,  1.0527e+00,\n",
       "           -1.2505e-02, -2.9370e-01],\n",
       "          [ 1.5186e-01,  3.7402e-01, -2.6489e-01,  ...,  5.4248e-01,\n",
       "            2.4182e-01, -1.9369e-03]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1707, -0.0944, -0.1392,  ..., -0.0215, -0.0944,  0.0490],\n",
       "          [-0.0997, -0.3735, -0.1008,  ..., -0.0328, -0.1313,  0.1831]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.6484e-01, 6.5565e-07, 3.3539e-02, 2.0266e-06, 1.4901e-06, 3.0577e-05,\n",
       "           1.5936e-03],\n",
       "          [5.4688e-01, 7.9274e-06, 1.2600e-04, 0.0000e+00, 0.0000e+00, 5.9605e-08,\n",
       "           4.5312e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   291.109344  124.199417  426.267639  278.955536    0.932091      0  person\n",
       "  1   134.300140   54.035854  237.067886  124.417641    0.893238     62      tv\n",
       "  2   311.031586  104.579735  465.511505  201.669647    0.877400      0  person\n",
       "  3   108.173035  232.266327  186.764023  280.199188    0.830335     73    book\n",
       "  4   209.118652  220.542557  248.143295  237.832565    0.813390     65  remote\n",
       "  5   192.943832  202.260422  218.746063  216.383331    0.742932     65  remote\n",
       "  6   407.228149  103.579971  499.905548  223.672897    0.683075     56   chair\n",
       "  7   193.530106  227.475616  240.429428  254.920410    0.670792     73    book\n",
       "  8   328.990845  180.880554  349.640656  201.583954    0.668644     65  remote\n",
       "  9   193.729324  227.650925  240.537430  255.144165    0.544363     65  remote\n",
       "  10  422.930054  193.459579  499.675262  279.491028    0.357063      0  person\n",
       "  11  147.069672   72.722816  171.329422  115.329124    0.270765      0  person,\n",
       "  'caption': ['An older lady sitting on a chair with her hands fold.',\n",
       "   'A woman sitting down facing the TV.'],\n",
       "  'bbox_target': [310.81, 105.14, 154.14, 104.02]},\n",
       " 619: {'image_emb': tensor([[ 0.3955, -0.0420, -0.2759,  ...,  0.7520, -0.0263, -0.1003],\n",
       "          [-0.1869,  0.6538, -0.0286,  ...,  0.7617,  0.1460, -0.1930],\n",
       "          [ 0.0170,  0.5830, -0.2710,  ...,  0.9976,  0.0593, -0.0795],\n",
       "          [-0.1582,  0.2251, -0.1044,  ...,  0.4482,  0.0779, -0.0936],\n",
       "          [-0.3901,  0.5176, -0.1836,  ...,  0.8589,  0.0241,  0.1126],\n",
       "          [-0.3538,  0.2744, -0.0598,  ...,  0.9985, -0.0322, -0.4988]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.7852,  0.3545,  0.4070,  ..., -0.1155, -0.0337, -0.4956],\n",
       "          [-0.3538,  0.2273, -0.3708,  ...,  0.3076, -0.1043, -0.2369]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.6499e-02, 7.7271e-02, 7.3340e-01, 1.1549e-03, 1.5125e-01, 5.8985e-04],\n",
       "          [2.6367e-01, 4.4403e-02, 1.3464e-01, 1.6785e-04, 4.1077e-02, 5.1611e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  175.516769   81.352295  315.822632  210.334229    0.909527     45   \n",
       "  1  316.880768   75.787476  451.930267  197.270142    0.900909     45   \n",
       "  2   27.042847   67.715363  180.965302  219.949493    0.896418     45   \n",
       "  3  568.405945  115.857635  623.199280  168.418671    0.887438     49   \n",
       "  4  450.768372   83.304474  571.729614  193.827545    0.886634     45   \n",
       "  5    6.890045    1.851776  633.409668  318.150787    0.563157     60   \n",
       "  6  454.948242   98.566177  481.165100  137.147766    0.502564     51   \n",
       "  7  478.571045   87.443008  543.387451  109.767273    0.470451     51   \n",
       "  \n",
       "             name  \n",
       "  0          bowl  \n",
       "  1          bowl  \n",
       "  2          bowl  \n",
       "  3        orange  \n",
       "  4          bowl  \n",
       "  5  dining table  \n",
       "  6        carrot  \n",
       "  7        carrot  ,\n",
       "  'caption': ['A small white bowl with tan wraps in it.',\n",
       "   'The second dish to the left with the whales in the bowl.'],\n",
       "  'bbox_target': [174.69, 77.39, 143.73, 134.89]},\n",
       " 620: {'image_emb': tensor([[-2.2858e-02,  7.1973e-01, -6.6260e-01,  ...,  8.4082e-01,\n",
       "            9.6069e-02,  5.2738e-04],\n",
       "          [ 5.1318e-01,  1.7188e-01, -4.4043e-01,  ...,  1.2148e+00,\n",
       "           -2.6270e-01,  2.5488e-01],\n",
       "          [ 1.9684e-03,  6.7432e-01, -4.2285e-01,  ...,  7.8955e-01,\n",
       "           -6.4636e-02,  4.0234e-01],\n",
       "          [ 3.2690e-01,  3.6011e-01, -5.4639e-01,  ...,  6.0010e-01,\n",
       "           -2.6465e-01,  3.4912e-01],\n",
       "          [ 3.3765e-01,  3.0347e-01, -6.0986e-01,  ...,  6.4111e-01,\n",
       "           -2.1558e-01,  4.2627e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2859,  0.2347, -0.3730,  ...,  0.0925, -0.6362,  0.0732],\n",
       "          [ 0.2769, -0.1134, -0.3733,  ...,  0.1096, -0.7241, -0.2437]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0082, 0.9370, 0.0132, 0.0180, 0.0238],\n",
       "          [0.0041, 0.9868, 0.0036, 0.0026, 0.0027]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   44.672630  117.633499  228.456360  271.532959    0.928848     54   \n",
       "  1  270.628204   99.923401  462.073151  250.719666    0.927355     54   \n",
       "  2  457.992096   91.112808  637.475342  250.053955    0.920822     54   \n",
       "  3    1.349792    9.215622  632.934448  476.273987    0.760173     60   \n",
       "  4   93.542084    0.777206  638.904419  158.893723    0.391060      0   \n",
       "  \n",
       "             name  \n",
       "  0         donut  \n",
       "  1         donut  \n",
       "  2         donut  \n",
       "  3  dining table  \n",
       "  4        person  ,\n",
       "  'caption': ['A dark brown doughnut on a plate.',\n",
       "   'A brown chocolate doughnut.'],\n",
       "  'bbox_target': [271.35, 101.62, 191.35, 151.35]},\n",
       " 621: {'image_emb': tensor([[-0.2805, -0.1764, -0.1340,  ...,  0.1475,  0.2551,  0.2393],\n",
       "          [-0.2458,  0.3123, -0.0569,  ...,  0.5400,  0.1455,  0.0881],\n",
       "          [-0.2996,  0.0956,  0.0735,  ...,  0.6216,  0.2908,  0.1231],\n",
       "          [ 0.1152, -0.0967, -0.1852,  ...,  0.8145,  0.0693, -0.4465],\n",
       "          [-0.1591, -0.2844, -0.0981,  ..., -0.3928, -0.1004,  0.0598]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2246, -0.1465,  0.0058,  ..., -0.0300,  0.1415,  0.2837],\n",
       "          [-0.0731,  0.0818,  0.1552,  ...,  0.4497, -0.0676,  0.3567]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.2236e-01, 4.6631e-02, 7.4524e-02, 9.3579e-06, 4.5654e-01],\n",
       "          [1.5625e-01, 1.5236e-02, 5.9326e-02, 1.1921e-07, 7.6904e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  392.656769  258.721252  583.945374  475.665894    0.940994     22  zebra\n",
       "  1   81.547676  277.391083  153.887421  380.036652    0.909212     22  zebra\n",
       "  2   59.841431  298.602631  118.822037  378.891754    0.795557     22  zebra\n",
       "  3    0.127270  346.427948   23.122070  373.961151    0.762242     22  zebra,\n",
       "  'caption': ['A zebra standing alone.',\n",
       "   'A zebra standing in a zoo enclosure with two other zebras.'],\n",
       "  'bbox_target': [394.94, 260.52, 188.88, 214.7]},\n",
       " 622: {'image_emb': tensor([[-0.1069,  0.0190, -0.0611,  ...,  0.6094,  0.4116, -0.0945],\n",
       "          [-0.0820,  0.1851, -0.1875,  ...,  1.1924, -0.2947, -0.0182],\n",
       "          [ 0.1345,  0.4080, -0.0633,  ...,  0.6196,  0.1614, -0.2070]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3694,  0.0039,  0.1499,  ...,  0.5298, -0.3220, -0.0779],\n",
       "          [ 0.0692, -0.0943,  0.0432,  ..., -0.3516,  0.0392,  0.0484]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0029, 0.9097, 0.0873],\n",
       "          [0.0051, 0.8291, 0.1658]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax       ymax  confidence  class   name\n",
       "  0    6.694508  48.068947  110.696434  83.546272    0.842287      2    car\n",
       "  1  124.400345   8.141762  347.159302  95.533257    0.702434      7  truck\n",
       "  2    0.117213  60.134811    9.311229  79.450600    0.618311      2    car\n",
       "  3  296.554688   1.304299  461.083405  79.921371    0.457944      7  truck\n",
       "  4  296.508087   1.451570  456.891693  79.218155    0.439444      2    car,\n",
       "  'caption': ['A gray SUV parked in the driveway.', \"a van that's in front\"],\n",
       "  'bbox_target': [297.02, 3.57, 164.92, 74.25]},\n",
       " 623: {'image_emb': tensor([[ 0.2888, -0.1312,  0.2551,  ...,  0.8354, -0.1669, -0.0762],\n",
       "          [ 0.5366, -0.0242,  0.3225,  ...,  0.8496, -0.2172,  0.0697],\n",
       "          [ 0.3818,  0.1061,  0.1472,  ...,  0.5425, -0.0579, -0.1014]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0622, -0.0242,  0.0709,  ...,  0.2563,  0.2729, -0.3179],\n",
       "          [ 0.2478, -0.0126,  0.0312,  ...,  0.1908, -0.0947, -0.4897]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1492, 0.8066, 0.0441],\n",
       "          [0.0779, 0.9053, 0.0171]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  name\n",
       "  0   33.029732  153.488678  232.927032  404.565765    0.831267      8  boat\n",
       "  1  425.962555  167.746597  572.978882  379.713623    0.760220      8  boat\n",
       "  2  186.004395   74.786285  416.047119  511.390961    0.559784      8  boat\n",
       "  3  205.244781  381.220856  445.017487  436.618683    0.384938      8  boat\n",
       "  4  153.424316  290.709808  166.078186  308.745514    0.292250      8  boat,\n",
       "  'caption': ['The boat in the back right with a light below the mast.',\n",
       "   'the yacht has light in the upper body'],\n",
       "  'bbox_target': [429.55, 165.05, 141.26, 214.77]},\n",
       " 624: {'image_emb': tensor([[ 0.0714,  0.3845,  0.0076,  ...,  0.9673, -0.0138, -0.1427],\n",
       "          [-0.0105,  0.0782, -0.3616,  ...,  0.9043, -0.1160, -0.3049],\n",
       "          [-0.3552,  0.2825, -0.3569,  ...,  0.9629,  0.0176, -0.0328],\n",
       "          ...,\n",
       "          [-0.1516,  0.2644, -0.1333,  ...,  0.8926,  0.1345,  0.2456],\n",
       "          [-0.0855, -0.0429, -0.3352,  ...,  0.9365, -0.0779, -0.1677],\n",
       "          [-0.0134,  0.2173, -0.2449,  ...,  0.4441, -0.2058, -0.1367]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-2.0050e-02, -8.8318e-02, -5.4718e-02,  ...,  4.8267e-01,\n",
       "           -4.4971e-01, -1.1475e-01],\n",
       "          [ 4.0436e-04,  2.1924e-01, -5.3375e-02,  ...,  2.7466e-01,\n",
       "           -3.8330e-01, -1.8579e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.7656e-01, 4.1008e-05, 2.3842e-07, 2.3331e-02, 1.7881e-07, 3.5763e-07,\n",
       "           1.7810e-04],\n",
       "          [7.9541e-01, 5.2512e-05, 2.3842e-07, 2.0422e-01, 0.0000e+00, 4.7684e-07,\n",
       "           3.0231e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  489.215851  120.290375  639.291016  456.971924    0.926785     56  chair\n",
       "  1  141.870636  164.918030  334.297699  291.790283    0.925026     16    dog\n",
       "  2  305.441223  175.403885  442.483521  247.435883    0.894450     16    dog\n",
       "  3    0.183743  282.229797   96.629135  460.859131    0.889928     56  chair\n",
       "  4   18.814438  132.282303   48.325676  173.166672    0.886202     41    cup\n",
       "  5   79.143677  159.540497  495.365601  440.962891    0.822859     57  couch\n",
       "  6  491.199615  155.917816  544.540283  175.700897    0.274908     73   book,\n",
       "  'caption': ['The wooden chair with a white cushion and a green throw pillow.',\n",
       "   'A wooden armchair with white cushions with a green pillow on it .'],\n",
       "  'bbox_target': [487.19, 120.36, 152.12, 337.13]},\n",
       " 625: {'image_emb': tensor([[ 0.0260,  0.2876, -0.2983,  ...,  0.7334, -0.2130, -0.1147],\n",
       "          [-0.0024, -0.0267, -0.2262,  ...,  0.6431, -0.2822,  0.1163],\n",
       "          [-0.0186,  0.1170, -0.3955,  ...,  0.7222, -0.2756, -0.0584]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0898, -0.2710, -0.0958,  ...,  0.1797,  0.1129, -0.1720],\n",
       "          [ 0.0555, -0.0545, -0.2891,  ..., -0.0092, -0.1073, -0.2195]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1094, 0.8081, 0.0826],\n",
       "          [0.7290, 0.1083, 0.1627]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class   name\n",
       "  0  430.934418  84.308472  638.993286  419.149109    0.926835     17  horse\n",
       "  1    9.950806  74.298615  443.205811  421.217529    0.903773     17  horse,\n",
       "  'caption': ['A short haired horse walking along a longer haired horse.',\n",
       "   'Horse to right of other horse with less hair'],\n",
       "  'bbox_target': [427.91, 84.21, 212.09, 337.39]},\n",
       " 626: {'image_emb': tensor([[-0.0784,  0.4653, -0.2358,  ...,  0.9136,  0.2507,  0.3345],\n",
       "          [ 0.0589,  0.5947,  0.2610,  ...,  0.9326,  0.3916, -0.1037],\n",
       "          [ 0.2043, -0.0282, -0.5430,  ...,  1.0234,  0.2234, -0.0083],\n",
       "          [-0.0224,  0.5142,  0.0750,  ...,  0.7930,  0.5308,  0.0120],\n",
       "          [ 0.0382, -0.1124, -0.3809,  ...,  0.7427,  0.0494,  0.1467],\n",
       "          [ 0.1079,  0.1030,  0.0890,  ...,  1.0703,  0.0823, -0.3542]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0021,  0.4375, -0.1270,  ..., -0.2603, -0.1295, -0.1182],\n",
       "          [-0.0713,  0.0406,  0.0721,  ..., -0.2527,  0.1470, -0.0925]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.5065e-02, 7.3828e-01, 1.6403e-03, 2.2168e-01, 6.9475e-04, 2.7046e-03],\n",
       "          [2.8670e-05, 2.8467e-01, 1.0133e-06, 7.1533e-01, 3.5763e-07, 1.5855e-05]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     0.301768   91.263245   91.538704  605.335754    0.902656     72   \n",
       "  1   189.931656  331.295319  442.758667  520.885864    0.873061     58   \n",
       "  2   536.012756  365.514984  585.094116  479.511261    0.844354     39   \n",
       "  3   370.899719  380.942078  512.734375  524.432861    0.759000     58   \n",
       "  4   581.901062  365.560394  611.787964  480.923279    0.731828     39   \n",
       "  5   224.228745  377.988220  610.463379  610.164368    0.686678     60   \n",
       "  6   366.986420  220.687469  378.010834  254.595474    0.671945     39   \n",
       "  7   356.323273  220.763870  366.850128  254.300797    0.604149     39   \n",
       "  8   191.919296  165.963715  313.779480  248.531342    0.578286     68   \n",
       "  9   579.453369  218.268509  611.790222  248.742126    0.570212     58   \n",
       "  10  259.675964  280.195496  546.977295  389.551300    0.550370     60   \n",
       "  11  191.433807  164.725418  317.439514  391.757538    0.543015     69   \n",
       "  12  194.857697  245.591187  315.878174  350.913971    0.421782     69   \n",
       "  \n",
       "              name  \n",
       "  0   refrigerator  \n",
       "  1   potted plant  \n",
       "  2         bottle  \n",
       "  3   potted plant  \n",
       "  4         bottle  \n",
       "  5   dining table  \n",
       "  6         bottle  \n",
       "  7         bottle  \n",
       "  8      microwave  \n",
       "  9   potted plant  \n",
       "  10  dining table  \n",
       "  11          oven  \n",
       "  12          oven  ,\n",
       "  'caption': ['The larger of two plants.',\n",
       "   'a plant with a flag in it on the side of a smaller identical plant'],\n",
       "  'bbox_target': [186.62, 331.52, 259.34, 192.11]},\n",
       " 627: {'image_emb': tensor([[ 0.1156,  0.1514, -0.0952,  ...,  0.8110, -0.2039, -0.0349],\n",
       "          [ 0.3372,  0.0691,  0.2469,  ...,  0.7197,  0.1487,  0.2581],\n",
       "          [ 0.0986,  0.3442, -0.0834,  ...,  0.6030,  0.1941,  0.4951],\n",
       "          ...,\n",
       "          [ 0.1411,  0.3679, -0.0743,  ...,  0.8086,  0.0776, -0.1061],\n",
       "          [-0.2129, -0.0131, -0.1995,  ...,  1.2949, -0.1697,  0.2881],\n",
       "          [ 0.1245, -0.1490,  0.0585,  ...,  0.7427, -0.1266,  0.2296]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1609, -0.1333, -0.2981,  ..., -0.3167, -0.0672,  0.2512],\n",
       "          [-0.0222, -0.1984, -0.1156,  ..., -0.0645, -0.4353,  0.2438]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.1586e-02, 1.1086e-02, 1.6832e-04, 2.0828e-03, 6.7616e-04, 9.3750e-01,\n",
       "           1.6907e-02],\n",
       "          [1.1053e-03, 1.6327e-03, 2.7351e-03, 3.0994e-03, 2.1629e-03, 9.8877e-01,\n",
       "           3.7599e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   26.730526   95.418808  294.907471  499.198822    0.922577      0   \n",
       "  1   81.642433   92.787949  228.184753  243.346481    0.903726     63   \n",
       "  2   36.112366  124.990829   72.278725  160.866852    0.887640     41   \n",
       "  3  265.353333  250.684906  316.992981  319.718445    0.842986     73   \n",
       "  4  221.671539  189.070114  247.459030  242.930725    0.819394     67   \n",
       "  5  424.674835    0.931966  499.540009  163.366623    0.742494     57   \n",
       "  6    0.000000  116.890671   27.807945  160.494614    0.659715     73   \n",
       "  7    0.000000  158.126999   76.509331  216.156143    0.526698     73   \n",
       "  8    0.959754  372.271667  435.134705  499.952606    0.460185     57   \n",
       "  9    1.199055  368.861115  434.869202  499.678040    0.364420     56   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      laptop  \n",
       "  2         cup  \n",
       "  3        book  \n",
       "  4  cell phone  \n",
       "  5       couch  \n",
       "  6        book  \n",
       "  7        book  \n",
       "  8       couch  \n",
       "  9       chair  ,\n",
       "  'caption': ['There is a Couch in the right side of the guy which is blue in color',\n",
       "   'Blue chair.'],\n",
       "  'bbox_target': [421.18, 1.38, 78.82, 161.33]},\n",
       " 628: {'image_emb': tensor([[ 0.1094,  0.2310, -0.0658,  ...,  1.2920,  0.1404, -0.1109],\n",
       "          [ 0.2632, -0.1310,  0.1233,  ...,  0.5391,  0.2054,  0.0481],\n",
       "          [-0.1940,  0.3042, -0.2119,  ...,  1.4531, -0.0211,  0.0556],\n",
       "          [-0.2798,  0.0629, -0.2771,  ...,  0.9697,  0.0989, -0.0959],\n",
       "          [ 0.2549,  0.2922,  0.0490,  ...,  0.7769,  0.0790,  0.0483]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3467, -0.2462, -0.2008,  ..., -0.1543, -0.3455, -0.2258],\n",
       "          [ 0.0690, -0.1768, -0.1595,  ..., -0.3398,  0.0080, -0.3169]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.9600e-02, 4.1431e-01, 3.1763e-01, 7.5439e-02, 1.0309e-01],\n",
       "          [2.4072e-01, 2.6440e-01, 2.3007e-04, 5.8746e-04, 4.9390e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   33.904877    9.881042  317.049591  131.778503    0.912090     53   \n",
       "  1  109.480408   84.489273  612.568420  392.712646    0.911463     53   \n",
       "  2    0.140518  222.090027   94.398849  354.466248    0.896004     43   \n",
       "  3    0.000000   85.521011   34.815948  221.948547    0.798770     41   \n",
       "  4  296.010315    0.000000  598.097351   54.879547    0.671765      0   \n",
       "  5    3.817169   19.827652  640.000000  427.000000    0.633897     60   \n",
       "  \n",
       "             name  \n",
       "  0         pizza  \n",
       "  1         pizza  \n",
       "  2         knife  \n",
       "  3           cup  \n",
       "  4        person  \n",
       "  5  dining table  ,\n",
       "  'caption': ['plate of food behind the front plate',\n",
       "   'a pizza in the background of another pizza'],\n",
       "  'bbox_target': [38.38, 12.47, 279.23, 120.91]},\n",
       " 629: {'image_emb': tensor([[ 0.1732,  0.6299,  0.1218,  ...,  1.1338, -0.1266, -0.3145],\n",
       "          [ 0.1964,  0.0863, -0.5034,  ...,  0.8247, -0.1556, -0.2115],\n",
       "          [ 0.2710,  0.3989,  0.0374,  ...,  1.2578,  0.0547,  0.1276],\n",
       "          [ 0.4282,  0.3604, -0.2023,  ...,  0.2632, -0.0524, -0.1736]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0967,  0.0337,  0.1294,  ...,  0.1179,  0.2715, -0.3938],\n",
       "          [-0.0794,  0.0251,  0.4243,  ...,  0.0845, -0.1646, -0.0648]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.4668e-03, 5.3027e-01, 2.9802e-06, 4.6802e-01],\n",
       "          [4.0398e-03, 2.2537e-02, 1.2517e-06, 9.7363e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  461.822418    0.000000  638.325012  236.018890    0.809037      2      car\n",
       "  1   65.954727   38.769608  554.787170  413.286560    0.803324      1  bicycle\n",
       "  2    0.208855   51.662399  123.787773  223.481079    0.722490      2      car\n",
       "  3  324.109192   96.553223  628.747498  313.765320    0.659724      1  bicycle\n",
       "  4    0.741096    1.231506  128.350067   54.936859    0.646217      2      car\n",
       "  5    0.150307  103.084290   34.391190  315.655823    0.483927      0   person,\n",
       "  'caption': ['A heavily decorated bicycle.',\n",
       "   \"the bike that looks like it's made out of bush\"],\n",
       "  'bbox_target': [67.48, 40.49, 496.0, 377.9]},\n",
       " 630: {'image_emb': tensor([[-0.3794,  0.0266, -0.0701,  ...,  0.6313, -0.0182, -0.0998],\n",
       "          [-0.1095,  0.0196, -0.1869,  ...,  0.9375,  0.0104,  0.0883],\n",
       "          [-0.3274,  0.0143, -0.0117,  ...,  0.6323, -0.1926, -0.2659]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.4255, -0.1367, -0.1971,  ..., -0.1733,  0.0507,  0.4224],\n",
       "          [-0.3093, -0.5171, -0.0711,  ...,  0.5308,  0.1814,  0.5254]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5620, 0.2869, 0.1512],\n",
       "          [0.3521, 0.5806, 0.0672]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class   name\n",
       "  0  127.264725  52.450150  568.045166  361.347107    0.952629     22  zebra\n",
       "  1  551.982544  65.554291  639.890503  226.861176    0.912109     22  zebra,\n",
       "  'caption': ['The large zebra at the forefront of the photo eating off the ground',\n",
       "   'A zebra stands directly in front of the camera, we face its side.'],\n",
       "  'bbox_target': [125.8, 52.51, 440.29, 299.25]},\n",
       " 631: {'image_emb': tensor([[ 0.2482,  0.0832, -0.2305,  ...,  0.2549,  0.0963, -0.0603],\n",
       "          [ 0.3718, -0.0488, -0.2786,  ...,  0.1334,  0.1895, -0.0327],\n",
       "          [-0.2659, -0.0654, -0.1327,  ..., -0.0911,  0.2397, -0.1209],\n",
       "          [-0.0376, -0.4146, -0.0397,  ..., -0.0458, -0.0275, -0.1130]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1973, -0.3655, -0.1334,  ...,  0.1794, -0.0588,  0.5439],\n",
       "          [-0.1752, -0.2126, -0.0143,  ...,  0.2632, -0.0024,  0.3208]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0761, 0.0978, 0.2273, 0.5986],\n",
       "          [0.0899, 0.1119, 0.5254, 0.2727]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  410.114136  133.161346  557.562073  411.036987    0.940399     22  zebra\n",
       "  1  231.634552  153.134766  405.395599  373.458496    0.929615     22  zebra\n",
       "  2   57.749237  141.039246  256.498444  373.114136    0.912350     22  zebra,\n",
       "  'caption': ['the back end of a zebra to the right of 3 zebras',\n",
       "   'The zebra standing in the right side placed the tail in the left direction.'],\n",
       "  'bbox_target': [413.5, 137.27, 149.05, 273.1]},\n",
       " 632: {'image_emb': tensor([[ 0.3884,  0.0585, -0.0278,  ...,  0.2795,  0.1030, -0.1853],\n",
       "          [ 0.0830,  0.0884,  0.1196,  ...,  0.4172,  0.1077, -0.2474],\n",
       "          [ 0.0078,  0.1281,  0.4578,  ...,  0.5068,  0.0508,  0.0198],\n",
       "          ...,\n",
       "          [ 0.1302, -0.2515, -0.4509,  ...,  1.0654,  0.0358, -0.0047],\n",
       "          [ 0.1786,  0.1037, -0.2664,  ...,  1.0430,  0.0434,  0.1087],\n",
       "          [ 0.3516, -0.0920,  0.3267,  ...,  0.4075,  0.2969, -0.1874]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 3.2684e-02, -1.8600e-02, -1.9983e-01, -5.3772e-02, -1.8750e-01,\n",
       "           -3.3154e-01, -6.9397e-02, -1.1934e+00, -3.8184e-01,  3.4180e-01,\n",
       "           -1.2793e-01,  2.0227e-01,  6.0010e-01, -3.1494e-01,  2.0447e-01,\n",
       "            9.0698e-02, -1.4244e-02,  1.0767e-01, -3.5205e-01, -1.8799e-01,\n",
       "            5.0195e-01,  2.0813e-01,  1.4938e-02,  1.3257e-01,  2.4231e-01,\n",
       "            3.8025e-02,  4.9469e-02,  5.3516e-01, -1.2598e-01,  1.6248e-01,\n",
       "            1.2201e-01, -6.4453e-02, -2.7368e-01, -3.6804e-02,  1.7029e-01,\n",
       "           -2.1515e-02, -1.3611e-01,  8.0933e-02,  1.9958e-01,  5.2338e-03,\n",
       "            2.3901e-01, -1.3708e-01, -2.7637e-01,  1.0895e-01,  3.3545e-01,\n",
       "            3.6475e-01, -4.6326e-02,  2.8732e-02,  3.3643e-01,  3.4695e-03,\n",
       "           -2.8540e-01, -1.5710e-01, -5.5847e-03,  3.9734e-02,  9.6008e-02,\n",
       "           -1.2006e-01, -4.7681e-01,  3.7866e-01,  1.0657e-01,  3.4424e-01,\n",
       "            5.0732e-01, -3.0737e-01,  5.9619e-01,  1.6708e-02,  1.0492e-01,\n",
       "           -2.9892e-02,  1.2415e-01,  2.9541e-01,  7.6782e-02,  1.0034e-01,\n",
       "           -3.3838e-01,  1.0938e-01,  1.1456e-01, -1.0071e-02, -1.8463e-02,\n",
       "           -2.4402e-01,  2.2449e-01,  2.4658e-01,  3.1189e-02, -2.0947e-01,\n",
       "           -1.7261e-01,  2.3999e-01, -3.2745e-02,  4.6478e-02, -2.7148e-01,\n",
       "           -1.5869e-01, -5.8746e-02,  4.2505e-01,  2.0966e-02, -7.3486e-02,\n",
       "           -4.1718e-02, -1.4368e-01, -1.5303e+00,  5.1172e-01, -5.0732e-01,\n",
       "            9.0393e-02, -6.1798e-02, -1.9751e-01,  3.0200e-01,  4.9219e-01,\n",
       "            2.8296e-01, -9.9792e-02,  2.4146e-01,  1.5063e-01,  1.0059e-01,\n",
       "           -7.7362e-03,  4.4289e-03, -3.4619e-01, -3.3911e-01, -1.1395e-01,\n",
       "            3.7793e-01,  1.1615e-01, -4.8608e-01, -2.7637e-01,  2.0801e-01,\n",
       "            2.6343e-01, -2.1095e-03, -8.8425e-03, -7.5012e-02, -1.8250e-01,\n",
       "           -3.8550e-01, -4.9243e-01,  4.2114e-01, -9.9182e-02, -2.4475e-02,\n",
       "           -3.3813e-01, -2.4890e-01, -2.8271e-01, -6.5247e-02,  5.7324e-01,\n",
       "           -1.2781e-01, -1.4319e-01,  2.6703e-02,  5.8359e+00,  1.6541e-02,\n",
       "           -2.4719e-03, -1.7615e-01, -2.6172e-01,  4.6814e-02, -3.7207e-01,\n",
       "           -3.3813e-01,  5.9845e-02, -2.9932e-01, -1.7981e-01, -2.7271e-01,\n",
       "           -4.1943e-01,  1.2093e-02, -4.5807e-02, -2.1277e-01,  5.5420e-02,\n",
       "           -4.6921e-04, -2.1350e-01,  5.7129e-01,  4.1138e-02,  1.5906e-01,\n",
       "            5.2185e-02,  1.6357e-01, -3.8672e-01,  1.2744e-01,  1.1188e-01,\n",
       "           -1.0956e-02,  9.1431e-02,  8.0566e-02, -8.5205e-02, -1.0376e-02,\n",
       "           -4.3121e-02,  2.4255e-01,  1.1346e-01, -2.1057e-01,  3.6499e-01,\n",
       "           -1.7065e-01,  3.6438e-02,  7.5500e-02,  5.3223e-02, -2.2253e-01,\n",
       "           -1.3269e-01, -1.4465e-01,  4.4128e-02,  1.4307e-01, -3.0811e-01,\n",
       "            3.1763e-01,  3.6597e-01, -3.4351e-01,  4.6338e-01,  7.1411e-02,\n",
       "            1.6309e-01,  2.6245e-01, -5.5762e-01, -4.3286e-01, -8.7769e-02,\n",
       "            1.4929e-01,  1.7542e-01,  6.7627e-02, -4.9095e-03,  4.1064e-01,\n",
       "           -5.5420e-02, -3.4302e-01,  8.9905e-02, -1.3147e-01,  3.0347e-01,\n",
       "            4.8901e-01, -1.9067e-01, -2.6855e-01,  2.0309e-02, -1.3684e-01,\n",
       "           -5.0720e-02, -7.3181e-02, -7.6538e-02,  1.3390e-02, -1.3535e-02,\n",
       "           -1.5759e-01,  2.7197e-01, -1.4050e-01, -6.6223e-02,  2.1118e-01,\n",
       "            9.4849e-02, -2.2375e-01, -3.6401e-01,  3.2324e-01,  1.6956e-01,\n",
       "            2.3083e-01, -1.5979e-01, -3.1079e-01,  2.2290e-01, -5.1819e-02,\n",
       "           -1.6919e-01, -2.0154e-01, -3.3844e-02,  7.5928e-02, -4.0527e-01,\n",
       "           -3.1067e-02,  4.9377e-02, -2.6465e-01, -8.3313e-02,  1.0748e-01,\n",
       "           -6.0974e-02,  1.5906e-01,  7.1533e-02, -2.0667e-01,  2.4219e-01,\n",
       "           -1.8701e-01,  1.4673e-01,  1.8555e-01,  9.4177e-02,  2.8979e-01,\n",
       "            6.3660e-02,  2.0233e-02,  3.9502e-01, -2.3352e-01, -2.8198e-01,\n",
       "            1.1871e-01,  1.1742e-02,  4.8169e-01,  7.3486e-02, -1.9824e-01,\n",
       "           -1.5112e-01, -2.9297e-01,  3.3283e-03,  1.4575e-01,  1.1322e-01,\n",
       "           -4.3488e-02,  5.8502e-02,  1.6370e-01, -1.6528e-01,  9.1614e-02,\n",
       "           -7.8491e-02,  1.5450e-02, -5.6915e-02,  2.1765e-01, -1.3464e-01,\n",
       "            9.9915e-02,  1.6553e-01,  1.0291e-01, -3.9136e-01, -2.5073e-01,\n",
       "            1.1896e-01,  3.8354e-01, -1.0291e-01,  3.1982e-01, -4.1382e-01,\n",
       "           -5.4639e-01,  8.7891e-03,  6.9873e-01,  2.2278e-02,  9.8694e-02,\n",
       "            1.7773e-01, -2.3972e-02,  5.6183e-02,  1.5442e-01, -8.1665e-02,\n",
       "            1.4856e-01,  3.6206e-01,  1.3660e-01,  1.8420e-01,  2.3450e-01,\n",
       "           -1.1246e-02,  1.6150e-01, -1.1304e-01,  6.6452e-03, -3.2373e-01,\n",
       "            8.6365e-02, -3.1470e-01,  2.0911e-01,  3.7500e-01, -2.4451e-01,\n",
       "           -3.6548e-01,  5.5518e-01, -3.9502e-01, -8.4045e-02, -5.8807e-02,\n",
       "           -8.5754e-02,  3.2080e-01,  5.8281e+00, -1.2695e-01,  5.3564e-01,\n",
       "            2.6343e-01,  2.1692e-01,  5.3314e-02,  5.0720e-02,  8.4961e-01,\n",
       "           -6.6223e-02, -3.4204e-01,  5.8990e-02,  2.3401e-01,  1.6919e-01,\n",
       "            8.5571e-02,  3.7256e-01,  2.3511e-01, -4.3915e-02, -2.4766e+00,\n",
       "            6.5002e-02,  2.3950e-01,  2.5781e-01,  2.0767e-02,  2.9028e-01,\n",
       "           -3.5889e-01, -3.4570e-01, -1.9730e-02, -3.8232e-01, -2.3514e-02,\n",
       "           -8.5693e-02, -5.9326e-02, -2.2815e-01,  2.0435e-01, -1.3684e-01,\n",
       "            9.1675e-02,  2.6953e-01,  1.8567e-01, -7.5500e-02, -6.5430e-02,\n",
       "           -1.6223e-01, -2.7051e-01, -1.4990e-01,  3.3618e-01,  6.5979e-02,\n",
       "            1.8555e-01, -5.8441e-02, -1.6919e-01,  7.5562e-02,  1.5417e-01,\n",
       "            2.4268e-01, -7.3338e-04,  1.9562e-02,  2.8613e-01,  4.9390e-01,\n",
       "            1.3588e-02, -4.9805e-02, -2.3022e-01,  1.1151e-01,  5.3558e-02,\n",
       "           -1.3757e-01,  2.9755e-02,  3.3398e-01, -9.8328e-02,  1.8848e-01,\n",
       "           -2.6123e-01,  3.3960e-01, -4.4312e-01, -4.3164e-01, -3.8727e-02,\n",
       "            1.6577e-01,  2.4695e-01, -1.5930e-01, -1.4941e-01,  1.1041e-01,\n",
       "           -2.0251e-01, -1.8814e-02,  6.0059e-01, -5.9668e-01, -2.2375e-01,\n",
       "           -4.6216e-01,  1.1890e-01, -1.4075e-01,  3.7891e-01, -6.1890e-02,\n",
       "           -2.3303e-01, -3.2349e-01,  3.6401e-01, -3.9600e-01,  4.9487e-01,\n",
       "            4.4019e-01,  5.6458e-02,  1.8091e-01, -2.1362e-02,  1.5430e-01,\n",
       "            1.1823e-01, -5.8929e-02,  1.1450e-01, -1.1879e-02,  3.9404e-01,\n",
       "           -3.7451e-01, -1.1737e-01, -5.3040e-02,  2.4609e-01, -9.6252e-02,\n",
       "           -3.2129e-01,  9.6069e-02,  4.5703e-01, -6.2256e-02, -4.3286e-01,\n",
       "           -1.0583e-01, -1.8787e-01,  7.4951e-02, -4.0894e-02, -7.0068e-02,\n",
       "            3.3936e-01,  5.2795e-02,  2.1957e-02,  2.7295e-01,  2.5122e-01,\n",
       "            5.6381e-03,  1.0028e-01, -1.1548e-01,  3.6694e-01, -1.9775e-01,\n",
       "            3.6545e-03,  1.5894e-01,  2.8320e-01,  1.9055e-01, -1.0675e-01,\n",
       "           -2.3083e-01, -6.5613e-02, -1.8176e-01, -7.5378e-02,  1.0931e-01,\n",
       "            2.8394e-01, -2.7612e-01, -3.3740e-01,  2.0142e-01,  7.3051e-03,\n",
       "            2.2980e-02, -2.3315e-01, -2.4292e-01, -2.7856e-01, -1.0419e-01,\n",
       "            2.2961e-01, -3.4106e-01, -3.6133e-01, -5.6274e-02, -3.8330e-01,\n",
       "           -1.5845e-01,  3.8727e-02, -1.1603e-01,  9.6054e-03, -2.5177e-02,\n",
       "           -2.2180e-01,  2.6929e-01, -1.6693e-02, -1.7847e-01,  1.1890e-01,\n",
       "            4.0967e-01,  2.2705e-02,  3.7231e-01,  1.0309e-01,  4.2090e-01,\n",
       "           -3.4717e-01, -2.3767e-01, -1.0089e-01,  1.6370e-01,  6.4209e-01,\n",
       "           -2.1741e-01, -3.4473e-01, -1.5100e-01, -2.6188e-03,  1.0319e-03,\n",
       "            1.7288e-02,  5.2643e-02,  2.3303e-01, -1.3818e-01,  6.9641e-02,\n",
       "            2.7100e-01, -1.3989e-01, -9.0027e-02,  8.8818e-01,  1.8262e-01,\n",
       "           -1.1151e-01,  1.4526e-01, -4.2310e-01,  8.2520e-02,  3.1860e-01,\n",
       "            2.0703e-01, -1.4732e-02, -2.8906e-01,  2.2095e-02,  1.5344e-01,\n",
       "           -1.5698e-01, -8.3191e-02,  1.6479e-01, -3.6914e-01, -6.3525e-01,\n",
       "            4.6349e-03, -2.7222e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0094, 0.0249, 0.4478, 0.0039, 0.3889, 0.0295, 0.0300, 0.0655]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  475.327576  134.485184  640.000000  420.911255    0.937303      0   \n",
       "  1    0.535934   78.931793  264.598022  421.211304    0.928281      0   \n",
       "  2  251.319809  193.141357  551.823242  422.038452    0.920977      0   \n",
       "  3  252.162415  378.756470  308.773010  421.938293    0.791310     67   \n",
       "  4  199.404510  129.234787  417.115265  420.233276    0.767125      0   \n",
       "  5  214.378769  350.527924  251.153915  371.366669    0.718976     67   \n",
       "  6  576.528320  244.945801  606.266968  262.930054    0.718464     67   \n",
       "  7  566.851440  273.509094  640.000000  423.410461    0.666872     26   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2      person  \n",
       "  3  cell phone  \n",
       "  4      person  \n",
       "  5  cell phone  \n",
       "  6  cell phone  \n",
       "  7     handbag  ,\n",
       "  'caption': ['The person stnading behind the person wearing a cap'],\n",
       "  'bbox_target': [202.18, 131.42, 219.54, 294.58]},\n",
       " 633: {'image_emb': tensor([[-0.0052, -0.1477,  0.0527,  ...,  0.1982,  0.2070,  0.1747],\n",
       "          [ 0.3240,  0.0196, -0.0447,  ...,  0.2771,  0.0453,  0.1702],\n",
       "          [-0.0370,  0.0856,  0.2522,  ...,  0.2108,  0.3027,  0.1923],\n",
       "          [ 0.5391,  0.2174,  0.0626,  ...,  0.6021,  0.0570,  0.4531],\n",
       "          [ 0.3455, -0.3538, -0.2791,  ...,  0.6938,  0.2639,  0.2798]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0015, -0.4500, -0.1564,  ...,  0.1067, -0.1544, -0.3789],\n",
       "          [ 0.1187, -0.3596, -0.0715,  ..., -0.3069, -0.1611, -0.5352]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.8325, 0.0824, 0.0441, 0.0318, 0.0092],\n",
       "          [0.3752, 0.4187, 0.0492, 0.0964, 0.0603]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  142.993927  286.275360  449.049591  571.511169    0.889233     48   \n",
       "  1  374.402802  160.246597  588.312622  398.280457    0.888887     48   \n",
       "  2  219.515457   63.512611  431.501556  221.458267    0.883616     48   \n",
       "  3   18.506674  181.312347  242.542114  416.353271    0.878426     48   \n",
       "  4    8.924426   16.946560  589.835938  594.452820    0.549475     60   \n",
       "  5  338.160126   19.575817  588.165283  209.659851    0.484042     60   \n",
       "  6  465.998718   17.757566  589.028259   50.364544    0.258694     43   \n",
       "  \n",
       "             name  \n",
       "  0      sandwich  \n",
       "  1      sandwich  \n",
       "  2      sandwich  \n",
       "  3      sandwich  \n",
       "  4  dining table  \n",
       "  5  dining table  \n",
       "  6         knife  ,\n",
       "  'caption': ['The piece of toast with a jalapeno on the very top near the white plate.',\n",
       "   'The bread that is on the bottom right hand corner.'],\n",
       "  'bbox_target': [374.92, 159.89, 216.4, 241.22]},\n",
       " 634: {'image_emb': tensor([[ 0.1744,  0.3860,  0.1013,  ...,  1.3623,  0.1018, -0.2330],\n",
       "          [-0.2969, -0.1205, -0.1157,  ...,  1.2227,  0.0279, -0.1836],\n",
       "          [-0.3267,  0.2316, -0.0227,  ...,  0.6714,  0.1305,  0.1439]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1483, -0.2864, -0.2500,  ..., -0.2001,  0.3123, -0.3525],\n",
       "          [-0.1899, -0.0226, -0.1002,  ...,  0.2908,  0.3765,  0.0529]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5884, 0.2959, 0.1158],\n",
       "          [0.0099, 0.8511, 0.1389]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   496.488037  132.968292  640.000000  421.398560    0.946893      0  person\n",
       "  1   208.025421  211.304749  400.356293  420.977112    0.934233      0  person\n",
       "  2   316.822205  158.407776  346.386169  186.014709    0.697062     46  banana\n",
       "  3   264.336853  143.579437  292.326416  195.825958    0.693749     46  banana\n",
       "  4   298.696960  159.368530  330.134583  189.407715    0.667057     46  banana\n",
       "  5   334.026306  151.445740  357.250549  180.732727    0.648290     46  banana\n",
       "  6   243.213379  157.135773  270.928833  195.851349    0.631741     46  banana\n",
       "  7   282.211243  150.245209  307.184265  193.254303    0.620921     46  banana\n",
       "  8   224.945221  149.986176  255.408447  197.364899    0.551247     46  banana\n",
       "  9   354.190979  120.386856  378.318726  151.775208    0.536559     46  banana\n",
       "  10  321.374054  121.813797  354.960846  155.668152    0.522370     46  banana\n",
       "  11  345.422760  125.704391  360.961823  159.814331    0.398098     46  banana\n",
       "  12  291.409210  127.766006  314.689178  157.991272    0.362960     46  banana\n",
       "  13  259.781952  130.618469  283.242584  161.532074    0.345837     46  banana\n",
       "  14  217.702576  136.259674  237.650116  165.714325    0.316507     46  banana\n",
       "  15  308.209656  131.178162  325.325012  160.397614    0.314359     46  banana\n",
       "  16  366.737549  144.179596  383.599487  163.036102    0.280486     46  banana,\n",
       "  'caption': ['A MAN CARRYING BANANA BOXES',\n",
       "   'An old man in a striped shirt with a red box of bananas.'],\n",
       "  'bbox_target': [212.02, 209.62, 179.4, 215.38]},\n",
       " 635: {'image_emb': tensor([[-0.0428,  0.4312,  0.0638,  ...,  0.7363, -0.4443, -0.2515],\n",
       "          [ 0.0859,  0.5435,  0.0720,  ...,  0.5371, -0.3008, -0.1057],\n",
       "          [ 0.1488,  0.4507,  0.1183,  ...,  0.1877, -0.5176, -0.1919],\n",
       "          [-0.0530,  0.1832, -0.2478,  ...,  1.2861,  0.1649, -0.3311],\n",
       "          [ 0.1497,  0.0257,  0.3904,  ...,  0.4202, -0.2057, -0.0363]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1925, -0.1478,  0.1442,  ..., -0.1166, -0.0324, -0.1562],\n",
       "          [-0.1337,  0.2371,  0.1638,  ...,  0.5933, -0.3333, -0.1976]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9707e-01, 1.0138e-03, 1.5955e-03, 5.9605e-07, 4.4298e-04],\n",
       "          [9.9756e-01, 4.5037e-04, 1.5965e-03, 1.1921e-07, 1.5330e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  427.939789  275.449280  554.930115  477.182251    0.937279      0   \n",
       "  1  255.942017  312.397827  436.975525  478.649353    0.919487      0   \n",
       "  2  141.949615  162.625671  322.342651  421.675537    0.857524      0   \n",
       "  3  253.486511  309.936340  294.035950  349.864624    0.738466     35   \n",
       "  \n",
       "               name  \n",
       "  0          person  \n",
       "  1          person  \n",
       "  2          person  \n",
       "  3  baseball glove  ,\n",
       "  'caption': ['An umpire watching a play.', 'An umpire at a baseball game'],\n",
       "  'bbox_target': [428.22, 275.42, 125.7, 204.58]},\n",
       " 636: {'image_emb': tensor([[-7.9895e-02,  4.4482e-01,  1.7017e-01,  ...,  9.9658e-01,\n",
       "            1.5088e-01, -3.9429e-01],\n",
       "          [ 2.5098e-01,  4.9829e-01, -2.4243e-01,  ...,  1.3525e+00,\n",
       "           -7.7881e-02, -1.7395e-01],\n",
       "          [-3.5498e-01,  4.4971e-01, -2.4796e-02,  ...,  6.5771e-01,\n",
       "            2.0325e-01, -7.3730e-02],\n",
       "          ...,\n",
       "          [-4.6899e-01,  2.2229e-01,  1.5857e-01,  ...,  9.2871e-01,\n",
       "            1.5833e-01, -1.3904e-01],\n",
       "          [-7.2746e-03,  5.0830e-01, -2.4500e-01,  ...,  1.0781e+00,\n",
       "            9.1614e-02, -6.5674e-02],\n",
       "          [ 2.9404e-02,  2.1704e-01,  2.9419e-01,  ...,  5.8594e-01,\n",
       "           -1.2674e-03, -1.4941e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1619,  0.0645, -0.1848,  ...,  0.3469,  0.3298, -0.1476],\n",
       "          [-0.0349,  0.2690, -0.2162,  ...,  0.1614,  0.1112, -0.1969]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0482, 0.0093, 0.7188, 0.0746, 0.1137, 0.0149, 0.0016, 0.0191],\n",
       "          [0.0470, 0.0285, 0.5298, 0.2141, 0.1667, 0.0040, 0.0032, 0.0066]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  312.339294    0.672943  552.063232  387.646881    0.931113      0   \n",
       "  1  132.407928    0.000000  262.637299  247.174652    0.923997      0   \n",
       "  2  119.321106  220.609619  322.402588  291.811157    0.901924     36   \n",
       "  3  249.182724    0.195442  344.173248  212.439240    0.871257      0   \n",
       "  4  293.768585    0.321564  428.379364  361.144562    0.869932     36   \n",
       "  5  248.871719  181.587067  345.844177  238.389923    0.857843     36   \n",
       "  6  561.226685  155.980164  639.955811  315.726868    0.809695     24   \n",
       "  7  616.932739   36.138489  639.869141  124.126190    0.370246      0   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2  skateboard  \n",
       "  3      person  \n",
       "  4  skateboard  \n",
       "  5  skateboard  \n",
       "  6    backpack  \n",
       "  7      person  ,\n",
       "  'caption': ['A kid wearing black pants with one foot on a skateboard.',\n",
       "   'a man with black pants standing on skateboard'],\n",
       "  'bbox_target': [130.5, 2.88, 135.3, 244.68]},\n",
       " 637: {'image_emb': tensor([[-0.1375, -0.2047,  0.1201,  ...,  0.5361,  0.1394, -0.3564],\n",
       "          [-0.3110, -0.2354,  0.1110,  ...,  0.5371,  0.1232, -0.0833],\n",
       "          [ 0.1514, -0.0021,  0.0345,  ...,  0.6357,  0.0965, -0.2627],\n",
       "          [-0.2440, -0.1887,  0.0426,  ...,  0.1136,  0.0138, -0.2467]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1997, -0.5181, -0.2620,  ...,  0.2671, -0.3098, -0.1021],\n",
       "          [-0.3088, -0.4927, -0.2942,  ...,  0.1644, -0.1501, -0.2947]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0337, 0.2527, 0.0887, 0.6250],\n",
       "          [0.0405, 0.8662, 0.0197, 0.0734]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0    0.000000   34.699905  260.793365  478.670654    0.943636     23  giraffe\n",
       "  1  155.067795  230.595184  408.028076  499.487152    0.915327     23  giraffe\n",
       "  2  146.026260   17.062637  380.437012  264.967712    0.907271     23  giraffe,\n",
       "  'caption': ['A giraffe bending down near flowers behind two other giraffes',\n",
       "   'The smallest giraffe, looking down at some flowers.'],\n",
       "  'bbox_target': [153.89, 232.75, 256.0, 271.82]},\n",
       " 638: {'image_emb': tensor([[-0.3884, -0.0228, -0.1329,  ...,  0.9868,  0.1328, -0.4478],\n",
       "          [-0.4133,  0.1729, -0.0648,  ...,  0.9321,  0.0786, -0.5181],\n",
       "          [-0.0372,  0.5103,  0.0534,  ...,  1.2148, -0.3274, -0.1558],\n",
       "          ...,\n",
       "          [ 0.2432,  0.4504, -0.1844,  ...,  0.9575,  0.0523, -0.0065],\n",
       "          [-0.0778,  0.3589,  0.0518,  ...,  1.1934, -0.1565, -0.1240],\n",
       "          [-0.0369,  0.2229, -0.4844,  ...,  0.7661,  0.1538, -0.1077]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3352,  0.3804,  0.0473,  ...,  0.1152, -0.3916, -0.3450],\n",
       "          [-0.3352,  0.3804,  0.0473,  ...,  0.1152, -0.3916, -0.3450]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0054, 0.0007, 0.2793, 0.0509, 0.0313, 0.3818, 0.0115, 0.2389],\n",
       "          [0.0054, 0.0007, 0.2793, 0.0509, 0.0313, 0.3818, 0.0115, 0.2389]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0   173.689941    0.910763  297.559570  114.184753    0.866085     55   cake\n",
       "  1    48.599121    1.214748  176.974121  106.882996    0.850990     55   cake\n",
       "  2   220.778717  242.080902  357.186432  319.715485    0.850399     55   cake\n",
       "  3   292.224609    0.365765  422.859375  108.476700    0.836130     55   cake\n",
       "  4   304.026886  220.428467  415.961548  295.747864    0.789150     55   cake\n",
       "  5   302.293304  321.399048  425.995178  419.626343    0.765411     55   cake\n",
       "  6   283.072388  417.837067  426.559082  634.631470    0.713428     55   cake\n",
       "  7   160.537354  296.086975  313.503296  353.247253    0.674853     55   cake\n",
       "  8   335.139862  290.475037  426.344788  345.775879    0.608577     55   cake\n",
       "  9   133.759064  245.029968  234.335236  299.168945    0.556629     55   cake\n",
       "  10  166.658600  349.368805  321.951752  425.233368    0.441868     55   cake\n",
       "  11   24.402328  309.985626  130.256454  411.477325    0.430226     55   cake\n",
       "  12    0.000000  374.823822   81.684418  509.630035    0.405608     55   cake\n",
       "  13   29.450684  278.664520  120.800507  327.345428    0.377233     55   cake\n",
       "  14  112.413727  268.834290  224.367218  640.000000    0.288549     43  knife,\n",
       "  'caption': ['A brown color table with bowls',\n",
       "   'A brown color table with bowls'],\n",
       "  'bbox_target': [0.0, 130.74, 427.0, 97.44]},\n",
       " 639: {'image_emb': tensor([[ 0.1071,  0.0807, -0.1475,  ...,  1.1660, -0.0347,  0.3313],\n",
       "          [ 0.1199,  0.1864,  0.0759,  ...,  1.0156, -0.0348,  0.1160],\n",
       "          [-0.0630,  0.1725, -0.2098,  ...,  1.5068,  0.3547,  0.0125],\n",
       "          ...,\n",
       "          [ 0.2333,  0.3381, -0.2493,  ...,  1.3145,  0.5215,  0.0731],\n",
       "          [ 0.1151, -0.1794, -0.3611,  ...,  0.8013,  0.0553, -0.1104],\n",
       "          [-0.1005,  0.2319, -0.1381,  ...,  1.1611, -0.0391,  0.4133]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1526,  0.0158,  0.1831,  ..., -0.2118,  0.0884,  0.4302],\n",
       "          [-0.2664, -0.0149, -0.3521,  ...,  0.2012,  0.1300,  0.2517]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.5820e-03, 1.6571e-02, 9.7803e-01, 5.0831e-04, 3.7968e-05, 8.1205e-04,\n",
       "           2.1279e-05, 3.4380e-04, 3.5703e-05, 1.3053e-04],\n",
       "          [2.7142e-03, 2.9932e-01, 6.6406e-01, 1.1246e-02, 2.0933e-04, 1.2161e-02,\n",
       "           3.7885e-04, 6.4087e-03, 1.9658e-04, 3.2730e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     1.226089  156.880768  359.296204  475.779297    0.944972      0   \n",
       "  1   341.216461  171.784378  442.430634  335.222961    0.920288      0   \n",
       "  2   431.208252  172.645462  582.438538  476.258545    0.869514      0   \n",
       "  3   511.557587  288.898926  638.965698  476.852783    0.867807     56   \n",
       "  4   461.545624  342.071503  495.334564  398.932159    0.861858     41   \n",
       "  5   429.976044  266.197418  455.847198  336.671906    0.820761     56   \n",
       "  6   440.439270  295.091217  467.013733  390.245941    0.818117     39   \n",
       "  7   216.223389  286.326233  572.592346  462.482422    0.786595     60   \n",
       "  8   265.324341  283.039978  288.232910  338.625488    0.775652     41   \n",
       "  9   174.714371  177.690369  243.027878  293.950317    0.698416      0   \n",
       "  10  398.248077  335.713715  423.722992  409.651215    0.692288     41   \n",
       "  11  419.706757  334.641541  446.090973  393.252136    0.665463     41   \n",
       "  12  288.976685  263.745270  310.681824  305.049957    0.486797     41   \n",
       "  13  333.383698  303.387939  382.139496  324.830566    0.423055     45   \n",
       "  14    0.000000  274.601532   67.744232  477.947571    0.319522     56   \n",
       "  15  297.784058  361.256836  348.560730  389.391357    0.288008     45   \n",
       "  16  362.869904  301.107819  390.426300  316.743622    0.286914     44   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3          chair  \n",
       "  4            cup  \n",
       "  5          chair  \n",
       "  6         bottle  \n",
       "  7   dining table  \n",
       "  8            cup  \n",
       "  9         person  \n",
       "  10           cup  \n",
       "  11           cup  \n",
       "  12           cup  \n",
       "  13          bowl  \n",
       "  14         chair  \n",
       "  15          bowl  \n",
       "  16         spoon  ,\n",
       "  'caption': ['a man in purple',\n",
       "   'the man with the striped polo shirt and gray hair'],\n",
       "  'bbox_target': [410.5, 172.86, 173.25, 307.14]},\n",
       " 640: {'image_emb': tensor([[ 0.0649,  0.3977, -0.1599,  ...,  0.7949,  0.4517, -0.0231],\n",
       "          [ 0.2891,  0.0116, -0.3899,  ...,  1.0576,  0.1346, -0.0747],\n",
       "          [-0.1444,  0.3545, -0.0871,  ...,  1.0479, -0.0440, -0.0462],\n",
       "          [ 0.2242,  0.0977,  0.0436,  ...,  0.8477,  0.3125,  0.0213]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2411, -0.0537, -0.3176,  ...,  0.0838, -0.4597,  0.1462],\n",
       "          [ 0.2788, -0.2173, -0.3726,  ...,  0.3899, -0.1478, -0.1254]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.8895e-05, 9.9902e-01, 7.4387e-04, 3.5346e-05],\n",
       "          [8.4817e-05, 9.9951e-01, 9.5367e-07, 2.3413e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  233.762512   51.395386  583.026550  421.361450    0.949476      0   \n",
       "  1   15.332870  310.985657  228.638214  423.935974    0.845297      0   \n",
       "  2   80.380859  262.695862  260.965332  353.821960    0.826137     16   \n",
       "  3  462.492432  342.508514  615.774170  425.607239    0.660290     56   \n",
       "  4  223.680771  239.767365  280.694702  278.821625    0.654879     65   \n",
       "  5    8.471375  305.103394  308.513855  422.384460    0.572991     57   \n",
       "  6    0.000000  253.764221   74.080353  344.466675    0.565662     77   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2         dog  \n",
       "  3       chair  \n",
       "  4      remote  \n",
       "  5       couch  \n",
       "  6  teddy bear  ,\n",
       "  'caption': ['A woman sitting on a blue couch.',\n",
       "   'Girl in black tank top sitting'],\n",
       "  'bbox_target': [13.89, 310.69, 212.53, 115.31]},\n",
       " 641: {'image_emb': tensor([[ 0.0359,  0.0621, -0.1525,  ...,  0.8315, -0.0526, -0.1758],\n",
       "          [ 0.1964,  0.0079, -0.2249,  ...,  0.5889, -0.0887, -0.0150],\n",
       "          [ 0.2404,  0.1305, -0.1558,  ...,  0.8350, -0.0861, -0.0516],\n",
       "          ...,\n",
       "          [ 0.2612, -0.3977, -0.4487,  ...,  0.8945,  0.1184, -0.3284],\n",
       "          [ 0.2257, -0.0855, -0.5015,  ...,  0.8696, -0.0297, -0.0147],\n",
       "          [ 0.0310,  0.0832, -0.7212,  ..., -0.1356,  0.1619,  0.0426]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-1.0815e-01, -1.4893e-01, -1.7847e-01,  2.3608e-01, -3.2074e-02,\n",
       "           -7.0129e-02, -2.1826e-01, -8.8721e-01,  1.5479e-01,  1.1865e-01,\n",
       "            8.7097e-02, -1.7395e-01,  7.3669e-02, -6.1066e-02,  4.1138e-01,\n",
       "           -1.4697e-01,  6.8359e-02, -1.2274e-01, -2.3438e-01, -7.4043e-03,\n",
       "            2.4023e-01,  6.0010e-01,  6.8176e-02, -2.0520e-01, -3.2275e-01,\n",
       "           -2.1741e-01, -1.9226e-01,  2.2083e-01, -1.6541e-01, -4.8859e-02,\n",
       "           -3.2886e-01,  1.9397e-01,  2.7771e-02, -8.0566e-02, -4.8511e-01,\n",
       "            1.2244e-01, -2.1652e-02,  3.1860e-01,  1.7405e-03,  2.4341e-01,\n",
       "           -2.9419e-01,  4.6387e-02,  1.1615e-01,  2.5415e-01,  2.2754e-01,\n",
       "           -1.7432e-01,  1.3965e-01, -1.1621e-01, -2.3853e-01, -3.4790e-01,\n",
       "           -2.2339e-01, -4.9896e-02,  1.4148e-01, -2.9443e-01, -6.3477e-01,\n",
       "            4.8599e-03,  1.6298e-03,  9.5459e-02,  1.9336e-01,  1.5198e-01,\n",
       "           -1.7981e-01,  1.1053e-01,  8.2214e-02,  1.8835e-01,  1.9604e-01,\n",
       "           -3.8623e-01, -1.3220e-01,  8.8867e-02,  2.1021e-01, -8.3252e-02,\n",
       "           -1.6541e-01, -2.8711e-01,  2.6538e-01,  6.7322e-02,  4.4604e-01,\n",
       "           -2.8662e-01, -5.2216e-02,  1.3977e-02,  1.7529e-01, -2.3816e-01,\n",
       "           -2.7246e-01, -3.5522e-01,  9.4238e-02,  5.5615e-01, -5.6152e-02,\n",
       "           -4.3121e-02,  1.8860e-01, -7.9712e-02,  9.8343e-03,  1.0773e-01,\n",
       "           -3.0811e-01, -1.3037e-01, -1.4229e+00,  7.3926e-01, -2.5586e-01,\n",
       "            1.7346e-01,  1.4612e-01,  3.1567e-01,  2.1655e-01, -5.0098e-01,\n",
       "           -6.1676e-02, -3.1714e-01,  2.5171e-01, -3.7134e-01, -4.3945e-01,\n",
       "           -1.5369e-01, -2.1814e-01,  3.3838e-01,  1.2158e-01,  1.0718e-01,\n",
       "           -2.0349e-01, -2.3608e-01, -7.9041e-02,  2.5940e-02,  3.9215e-02,\n",
       "           -2.5830e-01,  1.7700e-02,  1.0602e-01, -2.7002e-01, -7.2754e-02,\n",
       "           -1.9263e-01, -3.5913e-01, -1.5747e-02, -1.1639e-01,  3.2202e-01,\n",
       "            1.9250e-01, -3.3997e-02,  1.6162e-01, -1.5137e-01,  3.7695e-01,\n",
       "           -6.4148e-02, -5.7373e-02, -2.7222e-01,  6.1523e+00,  1.4502e-01,\n",
       "           -2.8784e-01, -1.6394e-01, -3.6206e-01, -4.6973e-01,  1.2482e-01,\n",
       "            1.1938e-01,  1.3171e-01, -3.0737e-01, -2.2083e-01, -1.8970e-01,\n",
       "            1.1951e-01, -1.3879e-01, -3.5962e-01, -1.1761e-01,  1.2292e-01,\n",
       "           -2.5439e-01, -4.6997e-02,  3.3472e-01,  4.2578e-01,  2.2620e-01,\n",
       "           -8.7036e-02,  6.5063e-02, -2.4805e-01,  2.6465e-01, -5.9540e-02,\n",
       "            1.1316e-01, -2.5650e-02, -7.3669e-02, -4.6692e-02, -7.7881e-02,\n",
       "           -1.1505e-01,  3.9355e-01, -1.5283e-01, -1.2457e-01, -3.8452e-01,\n",
       "            2.7759e-01, -3.0176e-01,  1.1034e-03,  3.7061e-01, -4.5410e-01,\n",
       "           -1.3855e-01, -2.4817e-01, -9.6619e-02,  6.2073e-02,  2.7588e-01,\n",
       "           -9.3323e-02,  5.7922e-02, -3.3936e-01,  6.2943e-03,  1.4282e-01,\n",
       "           -3.1445e-01, -4.5166e-03, -8.3740e-02, -1.8359e-01, -3.1250e-01,\n",
       "            1.0724e-01,  2.9395e-01, -1.4807e-01, -2.6660e-01,  9.5886e-02,\n",
       "           -1.0431e-01,  9.7900e-02,  5.3906e-01,  1.3232e-01, -1.1078e-01,\n",
       "            7.3120e-02, -1.7529e-01,  4.0479e-01, -3.3423e-01,  7.0496e-02,\n",
       "           -3.2275e-01,  2.7267e-02, -9.6436e-02, -2.6099e-01,  5.6000e-02,\n",
       "            1.8457e-01, -1.7078e-01,  1.6052e-01,  1.0400e-01,  1.2323e-01,\n",
       "           -3.4637e-02, -1.2402e-01, -4.4458e-01,  3.4912e-02, -7.2144e-02,\n",
       "            4.3915e-02, -8.8989e-02,  1.1154e-02, -1.7914e-02, -1.6333e-01,\n",
       "           -1.4893e-01,  3.2617e-01, -2.5806e-01, -4.7070e-01, -3.9948e-02,\n",
       "           -3.8550e-01,  3.1104e-01,  3.0566e-01, -6.7810e-02,  2.3364e-01,\n",
       "            4.0527e-01, -5.8079e-04,  8.1299e-02,  7.0007e-02,  8.7158e-02,\n",
       "            6.4026e-02,  2.7588e-01,  2.1704e-01,  2.0218e-02, -1.6418e-01,\n",
       "            2.6294e-01, -4.7089e-02,  7.0618e-02,  1.1749e-01,  3.4521e-01,\n",
       "           -4.6875e-01,  1.7053e-01,  1.8469e-01, -3.6328e-01, -3.7549e-01,\n",
       "            5.8563e-02, -1.6357e-01,  4.7302e-02,  4.3140e-01,  2.0288e-01,\n",
       "           -2.7191e-02, -7.2205e-02,  1.0760e-01,  1.7676e-01,  5.3223e-02,\n",
       "            9.8816e-02,  2.6367e-01, -3.1934e-01,  3.7598e-02,  5.4834e-01,\n",
       "           -1.8115e-01,  2.8442e-01,  4.5624e-02, -2.5977e-01, -2.7856e-01,\n",
       "            1.5955e-01,  7.7881e-02, -3.6224e-02,  2.0276e-01,  3.3844e-02,\n",
       "            2.9495e-02, -1.2793e-01,  1.9934e-01,  1.1981e-01, -1.7371e-01,\n",
       "            3.3966e-02,  2.8101e-01,  2.5977e-01,  2.6758e-01, -1.2415e-01,\n",
       "           -1.4124e-01,  8.6060e-02,  3.4326e-01, -9.8816e-02, -2.2827e-01,\n",
       "           -1.6235e-01, -5.3809e-01, -1.8103e-01,  4.3762e-02,  2.9419e-01,\n",
       "           -1.4868e-01,  3.0228e-02,  9.0149e-02, -3.0640e-01,  2.4634e-01,\n",
       "           -2.2742e-01, -1.7700e-01, -1.1517e-01,  3.0228e-02,  2.6343e-01,\n",
       "           -3.8354e-01,  4.3237e-01,  6.1445e+00, -1.2189e-01, -2.6831e-01,\n",
       "           -8.2458e-02,  2.0776e-01, -1.1665e-02,  3.2593e-01,  5.5713e-01,\n",
       "           -8.3130e-02,  9.9121e-02,  3.1274e-01, -2.9492e-01, -6.0638e-02,\n",
       "            8.3557e-02, -2.0276e-01, -5.7812e-01,  7.5562e-02, -2.4746e+00,\n",
       "            3.6621e-01, -1.9067e-01,  4.6082e-02,  4.5776e-02, -1.7737e-01,\n",
       "           -3.1396e-01, -2.2498e-01,  1.7419e-01,  3.4973e-02,  1.6516e-01,\n",
       "           -2.7759e-01, -5.1636e-02,  2.4805e-01, -7.0166e-01, -2.4341e-01,\n",
       "           -3.8696e-02,  2.6050e-01,  2.5195e-01, -1.8750e-01, -9.1370e-02,\n",
       "            1.5125e-01,  1.5149e-01,  2.0227e-01,  5.7251e-02, -9.1919e-02,\n",
       "            9.4299e-02, -1.8091e-01,  7.4036e-02, -1.0046e-01,  4.3335e-02,\n",
       "            8.7280e-02, -3.2666e-01, -9.8694e-02,  7.4158e-02, -2.7026e-01,\n",
       "           -5.3833e-02, -2.7612e-01,  1.2830e-01, -9.8511e-02, -5.6244e-02,\n",
       "            3.0615e-01,  1.0461e-01,  1.3721e-01,  1.7163e-01, -1.4758e-01,\n",
       "            2.9068e-02, -3.6475e-01, -1.0828e-01,  1.4331e-01,  1.7542e-01,\n",
       "           -7.6782e-02,  2.9907e-01,  1.1353e-01, -2.6978e-01, -1.6357e-01,\n",
       "            3.0563e-02, -2.1057e-01,  1.1707e-01, -3.2251e-01, -2.6172e-01,\n",
       "           -6.9678e-01,  3.1519e-01, -5.0125e-03, -1.8829e-02, -9.7122e-03,\n",
       "            4.5874e-01, -9.3689e-02,  1.1401e-01, -5.5969e-02,  8.0420e-01,\n",
       "            2.7222e-01, -2.3840e-01, -1.3965e-01,  1.7249e-01,  4.0039e-01,\n",
       "            6.7322e-02, -1.5808e-01, -2.2034e-01,  1.5002e-01, -5.4443e-02,\n",
       "            8.6121e-02, -2.8345e-01, -2.5854e-01,  2.7832e-01,  1.1734e-02,\n",
       "           -2.5171e-01, -3.0249e-01,  5.1880e-02,  2.5806e-01,  9.6359e-03,\n",
       "           -3.9337e-02, -7.1838e-02, -9.2163e-02, -4.2871e-01,  1.7896e-01,\n",
       "           -2.1729e-01, -2.8003e-01, -5.6610e-02,  2.6099e-01,  9.0454e-02,\n",
       "           -3.7817e-01, -1.0095e-01, -3.9380e-01, -1.3110e-01,  1.8164e-01,\n",
       "           -4.9255e-02,  2.4146e-01,  1.5271e-01, -1.4734e-01, -5.0659e-02,\n",
       "           -6.1279e-01, -3.8135e-01, -2.9565e-01, -1.2476e-01,  9.6375e-02,\n",
       "            2.0972e-01, -1.1505e-01, -2.3303e-01, -1.4941e-01, -8.7097e-02,\n",
       "            3.2291e-03, -1.4172e-01, -1.7139e-01, -1.6211e-01,  8.8013e-02,\n",
       "           -3.0322e-01,  4.9706e-03, -1.5198e-01, -2.6074e-01, -3.0127e-01,\n",
       "           -1.2341e-01,  3.0975e-02, -3.2568e-01,  2.3816e-01,  2.2412e-01,\n",
       "            6.0852e-02,  1.4819e-01,  5.8563e-02, -2.9395e-01, -2.3206e-01,\n",
       "            3.5156e-01, -1.8457e-01,  5.2887e-02, -1.6800e-02, -1.1462e-01,\n",
       "            2.4951e-01,  5.6671e-02,  5.1178e-02,  3.7201e-02,  3.4863e-01,\n",
       "            9.9609e-02, -9.1699e-01,  4.3652e-01,  2.0935e-01,  9.9915e-02,\n",
       "           -7.9773e-02, -1.3452e-01, -2.8275e-02, -4.7485e-02, -2.3328e-01,\n",
       "            3.8525e-01,  1.3623e-01,  1.6553e-01,  8.9893e-01,  5.0000e-01,\n",
       "            4.0497e-02, -2.2961e-01,  2.2656e-01, -3.6545e-03,  2.6245e-01,\n",
       "           -3.2562e-02,  1.8958e-01, -2.5223e-02,  2.5635e-02,  2.2171e-02,\n",
       "           -2.4463e-01,  3.9282e-01, -1.5601e-01,  3.5034e-02, -2.9099e-02,\n",
       "            8.3191e-02, -1.4612e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0008, 0.0006, 0.0015, 0.0859, 0.0052, 0.0746, 0.0028, 0.2773, 0.0009,\n",
       "           0.1532, 0.0012, 0.0015, 0.0033, 0.3911]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   555.057068  282.740051  578.285217  349.391235    0.883650      0  person\n",
       "  1   473.867920  282.450989  492.141052  333.405701    0.866639      0  person\n",
       "  2    95.034706  316.206543  117.625008  393.579346    0.862364      0  person\n",
       "  3     3.101410  247.476562  392.734650  380.617126    0.855112      6   train\n",
       "  4   457.104645  280.713806  474.617584  329.838135    0.840904      0  person\n",
       "  5     0.231026  275.874023   82.940140  438.212830    0.835213      6   train\n",
       "  6   521.158691  291.055542  540.442993  354.196411    0.831133      0  person\n",
       "  7   210.143906  303.327759  304.743683  404.859253    0.823835      6   train\n",
       "  8   537.360657  285.096039  555.600037  347.518036    0.811446      0  person\n",
       "  9   465.031860  232.624908  639.472412  340.225922    0.781584      6   train\n",
       "  10  454.270844  248.113434  467.927399  290.104156    0.766252      0  person\n",
       "  11   70.584770  293.605743   82.996513  337.371063    0.762183      0  person\n",
       "  12  507.732361  287.492584  523.995911  348.546112    0.720479      0  person\n",
       "  13  433.368713  259.501617  444.623840  290.153473    0.669350      0  person\n",
       "  14  400.202087  294.224457  425.341858  328.774323    0.649592      0  person\n",
       "  15  385.934753  302.054138  413.306702  330.544067    0.644915     13   bench\n",
       "  16  111.794434  308.509003  134.482956  380.150116    0.597044      0  person,\n",
       "  'caption': ['train on the far right'],\n",
       "  'bbox_target': [464.9, 239.53, 174.74, 98.16]},\n",
       " 642: {'image_emb': tensor([[-0.7915,  0.4839, -0.3279,  ...,  0.5068,  0.2157,  0.0867],\n",
       "          [-0.0070,  0.7378, -0.3350,  ...,  0.6128, -0.0612,  0.0108],\n",
       "          [ 0.0009,  0.4172, -0.0548,  ...,  0.6631, -0.0740,  0.1731],\n",
       "          [-0.4526,  0.2344, -0.2473,  ...,  0.8101,  0.3640, -0.0230],\n",
       "          [-0.3250,  0.5186, -0.3857,  ...,  0.5845, -0.1459,  0.3972]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2690,  0.1940, -0.2473,  ...,  0.2095, -0.3513,  0.1580],\n",
       "          [ 0.3311,  0.1440, -0.0055,  ...,  0.5088,  0.2898,  0.0853]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.4041, 0.0246, 0.1022, 0.0321, 0.4370],\n",
       "          [0.0378, 0.0047, 0.7241, 0.0229, 0.2107]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   21.559572   65.891251  495.358215  291.372742    0.938973     73   \n",
       "  1  336.058685  171.134216  485.278076  372.456268    0.933777     67   \n",
       "  2   22.586649    0.354844  207.637527  139.712067    0.913296     73   \n",
       "  3  260.240784  178.533813  343.284546  291.112823    0.845336      0   \n",
       "  4  113.307739  205.338074  140.866348  244.535614    0.545309      0   \n",
       "  5   83.472153  167.767807  108.696838  204.921631    0.513257      0   \n",
       "  6  137.604706  217.402786  171.258316  252.616028    0.471893      0   \n",
       "  7  252.816101    0.858092  499.375336  319.124329    0.459273     73   \n",
       "  8  154.847122  286.169983  500.000000  373.789917    0.406619     73   \n",
       "  9   99.159576  182.397598  123.544357  219.693619    0.384781      0   \n",
       "  \n",
       "           name  \n",
       "  0        book  \n",
       "  1  cell phone  \n",
       "  2        book  \n",
       "  3      person  \n",
       "  4      person  \n",
       "  5      person  \n",
       "  6      person  \n",
       "  7        book  \n",
       "  8        book  \n",
       "  9      person  ,\n",
       "  'caption': ['Phone manual with green and pink pages opened.',\n",
       "   'tigo add book of page number 6 and 7'],\n",
       "  'bbox_target': [26.91, 61.38, 467.48, 235.42]},\n",
       " 643: {'image_emb': tensor([[-0.1815,  0.4312, -0.3347,  ...,  0.8013,  0.5449,  0.1840],\n",
       "          [-0.0656,  0.3972, -0.2192,  ...,  1.4463, -0.1093,  0.0701],\n",
       "          [ 0.1731, -0.3628, -0.3701,  ...,  1.1260, -0.0240, -0.2461],\n",
       "          [-0.1886,  0.5298, -0.1534,  ...,  0.8999,  0.2756,  0.3838]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1141, -0.0093, -0.3589,  ..., -0.2448, -0.0238, -0.0042],\n",
       "          [ 0.0163, -0.1295, -0.5742,  ...,  0.3486,  0.0491,  0.1022]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.1025e-01, 1.5088e-01, 9.3460e-03, 3.2959e-01],\n",
       "          [1.2268e-01, 1.2527e-02, 4.2439e-05, 8.6475e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   295.640472   92.756500  606.455566  476.632812    0.952132      0  person\n",
       "  1   308.087830  118.979431  446.573425  478.736450    0.874577      0  person\n",
       "  2   319.902466  212.488098  337.905029  227.886414    0.701626     65  remote\n",
       "  3   158.448654  359.601471  365.947083  477.368347    0.678229     56   chair\n",
       "  4   393.719269  349.393768  425.910919  369.411896    0.653909     65  remote\n",
       "  5     0.136892  276.292389   44.758202  291.792145    0.495722     73    book\n",
       "  6   309.106995  339.866455  324.195740  354.248535    0.445924     65  remote\n",
       "  7     0.000000  267.817108   42.420380  285.939789    0.363519     73    book\n",
       "  8     0.097843  262.404144   40.583908  274.616547    0.352964     73    book\n",
       "  9   581.733643  102.602875  639.772461  147.867310    0.352804     73    book\n",
       "  10   73.272156  392.078369  117.998581  415.029907    0.302702     65  remote\n",
       "  11    0.000000  300.981079   52.800438  318.437256    0.274506     73    book\n",
       "  12   73.369843  391.930176  118.414963  415.375732    0.267383     73    book\n",
       "  13  251.785965  454.122742  286.247559  479.017761    0.256306     65  remote,\n",
       "  'caption': ['the girl closest to camera',\n",
       "   'The woman in the light colored shirt playing Wii.'],\n",
       "  'bbox_target': [295.14, 96.76, 314.59, 374.05]},\n",
       " 644: {'image_emb': tensor([[-0.5259, -0.0701,  0.2378,  ...,  0.7090,  0.4453,  0.3030],\n",
       "          [ 0.0222,  0.3979, -0.2520,  ...,  1.4346, -0.0496,  0.0218],\n",
       "          [ 0.1500,  0.4009,  0.0168,  ...,  0.7915,  0.0074,  0.0990],\n",
       "          [ 0.5122, -0.5635, -0.4365,  ...,  0.9106,  0.0594, -0.0266],\n",
       "          [-0.0657,  0.4397,  0.1733,  ...,  0.4768,  0.0709,  0.0018]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0293,  0.0617, -0.0895,  ..., -0.0264, -0.4148, -0.1027],\n",
       "          [-0.1847,  0.2913, -0.4077,  ..., -0.1030, -0.3594, -0.1510]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.7217e-01, 1.4472e-04, 1.0803e-02, 1.9670e-06, 1.6724e-02],\n",
       "          [2.1744e-02, 3.1319e-03, 9.6875e-01, 3.5882e-05, 6.4240e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  336.217529   56.026932  518.819702  310.004395    0.947997      0   \n",
       "  1   41.276260  233.527161  298.175293  332.861389    0.920122     63   \n",
       "  2  102.478226   38.393608  345.346558  290.070190    0.864934      0   \n",
       "  3  471.625732  277.316376  502.190125  293.547638    0.777354     67   \n",
       "  4  292.356659  318.402313  342.190338  387.646088    0.629804     41   \n",
       "  5   78.568153  277.540039  332.033478  412.885864    0.542994     24   \n",
       "  6   79.015541  277.233734  334.686340  411.369232    0.524787     26   \n",
       "  7  283.759888  277.508881  316.031860  307.015717    0.389423     67   \n",
       "  8   69.836937  257.673950  323.315887  381.051453    0.288838     63   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      laptop  \n",
       "  2      person  \n",
       "  3  cell phone  \n",
       "  4         cup  \n",
       "  5    backpack  \n",
       "  6     handbag  \n",
       "  7  cell phone  \n",
       "  8      laptop  ,\n",
       "  'caption': ['a man laying in the grass playing with his cellphone',\n",
       "   'A MAN OPERATING MOBILE PHONES'],\n",
       "  'bbox_target': [331.27, 55.05, 187.93, 255.13]},\n",
       " 645: {'image_emb': tensor([[-0.0436, -0.2094, -0.0455,  ...,  0.0036, -0.2319,  0.0251],\n",
       "          [-0.1112, -0.2649, -0.1436,  ...,  0.2505, -0.2173, -0.1016],\n",
       "          [-0.2140, -0.4006, -0.0548,  ...,  0.1582, -0.3359,  0.1544]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1497, -0.3611, -0.4995,  ..., -0.1148, -0.1219, -0.0892],\n",
       "          [ 0.3291, -0.4080, -0.4678,  ..., -0.1488,  0.1195, -0.2861]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5630, 0.3259, 0.1109],\n",
       "          [0.2318, 0.6011, 0.1670]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  187.861450  168.261902  366.249939  377.127197    0.921268     23  giraffe\n",
       "  1  314.034821  174.970215  481.364288  379.140381    0.916083     23  giraffe,\n",
       "  'caption': ['a giraffe behind another giraffe',\n",
       "   'A giraffe following another giraffe.'],\n",
       "  'bbox_target': [316.04, 172.58, 169.35, 201.71]},\n",
       " 646: {'image_emb': tensor([[-0.1138,  0.3647, -0.1466,  ...,  0.4766, -0.0228, -0.1044],\n",
       "          [-0.2365, -0.0749, -0.1764,  ...,  0.7871, -0.1384,  0.2510],\n",
       "          [-0.0475,  0.2227,  0.1016,  ...,  0.9033,  0.2136, -0.2842],\n",
       "          [ 0.1696, -0.1754, -0.4072,  ...,  1.0303, -0.1177, -0.2037],\n",
       "          [-0.0769,  0.0125, -0.0795,  ...,  0.3416, -0.0213,  0.0688]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2261, -0.1351,  0.0919,  ..., -0.0408, -0.1032, -0.0881],\n",
       "          [-0.3452, -0.2915,  0.1687,  ..., -0.5269, -0.6436, -0.1681]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.2871e-01, 6.6223e-02, 2.8648e-03, 1.7376e-03, 2.9731e-04],\n",
       "          [8.3256e-04, 8.8477e-01, 6.3181e-05, 6.5207e-05, 1.1426e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  353.881744   34.349594  461.643463  419.484009    0.941382      0  person\n",
       "  1  145.465164  224.987915  373.168274  478.077637    0.939226      0  person\n",
       "  2   59.987274  117.673065   94.831177  214.501495    0.860242     39  bottle\n",
       "  3  425.773071  192.013458  443.109009  202.130951    0.713521     65  remote,\n",
       "  'caption': ['Man who is mad he lost his game.',\n",
       "   'a man sitting on the floor'],\n",
       "  'bbox_target': [137.48, 227.72, 233.78, 252.09]},\n",
       " 647: {'image_emb': tensor([[ 8.8692e-04,  4.4238e-01, -1.6333e-01,  ...,  1.3164e+00,\n",
       "            9.1675e-02,  4.3182e-02],\n",
       "          [ 1.2146e-01,  2.5513e-01, -5.1758e-01,  ...,  1.0449e+00,\n",
       "            1.7725e-01,  8.2947e-02],\n",
       "          [ 1.2622e-01,  5.1855e-01, -3.5278e-01,  ...,  7.7930e-01,\n",
       "           -1.6211e-01,  2.1338e-01],\n",
       "          ...,\n",
       "          [ 1.5674e-01, -1.0712e-01, -1.5369e-01,  ...,  6.6211e-01,\n",
       "            1.0046e-01, -5.9296e-02],\n",
       "          [ 8.5144e-02, -1.2537e-01, -1.5332e-01,  ...,  8.3252e-01,\n",
       "           -4.1199e-02, -7.0435e-02],\n",
       "          [-1.6064e-01,  2.0691e-01,  3.1470e-01,  ...,  5.2734e-01,\n",
       "           -3.7079e-02,  4.1138e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0619,  0.3440,  0.0934,  ...,  0.2250, -0.1260, -0.3806],\n",
       "          [ 0.0104,  0.1371, -0.1031,  ...,  0.0831, -0.0200, -0.3442]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.8426e-05, 6.7920e-01, 3.2080e-01, 2.3842e-07, 1.0669e-05, 0.0000e+00,\n",
       "           0.0000e+00, 1.7881e-07],\n",
       "          [1.0169e-04, 4.9194e-01, 5.0781e-01, 2.8014e-06, 1.3351e-05, 1.6093e-06,\n",
       "           5.3644e-07, 2.4021e-04]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   230.176636  141.159760  363.702698  423.418945    0.945562      0   \n",
       "  1   383.382782  152.084351  490.585663  422.871399    0.935642      0   \n",
       "  2   472.613190  105.575363  581.157959  423.302307    0.925521      0   \n",
       "  3     1.157764  151.748413   92.294342  423.553406    0.811681     72   \n",
       "  4   572.711182  211.352020  639.968750  293.532379    0.800894     68   \n",
       "  5   131.377502  316.613159  164.221741  343.026489    0.775783     41   \n",
       "  6   119.337120  327.884064  147.570831  355.230804    0.704126     41   \n",
       "  7    89.135399  357.829559  151.784515  382.366547    0.390826     44   \n",
       "  8   159.156830  236.414703  236.896881  263.974762    0.366349     71   \n",
       "  9    90.615417  270.500580  151.150864  302.622833    0.307268     45   \n",
       "  10  205.884308  263.446442  228.833740  290.793671    0.303931     41   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3   refrigerator  \n",
       "  4      microwave  \n",
       "  5            cup  \n",
       "  6            cup  \n",
       "  7          spoon  \n",
       "  8           sink  \n",
       "  9           bowl  \n",
       "  10           cup  ,\n",
       "  'caption': ['A woman wearing an orange hijab.', 'woman in orange garb'],\n",
       "  'bbox_target': [382.65, 149.86, 108.8, 276.65]},\n",
       " 648: {'image_emb': tensor([[ 0.0120, -0.1218, -0.0573,  ...,  0.4976,  0.0288, -0.2170],\n",
       "          [-0.0615,  0.1638,  0.0907,  ...,  0.3755,  0.0447, -0.2593],\n",
       "          [-0.1044, -0.2139, -0.1229,  ...,  0.9194,  0.2362, -0.0109],\n",
       "          [-0.1960, -0.3396,  0.0531,  ...,  0.3345,  0.1057,  0.2039]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0288, -0.0582, -0.3645,  ..., -0.0222, -0.1914, -0.0984],\n",
       "          [ 0.2842, -0.2172, -0.3191,  ..., -0.1283, -0.0958, -0.1807]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0634, 0.0789, 0.0853, 0.7725],\n",
       "          [0.6030, 0.0200, 0.0110, 0.3657]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  135.610886  161.624817  257.476715  354.273621    0.914194     23  giraffe\n",
       "  1  409.974121  184.968109  638.779419  377.366364    0.754382     23  giraffe\n",
       "  2  567.912048  128.002945  639.857971  203.971100    0.705183     23  giraffe\n",
       "  3  411.846924  206.794708  558.974670  315.001801    0.266431     23  giraffe,\n",
       "  'caption': ['The giraffe standing behind the fallen log, against the wall, on the left',\n",
       "   'a giraffe in front of a wall'],\n",
       "  'bbox_target': [131.69, 161.21, 123.7, 200.0]},\n",
       " 649: {'image_emb': tensor([[-0.3855,  0.0883, -0.1175,  ...,  0.6191,  0.0939,  0.5303],\n",
       "          [ 0.4089,  0.4863, -0.2947,  ...,  0.6792,  0.3416,  0.1278],\n",
       "          [-0.0793,  0.1926,  0.2299,  ...,  0.7178,  0.3086, -0.0674],\n",
       "          [-0.0033,  0.1294, -0.0193,  ...,  1.0684,  0.4104,  0.1223],\n",
       "          [-0.1941,  0.3730,  0.0895,  ...,  0.8701,  0.1871,  0.4023],\n",
       "          [ 0.3503, -0.1299,  0.0504,  ...,  0.2080,  0.1953,  0.3674]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2952, -0.2607, -0.0193,  ..., -0.0307,  0.0201, -0.0636],\n",
       "          [-0.2385,  0.0083, -0.2383,  ..., -0.5352, -0.0724,  0.0868]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0000e+00, 7.7486e-07, 5.8413e-06, 3.8147e-06, 2.4319e-05, 8.6129e-05],\n",
       "          [6.7041e-01, 5.0392e-03, 5.4199e-02, 3.4637e-03, 8.1787e-03, 2.5854e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   332.988953   76.345169  621.642151  421.666504    0.936884      0   \n",
       "  1   175.341217   58.850464  329.840698  420.694946    0.919516      0   \n",
       "  2     0.316730  175.553345  124.269135  419.180603    0.892701     56   \n",
       "  3   260.885376  132.330292  358.777527  298.311310    0.762872     75   \n",
       "  4     0.379433  262.014679  102.149315  420.278442    0.761820      0   \n",
       "  5   423.483185   12.056259  451.016449   44.334854    0.516732     74   \n",
       "  6   332.650543  185.988708  638.137329  417.288879    0.466373     57   \n",
       "  7   581.933167  188.378113  640.000000  419.788086    0.431272     57   \n",
       "  8   260.691589  132.614792  358.599854  298.175598    0.409616     78   \n",
       "  9    68.124741  232.104462  179.225784  279.628021    0.379856     73   \n",
       "  10   65.043137  174.614899  189.991776  231.885162    0.338039     73   \n",
       "  \n",
       "            name  \n",
       "  0       person  \n",
       "  1       person  \n",
       "  2        chair  \n",
       "  3         vase  \n",
       "  4       person  \n",
       "  5        clock  \n",
       "  6        couch  \n",
       "  7        couch  \n",
       "  8   hair drier  \n",
       "  9         book  \n",
       "  10        book  ,\n",
       "  'caption': ['The man sitting down and wearing a reindeer sweater',\n",
       "   'The Dad laughing with his children.'],\n",
       "  'bbox_target': [332.58, 73.89, 295.16, 344.01]},\n",
       " 650: {'image_emb': tensor([[-0.1787,  0.0211, -0.1824,  ...,  1.2686,  0.1187,  0.2329],\n",
       "          [-0.1230,  0.3547, -0.0376,  ...,  0.4072, -0.1708, -0.0840],\n",
       "          [-0.1941,  0.4170, -0.0912,  ...,  0.9839, -0.3289, -0.2009],\n",
       "          [ 0.0049, -0.0598, -0.2302,  ...,  0.7319, -0.1990, -0.0280]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0007, -0.1603, -0.0931,  ...,  0.3423, -0.3496, -0.2222],\n",
       "          [ 0.2272, -0.0241, -0.2729,  ...,  0.3906, -0.4038, -0.1699]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.1764e-05, 9.2041e-02, 7.9492e-01, 1.1279e-01],\n",
       "          [8.4162e-05, 2.7124e-01, 7.2607e-01, 2.3842e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   466.169586  116.028275  529.704651  172.970215    0.895684     62   \n",
       "  1    55.030479  143.722244  252.012421  296.138428    0.855166     57   \n",
       "  2   403.118896  158.862213  543.221680  286.354828    0.778414     56   \n",
       "  3   313.735992  208.512756  632.963013  421.426880    0.586258     57   \n",
       "  4     0.600960  211.585297  106.749870  417.785767    0.575364     57   \n",
       "  5   316.112976  208.503174  632.332275  421.261780    0.551878     56   \n",
       "  6   163.729736  108.547287  179.629120  132.794937    0.395780     73   \n",
       "  7   157.300690   67.546417  172.282196   94.954437    0.359200     73   \n",
       "  8   172.939651   69.773468  182.103256   93.938812    0.341660     73   \n",
       "  9   156.803757   31.148407  172.556442   56.797882    0.331737     73   \n",
       "  10  164.269882   68.252655  179.980240   94.359283    0.316951     73   \n",
       "  11  173.572372   33.153915  182.941238   57.750565    0.314529     73   \n",
       "  12  163.608017   31.741043  176.550278   56.973251    0.305098     73   \n",
       "  13  147.340714   67.791733  157.305801   95.463852    0.296065     73   \n",
       "  14  149.457214  109.452393  160.956451  135.643555    0.291410     73   \n",
       "  15  264.491241  208.454834  356.205536  290.883911    0.274628     60   \n",
       "  16  277.835663   71.392181  303.093964   93.501221    0.268903     75   \n",
       "  \n",
       "              name  \n",
       "  0             tv  \n",
       "  1          couch  \n",
       "  2          chair  \n",
       "  3          couch  \n",
       "  4          couch  \n",
       "  5          chair  \n",
       "  6           book  \n",
       "  7           book  \n",
       "  8           book  \n",
       "  9           book  \n",
       "  10          book  \n",
       "  11          book  \n",
       "  12          book  \n",
       "  13          book  \n",
       "  14          book  \n",
       "  15  dining table  \n",
       "  16          vase  ,\n",
       "  'caption': ['A floral sofa farthest from the fire place.',\n",
       "   'Floral designed sofa in the Living room'],\n",
       "  'bbox_target': [316.98, 207.05, 315.15, 213.45]},\n",
       " 651: {'image_emb': tensor([[-0.4111,  0.1737, -0.2705,  ...,  0.6963,  0.7148, -0.4868],\n",
       "          [-0.6401,  0.4670,  0.0413,  ...,  1.0195,  0.1266, -0.3943],\n",
       "          [-0.4126,  0.2939, -0.3132,  ...,  0.5200,  0.5518, -0.4043]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1051,  0.3926, -0.3083,  ..., -0.1375, -0.0147, -0.4121],\n",
       "          [-0.1288,  0.1567, -0.1616,  ...,  0.2271, -0.0586, -0.3308]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0034, 0.1495, 0.8472],\n",
       "          [0.0198, 0.1121, 0.8682]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    0.419342   59.605865  345.959869  636.167969    0.903763      0  person\n",
       "  1  167.791656  284.935242  423.820068  639.112366    0.872602     17   horse,\n",
       "  'caption': ['A man in back of a horse wearing a grey suit and hat',\n",
       "   'a man looks his baby in horse ride'],\n",
       "  'bbox_target': [0.0, 62.7, 351.71, 567.93]},\n",
       " 652: {'image_emb': tensor([[-0.3281,  0.1146, -0.3125,  ...,  0.7915,  0.0672, -0.1210],\n",
       "          [-0.4163, -0.1040, -0.3364,  ...,  0.6929,  0.1754, -0.0185],\n",
       "          [-0.2935, -0.3171, -0.2915,  ...,  0.7354, -0.1105,  0.4355]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2808, -0.4146, -0.0262,  ...,  0.3108,  0.0672,  0.5605],\n",
       "          [-0.1892, -0.1893, -0.0956,  ...,  0.2379,  0.0061,  0.5923]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1859, 0.5640, 0.2502],\n",
       "          [0.3311, 0.4744, 0.1946]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0   86.706017  237.299011  267.252136  424.617188    0.945961     22  zebra\n",
       "  1  244.919754  147.042252  474.302124  269.595703    0.864227     22  zebra,\n",
       "  'caption': ['a zebra in a zoo nibbling on some wood', 'zebra by log first'],\n",
       "  'bbox_target': [278.01, 152.57, 195.24, 119.02]},\n",
       " 653: {'image_emb': tensor([[-0.2352,  0.4661, -0.2478,  ...,  1.2148, -0.0933,  0.0181],\n",
       "          [ 0.2856,  0.5640, -0.2524,  ...,  0.5884,  0.1865, -0.1636]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1157,  0.1451, -0.0598,  ..., -0.1478, -0.1311, -0.3601],\n",
       "          [-0.0616,  0.2200, -0.0460,  ..., -0.0531, -0.0528,  0.1082]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2568, 0.7432],\n",
       "          [0.9980, 0.0018]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  115.760117  270.727905  238.872330  413.164368    0.854231     50  broccoli\n",
       "  1  522.583679   69.471420  640.000000  230.891113    0.633879     50  broccoli\n",
       "  2  386.836334  159.290527  440.345428  242.701782    0.617156     50  broccoli\n",
       "  3    0.000000  126.443893  235.583710  249.531158    0.494694     50  broccoli\n",
       "  4    0.061852  235.850586   93.809334  353.704407    0.485851     50  broccoli\n",
       "  5    0.645058  186.771179   89.316505  253.096558    0.343003     50  broccoli,\n",
       "  'caption': ['The broccoli on the left center of picture.',\n",
       "   'The piece of broccoli that is in focus'],\n",
       "  'bbox_target': [63.88, 128.01, 172.28, 98.33]},\n",
       " 654: {'image_emb': tensor([[ 0.3193,  0.5405,  0.2705,  ...,  1.0596,  0.5820,  0.3242],\n",
       "          [ 0.3369,  0.2222, -0.2379,  ...,  0.8584,  0.3879,  0.0087],\n",
       "          [ 0.1909,  0.1445, -0.3535,  ...,  0.8784,  0.0112,  0.0039],\n",
       "          ...,\n",
       "          [ 0.2181,  0.3962, -0.2947,  ...,  1.0088,  0.3655,  0.1879],\n",
       "          [-0.1302,  0.3406, -0.3599,  ...,  1.0273, -0.0125, -0.0909],\n",
       "          [ 0.1190,  0.4688, -0.2019,  ...,  1.0420,  0.2260,  0.1934]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-2.2241e-01,  1.8750e-01, -1.0431e-01,  6.8787e-02,  1.0699e-01,\n",
       "            1.5027e-01,  1.7273e-01, -1.5283e+00,  3.4790e-02,  4.8096e-01,\n",
       "           -1.0361e-02, -2.5659e-01, -3.7689e-02, -1.9092e-01,  3.3936e-01,\n",
       "           -4.0088e-01, -1.4990e-01, -1.7371e-01, -8.6182e-02,  1.3596e-02,\n",
       "            6.4258e-01,  1.5884e-02, -1.4153e-02,  2.7808e-01, -6.5552e-02,\n",
       "            1.3098e-01,  4.6631e-02,  1.2439e-01, -2.1350e-01, -2.1716e-01,\n",
       "           -1.6431e-01, -1.3538e-01, -1.8158e-02, -8.3313e-02, -3.0933e-01,\n",
       "            9.6619e-02,  4.9629e-03,  4.9782e-03, -5.8746e-02,  2.6440e-01,\n",
       "           -2.8149e-01, -1.6968e-02,  3.8281e-01, -3.3813e-01,  7.5012e-02,\n",
       "            2.2742e-01,  2.7783e-01,  8.8196e-02,  1.2347e-01, -8.8623e-02,\n",
       "           -1.1249e-01, -3.3521e-01,  8.5297e-03, -1.4563e-01, -1.8265e-02,\n",
       "           -4.6021e-02, -2.6001e-01,  1.7664e-01, -1.1609e-01,  3.1543e-01,\n",
       "            6.5491e-02, -3.1348e-01,  2.9272e-01, -1.0162e-01,  1.1621e-01,\n",
       "           -1.5533e-02,  6.2988e-01, -2.9297e-01, -7.9269e-03,  1.3013e-01,\n",
       "           -3.6041e-02,  9.1980e-02,  3.6011e-01,  1.7261e-01, -6.4209e-02,\n",
       "           -4.7485e-02,  2.0190e-01,  3.5718e-01,  1.4050e-01, -8.3542e-03,\n",
       "            8.2703e-02,  7.5317e-02, -1.0706e-01,  7.5745e-02, -1.5942e-01,\n",
       "           -1.6504e-01,  2.8516e-01,  3.3032e-01, -2.0544e-01, -3.3936e-02,\n",
       "            1.4868e-01,  1.5906e-01, -1.8418e+00,  4.8438e-01, -2.9327e-02,\n",
       "           -1.6830e-02,  3.5181e-01,  1.3892e-01,  5.0049e-02, -1.2720e-01,\n",
       "            2.1033e-01, -2.1155e-01, -7.0496e-02, -2.7856e-01, -2.9224e-01,\n",
       "           -5.6305e-02,  2.0935e-01, -1.2543e-02,  2.8442e-01,  4.0649e-02,\n",
       "            5.8655e-02,  4.1406e-01, -3.0632e-03, -9.6741e-02, -1.8481e-01,\n",
       "            1.5247e-01,  5.1636e-02, -4.8523e-02, -2.2791e-01, -1.3660e-01,\n",
       "           -1.5088e-01, -3.8818e-01,  4.3066e-01, -1.3330e-01,  6.2439e-02,\n",
       "            1.6312e-02, -5.0684e-01, -2.6514e-01,  1.3992e-02, -1.4917e-01,\n",
       "           -6.7505e-02, -1.8774e-01, -1.2830e-01,  6.1172e+00,  1.4130e-02,\n",
       "            3.4576e-02,  3.7964e-02, -1.5747e-01, -1.3086e-01, -7.8796e-02,\n",
       "           -1.0773e-01,  1.7407e-01, -3.6157e-01,  7.6025e-01, -5.3369e-01,\n",
       "           -2.7954e-01,  3.3081e-02, -1.9836e-01, -2.2003e-02,  3.6499e-01,\n",
       "            1.9714e-01, -1.8030e-01,  2.5977e-01,  1.1115e-01, -6.0577e-02,\n",
       "           -6.1230e-01,  6.0516e-02,  6.2134e-02,  4.4751e-01, -1.2610e-01,\n",
       "            5.6244e-02,  3.5132e-01, -2.2949e-01,  2.6221e-01, -2.5781e-01,\n",
       "            4.3237e-01,  4.3823e-01, -2.8394e-01,  9.1309e-02, -2.0096e-02,\n",
       "           -9.4116e-02, -2.4341e-01,  3.3813e-01,  4.4678e-01, -4.1235e-01,\n",
       "           -7.2510e-02, -1.7468e-01, -3.6049e-03,  8.7341e-02,  6.8665e-02,\n",
       "           -1.4725e-02, -3.3356e-02,  3.7432e-04,  1.4832e-01, -1.0522e-01,\n",
       "           -2.1985e-01, -4.4312e-02, -4.5044e-01, -2.3651e-02, -1.0938e-01,\n",
       "            1.9226e-02, -5.0354e-02,  1.0455e-01, -3.1787e-01,  6.0141e-05,\n",
       "            4.5166e-02,  8.3008e-02, -6.0959e-03, -2.6294e-01,  4.0259e-01,\n",
       "           -3.5736e-02,  3.1421e-01,  2.2449e-01, -1.1188e-01, -2.2656e-01,\n",
       "           -1.2061e-01,  3.4766e-01, -5.0439e-01,  3.5498e-01,  4.7333e-02,\n",
       "            2.8702e-02,  3.2275e-01,  2.0374e-01,  2.3792e-01, -5.0385e-02,\n",
       "           -3.9444e-03,  1.3054e-02,  1.3145e-02,  2.1460e-01,  7.5134e-02,\n",
       "            1.0284e-01, -3.1860e-01,  2.1838e-01,  2.2168e-01, -9.7473e-02,\n",
       "            4.2511e-02,  2.2632e-01,  8.7097e-02, -2.5952e-01,  3.7012e-01,\n",
       "           -1.5527e-01,  1.3989e-01,  1.3672e-01, -1.6589e-01,  9.3689e-02,\n",
       "            2.3779e-01,  2.2034e-01,  4.7949e-01, -9.1614e-02, -1.7871e-01,\n",
       "            3.3105e-01,  2.3279e-01, -5.1849e-02, -5.7770e-02, -3.5797e-02,\n",
       "            3.4448e-01,  2.2510e-01,  4.5593e-02, -1.1835e-01,  4.2572e-02,\n",
       "           -4.8218e-02,  9.8938e-02, -1.5015e-01,  2.6270e-01, -1.9336e-01,\n",
       "           -1.7944e-01,  7.8064e-02, -9.4482e-02,  2.0117e-01, -4.4006e-02,\n",
       "           -3.6060e-01, -7.4280e-02, -1.0796e-02, -1.9055e-01,  6.8237e-02,\n",
       "           -1.4966e-01,  2.2534e-01,  2.6108e-02, -2.5415e-01, -1.2067e-01,\n",
       "           -3.2867e-02,  2.9443e-01, -2.7344e-01, -4.4238e-01,  4.1406e-01,\n",
       "           -1.9165e-01,  2.7145e-02, -1.3342e-01, -4.0680e-02, -3.9868e-01,\n",
       "            1.6602e-02, -2.2778e-01,  2.7832e-01,  1.5466e-01,  5.5786e-02,\n",
       "            2.9077e-01,  2.4185e-02,  1.3428e-01, -1.2610e-01, -2.7563e-01,\n",
       "           -3.1921e-02,  1.3428e-01,  2.9028e-01,  1.2482e-02, -1.5356e-01,\n",
       "            2.8854e-02, -3.7598e-01,  7.3975e-02,  8.8562e-02, -1.6006e-02,\n",
       "           -2.5928e-01, -9.3140e-02,  4.9170e-01,  1.2756e-01, -1.1237e-01,\n",
       "           -5.5084e-02, -4.3579e-02, -2.1716e-01,  2.4646e-01, -1.7896e-01,\n",
       "            1.9867e-02,  2.7075e-01,  6.1055e+00, -8.7097e-02,  3.7183e-01,\n",
       "            2.7515e-01,  5.1666e-02,  2.4353e-01,  2.6831e-01,  2.1326e-01,\n",
       "            2.1375e-01, -1.1879e-02,  1.3220e-01, -1.2061e-01,  2.0532e-01,\n",
       "            1.0022e-01, -1.1627e-01, -1.2189e-01,  3.5254e-01, -3.3750e+00,\n",
       "           -1.6931e-01,  2.6416e-01,  1.0150e-01, -2.9004e-01,  9.1370e-02,\n",
       "            8.3130e-02, -8.6121e-02,  2.7197e-01, -1.6333e-01,  1.7529e-01,\n",
       "           -2.9126e-01, -2.2266e-01, -3.9959e-04,  2.9419e-02,  1.1371e-01,\n",
       "            3.6804e-02,  3.2812e-01, -2.1863e-01,  6.0913e-02, -3.8574e-01,\n",
       "            7.5000e-01, -1.5198e-01, -6.3782e-03,  4.2627e-01, -1.9214e-01,\n",
       "            2.6025e-01,  1.3330e-01,  4.1229e-02, -1.4148e-01, -1.0028e-01,\n",
       "           -2.6953e-01, -1.4368e-01, -2.5366e-01,  2.8784e-01,  3.4424e-01,\n",
       "           -6.7017e-02,  2.0410e-01, -2.6440e-01,  4.9103e-02,  3.4668e-01,\n",
       "           -6.9519e-02, -1.1310e-01,  2.8125e-01,  3.1204e-02,  3.4644e-01,\n",
       "           -1.5540e-01,  1.3452e-01, -1.7749e-01, -2.0935e-01,  1.7310e-01,\n",
       "            2.9590e-01,  2.0728e-01,  1.0693e-01, -8.5693e-02,  5.3223e-01,\n",
       "            3.8391e-02, -1.1670e-01,  3.6304e-01, -2.1680e-01,  2.2876e-01,\n",
       "           -6.0645e-01,  1.4392e-01, -1.9800e-01,  1.2036e-01, -1.6248e-01,\n",
       "            1.8604e-01, -1.0156e-01,  1.8506e-01, -1.0419e-01,  4.2261e-01,\n",
       "            4.6753e-02,  2.2919e-02,  3.1470e-01, -1.7566e-01, -2.0422e-01,\n",
       "           -1.6211e-01,  2.8589e-01, -1.6443e-01,  3.3905e-02, -3.0322e-01,\n",
       "            6.4453e-02, -1.9482e-01, -4.9622e-02,  4.1089e-01,  4.6289e-01,\n",
       "           -1.8494e-01,  9.7412e-02, -5.6549e-02, -1.3074e-01,  2.5903e-01,\n",
       "            3.0664e-01, -6.0539e-03,  2.8503e-02, -2.7344e-01,  2.6562e-01,\n",
       "           -4.0344e-02, -6.9397e-02, -2.8296e-01,  5.1788e-02,  1.3171e-01,\n",
       "           -3.6499e-02, -9.2163e-02,  1.1450e-01,  6.3477e-02, -1.6321e-01,\n",
       "            1.3477e-01, -1.7395e-01, -1.9150e-02, -7.8064e-02,  3.0713e-01,\n",
       "           -3.5034e-01, -3.7720e-01, -2.2571e-01, -1.0730e-01, -4.5874e-01,\n",
       "            1.9250e-01, -5.1660e-01, -1.6870e-01, -2.2430e-02,  2.1936e-01,\n",
       "            1.9519e-01, -2.9541e-01,  1.4374e-02,  9.6985e-02,  3.9282e-01,\n",
       "           -4.1260e-01, -1.6205e-02,  2.5000e-01,  5.3802e-02,  7.0374e-02,\n",
       "           -1.7151e-01, -1.7664e-01,  2.9224e-01,  3.4595e-01,  2.9688e-01,\n",
       "            1.0083e-01, -2.0374e-01, -1.0269e-02, -1.3562e-01, -3.8879e-02,\n",
       "            3.0005e-01, -2.1082e-01,  4.4220e-02, -8.4991e-03,  3.6072e-02,\n",
       "           -1.4905e-01,  2.1973e-01,  4.0283e-02,  3.1421e-01,  1.1157e-01,\n",
       "            1.3618e-02, -9.0527e-01, -4.6814e-02, -3.2654e-02, -2.8625e-02,\n",
       "            2.3059e-01, -1.7468e-01, -4.0527e-01,  8.6426e-02, -8.3740e-02,\n",
       "            3.2251e-01,  1.4087e-01,  9.0149e-02,  2.4072e-01,  4.7559e-01,\n",
       "           -1.9617e-01,  7.8735e-02, -1.2140e-01, -2.5586e-01,  1.0925e-02,\n",
       "            3.1055e-01, -3.0200e-01, -1.1511e-01,  2.8564e-01,  2.6855e-01,\n",
       "            4.4653e-01, -5.9753e-02, -1.3695e-02, -1.5076e-01, -4.7217e-01,\n",
       "           -4.6082e-02,  1.0040e-02]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0009, 0.0173, 0.0252, 0.1322, 0.0073, 0.7256, 0.0053, 0.0137, 0.0707,\n",
       "           0.0017]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   293.214661   48.576584  561.993958  381.406128    0.929947      0   \n",
       "  1   106.648987   89.906616  151.173370  206.837433    0.883486      0   \n",
       "  2    74.683327   86.743683  118.113792  207.065216    0.879327      0   \n",
       "  3    14.608261  111.588303  107.332268  207.331390    0.878334      0   \n",
       "  4     1.537323  201.705322  409.665009  475.051575    0.866191      3   \n",
       "  5   304.999817   33.608299  407.275879  208.662537    0.864471      0   \n",
       "  6   431.493958   10.817223  502.282349  142.346039    0.818667     46   \n",
       "  7   404.549408  348.410217  512.509888  478.180847    0.763421     46   \n",
       "  8   190.925720  192.297913  270.940826  263.910461    0.702960     56   \n",
       "  9    93.673279   43.160538  357.651794  196.757141    0.638933      8   \n",
       "  10  520.603882   28.718575  614.658447   82.672745    0.595278     46   \n",
       "  11  433.536469  313.320160  523.596130  386.958954    0.423797     46   \n",
       "  12  523.711609  113.296249  581.670837  175.890305    0.388447     46   \n",
       "  13  599.982849  333.388611  639.640198  367.891541    0.386580     49   \n",
       "  14   61.698051  354.701965  109.741463  412.667847    0.364852     46   \n",
       "  15  533.879456  313.031830  620.082458  361.644867    0.312923     49   \n",
       "  16  546.109680  275.665833  638.534363  307.745789    0.270498     47   \n",
       "  \n",
       "            name  \n",
       "  0       person  \n",
       "  1       person  \n",
       "  2       person  \n",
       "  3       person  \n",
       "  4   motorcycle  \n",
       "  5       person  \n",
       "  6       banana  \n",
       "  7       banana  \n",
       "  8        chair  \n",
       "  9         boat  \n",
       "  10      banana  \n",
       "  11      banana  \n",
       "  12      banana  \n",
       "  13      orange  \n",
       "  14      banana  \n",
       "  15      orange  \n",
       "  16       apple  ,\n",
       "  'caption': ['The white walls in the background'],\n",
       "  'bbox_target': [60.44, 82.96, 297.49, 123.26]},\n",
       " 655: {'image_emb': tensor([[-0.4402,  0.4883, -0.1141,  ...,  0.6494,  0.1863,  0.3154],\n",
       "          [-0.4595,  0.2725, -0.1689,  ...,  0.6753,  0.2896,  0.1959],\n",
       "          [-0.4717,  0.1324, -0.0637,  ...,  0.6826,  0.2222,  0.1570]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0128,  0.8403, -0.3889,  ...,  0.4792,  0.5103, -0.2004],\n",
       "          [-0.0967,  0.3027, -0.1989,  ...,  0.1879,  0.1248, -0.3516]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.9321, 0.0543, 0.0137],\n",
       "          [0.9512, 0.0418, 0.0068]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class  name\n",
       "  0  309.263916  53.769547  557.395447  420.083435    0.948310     75  vase\n",
       "  1   30.964874  93.774902  306.833313  394.885925    0.929532     75  vase,\n",
       "  'caption': ['A Grecian urn with the a man on a horse in a glass case.',\n",
       "   'A vase picturing several men and a single horse.'],\n",
       "  'bbox_target': [309.41, 53.95, 246.37, 364.8]},\n",
       " 656: {'image_emb': tensor([[-0.0132, -0.1217, -0.1687,  ...,  0.7925, -0.1403,  0.1268],\n",
       "          [ 0.3281,  0.2367, -0.3428,  ...,  0.6904,  0.1145, -0.1231],\n",
       "          [ 0.0810,  0.4924, -0.6382,  ...,  1.3223, -0.2354,  0.0087],\n",
       "          [ 0.2432,  0.0992, -0.5405,  ...,  0.6318,  0.1677, -0.3252],\n",
       "          [ 0.0599, -0.2998, -0.4355,  ...,  1.0811, -0.0742, -0.1405],\n",
       "          [-0.1500, -0.1293, -0.0363,  ...,  0.7925, -0.1780,  0.0070]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0382, -0.0391, -0.1979,  ..., -0.3171,  0.2634, -0.4954],\n",
       "          [-0.0068,  0.0293, -0.4465,  ...,  0.1267,  0.1541,  0.0514]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.1960e-02, 5.0732e-01, 5.3048e-06, 9.8944e-06, 4.6551e-05, 4.2065e-01],\n",
       "          [7.8278e-03, 9.6289e-01, 2.0325e-05, 9.5367e-07, 9.7752e-06, 2.9083e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   60.139824   30.101252  434.816925  271.999939    0.937139      0   \n",
       "  1  163.429047    0.355667  234.505585  134.420364    0.929238      0   \n",
       "  2  313.944000  193.452484  421.366211  257.046051    0.804272     26   \n",
       "  3  373.715668   64.962982  396.354553   90.334030    0.772861     67   \n",
       "  4  302.674164  178.746414  317.630798  197.477051    0.725394     39   \n",
       "  5  107.763885    0.576848  162.912018   33.179070    0.513601      0   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2     handbag  \n",
       "  3  cell phone  \n",
       "  4      bottle  \n",
       "  5      person  ,\n",
       "  'caption': ['A woman wearing a dress and heels walking around the corner of a building',\n",
       "   'The walking women is wearing a yellow skirt with black heels.'],\n",
       "  'bbox_target': [154.94, 0.0, 81.8, 133.71]},\n",
       " 657: {'image_emb': tensor([[ 0.0137,  0.4302, -0.0435,  ...,  0.6587, -0.1768,  0.0986],\n",
       "          [ 0.0695,  0.3872, -0.2703,  ...,  0.6948,  0.0671,  0.0792],\n",
       "          [-0.1394,  0.4446,  0.0290,  ...,  0.6143,  0.1797,  0.3044],\n",
       "          ...,\n",
       "          [ 0.0845,  0.0803, -0.3853,  ...,  1.0303,  0.0851, -0.0928],\n",
       "          [ 0.1216, -0.2803, -0.3962,  ...,  1.0059, -0.1172, -0.0875],\n",
       "          [-0.1764,  0.3457,  0.0731,  ...,  0.5396, -0.1768,  0.1388]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1045,  0.2812,  0.0637,  ...,  0.0106,  0.2766, -0.2152],\n",
       "          [-0.0788, -0.0150, -0.0979,  ...,  0.3721,  0.3926,  0.0120]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.2329e-02, 1.5192e-03, 9.3457e-01, 4.1533e-04, 7.8022e-05, 5.4479e-05,\n",
       "           1.3173e-05, 5.1086e-02],\n",
       "          [3.1677e-02, 1.7319e-03, 9.2529e-01, 4.4465e-04, 8.8930e-05, 3.3736e-05,\n",
       "           1.5438e-05, 4.0649e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   99.785400  120.294128  307.961426  396.176270    0.933942      0   \n",
       "  1    0.000000  354.670898   36.304611  416.733948    0.893703      0   \n",
       "  2  135.380127  321.844238  286.190674  432.808105    0.884520     36   \n",
       "  3  322.831329  347.133728  351.084930  415.866699    0.871657      0   \n",
       "  4  265.675781  351.024567  321.511353  457.743134    0.835362      0   \n",
       "  5  167.533081  352.729828  189.400757  408.133026    0.770205      0   \n",
       "  6  256.359192  452.295502  346.441711  467.982269    0.765839     36   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2  skateboard  \n",
       "  3      person  \n",
       "  4      person  \n",
       "  5      person  \n",
       "  6  skateboard  ,\n",
       "  'caption': ['the skateboard in the air touching the pole',\n",
       "   'skateboard going down the rail'],\n",
       "  'bbox_target': [135.75, 321.68, 148.83, 110.81]},\n",
       " 658: {'image_emb': tensor([[-0.0433,  0.7529,  0.0228,  ...,  0.8955, -0.4307,  0.0773],\n",
       "          [-0.0408,  0.0487,  0.1985,  ...,  0.7930,  0.1597, -0.0560],\n",
       "          [-0.1877,  0.2456, -0.1770,  ...,  1.0918, -0.0902, -0.0662],\n",
       "          [-0.0621,  0.1570, -0.3757,  ...,  1.0469,  0.0151, -0.1306],\n",
       "          [ 0.0229,  0.6689,  0.4353,  ...,  1.0635, -0.2463,  0.0175]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0690, -0.0844,  0.0583,  ..., -0.3687, -0.1414, -0.2524],\n",
       "          [-0.0852, -0.0901, -0.1106,  ..., -0.1133, -0.0843, -0.0972]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.7760e-02, 9.1696e-04, 3.9215e-03, 2.8248e-03, 9.4434e-01],\n",
       "          [6.8420e-02, 1.3840e-04, 5.6458e-04, 1.1063e-03, 9.2969e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    1.491119  181.655823  329.553619  400.995056    0.912285      2   \n",
       "  1  588.182434  201.351898  640.000000  316.309540    0.832897      2   \n",
       "  2    0.249205  133.198151   61.178093  399.668579    0.768381      0   \n",
       "  3  481.288940  137.631470  549.880737  364.227234    0.762054     12   \n",
       "  4  256.175903  108.008942  530.229309  265.712311    0.692262      7   \n",
       "  5    1.153435  123.281342  138.706909  186.263702    0.478642      7   \n",
       "  6    1.579506  123.034760  137.439362  186.756378    0.433427      2   \n",
       "  \n",
       "              name  \n",
       "  0            car  \n",
       "  1            car  \n",
       "  2         person  \n",
       "  3  parking meter  \n",
       "  4          truck  \n",
       "  5          truck  \n",
       "  6            car  ,\n",
       "  'caption': ['A grey van parked next to the sign.',\n",
       "   'A van parked in front of the open car.'],\n",
       "  'bbox_target': [255.28, 113.55, 235.15, 156.46]},\n",
       " 659: {'image_emb': tensor([[ 0.0432,  0.1620, -0.0865,  ...,  1.1104,  0.4246,  0.0440],\n",
       "          [ 0.0620,  0.0177, -0.0541,  ...,  1.4785,  0.1456,  0.0066],\n",
       "          [ 0.0861,  0.0726, -0.4563,  ...,  1.1875,  0.2798,  0.1560],\n",
       "          [-0.0883, -0.0124, -0.4341,  ...,  1.4756,  0.3650,  0.3125],\n",
       "          [ 0.3669, -0.0225, -0.0066,  ...,  0.9541,  0.1758, -0.0886]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0649, -0.2295, -0.3816,  ...,  0.0256, -0.0044, -0.2090],\n",
       "          [ 0.0082,  0.0180, -0.1088,  ...,  0.1031, -0.1539, -0.0994]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.3716e-01, 2.5749e-03, 3.0396e-02, 2.9981e-05, 6.2988e-01],\n",
       "          [9.3750e-01, 1.7166e-02, 1.8377e-03, 4.5037e-04, 4.3152e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   99.780884  162.375809  309.530823  637.341370    0.952827      0  person\n",
       "  1  282.878174  354.580078  361.976685  545.187012    0.845995      0  person\n",
       "  2    0.226204  384.235596  201.142242  534.491211    0.746913     57   couch\n",
       "  3  331.379059  394.465363  478.071350  538.925659    0.722531     57   couch\n",
       "  4  101.748856  376.263214  140.064865  405.052826    0.630537     65  remote\n",
       "  5  100.444977  390.273804  117.764801  405.832214    0.504215     65  remote\n",
       "  6  385.643280  323.961243  413.750153  350.395996    0.357542     73    book\n",
       "  7  372.646179  322.083557  392.376770  351.799622    0.253867     73    book,\n",
       "  'caption': ['A woman in white playing the wii.',\n",
       "   'a women was on white t-shirt'],\n",
       "  'bbox_target': [100.04, 162.15, 207.83, 477.85]},\n",
       " 660: {'image_emb': tensor([[-0.7744, -0.2488,  0.0871,  ...,  0.8188,  0.1946, -0.2632],\n",
       "          [-0.6616,  0.3354,  0.0795,  ...,  0.8940, -0.0436, -0.1027],\n",
       "          [-0.4326, -0.0132, -0.2152,  ...,  0.9004,  0.1711, -0.1364],\n",
       "          [-0.0881,  0.1107, -0.2180,  ...,  1.2627, -0.0379, -0.1484],\n",
       "          [-0.8086, -0.0072, -0.0150,  ...,  0.9443,  0.0538, -0.3337]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2433,  0.0484, -0.0717,  ..., -0.0114,  0.1367, -0.3726],\n",
       "          [-0.7402, -0.2112, -0.3271,  ...,  0.2031, -0.4294, -0.2859]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0513, 0.2125, 0.4229, 0.2952, 0.0183],\n",
       "          [0.1226, 0.5938, 0.0140, 0.0062, 0.2634]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  132.382080    9.071060  410.004822  418.774780    0.947052      0   \n",
       "  1    0.433544   95.404633   87.384872  308.365753    0.915115      0   \n",
       "  2  229.995514  211.283966  402.162750  421.558044    0.851294     69   \n",
       "  3   99.175705   55.261307  183.244461  285.924316    0.757383     72   \n",
       "  4   98.424896   55.474915  183.632355  285.899231    0.584407     69   \n",
       "  5  337.931274  258.143127  401.405151  322.227905    0.420829     71   \n",
       "  6  380.048523    1.520935  519.216614  211.763763    0.379930     72   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2          oven  \n",
       "  3  refrigerator  \n",
       "  4          oven  \n",
       "  5          sink  \n",
       "  6  refrigerator  ,\n",
       "  'caption': ['A man with his back to the camera wearing a white shirt, head cover and black and white striped apron looking in a refrigerator.',\n",
       "   'A cook in a stripped apron.'],\n",
       "  'bbox_target': [0.0, 94.54, 87.86, 211.05]},\n",
       " 661: {'image_emb': tensor([[ 0.2888,  0.2037, -0.1559,  ...,  0.7505,  0.0088,  0.3362],\n",
       "          [-0.0602,  0.3333, -0.2969,  ...,  0.8696,  0.0616,  0.5059],\n",
       "          [ 0.1830,  0.2249, -0.1388,  ...,  0.8130, -0.1460,  0.0827]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0780,  0.0080, -0.2878,  ..., -0.1934,  0.4814,  0.2478],\n",
       "          [ 0.0145,  0.0890, -0.3315,  ...,  0.2153,  0.5229,  0.2957]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2517, 0.7397, 0.0085],\n",
       "          [0.5352, 0.4299, 0.0348]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  171.927734  188.939240  309.220337  562.303833    0.926689      0   \n",
       "  1  211.309235  303.268066  276.947723  390.005981    0.712069     46   \n",
       "  2  212.653442  303.328064  277.745728  387.359985    0.663565     79   \n",
       "  3  290.903046  165.443878  392.310089  257.667206    0.662732     58   \n",
       "  4    0.527710    1.460693   42.593658  131.845993    0.609764     68   \n",
       "  5  275.501831  565.183777  478.537415  639.821228    0.536720     25   \n",
       "  6  263.291473  148.250153  309.191803  255.029083    0.456314     58   \n",
       "  7  211.660706  306.320038  277.111267  385.732086    0.354796      0   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        banana  \n",
       "  2    toothbrush  \n",
       "  3  potted plant  \n",
       "  4     microwave  \n",
       "  5      umbrella  \n",
       "  6  potted plant  \n",
       "  7        person  ,\n",
       "  'caption': ['The girl in pink holding a banana',\n",
       "   'A little girl in a pink shirt holding a banana.'],\n",
       "  'bbox_target': [165.86, 162.09, 156.3, 402.25]},\n",
       " 662: {'image_emb': tensor([[-0.3296,  0.0925, -0.3020,  ...,  0.9067, -0.0704,  0.0189],\n",
       "          [-0.3452,  0.3188, -0.1322,  ...,  0.7798,  0.0039,  0.0630],\n",
       "          [-0.5396, -0.0046, -0.1265,  ...,  0.2957, -0.0609,  0.1218]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2236,  0.1986,  0.1610,  ...,  0.2900,  0.2135, -0.5308],\n",
       "          [ 0.1052, -0.1595,  0.0349,  ...,  0.2676,  0.1780, -0.0587]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2389, 0.7593, 0.0019],\n",
       "          [0.5044, 0.4521, 0.0434]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   69.576309   96.613892  304.507324  469.769531    0.946122      0  person\n",
       "  1  399.021088   61.764496  586.841187  448.951050    0.941045      0  person\n",
       "  2  324.715637  414.808594  635.855774  477.298462    0.671134     30    skis\n",
       "  3    0.420181  448.912903  373.775146  479.775696    0.439456     30    skis,\n",
       "  'caption': ['skis under man with orange coat', 'black skiis.'],\n",
       "  'bbox_target': [330.9, 414.78, 309.1, 60.72]},\n",
       " 663: {'image_emb': tensor([[-0.2180,  0.1941, -0.1431,  ...,  0.9067,  0.3223, -0.0345],\n",
       "          [-0.1853,  0.1094, -0.2245,  ...,  1.1758,  0.1549,  0.1432],\n",
       "          [-0.0584, -0.0836, -0.2053,  ...,  0.6875,  0.0402, -0.0444]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0323,  0.1088,  0.0833,  ..., -0.6387,  0.0687, -0.1019],\n",
       "          [ 0.3645,  0.4883, -0.3647,  ...,  0.0510, -0.6440,  0.1251]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5225, 0.2886, 0.1892],\n",
       "          [0.3708, 0.5068, 0.1223]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   64.425385  111.876617  385.163849  639.425537    0.923195      0   \n",
       "  1  273.933044  166.780518  426.000000  588.546204    0.891542     77   \n",
       "  2   86.826553  221.627884  151.101807  347.083557    0.679974      0   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1  teddy bear  \n",
       "  2      person  ,\n",
       "  'caption': ['the boy', 'A boy with curly hair and brown pants.'],\n",
       "  'bbox_target': [60.2, 108.67, 325.99, 527.33]},\n",
       " 664: {'image_emb': tensor([[-0.1353,  0.4534, -0.3215,  ...,  1.3252,  0.1753, -0.4590],\n",
       "          [-0.4587,  0.2079,  0.6528,  ...,  0.9829, -0.2014, -0.2820]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2544,  0.4888, -0.1176,  ...,  0.0131, -0.0048, -0.4226],\n",
       "          [ 0.2668,  0.2275, -0.0024,  ...,  0.0638,  0.1693, -0.2496]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.9390, 0.0610],\n",
       "          [0.8008, 0.1993]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   71.718086  165.923737  163.605682  320.772461    0.936941     11   \n",
       "  1    3.422862  301.488953   81.579903  332.720581    0.578725     25   \n",
       "  2  192.229340  254.605835  346.564545  373.614868    0.578174     25   \n",
       "  3  422.885956  303.716003  498.589172  331.408630    0.536828     25   \n",
       "  4    3.966591  300.853271   80.807793  374.021149    0.531805     25   \n",
       "  5  423.399689  303.226196  498.559143  364.859375    0.484086     25   \n",
       "  6  137.718796  289.560791  229.244110  322.161499    0.439907     25   \n",
       "  7  191.314606  254.326416  346.810974  304.711365    0.318767     25   \n",
       "  \n",
       "          name  \n",
       "  0  stop sign  \n",
       "  1   umbrella  \n",
       "  2   umbrella  \n",
       "  3   umbrella  \n",
       "  4   umbrella  \n",
       "  5   umbrella  \n",
       "  6   umbrella  \n",
       "  7   umbrella  ,\n",
       "  'caption': ['maroon umbrella closest to the street',\n",
       "   'Maroon umbrella furthest from shop.'],\n",
       "  'bbox_target': [195.06, 256.64, 154.3, 117.53]},\n",
       " 665: {'image_emb': tensor([[-0.0725,  0.4546,  0.2600,  ...,  1.1631, -0.0520,  0.0934],\n",
       "          [ 0.1234,  0.3369, -0.1196,  ...,  1.1846, -0.1342, -0.0492],\n",
       "          [ 0.5713,  0.3779,  0.1818,  ...,  0.7368, -0.1573,  0.1483]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0371,  0.0820,  0.0198,  ..., -0.2323, -0.7280, -0.3674],\n",
       "          [ 0.0442,  0.1371,  0.2244,  ..., -0.2185, -0.6841, -0.4146]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.8535, 0.0794, 0.0669],\n",
       "          [0.6733, 0.0374, 0.2896]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  340.862183  123.054688  635.509888  303.272095    0.883226     48   \n",
       "  1  322.041107  270.494385  592.403076  413.023193    0.848628     48   \n",
       "  2  147.269241   53.312286  342.537109  148.284729    0.691835     55   \n",
       "  3    0.380585  258.009613  117.454880  297.951263    0.660321     44   \n",
       "  4    0.000000  102.469406  640.000000  464.562805    0.625607     60   \n",
       "  5   96.423615  137.942581  363.996033  371.761841    0.605250     41   \n",
       "  6   97.814346  138.934402  362.465698  371.773743    0.539757     45   \n",
       "  7   94.916550  136.131042  366.168762  372.741882    0.327524     60   \n",
       "  \n",
       "             name  \n",
       "  0      sandwich  \n",
       "  1      sandwich  \n",
       "  2          cake  \n",
       "  3         spoon  \n",
       "  4  dining table  \n",
       "  5           cup  \n",
       "  6          bowl  \n",
       "  7  dining table  ,\n",
       "  'caption': ['half a sandwich sitting on top',\n",
       "   'A sandwich on another sandwich on dish.'],\n",
       "  'bbox_target': [340.0, 121.88, 295.59, 179.39]},\n",
       " 666: {'image_emb': tensor([[-0.0690, -0.1456, -0.2419,  ...,  0.3860,  0.2583, -0.1713],\n",
       "          [-0.1198,  0.0117, -0.2289,  ...,  0.2695,  0.3223, -0.0982],\n",
       "          [ 0.0194, -0.0725, -0.2185,  ...,  0.3955,  0.2198, -0.1097]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3220, -0.0302, -0.2181,  ..., -0.3684,  0.0458,  0.6069],\n",
       "          [-0.3264, -0.2700, -0.1214,  ...,  0.0177,  0.0842,  0.5361]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2241, 0.4744, 0.3015],\n",
       "          [0.3811, 0.5210, 0.0979]], dtype=torch.float16),\n",
       "  'df_boxes':         xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  85.131546  211.104340  445.081360  442.128906    0.949325     22  zebra\n",
       "  1  82.817818  179.220764  254.206848  354.813416    0.888568     22  zebra,\n",
       "  'caption': ['The zebra in the back.', 'zebra in the back side'],\n",
       "  'bbox_target': [83.57, 180.57, 174.13, 182.38]},\n",
       " 667: {'image_emb': tensor([[ 0.1500,  0.2793, -0.1279,  ...,  0.8789, -0.0835,  0.1313],\n",
       "          [ 0.1171,  0.5483, -0.1300,  ...,  0.9922,  0.0639,  0.0440],\n",
       "          [-0.0231,  0.4829, -0.1630,  ...,  1.0322,  0.1094, -0.0396]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1302,  0.3501, -0.6504,  ..., -0.1475, -0.0322,  0.0100],\n",
       "          [ 0.1284,  0.2852, -0.5146,  ..., -0.0289, -0.2299,  0.0933]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.8662, 0.0668, 0.0668],\n",
       "          [0.7065, 0.1902, 0.1034]], dtype=torch.float16),\n",
       "  'df_boxes':         xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  75.385086   57.902527  418.978149  328.997314    0.918543      0  person\n",
       "  1  60.015045  112.136612  529.307129  477.245361    0.832221      0  person\n",
       "  2   8.212341    2.951660  634.160828  473.240417    0.626121     59     bed,\n",
       "  'caption': ['A sleeping child in a red shirt.',\n",
       "   'A boy with a red shirt sleeping.'],\n",
       "  'bbox_target': [58.25, 112.18, 471.37, 367.82]},\n",
       " 668: {'image_emb': tensor([[-0.2383,  0.3037,  0.0213,  ...,  0.5244, -0.0523, -0.0936],\n",
       "          [-0.6069,  0.1591, -0.1042,  ...,  0.6348, -0.0845, -0.5005],\n",
       "          [-0.5859,  0.2281, -0.1692,  ...,  0.5225, -0.0897, -0.2063],\n",
       "          [-0.4438,  0.2786, -0.2412,  ...,  1.0361,  0.0602, -0.1559],\n",
       "          [-0.4060,  0.1027, -0.0070,  ..., -0.0204, -0.1368, -0.3350]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2576,  0.0547,  0.0261,  ...,  0.1458,  0.1653, -0.2410],\n",
       "          [-0.2009, -0.0726, -0.2407,  ...,  0.6377, -0.0978, -0.4346]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1008, 0.8711, 0.0160, 0.0011, 0.0111],\n",
       "          [0.7588, 0.1747, 0.0485, 0.0037, 0.0143]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  139.089783   36.103775  289.083069  207.500488    0.944800      0   \n",
       "  1  272.726227  136.276550  351.123322  340.422852    0.927880      0   \n",
       "  2  339.049164  186.980408  374.654694  228.456848    0.796923     38   \n",
       "  3  274.982941   51.134918  325.077972   84.182098    0.700580     38   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2  tennis racket  \n",
       "  3  tennis racket  ,\n",
       "  'caption': ['A man with a black wrist band is holding a tennis rack with his right leg up.',\n",
       "   'A tennis player in full extension after just hitting a tennis ball.'],\n",
       "  'bbox_target': [137.21, 36.77, 149.88, 169.36]},\n",
       " 669: {'image_emb': tensor([[-0.3430,  0.5161, -0.1055,  ...,  1.0801,  0.5288,  0.0538],\n",
       "          [-0.5176,  0.7539,  0.0525,  ...,  0.3892,  0.2939,  0.1209],\n",
       "          [-0.6499,  0.3945,  0.0992,  ...,  0.7500, -0.0817, -0.1069],\n",
       "          [-0.6299,  0.5166,  0.1593,  ...,  1.1426,  0.1833, -0.2124],\n",
       "          [-0.5215,  0.4214,  0.1010,  ...,  0.5210,  0.3950,  0.0131]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1647,  0.1779,  0.1946,  ..., -0.4373, -0.5142, -0.1913],\n",
       "          [-0.0914,  0.4714,  0.1058,  ..., -0.3779, -0.2275, -0.2749]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0034, 0.6782, 0.1400, 0.0469, 0.1315],\n",
       "          [0.0030, 0.1002, 0.0051, 0.7881, 0.1034]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  497.281677  227.096405  639.737122  462.428101    0.929733     41   \n",
       "  1    2.750473  156.062317  406.115479  473.571594    0.925961     45   \n",
       "  2  378.036285  132.552582  569.551758  293.005463    0.904256     45   \n",
       "  3  305.367828  389.226196  379.952118  478.983398    0.765661     44   \n",
       "  4  562.275757  264.575928  640.000000  339.706665    0.422481     44   \n",
       "  5    2.929138    7.523254  640.000000  473.308105    0.348898     60   \n",
       "  6  377.561584  269.434448  639.255554  476.233643    0.266655     60   \n",
       "  \n",
       "             name  \n",
       "  0           cup  \n",
       "  1          bowl  \n",
       "  2          bowl  \n",
       "  3         spoon  \n",
       "  4         spoon  \n",
       "  5  dining table  \n",
       "  6  dining table  ,\n",
       "  'caption': ['A bowl of soup.', 'the largest plate'],\n",
       "  'bbox_target': [1.08, 158.56, 405.57, 312.81]},\n",
       " 670: {'image_emb': tensor([[-0.0837,  0.4180,  0.1481,  ...,  1.2324,  0.0328, -0.0609],\n",
       "          [-0.1328,  0.5312,  0.2172,  ...,  1.0693, -0.3208, -0.0689],\n",
       "          [-0.4224,  0.2793,  0.2839,  ...,  0.6729,  0.0820, -0.0760]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0198,  0.2284, -0.0664,  ...,  0.3206, -0.4744, -0.4038],\n",
       "          [-0.3530,  0.2083,  0.0661,  ..., -0.3638, -0.3118, -0.2472]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0709, 0.5747, 0.3542],\n",
       "          [0.1699, 0.2083, 0.6216]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0    12.824860  367.134644  296.501740  631.755493    0.915183     59      bed\n",
       "  1   114.595505  135.245605  460.128235  309.627197    0.892739     59      bed\n",
       "  2   235.548035  499.990509  450.984436  633.564331    0.696492     59      bed\n",
       "  3    72.626968  245.001221   97.611160  278.335449    0.570440     73     book\n",
       "  4    59.061951  195.679611   97.722168  211.701096    0.481352     73     book\n",
       "  5    24.948730  189.843994   61.384918  206.079285    0.456709     73     book\n",
       "  6    26.291054  197.989670   61.781372  210.883560    0.423978     73     book\n",
       "  7    23.056633  235.799149   62.005905  248.325455    0.422970     73     book\n",
       "  8    61.726273  220.545853   90.362747  234.185043    0.422333     73     book\n",
       "  9    41.130798  250.085342   54.754578  285.682129    0.403966     73     book\n",
       "  10   26.742889  220.613861   60.669800  235.435303    0.395921     73     book\n",
       "  11   64.805954  248.028412   87.313705  280.241913    0.380129     73     book\n",
       "  12  383.784058  323.787567  454.878113  428.916168    0.374287     26  handbag\n",
       "  13   28.312622  212.341583   60.417877  225.532135    0.358527     73     book\n",
       "  14   27.598694  205.575317   61.732666  219.972778    0.356330     73     book\n",
       "  15   61.095001  203.954285   95.989044  217.120911    0.354101     73     book\n",
       "  16   54.291351  250.028870   68.997314  282.378723    0.347120     73     book,\n",
       "  'caption': ['a dark wooden bed whith a white and beige checkered comforter in a white-walled room',\n",
       "   'An empty made bed.'],\n",
       "  'bbox_target': [113.72, 133.69, 346.61, 179.86]},\n",
       " 671: {'image_emb': tensor([[ 0.0925,  0.3098, -0.0182,  ...,  0.6401,  0.1284, -0.2482],\n",
       "          [ 0.2274,  0.3787, -0.0780,  ...,  0.6724,  0.0499, -0.1918],\n",
       "          [-0.2583,  0.3191, -0.0646,  ...,  0.6753, -0.0076, -0.0827],\n",
       "          [-0.0732,  0.0475, -0.1875,  ...,  0.7671,  0.1571,  0.0247],\n",
       "          [-0.0815, -0.0496, -0.1245,  ...,  0.0427,  0.0443, -0.0944]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 3.0859e-01,  3.5083e-01, -4.1162e-01,  2.7368e-01,  6.2927e-02,\n",
       "           -4.4727e-01, -1.4368e-01, -1.4819e-01, -2.5854e-01,  2.5610e-01,\n",
       "           -1.8628e-01, -9.8022e-02,  3.8965e-01,  1.3745e-01, -1.2140e-01,\n",
       "           -3.4576e-02,  3.2532e-02, -2.2534e-01, -9.4971e-02,  8.2703e-02,\n",
       "            1.1243e-01,  2.8296e-01, -1.2164e-01,  1.8909e-01,  2.2168e-01,\n",
       "           -1.5149e-01, -1.3269e-01, -2.1179e-02,  4.8370e-02, -1.1389e-01,\n",
       "           -2.0239e-01, -3.4155e-01,  2.4976e-01,  1.9678e-01,  4.3335e-01,\n",
       "           -2.3999e-01, -4.3848e-01, -1.3538e-01, -6.1426e-01,  1.4856e-01,\n",
       "            4.7363e-01, -3.9038e-01, -6.1676e-02,  3.2593e-01,  5.9631e-02,\n",
       "            2.8979e-01,  1.9653e-01, -1.4050e-01, -8.3557e-02, -1.8713e-01,\n",
       "            1.3782e-01, -2.0508e-01,  1.2634e-01, -2.3352e-01, -4.1968e-01,\n",
       "           -1.0455e-01,  5.0098e-01,  2.5558e-02,  2.1777e-01,  2.1863e-01,\n",
       "            6.0645e-01, -1.6943e-01,  1.9360e-01,  2.5497e-02, -7.7881e-02,\n",
       "           -3.7085e-01, -1.4954e-01,  7.2314e-01,  2.2339e-01, -1.8726e-01,\n",
       "            2.6709e-01, -1.1322e-01,  3.2104e-01,  3.4570e-01,  2.9248e-01,\n",
       "           -5.5420e-02, -1.1566e-01,  2.0166e-01, -1.4001e-01, -1.0608e-01,\n",
       "           -1.7871e-01, -1.6846e-01, -1.9348e-01,  4.9976e-01,  3.3521e-01,\n",
       "           -1.6296e-01,  1.3232e-01, -2.2107e-01,  1.3171e-01,  2.0447e-01,\n",
       "            3.2471e-01, -6.1963e-01, -9.7607e-01, -4.1382e-01,  1.5930e-01,\n",
       "            1.0461e-01, -2.3364e-01, -1.3745e-01,  6.9946e-02,  1.0822e-01,\n",
       "            1.0643e-02,  3.2080e-01,  7.8369e-02,  2.1362e-02,  3.0347e-01,\n",
       "           -1.4099e-01, -1.0486e-01,  7.1777e-02, -1.1890e-01, -3.9941e-01,\n",
       "           -3.5742e-01, -3.4106e-01,  2.8259e-02,  1.3501e-01, -2.2180e-01,\n",
       "           -1.9458e-01, -5.0000e-01,  3.1274e-01, -2.1741e-01,  2.2693e-01,\n",
       "            5.9143e-02, -3.5864e-01, -1.6467e-01, -3.0957e-01, -5.0568e-02,\n",
       "           -3.0200e-01,  2.3239e-02,  2.3840e-01,  1.6809e-01,  1.3391e-01,\n",
       "            2.0728e-01,  3.4790e-01,  1.7053e-01,  1.0107e+00, -1.3184e-01,\n",
       "            8.0505e-02, -5.4443e-01, -5.1904e-01,  2.7661e-01, -3.6572e-01,\n",
       "           -7.3975e-02,  1.6150e-01, -6.8115e-01,  1.8604e-01,  6.8359e-02,\n",
       "            1.2756e-01,  3.0444e-01, -1.8799e-01, -1.9397e-01,  9.6008e-02,\n",
       "            7.7576e-02, -7.0374e-02,  4.0112e-01, -6.1475e-01, -1.9031e-01,\n",
       "            4.2676e-01, -2.1057e-01, -1.0132e-01, -6.0449e-01,  6.3110e-02,\n",
       "            1.6138e-01, -1.1725e-01,  4.6875e-02,  1.5991e-01,  3.0737e-01,\n",
       "            4.7333e-02,  5.7861e-01,  1.0181e-01,  2.3804e-02,  3.3154e-01,\n",
       "            1.9397e-01,  4.3121e-02, -8.9722e-02,  1.0553e-01, -1.4612e-01,\n",
       "            7.9102e-01,  2.4768e-01,  3.9337e-02, -5.7129e-02, -3.5913e-01,\n",
       "           -2.2900e-01,  1.5271e-01,  5.7129e-02, -3.7427e-01, -5.2686e-01,\n",
       "            2.9443e-01, -1.2079e-01, -2.1759e-02,  2.2021e-01,  6.0645e-01,\n",
       "            3.6499e-01,  1.9849e-01, -2.1765e-01, -1.3489e-01,  1.0126e-01,\n",
       "           -5.7373e-01, -1.3443e-02,  4.0967e-01,  2.8662e-01,  1.9824e-01,\n",
       "           -1.7700e-01,  3.8477e-01, -3.1445e-01,  5.1318e-01, -3.8818e-01,\n",
       "           -5.2490e-02,  3.9355e-01, -5.5481e-02,  4.0308e-01,  1.1841e-01,\n",
       "           -1.6235e-01,  2.7344e-01, -3.7695e-01, -2.0251e-01, -8.5632e-02,\n",
       "           -1.1743e-01,  1.7371e-01,  2.8503e-02, -7.1960e-02, -5.4736e-01,\n",
       "            1.4075e-01, -3.0835e-01, -2.9980e-01, -9.1675e-02, -3.6670e-01,\n",
       "           -6.1083e-04, -2.6782e-01,  2.1277e-01,  2.6514e-01, -3.2617e-01,\n",
       "            2.6782e-01, -7.0984e-02,  3.5596e-01,  1.1346e-01,  2.5854e-01,\n",
       "           -2.5220e-01,  1.5332e-01, -2.8503e-02, -4.7699e-02, -4.0796e-01,\n",
       "            5.9357e-02,  1.9470e-01, -3.0005e-01, -8.3374e-02, -1.4185e-01,\n",
       "            7.8552e-02,  1.2610e-01,  2.9175e-01, -1.5662e-01,  3.5034e-01,\n",
       "           -6.1676e-02,  6.1188e-02, -1.8970e-01,  2.7856e-01, -4.0137e-01,\n",
       "           -2.1826e-01,  3.5126e-02, -2.3926e-01,  1.0004e-01,  5.5127e-01,\n",
       "            1.6528e-01, -7.8796e-02, -8.3435e-02, -1.6785e-01,  2.0447e-01,\n",
       "           -1.0760e-01,  6.7810e-02, -4.0576e-01, -9.2896e-02,  2.9541e-01,\n",
       "           -1.5942e-01,  2.2595e-01,  4.3555e-01, -2.1790e-01, -1.1652e-01,\n",
       "            3.7378e-01,  9.3079e-03,  5.4718e-02, -1.2672e-02, -3.6646e-01,\n",
       "            2.0386e-01, -7.2083e-02, -2.9834e-01,  7.7026e-02,  3.1586e-02,\n",
       "            5.4053e-01, -2.5903e-01, -1.9495e-01,  3.4277e-01, -3.6060e-01,\n",
       "            6.1279e-02,  2.2021e-01, -1.4355e-01, -2.0862e-01,  1.3745e-01,\n",
       "            2.5024e-01, -2.4292e-01,  2.8345e-01, -9.7122e-03, -3.9819e-01,\n",
       "           -1.0199e-01,  1.4519e-02, -3.1958e-01,  7.6538e-02, -4.0918e-01,\n",
       "           -2.6831e-01,  5.3174e-01,  2.4780e-01,  5.8740e-01,  1.8921e-01,\n",
       "           -8.1543e-02,  2.1387e-01,  1.0137e+00,  1.4014e-01,  2.0935e-01,\n",
       "            4.2993e-01,  1.6211e-01, -4.0674e-01, -2.8027e-01, -2.2705e-02,\n",
       "           -2.1887e-01, -2.1960e-01, -2.2803e-01,  5.3650e-02,  8.5266e-02,\n",
       "           -1.2903e-01, -1.6028e-01,  2.5940e-02,  2.1252e-01, -9.1699e-01,\n",
       "            2.3914e-01,  3.8916e-01,  5.1953e-01, -3.5889e-01, -1.7017e-01,\n",
       "           -2.5903e-01, -6.7261e-02,  4.9820e-03, -1.7554e-01,  6.6943e-01,\n",
       "            2.0862e-01, -1.7993e-01,  2.1021e-01,  7.9407e-02,  3.6035e-01,\n",
       "            3.0054e-01,  3.6816e-01,  2.9028e-01,  4.8901e-01, -2.5464e-01,\n",
       "            3.8403e-01,  4.8169e-01,  2.0813e-01,  2.1912e-01, -4.4037e-02,\n",
       "           -5.7324e-01,  1.7029e-01,  2.0874e-01,  2.8580e-02, -3.6285e-02,\n",
       "            4.1840e-02, -3.1372e-01,  4.1577e-01,  1.1475e-01,  1.0364e-01,\n",
       "            1.8234e-02, -1.8567e-01, -1.5625e-02,  5.7275e-01,  5.9082e-01,\n",
       "           -3.6816e-01, -8.7158e-02,  7.0435e-02,  1.2817e-01,  4.5264e-01,\n",
       "            1.5366e-02,  3.1958e-01,  1.7456e-02, -4.9561e-01,  2.8027e-01,\n",
       "            7.0740e-02,  1.8646e-02, -9.9121e-02, -2.6392e-01, -2.3254e-02,\n",
       "            3.2349e-01,  1.5564e-01,  1.2299e-01, -2.9346e-01,  2.4243e-01,\n",
       "           -7.9932e-01, -3.8745e-01, -3.3521e-01, -2.0081e-01,  3.4180e-02,\n",
       "            4.1040e-01,  3.2935e-01,  2.8320e-01, -2.5940e-02,  8.6060e-02,\n",
       "            2.3767e-01, -2.1826e-01, -1.6431e-01, -2.2046e-01, -1.2535e-02,\n",
       "           -2.7222e-01,  3.5547e-01, -5.1025e-01, -2.3743e-01, -7.4524e-02,\n",
       "           -2.3181e-01,  3.9581e-02, -1.7126e-01,  2.2974e-01, -9.3445e-02,\n",
       "           -1.7212e-01,  2.1454e-02,  1.8616e-01, -2.2607e-01, -3.5693e-01,\n",
       "            2.1957e-02, -4.4946e-01, -8.0566e-02, -5.5518e-01,  3.9648e-01,\n",
       "           -1.7419e-01,  7.3181e-02,  2.8174e-01,  2.5513e-01, -3.4210e-02,\n",
       "           -2.6147e-01, -1.0156e-01,  1.2659e-01,  6.7322e-02, -1.1517e-01,\n",
       "           -2.7466e-01,  3.1616e-02, -8.7708e-02, -4.9170e-01,  2.1277e-01,\n",
       "            7.2205e-02, -5.7129e-02,  5.5811e-01,  1.8616e-01,  2.7466e-02,\n",
       "            4.0527e-01, -2.9053e-01, -2.3389e-01, -1.6443e-01,  2.7759e-01,\n",
       "           -1.7200e-01, -1.5686e-01, -1.9189e-01, -4.3652e-01, -2.2168e-01,\n",
       "            7.2510e-02, -7.4341e-02, -1.4209e-01,  3.4033e-01, -2.3193e-01,\n",
       "            7.1350e-02, -1.8909e-01,  1.3367e-01,  2.8906e-01,  4.0796e-01,\n",
       "           -2.1204e-01,  4.5996e-01, -4.7803e-01, -3.5376e-01, -3.6157e-01,\n",
       "            1.6373e-02, -2.0422e-01,  8.9172e-02,  1.3538e-01,  2.8589e-01,\n",
       "            2.0068e-01,  3.7476e-01, -1.7786e-01, -8.9844e-02,  6.6504e-01,\n",
       "           -1.0590e-01, -3.4204e-01, -2.0300e-01,  1.7126e-01, -9.8145e-02,\n",
       "           -7.0534e-03,  3.0127e-01, -9.6130e-02, -1.6797e-01, -1.0858e-01,\n",
       "            1.2219e-01,  4.5312e-01,  1.0803e-01,  6.3184e-01,  5.6445e-01,\n",
       "            4.7437e-01,  2.1008e-01, -1.2561e-01, -4.2261e-01,  2.9834e-01,\n",
       "            1.3269e-01, -1.3757e-01, -6.8420e-02,  4.9414e-01,  4.6436e-01,\n",
       "           -1.1377e-01, -2.2583e-01, -6.8420e-02, -1.1615e-01, -1.8628e-01,\n",
       "            2.1936e-01, -4.1553e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.0117e-01, 2.9224e-01, 2.5845e-04, 1.8609e-04, 5.9738e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  327.882324   44.439804  525.936768  253.217224    0.948949     25  umbrella\n",
       "  1  117.052490   39.119614  346.919861  256.577271    0.947081     25  umbrella\n",
       "  2  352.721222  236.356812  452.715485  378.341675    0.882839      0    person\n",
       "  3  181.355438  253.816620  261.654449  377.184235    0.861155      0    person,\n",
       "  'caption': ['A yellow umbrella being held by a person in a black shirt with white dots'],\n",
       "  'bbox_target': [117.47, 35.58, 229.22, 224.43]},\n",
       " 672: {'image_emb': tensor([[-0.0366,  0.1121, -0.1245,  ...,  0.8389,  0.2196, -0.2255],\n",
       "          [ 0.1064,  0.5249, -0.2896,  ...,  1.1484,  0.2974, -0.1534],\n",
       "          [ 0.0391, -0.0403, -0.0132,  ...,  0.3645,  0.3381, -0.0645]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0651,  0.5366, -0.5493,  ..., -0.0034,  0.1633, -0.5449],\n",
       "          [-0.2020,  0.3474, -0.3135,  ...,  0.0519,  0.2949, -0.4807]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0319, 0.6704, 0.2976],\n",
       "          [0.0176, 0.8462, 0.1360]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  name\n",
       "  0  149.795883  292.814087  225.350021  465.279785    0.912104     75  vase\n",
       "  1  255.679276  299.239960  342.756226  499.007904    0.910877     75  vase,\n",
       "  'caption': ['a glass vase on the left side of two others on top of a table',\n",
       "   'A crystal vase to the left of two other vases'],\n",
       "  'bbox_target': [151.98, 293.47, 74.87, 173.46]},\n",
       " 673: {'image_emb': tensor([[ 0.4043,  0.1324,  0.0222,  ...,  0.9360, -0.0511,  0.0136],\n",
       "          [ 0.1445,  0.1210, -0.2546,  ...,  0.7432, -0.1065, -0.0573],\n",
       "          [-0.2737,  0.7896, -0.3269,  ...,  0.8735, -0.0674,  0.1120],\n",
       "          [ 0.4705,  0.0071,  0.0407,  ...,  0.6416, -0.1741, -0.0952]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1055,  0.0513,  0.3684,  ..., -0.0561, -0.1646, -0.0126],\n",
       "          [ 0.0918,  0.0288,  0.0388,  ..., -0.0994, -0.8286, -0.3242]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.5528e-02, 1.1804e-01, 8.0664e-01, 4.9957e-02],\n",
       "          [4.9347e-02, 9.1650e-01, 1.7798e-04, 3.3905e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   63.601475  193.573196  258.751740  445.533234    0.881419     48   \n",
       "  1  171.981567   83.651772  331.994110  458.857971    0.863502     48   \n",
       "  2  200.873093   31.486303  332.203583   93.585655    0.857230     45   \n",
       "  3   31.477695   49.124573  133.981140  151.474243    0.681766     40   \n",
       "  4  165.212936  102.951660  289.565155  204.757782    0.547729     48   \n",
       "  5    0.629872    4.344296  331.689270  486.862762    0.394866     60   \n",
       "  \n",
       "             name  \n",
       "  0      sandwich  \n",
       "  1      sandwich  \n",
       "  2          bowl  \n",
       "  3    wine glass  \n",
       "  4      sandwich  \n",
       "  5  dining table  ,\n",
       "  'caption': ['eatables in the plate', 'Half a sandwich lying flat'],\n",
       "  'bbox_target': [66.44, 192.0, 192.57, 252.26]},\n",
       " 674: {'image_emb': tensor([[ 0.0424,  0.0923,  0.0763,  ...,  0.8887,  0.0994,  0.1666],\n",
       "          [-0.1598, -0.1937, -0.1021,  ...,  0.6611,  0.3879, -0.1348],\n",
       "          [ 0.1469, -0.2654, -0.1841,  ...,  0.3496, -0.1047,  0.2524]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0961,  0.0054, -0.2010,  ...,  0.0565, -0.1539, -0.0140],\n",
       "          [ 0.0839, -0.0818, -0.1136,  ...,  0.1189, -0.2297, -0.2150]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1688, 0.6274, 0.2037],\n",
       "          [0.2323, 0.4692, 0.2983]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  251.323349  254.761841  539.441467  386.404297    0.933382     20  elephant\n",
       "  1   81.905876  254.260223  297.800201  370.967255    0.888873     20  elephant,\n",
       "  'caption': ['A small elephant with its trunk behind another elephant',\n",
       "   'An elephant from the left kissing another elephant'],\n",
       "  'bbox_target': [81.99, 255.7, 226.29, 125.18]},\n",
       " 675: {'image_emb': tensor([[ 0.5020,  0.1663,  0.2861,  ...,  0.3215,  0.0049,  0.3931],\n",
       "          [-0.2646,  0.1935,  0.2905,  ...,  0.1838,  0.1819,  0.2976],\n",
       "          [ 0.0128,  0.6797,  0.0052,  ...,  0.2969,  0.1841,  0.5410],\n",
       "          ...,\n",
       "          [-0.2078,  0.0996, -0.0371,  ...,  0.7783,  0.0798,  0.2028],\n",
       "          [ 0.0941,  0.0464, -0.2732,  ...,  0.4497,  0.2695,  0.0321],\n",
       "          [-0.0364,  0.1099,  0.1300,  ...,  0.9614,  0.1866,  0.4832]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1044,  0.1530,  0.1465,  ...,  0.0012, -0.0807, -0.1088],\n",
       "          [-0.0407,  0.2834, -0.0537,  ..., -0.0259,  0.1234, -0.3240]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.1064e-01, 8.6188e-05, 6.5565e-07, 8.8745e-02, 3.2520e-04, 7.3671e-05,\n",
       "           1.5354e-04],\n",
       "          [8.1006e-01, 1.4544e-04, 2.3842e-07, 1.8359e-01, 5.5466e-03, 1.7548e-04,\n",
       "           3.1281e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    1.591829  316.110016  315.144714  505.392517    0.920679     53   \n",
       "  1  283.190582  207.834518  415.810608  401.607513    0.913730     41   \n",
       "  2  290.096130  120.146568  385.636871  219.174347    0.886436     41   \n",
       "  3  395.385529  271.667603  611.185303  401.756287    0.884479     53   \n",
       "  4  402.563141    0.263167  611.227539  265.238129    0.850786      0   \n",
       "  5   16.105698    0.048144  172.482132  243.377609    0.826581      0   \n",
       "  6    2.000864  154.602478  611.063477  608.529480    0.695989     60   \n",
       "  7  244.382050    0.828146  413.070435  199.344528    0.277780      0   \n",
       "  \n",
       "             name  \n",
       "  0         pizza  \n",
       "  1           cup  \n",
       "  2           cup  \n",
       "  3         pizza  \n",
       "  4        person  \n",
       "  5        person  \n",
       "  6  dining table  \n",
       "  7        person  ,\n",
       "  'caption': ['A slice of pepperoni pizza.',\n",
       "   \"A slice of pepperoni pizza in front of a person's hands.\"],\n",
       "  'bbox_target': [394.22, 271.54, 217.78, 130.95]},\n",
       " 676: {'image_emb': tensor([[ 0.0152, -0.1703, -0.0685,  ...,  0.9985,  0.5620, -0.2271],\n",
       "          [-0.4209,  0.1100, -0.1641,  ...,  0.8906,  0.4294, -0.2842],\n",
       "          [ 0.1949,  0.1439, -0.4382,  ...,  0.7100,  0.5903, -0.1213],\n",
       "          [ 0.0537,  0.2825, -0.0737,  ...,  0.9077,  0.1294,  0.2915],\n",
       "          [-0.1829,  0.0361, -0.0802,  ...,  0.8828,  0.3521, -0.5151],\n",
       "          [ 0.1964, -0.2412, -0.2233,  ...,  0.8037,  0.5835, -0.1175]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3926, -0.2607, -0.2632,  ..., -0.0718, -0.0647, -0.0042],\n",
       "          [-0.2747,  0.0076, -0.1074,  ..., -0.2788, -0.4529, -0.0512]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.9569e-03, 6.8604e-01, 1.6040e-01, 7.8186e-02, 6.7932e-02, 5.3215e-03],\n",
       "          [7.3385e-04, 5.3925e-02, 8.0469e-01, 9.9716e-03, 1.2939e-01, 1.1911e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  339.973389   14.097672  640.000000  423.481812    0.955212      0  person\n",
       "  1    0.494843  125.197815  294.977478  422.853943    0.949140      0  person\n",
       "  2  203.360489   61.526733  383.961914  424.601501    0.905510      0  person\n",
       "  3  355.148621  337.119965  446.627869  425.671021    0.900924     46  banana\n",
       "  4  349.500854  142.621124  448.499573  269.572113    0.745129     56   chair,\n",
       "  'caption': ['the little girl wearing a bow in her hair',\n",
       "   'A girl with a bow in her hair.'],\n",
       "  'bbox_target': [0.0, 126.95, 295.25, 300.05]},\n",
       " 677: {'image_emb': tensor([[-0.0575,  0.3699, -0.3262,  ...,  0.3574,  0.0357, -0.1698],\n",
       "          [-0.1935,  0.1984, -0.3054,  ...,  0.5845,  0.3735, -0.0081],\n",
       "          [ 0.1831,  0.2096, -0.2296,  ...,  0.5415,  0.4241,  0.5029],\n",
       "          [-0.0240, -0.0471, -0.1409,  ...,  0.3821, -0.1432, -0.0917]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3540,  0.3098, -0.2358,  ...,  0.1021, -0.7925, -0.1864],\n",
       "          [ 0.1425,  0.3154, -0.3369,  ...,  0.0262, -0.3076,  0.1075]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5933, 0.1409, 0.0026, 0.2632],\n",
       "          [0.0515, 0.0507, 0.8438, 0.0540]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0    2.289368   36.790359  639.441589  420.974182    0.900146     57  couch\n",
       "  1   25.837967  124.085724  285.144775  299.738129    0.897438     16    dog\n",
       "  2  411.704926  161.479889  605.799927  315.787628    0.832795     16    dog,\n",
       "  'caption': ['dog in dark dog bed', 'A brownish dogs showing its teeth.'],\n",
       "  'bbox_target': [409.88, 162.23, 203.64, 152.73]},\n",
       " 678: {'image_emb': tensor([[ 0.0696,  0.4626,  0.1902,  ...,  1.1191,  0.1085, -0.4978],\n",
       "          [ 0.3899, -0.2905, -0.4287,  ...,  1.3184, -0.0911, -0.0328],\n",
       "          [ 0.3918,  0.1084,  0.2771,  ...,  0.7773,  0.1796, -0.1479]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1373,  0.3765, -0.0834,  ...,  0.2006,  0.2935, -0.3945],\n",
       "          [-0.0978,  0.3438, -0.1560,  ...,  0.1736, -0.0127, -0.3918]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.1997e-01, 3.1590e-06, 7.7979e-01],\n",
       "          [1.9934e-01, 5.6624e-06, 8.0078e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  191.334290   52.263813  298.919403  236.960724    0.932423      0   \n",
       "  1  410.907349  125.581810  436.505890  138.504761    0.792170      0   \n",
       "  2   20.882988  204.498795  233.639343  250.691345    0.649071     37   \n",
       "  3  123.202347  113.828682  236.628860  176.657608    0.606490      0   \n",
       "  4  157.132904  150.494308  245.765350  187.159653    0.343896     37   \n",
       "  5  106.579460  134.702301  248.060516  186.527969    0.339786     37   \n",
       "  6  118.720268  115.839005  174.842789  140.934464    0.310271      0   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2  surfboard  \n",
       "  3     person  \n",
       "  4  surfboard  \n",
       "  5  surfboard  \n",
       "  6     person  ,\n",
       "  'caption': ['A teenage male riding a surfboard on his stomach.',\n",
       "   'A man lying down on a surf board.'],\n",
       "  'bbox_target': [114.43, 114.43, 127.22, 67.0]},\n",
       " 679: {'image_emb': tensor([[ 0.1902,  0.0117, -0.2849,  ...,  0.8716,  0.6807,  0.2991],\n",
       "          [ 0.1703,  0.1322, -0.3442,  ...,  0.7124,  0.1816,  0.1246],\n",
       "          [ 0.1997,  0.0388, -0.2524,  ...,  0.8589,  0.0152,  0.3757],\n",
       "          ...,\n",
       "          [ 0.2783,  0.0796,  0.1722,  ...,  1.1436,  0.2195, -0.0936],\n",
       "          [ 0.0703,  0.1831, -0.3232,  ...,  1.3818,  0.2671,  0.1182],\n",
       "          [-0.0263,  0.2603, -0.1798,  ...,  1.0928,  0.3562, -0.0059]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3367,  0.0714, -0.2167,  ..., -0.1987, -0.1361, -0.5894],\n",
       "          [ 0.1121,  0.0691, -0.2300,  ...,  0.1903, -0.2563, -0.5269]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.4703e-05, 5.4836e-06, 9.8438e-01, 1.5419e-02, 8.0943e-05, 5.9605e-08,\n",
       "           2.9743e-05, 5.9605e-07],\n",
       "          [5.9601e-02, 1.1435e-03, 7.9736e-01, 7.8918e-02, 4.1595e-02, 3.8700e-03,\n",
       "           1.5068e-02, 2.5368e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   450.066010  182.670715  574.092529  353.579346    0.912783      0   \n",
       "  1   384.725983  164.550049  467.257416  258.072266    0.880870      0   \n",
       "  2   144.543182  179.740234  246.671051  375.408142    0.871079      0   \n",
       "  3   337.572327  174.574097  392.188110  253.123047    0.825208      0   \n",
       "  4    92.702766  298.001984  246.300903  392.806976    0.801982     56   \n",
       "  5   537.542297  131.181824  639.450745  187.199219    0.779228      8   \n",
       "  6   502.657288  258.436951  594.503723  370.120972    0.775238     56   \n",
       "  7   301.468689  229.997772  313.888489  268.841583    0.694852     40   \n",
       "  8   448.831116  141.017349  539.795654  184.399139    0.649516      8   \n",
       "  9   395.426025  243.557739  407.183533  269.430420    0.627042     41   \n",
       "  10  211.803436  174.523315  281.190979  256.976624    0.621163      0   \n",
       "  11  367.727997  238.088409  383.373505  262.551971    0.595986     41   \n",
       "  12  232.188721  148.258133  302.138733  189.721954    0.590746      8   \n",
       "  13  337.527130  229.031311  352.065155  271.450195    0.555758     40   \n",
       "  14    9.639709  170.171204  629.340576  422.126526    0.533248      8   \n",
       "  15   87.859940  143.138550  194.613907  186.413452    0.491446      8   \n",
       "  16  193.385681  166.067078  235.436890  241.914856    0.468624      0   \n",
       "  17  448.024536  176.902405  522.475952  299.341064    0.467268      0   \n",
       "  18  324.518127  235.444855  339.051331  271.002960    0.455152     40   \n",
       "  19  285.402771  175.548187  340.102966  260.913849    0.445180      0   \n",
       "  20  257.195435  321.924194  391.429077  375.527893    0.428287     56   \n",
       "  21  278.325226  259.989502  292.613251  276.019836    0.406690     41   \n",
       "  22  255.902908  261.208405  269.879883  276.683990    0.400324     41   \n",
       "  23  232.503021  202.918121  274.260468  257.371429    0.378806      0   \n",
       "  24  229.413879  224.448212  468.497131  291.424225    0.315457     60   \n",
       "  25  352.682678  245.146423  365.861389  261.765991    0.311665     41   \n",
       "  26  247.800400  241.646637  258.451263  268.462860    0.307393     41   \n",
       "  27  258.658142  239.061066  271.983765  263.830841    0.303186     41   \n",
       "  28  539.001587  344.298401  639.913208  424.369690    0.302655     28   \n",
       "  29  270.315155  237.586792  282.286407  262.985168    0.292860     41   \n",
       "  30  248.045532  242.346558  259.169281  269.190552    0.278881     39   \n",
       "  31  344.617004  148.762054  412.221802  183.771149    0.276781      8   \n",
       "  32  305.032135  151.574310  349.849091  179.113800    0.272714      8   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3         person  \n",
       "  4          chair  \n",
       "  5           boat  \n",
       "  6          chair  \n",
       "  7     wine glass  \n",
       "  8           boat  \n",
       "  9            cup  \n",
       "  10        person  \n",
       "  11           cup  \n",
       "  12          boat  \n",
       "  13    wine glass  \n",
       "  14          boat  \n",
       "  15          boat  \n",
       "  16        person  \n",
       "  17        person  \n",
       "  18    wine glass  \n",
       "  19        person  \n",
       "  20         chair  \n",
       "  21           cup  \n",
       "  22           cup  \n",
       "  23        person  \n",
       "  24  dining table  \n",
       "  25           cup  \n",
       "  26           cup  \n",
       "  27           cup  \n",
       "  28      suitcase  \n",
       "  29           cup  \n",
       "  30        bottle  \n",
       "  31          boat  \n",
       "  32          boat  ,\n",
       "  'caption': ['Woman in a floral dress, wearing a captains hat',\n",
       "   'lady left with sailor hat'],\n",
       "  'bbox_target': [142.06, 179.62, 105.35, 173.17]},\n",
       " 680: {'image_emb': tensor([[-0.1747,  0.1576, -0.1479,  ...,  1.1006,  0.0324,  0.0651],\n",
       "          [-0.0632,  0.5288,  0.0344,  ...,  1.3496,  0.1525, -0.0054],\n",
       "          [-0.0269,  0.2039, -0.1157,  ...,  0.8013, -0.0924,  0.3677],\n",
       "          [-0.3733,  0.3372, -0.1182,  ...,  1.0742, -0.0052,  0.3066],\n",
       "          [-0.1954,  0.4185, -0.2244,  ...,  0.9067,  0.2438,  0.0038],\n",
       "          [-0.0401, -0.1133, -0.0397,  ...,  1.1797,  0.0337, -0.0711]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0394,  0.0214, -0.5991,  ..., -0.0490,  0.2086, -0.1628],\n",
       "          [ 0.1730,  0.0372, -0.1302,  ..., -0.2739,  0.1359,  0.0067]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.5556e-02, 7.1621e-04, 2.2185e-04, 6.4453e-02, 9.1797e-01, 8.3733e-04],\n",
       "          [1.5656e-02, 1.2074e-03, 2.6941e-04, 4.3213e-02, 9.3848e-01, 9.4032e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     0.709198  151.856583  547.283813  345.318298    0.904431     57   \n",
       "  1   328.730927  179.737061  409.756439  251.803650    0.875318     63   \n",
       "  2   496.009460  215.977692  640.000000  479.996948    0.868288     56   \n",
       "  3   291.713562  234.809540  425.456543  346.384491    0.800934     60   \n",
       "  4   109.831833  270.910583  275.201111  409.506531    0.790703     60   \n",
       "  5    62.376225   22.810410   77.184196   73.476547    0.645944     39   \n",
       "  6   144.385452  287.285034  225.543015  314.016296    0.555621     73   \n",
       "  7   163.824310   90.864975  174.623749  111.701797    0.455562     39   \n",
       "  8   552.679688  312.394226  638.937134  379.607178    0.389717     15   \n",
       "  9   578.453308  274.331543  607.532776  291.262512    0.378514     73   \n",
       "  10  130.902954  270.131317  174.133667  295.589264    0.368734     65   \n",
       "  11  604.688293  128.072769  617.654236  158.766373    0.335712     73   \n",
       "  12  612.161621  127.950302  626.817383  159.324020    0.325440     73   \n",
       "  13  148.080627   89.534775  157.439728  112.473526    0.296081     39   \n",
       "  14  600.506836  127.739960  611.288574  158.578949    0.280547     73   \n",
       "  15  630.780151  129.940933  639.578613  159.706650    0.269580     73   \n",
       "  \n",
       "              name  \n",
       "  0          couch  \n",
       "  1         laptop  \n",
       "  2          chair  \n",
       "  3   dining table  \n",
       "  4   dining table  \n",
       "  5         bottle  \n",
       "  6           book  \n",
       "  7         bottle  \n",
       "  8            cat  \n",
       "  9           book  \n",
       "  10        remote  \n",
       "  11          book  \n",
       "  12          book  \n",
       "  13        bottle  \n",
       "  14          book  \n",
       "  15          book  ,\n",
       "  'caption': ['Table above yellow box.',\n",
       "   'A table have a yellow box under it.'],\n",
       "  'bbox_target': [108.56, 266.43, 165.76, 142.3]},\n",
       " 681: {'image_emb': tensor([[ 1.8530e-01, -8.3923e-02, -2.3975e-01,  ...,  1.9849e-01,\n",
       "            2.0166e-01,  3.0884e-01],\n",
       "          [-1.7529e-01,  2.0349e-01, -4.2871e-01,  ...,  3.3325e-01,\n",
       "           -2.2852e-01, -3.4332e-04],\n",
       "          [ 1.0071e-03,  3.2178e-01, -2.2278e-01,  ...,  1.2432e+00,\n",
       "           -1.3977e-01, -3.0670e-02],\n",
       "          [ 4.5349e-02,  3.0957e-01, -1.6650e-01,  ...,  6.7480e-01,\n",
       "           -4.9072e-01,  5.2673e-02],\n",
       "          [ 3.4961e-01, -2.5537e-01, -6.7444e-02,  ..., -1.6125e-01,\n",
       "           -3.5645e-02,  4.3115e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0049, -0.0219, -0.2795,  ...,  0.2046, -0.0928, -0.2139],\n",
       "          [-0.0465, -0.3193, -0.0337,  ..., -0.0672, -0.5366, -0.3801]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.5276e-02, 4.3440e-04, 8.5533e-05, 1.8680e-04, 9.0381e-01],\n",
       "          [3.1250e-02, 4.0527e-01, 3.5205e-01, 1.2360e-01, 8.7646e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  146.560547   59.053284  456.301270  497.853271    0.941592     20  elephant\n",
       "  1    0.608109  371.803680  109.223312  578.674316    0.915168      0    person\n",
       "  2  123.016922  385.347534  265.870789  631.451721    0.876769      0    person\n",
       "  3    1.581863  329.199890  119.844986  460.365417    0.853079      0    person\n",
       "  4  215.285583  412.825531  309.887451  639.106384    0.655654     24  backpack\n",
       "  5    0.000000  355.159241   59.124557  417.436462    0.547098     24  backpack\n",
       "  6    0.000000  408.194000   47.260521  522.756470    0.388723     24  backpack\n",
       "  7    0.000000  409.531921   49.481079  519.896423    0.294255     26   handbag,\n",
       "  'caption': ['A lady with cap, reading the details about the elephant statue',\n",
       "   'woman in brown hat kneeling down'],\n",
       "  'bbox_target': [0.0, 373.61, 110.12, 204.01]},\n",
       " 682: {'image_emb': tensor([[ 0.5688,  0.6182, -0.2047,  ...,  1.3154,  0.1151,  0.4177],\n",
       "          [ 0.4470,  0.5498,  0.0244,  ...,  1.1904,  0.0898,  0.3533],\n",
       "          [ 0.3489,  0.4451, -0.1790,  ...,  0.5283, -0.1560,  0.1079],\n",
       "          [ 0.3660, -0.1215, -0.2114,  ...,  0.9370,  0.0699,  0.2173],\n",
       "          [ 0.6440,  0.0163, -0.4480,  ...,  0.8267, -0.1531,  0.0795],\n",
       "          [ 0.3245, -0.2100,  0.2979,  ...,  0.3958,  0.0506,  0.1392]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0887, -0.1548,  0.1455,  ...,  0.1776, -0.1896,  0.1614],\n",
       "          [-0.0744,  0.1090,  0.0082,  ..., -0.0930, -0.2024,  0.0164]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.7168e-01, 2.9964e-03, 6.3181e-05, 1.6041e-03, 2.3575e-02, 1.5163e-04],\n",
       "          [8.1641e-01, 1.8225e-01, 2.2709e-04, 4.6587e-04, 5.2786e-04, 3.3736e-05]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  189.548035  336.492310  348.730743  441.068420    0.936745      2   \n",
       "  1   26.309982  347.804840  197.958572  480.638184    0.908017      2   \n",
       "  2   46.382401  254.173126  324.138184  386.789490    0.889918      5   \n",
       "  3    0.000000  345.275787   40.929878  489.653229    0.873510      2   \n",
       "  4  327.016113  328.435516  366.239929  404.831360    0.872142      2   \n",
       "  5  280.545532  147.014587  300.656708  189.190826    0.667590      9   \n",
       "  6  330.958466  322.259766  343.656616  337.965881    0.472541      0   \n",
       "  7  322.074707  319.428833  329.969757  329.888031    0.441699      0   \n",
       "  \n",
       "              name  \n",
       "  0            car  \n",
       "  1            car  \n",
       "  2            bus  \n",
       "  3            car  \n",
       "  4            car  \n",
       "  5  traffic light  \n",
       "  6         person  \n",
       "  7         person  ,\n",
       "  'caption': ['The black BMW with its lights on that is in the middle of the two cars.',\n",
       "   'A black car with headlights on'],\n",
       "  'bbox_target': [195.56, 337.78, 152.22, 91.11]},\n",
       " 683: {'image_emb': tensor([[-0.1769,  0.5625, -0.0228,  ...,  0.7627,  0.0261, -0.5254],\n",
       "          [ 0.1638,  0.4719,  0.0601,  ...,  0.5767,  0.0113, -0.5132],\n",
       "          [ 0.0196,  0.4250, -0.0870,  ...,  0.7607, -0.0339, -0.1818],\n",
       "          [ 0.0911,  0.4697,  0.0250,  ...,  0.4036,  0.0763, -0.5767]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1158,  0.2000,  0.3865,  ...,  0.0745, -0.2561, -0.2778],\n",
       "          [-0.0423,  0.1927,  0.3826,  ...,  0.2100, -0.0925, -0.4424]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6426, 0.1729, 0.1479, 0.0368],\n",
       "          [0.1539, 0.0799, 0.7012, 0.0652]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  418.764191    9.655548  638.763855  347.927032    0.884385      4  airplane\n",
       "  1  191.829239  163.033295  562.030090  356.196808    0.796730      4  airplane\n",
       "  2  110.931877  292.354675  344.541992  342.434875    0.777612      4  airplane\n",
       "  3    0.259228  317.606232   25.058996  332.180756    0.307688      4  airplane,\n",
       "  'caption': ['An airplane with a red, white, and blue painted tail end.',\n",
       "   'Airplane that has a red, white, and white tail.'],\n",
       "  'bbox_target': [181.35, 161.94, 378.95, 196.43]},\n",
       " 684: {'image_emb': tensor([[ 0.1799,  0.7319,  0.0543,  ...,  0.7671, -0.0726,  0.1324],\n",
       "          [-0.1421,  0.5093,  0.0642,  ...,  0.9121,  0.3135,  0.3220],\n",
       "          [-0.3376,  0.0573, -0.1021,  ...,  0.3467, -0.2656, -0.0194],\n",
       "          ...,\n",
       "          [ 0.0334,  0.3191,  0.0238,  ...,  0.6592,  0.2866, -0.1720],\n",
       "          [ 0.1885,  0.4749, -0.0406,  ...,  0.7100,  0.2084,  0.4585],\n",
       "          [ 0.1907,  0.4246, -0.2517,  ...,  0.6509, -0.0622,  0.0245]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0497,  0.1025,  0.1779,  ..., -0.4478, -0.2096, -0.1017],\n",
       "          [ 0.0407, -0.1135,  0.0775,  ..., -0.1552, -0.5200, -0.1409]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.4524e-03, 9.3652e-01, 1.1557e-04, 1.8250e-02, 1.5854e-02, 2.6978e-02,\n",
       "           1.0138e-03],\n",
       "          [3.3116e-04, 9.8682e-01, 1.1861e-05, 5.5122e-03, 3.9101e-03, 3.0918e-03,\n",
       "           1.9157e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  315.461273  348.495789  521.191406  500.361298    0.905097     41     cup\n",
       "  1  219.180389  325.058075  405.205078  425.794495    0.835474     45    bowl\n",
       "  2   54.479210  255.679611  117.433655  353.155579    0.834854     39  bottle\n",
       "  3  406.218719  317.139282  611.497742  439.046143    0.816922     45    bowl\n",
       "  4  131.139206  279.615875  229.810043  388.073608    0.807115     39  bottle\n",
       "  5    0.765747  407.499817  275.235992  509.701019    0.752869     42    fork\n",
       "  6    1.725441  407.094604  270.331329  508.356415    0.279496     43   knife,\n",
       "  'caption': ['a bowl that has bread in it.',\n",
       "   'a piece of toast sitting in a bowl'],\n",
       "  'bbox_target': [406.05, 313.03, 203.86, 119.49]},\n",
       " 685: {'image_emb': tensor([[ 0.0580,  0.9033, -0.1991,  ...,  0.4375,  0.2725,  0.2231],\n",
       "          [ 0.1433,  0.0605, -0.1090,  ...,  0.9971, -0.1346, -0.1151],\n",
       "          [-0.2354,  0.6431,  0.1874,  ...,  0.8613,  0.2964, -0.0154],\n",
       "          [-0.1183,  0.6406, -0.1724,  ...,  1.3447, -0.0262,  0.1038],\n",
       "          [ 0.2930,  0.1221,  0.0787,  ...,  0.8677, -0.1447, -0.0219],\n",
       "          [ 0.0658,  0.2365,  0.1290,  ...,  0.7207, -0.0688, -0.1035]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1523, -0.1005,  0.7290,  ...,  0.3926, -0.0325,  0.1198],\n",
       "          [-0.3594,  0.3481,  0.4873,  ...,  0.1382, -0.2861, -0.6704]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.7292e-02, 2.0504e-04, 9.6143e-01, 1.9073e-05, 1.9872e-04, 6.5184e-04],\n",
       "          [9.9414e-01, 3.6955e-06, 5.8174e-03, 9.5367e-07, 2.2709e-05, 9.2626e-05]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    81.834671  195.745270  384.467773  468.833740    0.944437     45   \n",
       "  1   296.965454   63.150833  640.000000  422.542114    0.932614     45   \n",
       "  2    79.581367   32.059021  324.061096  215.999695    0.922931     45   \n",
       "  3    10.817181   92.217239  103.360596  386.827454    0.884981     44   \n",
       "  4   365.744781   38.852951  519.443359  214.751404    0.729307     48   \n",
       "  5    99.522453   73.975327  119.580544  142.474777    0.679459     51   \n",
       "  6   104.234947    0.039017  186.462891   27.451538    0.676617     41   \n",
       "  7    12.691620  183.925720  129.332077  443.250122    0.673013     43   \n",
       "  8   108.992386  138.164764  151.965317  180.136536    0.534248     51   \n",
       "  9   214.348175   63.705154  260.050232   95.593216    0.478333     51   \n",
       "  10  118.504829  110.106674  142.803177  140.350021    0.342793     51   \n",
       "  11    0.000000    0.000000  633.270508  477.667114    0.307280     60   \n",
       "  12  214.261810   64.139740  261.163483   95.467163    0.271924     47   \n",
       "  \n",
       "              name  \n",
       "  0           bowl  \n",
       "  1           bowl  \n",
       "  2           bowl  \n",
       "  3          spoon  \n",
       "  4       sandwich  \n",
       "  5         carrot  \n",
       "  6            cup  \n",
       "  7          knife  \n",
       "  8         carrot  \n",
       "  9         carrot  \n",
       "  10        carrot  \n",
       "  11  dining table  \n",
       "  12         apple  ,\n",
       "  'caption': ['Soup that is below the cucumbers.',\n",
       "   'A white soup bowl filled with soup and various vegetables and tofu'],\n",
       "  'bbox_target': [86.29, 196.31, 295.55, 265.35]},\n",
       " 686: {'image_emb': tensor([[ 0.0203,  0.4841, -0.0700,  ...,  1.0205, -0.1549, -0.0737],\n",
       "          [-0.0481,  0.4509, -0.1398,  ...,  1.1025, -0.2529, -0.3101],\n",
       "          [-0.4736, -0.0397, -0.2003,  ...,  0.7231, -0.1265, -0.0997],\n",
       "          ...,\n",
       "          [ 0.0105, -0.0670, -0.2529,  ...,  1.2812,  0.4871, -0.0223],\n",
       "          [-0.2539,  0.3193, -0.0025,  ...,  0.8770, -0.2305, -0.1401],\n",
       "          [-0.1378,  0.2006,  0.0597,  ...,  0.7100,  0.2338,  0.1031]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3660,  0.3674, -0.4390,  ...,  0.2314, -0.1637, -0.0159],\n",
       "          [ 0.0972,  0.2327, -0.2927,  ...,  0.0712,  0.0504,  0.0625]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.2846e-03, 9.9854e-01, 1.1921e-07, 6.0201e-06, 4.1723e-07, 1.7881e-07,\n",
       "           1.2577e-05, 5.9605e-08],\n",
       "          [5.6124e-04, 9.9951e-01, 5.9605e-08, 3.5763e-07, 5.9605e-08, 0.0000e+00,\n",
       "           1.1921e-06, 1.7762e-05]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   538.164978  118.373993  639.542419  477.438965    0.933210      0   \n",
       "  1   463.987366  146.865402  554.072083  473.441406    0.913590      0   \n",
       "  2   184.591324  296.924164  272.366119  381.590302    0.863796     77   \n",
       "  3   304.573486  263.161194  358.472595  330.380920    0.858219     77   \n",
       "  4   263.118195  276.513123  323.459137  333.680725    0.792270     77   \n",
       "  5   316.218445  406.240601  357.478271  445.840698    0.725703     16   \n",
       "  6   360.159851  269.639343  431.423096  348.153137    0.711175     77   \n",
       "  7   362.504120  404.221222  410.798370  448.675415    0.667468     16   \n",
       "  8   347.185211  377.139099  376.900482  432.578247    0.591797     16   \n",
       "  9   220.763901  269.174438  294.477844  411.095764    0.569000     77   \n",
       "  10  316.373535  348.670074  360.476196  412.120087    0.496848     77   \n",
       "  11  589.462830  131.751038  620.326721  162.542938    0.435010      0   \n",
       "  12  272.901001  400.190704  305.123413  437.867432    0.387222     16   \n",
       "  13  347.124207  377.636902  377.024597  432.252563    0.332196     77   \n",
       "  14  172.119324  267.066650  229.591034  353.097290    0.291416     77   \n",
       "  15  403.212921  390.097015  427.659882  434.191650    0.287763     16   \n",
       "  16  409.638458  419.344940  446.614471  451.006592    0.285243     16   \n",
       "  17  279.636078  351.433380  316.004242  384.795013    0.280924     77   \n",
       "  18  389.727997  357.768524  424.038971  402.602570    0.280737     77   \n",
       "  19  272.333679  386.514709  326.487122  436.456482    0.273940     16   \n",
       "  \n",
       "            name  \n",
       "  0       person  \n",
       "  1       person  \n",
       "  2   teddy bear  \n",
       "  3   teddy bear  \n",
       "  4   teddy bear  \n",
       "  5          dog  \n",
       "  6   teddy bear  \n",
       "  7          dog  \n",
       "  8          dog  \n",
       "  9   teddy bear  \n",
       "  10  teddy bear  \n",
       "  11      person  \n",
       "  12         dog  \n",
       "  13  teddy bear  \n",
       "  14  teddy bear  \n",
       "  15         dog  \n",
       "  16         dog  \n",
       "  17  teddy bear  \n",
       "  18  teddy bear  \n",
       "  19         dog  ,\n",
       "  'caption': ['A man with dark, curly hair wearing a plaid shirt and grey sweatpants.',\n",
       "   'A man wearing gray pants and a plaid shirt, looking at items for sale.'],\n",
       "  'bbox_target': [460.5, 144.92, 93.18, 323.97]},\n",
       " 687: {'image_emb': tensor([[ 0.1758, -0.0066, -0.5459,  ...,  0.1287,  0.0817,  0.7349],\n",
       "          [-0.3757,  0.2096, -0.1534,  ...,  1.3125,  0.2759,  0.1127],\n",
       "          [ 0.1572,  0.1416, -0.4194,  ...,  0.6865,  0.1554,  0.2004],\n",
       "          [ 0.1603, -0.0936, -0.4812,  ...,  0.1622,  0.0640,  0.7632]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1682,  0.3286, -0.0283,  ..., -0.0532, -0.1667,  0.0710],\n",
       "          [ 0.1500, -0.2101, -0.1798,  ...,  0.2306, -0.0473,  0.2245]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.2744e-01, 1.4534e-02, 2.4605e-04, 3.5767e-01],\n",
       "          [5.1807e-01, 9.0561e-03, 1.2836e-03, 4.7168e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    4.350400   41.545284  417.053040  371.837677    0.948134      0  person\n",
       "  1  290.817200    1.865900  499.373230  369.359253    0.855113      0  person\n",
       "  2  134.087967  312.122192  174.708557  374.622742    0.827612     27     tie\n",
       "  3  435.098877  274.512604  499.534943  372.709839    0.279216      0  person,\n",
       "  'caption': ['A woman in a military uniform.', 'An actress in an old movie.'],\n",
       "  'bbox_target': [5.04, 36.25, 415.36, 333.8]},\n",
       " 688: {'image_emb': tensor([[-3.9038e-01,  7.6025e-01,  4.3640e-02,  ...,  9.1211e-01,\n",
       "           -1.1456e-01, -6.0558e-04],\n",
       "          [-2.0227e-01,  5.9570e-01,  3.7573e-01,  ...,  9.6094e-01,\n",
       "           -3.6133e-01,  2.5562e-01],\n",
       "          [-6.1084e-01,  4.7949e-01, -7.1838e-02,  ...,  1.4473e+00,\n",
       "            1.2354e-01,  4.0833e-02],\n",
       "          [-1.1346e-01,  5.6738e-01, -1.4862e-02,  ...,  5.4053e-01,\n",
       "            9.6436e-02, -9.6130e-02],\n",
       "          [-3.4229e-01,  4.4849e-01,  8.5831e-03,  ...,  8.9697e-01,\n",
       "            9.4299e-02, -5.6793e-02],\n",
       "          [-1.5686e-01,  5.3223e-01,  1.1340e-01,  ...,  5.6104e-01,\n",
       "            1.2073e-01, -1.0675e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-4.7705e-01, -5.9875e-02,  2.2778e-01,  4.6265e-01,  3.4399e-01,\n",
       "            1.1707e-01, -2.1057e-01, -5.7178e-01, -2.6758e-01, -2.8467e-01,\n",
       "           -1.6833e-01,  1.1456e-01,  2.5098e-01, -7.3166e-03,  1.3586e-01,\n",
       "           -5.7343e-02,  2.9736e-01, -1.1078e-01, -2.6587e-01, -9.3746e-04,\n",
       "           -4.7485e-02, -5.1819e-02,  2.9956e-01, -4.0259e-01, -1.3379e-01,\n",
       "           -1.1066e-01,  3.0518e-02,  2.2656e-01, -6.5613e-02,  1.2427e-01,\n",
       "           -4.4556e-01, -3.9600e-01,  2.7466e-02, -1.5051e-01,  3.5858e-02,\n",
       "           -2.1912e-01, -1.0216e-02, -1.3390e-02, -1.9067e-01,  1.3452e-01,\n",
       "            2.9011e-03, -2.8259e-02,  3.2318e-02,  6.6260e-01, -4.4617e-02,\n",
       "            1.3257e-01,  3.2471e-01,  7.1106e-02,  2.2180e-01,  2.0129e-01,\n",
       "           -4.9414e-01,  1.0638e-01, -3.1128e-01, -2.2229e-01, -6.4697e-01,\n",
       "            2.9321e-01,  2.7954e-02,  2.3474e-01, -2.9761e-01,  2.0691e-02,\n",
       "            6.7017e-02, -6.3916e-01,  1.4575e-01, -5.4102e-01,  1.3000e-01,\n",
       "            1.9577e-02,  1.3672e-01,  5.9863e-01,  1.5198e-01, -3.1445e-01,\n",
       "            2.4390e-01, -1.9238e-01, -2.6147e-01, -3.4363e-02, -5.4230e-02,\n",
       "            2.6196e-01,  1.5662e-01, -4.6539e-02,  2.0032e-01, -1.2848e-02,\n",
       "           -7.2813e-04, -1.9690e-01,  1.6919e-01, -3.1677e-02, -4.2749e-01,\n",
       "            5.6738e-01,  8.4778e-02, -8.7662e-03, -1.7053e-01, -4.4769e-02,\n",
       "           -1.8079e-01, -3.1396e-01, -9.7852e-01,  3.9697e-01, -5.3223e-01,\n",
       "            1.5967e-01, -6.3818e-01, -3.1323e-01, -5.3558e-02,  1.3843e-01,\n",
       "            9.0881e-02, -2.1454e-02, -2.5757e-01, -2.1362e-01, -3.0054e-01,\n",
       "            9.6359e-03,  1.1652e-01, -1.5283e-01, -1.6187e-01, -7.2083e-02,\n",
       "           -9.1980e-02, -1.1591e-01,  5.1367e-01, -5.3125e-01, -6.9287e-01,\n",
       "            1.8848e-01, -9.9487e-02,  3.3276e-01,  9.0790e-03, -1.8823e-01,\n",
       "            8.7204e-03, -5.6396e-01,  2.6660e-01, -2.0740e-01,  1.3000e-01,\n",
       "            1.3818e-01, -1.8823e-01, -2.3950e-01, -2.4609e-01, -2.9126e-01,\n",
       "            1.1493e-01, -4.5410e-01,  1.4880e-01,  3.7754e+00, -2.5830e-01,\n",
       "           -6.4201e-03, -3.7158e-01,  1.2741e-02, -2.2217e-01,  9.3140e-02,\n",
       "           -1.4565e-02,  2.2308e-02, -4.7925e-01,  1.8909e-01,  1.2878e-01,\n",
       "           -6.8092e-03, -3.4131e-01, -5.3906e-01, -2.5195e-01, -2.1167e-01,\n",
       "            2.7661e-01, -1.5662e-01,  4.5288e-02, -3.6713e-02, -6.2317e-02,\n",
       "           -1.9763e-01, -2.7832e-01, -8.3069e-02, -1.8945e-01,  5.5859e-01,\n",
       "            1.1133e-01, -4.2261e-01, -2.2308e-02, -1.4954e-01,  1.7395e-01,\n",
       "            1.4844e-01,  5.3467e-02, -1.2817e-01,  3.7158e-01, -1.0333e-01,\n",
       "            1.7175e-01,  2.2864e-01,  4.5868e-02,  1.2903e-01,  1.5417e-01,\n",
       "            3.2104e-01, -1.6504e-01,  2.3779e-01,  2.0618e-01,  6.3354e-02,\n",
       "           -4.1772e-01, -4.0771e-01, -6.5137e-01, -3.9771e-01, -4.0869e-01,\n",
       "           -1.6565e-01,  4.5605e-01,  1.2927e-01,  4.5312e-01,  1.5833e-01,\n",
       "            5.7178e-01,  3.5840e-01, -1.0513e-02, -2.9150e-01,  2.1973e-01,\n",
       "           -1.9836e-01, -2.1838e-01, -1.6650e-01, -3.4253e-01, -1.9556e-01,\n",
       "           -8.0017e-02, -4.1235e-01,  7.7576e-02,  2.7490e-01,  5.9418e-02,\n",
       "            5.3711e-02,  3.3984e-01,  3.6426e-01,  3.1616e-02,  1.6101e-01,\n",
       "            8.0859e-01,  4.8242e-01,  5.1221e-01, -6.7932e-02,  6.0596e-01,\n",
       "           -1.8921e-01,  2.3486e-01,  5.7739e-02,  2.3392e-02,  1.2494e-01,\n",
       "            3.5645e-01,  1.5222e-01, -1.4633e-02, -5.3650e-02,  5.9668e-01,\n",
       "            2.5830e-01,  1.7456e-01, -1.3220e-01, -9.2407e-02, -4.6265e-02,\n",
       "           -1.4380e-01,  1.2952e-01,  4.4995e-01,  5.0293e-02,  2.3865e-01,\n",
       "            2.8589e-01,  1.5308e-01,  9.8389e-02, -2.1460e-01, -1.8384e-01,\n",
       "            3.2349e-01,  2.7075e-01,  2.1118e-01,  5.2277e-02, -5.7373e-01,\n",
       "           -1.0052e-01, -7.3608e-02,  2.7319e-01, -3.9893e-01, -4.7314e-01,\n",
       "            5.3497e-02,  3.2275e-01, -9.4727e-02,  3.0322e-01,  1.7053e-01,\n",
       "            1.2732e-01,  1.9958e-01,  1.8616e-01,  9.0210e-02,  5.3680e-02,\n",
       "            7.5867e-02,  4.2334e-01, -3.0640e-01, -2.1713e-02,  2.3804e-01,\n",
       "           -1.0364e-01,  1.8936e-02, -1.3660e-01, -3.7384e-02,  4.8180e-03,\n",
       "            6.8909e-02, -2.0081e-01, -1.5991e-01, -1.8005e-01,  2.3474e-01,\n",
       "            3.2739e-01, -1.8579e-01,  1.2238e-01, -3.9978e-02, -2.4780e-02,\n",
       "           -2.1899e-01, -7.4890e-02, -3.2020e-04, -8.0933e-02,  4.7437e-01,\n",
       "            3.2562e-02, -9.6375e-02,  2.7661e-01,  4.3604e-01, -3.4033e-01,\n",
       "            5.6427e-02,  4.5380e-02, -2.7893e-02,  4.0796e-01,  2.9150e-01,\n",
       "            4.7241e-01, -1.7175e-01,  1.5039e-01, -1.2085e-01, -2.5342e-01,\n",
       "            6.9397e-02, -2.5562e-01,  2.5610e-01,  3.8330e-01,  7.1533e-02,\n",
       "            1.0504e-01, -1.6150e-01,  3.3472e-01,  4.2114e-01, -2.4817e-01,\n",
       "            4.8413e-01,  3.4326e-01,  3.7754e+00, -2.9639e-01,  6.7177e-03,\n",
       "            3.8037e-01, -2.7441e-01, -1.4844e-01, -3.0493e-01,  5.2551e-02,\n",
       "            3.0493e-01,  1.5540e-01,  3.0167e-02,  1.2988e-01, -1.6443e-01,\n",
       "           -4.4739e-02,  7.4158e-02,  1.7603e-01,  2.3987e-01, -1.2285e+00,\n",
       "           -1.5723e-01,  1.0602e-01, -9.1431e-02, -2.3132e-01, -2.6566e-02,\n",
       "           -2.5082e-04,  2.1082e-01,  2.5757e-01, -2.9614e-01,  7.4768e-02,\n",
       "           -5.2686e-01, -3.6591e-02,  2.3657e-01, -1.1377e-01,  1.5526e-03,\n",
       "           -3.1372e-01,  1.4313e-02, -1.4844e-01,  2.0850e-01, -5.4291e-02,\n",
       "           -9.7961e-02,  2.3291e-01,  1.2573e-01,  3.9032e-02, -1.3354e-01,\n",
       "           -4.2871e-01, -1.8457e-01, -1.3782e-01,  3.4497e-01, -9.7595e-02,\n",
       "           -2.0410e-01, -1.5244e-02, -4.5361e-01,  2.5146e-01,  1.9080e-01,\n",
       "           -3.5498e-01, -2.1228e-01,  9.8938e-02, -3.2642e-01,  2.7490e-01,\n",
       "           -1.4209e-01,  6.8726e-02,  3.0908e-01,  1.8433e-01,  4.0674e-01,\n",
       "           -5.6299e-01, -2.2546e-01,  1.5051e-01, -1.9604e-01, -7.7539e-01,\n",
       "            2.6880e-01, -4.8242e-01,  7.4524e-02, -3.7036e-01,  3.8086e-02,\n",
       "            2.3758e-02, -4.1821e-01, -1.4478e-01, -4.6558e-01, -2.6443e-02,\n",
       "           -4.5068e-01, -2.6782e-01, -6.0730e-02, -9.1064e-02,  1.2184e-02,\n",
       "            2.0532e-01, -2.9221e-02, -5.3564e-01, -1.7181e-02,  6.0742e-01,\n",
       "           -9.1309e-02, -1.7871e-01,  6.2158e-01,  7.2449e-02, -4.2145e-02,\n",
       "           -1.7114e-01, -1.8555e-01, -8.3496e-02, -1.1438e-01, -2.5610e-01,\n",
       "           -4.6021e-01, -1.6321e-01,  7.7019e-03,  4.0381e-01,  3.5669e-01,\n",
       "            1.4941e-01, -2.6758e-01, -1.1635e-02, -1.5039e-01, -4.5142e-01,\n",
       "            1.3000e-01, -5.0568e-02,  2.3120e-01, -3.5278e-01, -4.4067e-01,\n",
       "           -1.3440e-01, -9.5093e-02, -4.6265e-01,  2.5925e-02,  6.3574e-01,\n",
       "            4.8486e-01, -1.9299e-01,  2.8854e-02,  1.6455e-01,  2.6782e-01,\n",
       "            1.2866e-01, -5.3027e-01,  1.4685e-01, -8.5022e-02,  4.1333e-01,\n",
       "           -2.9395e-01, -5.4736e-01,  1.6260e-01, -2.0117e-01, -2.7612e-01,\n",
       "            1.6434e-02, -7.5867e-02, -4.1284e-01, -3.5669e-01, -7.9895e-02,\n",
       "           -1.9678e-01,  3.9209e-01, -3.0713e-01, -2.0129e-01, -3.0200e-01,\n",
       "            1.7578e-01,  5.3076e-01,  3.5693e-01, -1.2720e-01, -4.9341e-01,\n",
       "            7.8964e-03,  1.0699e-01, -5.9662e-03,  9.5520e-02,  4.0137e-01,\n",
       "            9.8145e-02,  4.3384e-01, -4.6753e-01, -2.1167e-01, -4.8657e-01,\n",
       "           -3.3844e-02, -7.4524e-02, -1.2524e-01,  2.1875e-01, -1.3354e-01,\n",
       "            2.2995e-02, -9.8572e-02, -1.5857e-01,  1.7407e-01,  3.1104e-01,\n",
       "           -2.4524e-01, -3.6597e-01, -5.0830e-01,  1.9922e-01,  2.7222e-01,\n",
       "           -1.1902e-01,  1.0797e-01, -8.7708e-02,  3.1738e-01,  1.7200e-01,\n",
       "           -5.6396e-01, -1.0217e-01,  3.8623e-01,  9.0039e-01,  1.3196e-01,\n",
       "           -7.8583e-03,  7.9529e-02, -3.3911e-01, -6.2402e-01, -2.2205e-01,\n",
       "            2.3669e-01, -1.1517e-01,  1.3062e-01, -9.2651e-02,  3.4082e-01,\n",
       "           -2.2925e-01, -2.1240e-01, -2.3633e-01, -1.4893e-01,  2.3889e-01,\n",
       "           -2.0874e-01, -1.2494e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2018, 0.6719, 0.0351, 0.0430, 0.0040, 0.0443]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  407.789948   54.236237  583.608337  207.597870    0.919816     45   \n",
       "  1  218.731155    8.055237  350.372986  114.713318    0.875773     45   \n",
       "  2  139.698425  131.045197  219.926239  201.706512    0.843210     45   \n",
       "  3    6.653809    0.921631  639.723389  419.465454    0.771518     60   \n",
       "  4    0.895508  332.023407  330.331390  424.083862    0.717183     44   \n",
       "  5    0.000000  267.060455  130.778717  368.165558    0.356420     43   \n",
       "  \n",
       "             name  \n",
       "  0          bowl  \n",
       "  1          bowl  \n",
       "  2          bowl  \n",
       "  3  dining table  \n",
       "  4         spoon  \n",
       "  5         knife  ,\n",
       "  'caption': ['A small metal bowl full of white sauce.'],\n",
       "  'bbox_target': [218.0, 8.43, 133.25, 107.58]},\n",
       " 689: {'image_emb': tensor([[ 0.3042,  0.1138,  0.1527,  ...,  0.0036,  0.2389, -0.1698],\n",
       "          [ 0.2458, -0.2832, -0.0743,  ..., -0.3845,  0.1846,  0.0667],\n",
       "          [-0.0224, -0.1542, -0.0260,  ...,  0.3364,  0.5107,  0.1022],\n",
       "          [-0.0079,  0.1331, -0.1301,  ..., -0.0802,  0.0460,  0.1465],\n",
       "          [ 0.2081, -0.5845, -0.0768,  ..., -0.0693, -0.1267,  0.0089]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0344, -0.0872, -0.1919,  ..., -0.4158,  0.2267, -0.4031],\n",
       "          [ 0.2703, -0.1373, -0.2477,  ..., -0.1127,  0.1059,  0.1802]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.4165e-01, 2.8516e-01, 1.2463e-01, 4.2892e-04, 1.4795e-01],\n",
       "          [1.6809e-01, 6.5479e-01, 5.9021e-02, 2.8320e-02, 8.9966e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   62.816635  190.836487  354.439453  430.276672    0.942429     18   sheep\n",
       "  1  337.183014   40.785522  555.926453  407.856567    0.925315      0  person\n",
       "  2  289.246826  181.890915  487.877502  424.194397    0.914738     18   sheep\n",
       "  3  133.150711   85.241699  322.295959  257.495453    0.887671      0  person,\n",
       "  'caption': ['a sheep being help by the lady in the black shit; left side of the picture',\n",
       "   'The animal being petted by the blonde lady.'],\n",
       "  'bbox_target': [70.86, 184.89, 287.79, 239.82]},\n",
       " 690: {'image_emb': tensor([[ 9.6069e-02,  4.9146e-01, -1.4038e-01,  ...,  9.7754e-01,\n",
       "            2.3193e-01, -2.0386e-01],\n",
       "          [-2.6367e-02,  4.7827e-01, -2.4063e-02,  ...,  9.4727e-01,\n",
       "            1.0590e-01, -5.6915e-02],\n",
       "          [ 7.8583e-04,  2.4988e-01,  1.1084e-01,  ...,  7.2559e-01,\n",
       "            1.3647e-01, -1.0522e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1510,  0.2076,  0.1209,  ...,  0.3198, -0.3867,  0.0199],\n",
       "          [-0.0370, -0.0625, -0.0535,  ...,  0.3618, -0.4062,  0.1749]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.1035e-03, 9.0576e-01, 8.8257e-02],\n",
       "          [8.6355e-04, 3.4839e-01, 6.5088e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  218.871094   57.030060  344.699158  101.648193    0.906061      4  airplane\n",
       "  1  244.617661  213.901489  636.694580  316.521790    0.858442      4  airplane\n",
       "  2  368.917236  317.030243  397.524963  337.290802    0.572506      7     truck\n",
       "  3    0.343903  382.179443  636.427124  474.824890    0.384039      4  airplane\n",
       "  4  123.893669  335.441345  162.238602  349.438965    0.384039      7     truck\n",
       "  5  559.187195  309.865692  611.103088  330.015533    0.278676      7     truck\n",
       "  6  252.651535  317.143463  281.712738  335.810211    0.275780      7     truck,\n",
       "  'caption': ['A blue and white airplane with KLM on it',\n",
       "   'The blue and while KLM plane on the ground.'],\n",
       "  'bbox_target': [244.47, 217.84, 393.37, 99.17]},\n",
       " 691: {'image_emb': tensor([[-0.0961,  0.4912, -0.2268,  ...,  0.7622, -0.0671,  0.1394],\n",
       "          [ 0.0083,  0.4783, -0.2642,  ...,  0.7017, -0.2649,  0.1987],\n",
       "          [ 0.1919, -0.1783, -0.2817,  ...,  1.0000,  0.0681, -0.2343],\n",
       "          [ 0.1249,  0.0922, -0.3010,  ...,  0.7715,  0.0309,  0.2771],\n",
       "          [-0.4438,  0.1212, -0.2742,  ...,  1.0830,  0.0845, -0.3120],\n",
       "          [-0.1186,  0.3303, -0.1045,  ...,  0.5083, -0.1016, -0.2004]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.4131,  0.3232, -0.3438,  ...,  0.4475, -0.6538,  0.0659],\n",
       "          [ 0.2458,  0.0978, -0.4302,  ...,  0.1793, -0.3728,  0.1331]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.3340e-01, 2.5940e-03, 1.9073e-06, 6.6504e-01, 5.8174e-05, 9.8877e-02],\n",
       "          [1.4929e-01, 4.6062e-04, 6.8486e-05, 7.9443e-01, 5.9128e-04, 5.4932e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  361.073242  210.316101  599.133606  344.630066    0.907743     57   \n",
       "  1  322.742096  144.013168  395.460419  220.605164    0.863007     58   \n",
       "  2  258.780548  273.428345  295.452667  291.103760    0.846910     45   \n",
       "  3  168.685028  294.765869  620.233765  475.088196    0.841068     57   \n",
       "  4  143.734253  215.937836  347.351196  269.852875    0.763877     13   \n",
       "  5  416.764801  254.235870  461.953827  282.720551    0.496558     15   \n",
       "  6  349.252136  190.456116  377.655334  221.251953    0.419906     75   \n",
       "  7  207.761871  250.545013  367.842804  376.028595    0.375024     60   \n",
       "  \n",
       "             name  \n",
       "  0         couch  \n",
       "  1  potted plant  \n",
       "  2          bowl  \n",
       "  3         couch  \n",
       "  4         bench  \n",
       "  5           cat  \n",
       "  6          vase  \n",
       "  7  dining table  ,\n",
       "  'caption': ['A gray love seat with two blue throw pillows facing out a window',\n",
       "   'couch facing the window'],\n",
       "  'bbox_target': [168.57, 290.98, 451.22, 182.95]},\n",
       " 692: {'image_emb': tensor([[-0.0362,  0.2360, -0.1431,  ...,  0.9209,  0.2483, -0.2874],\n",
       "          [-0.1390,  0.6919, -0.1692,  ...,  0.9453,  0.1985, -0.0889],\n",
       "          [-0.1506,  0.3962, -0.1338,  ...,  1.0293, -0.2854,  0.2477],\n",
       "          [ 0.5137, -0.1321, -0.3635,  ...,  0.9854, -0.1104, -0.1268],\n",
       "          [ 0.1603,  0.0238, -0.5029,  ...,  0.7183, -0.0171,  0.1003]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3889,  0.1902, -0.0315,  ..., -0.2346, -0.4565, -0.3086],\n",
       "          [-0.2539,  0.0072, -0.0536,  ..., -0.1888, -0.2546, -0.0964]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.4782e-03, 1.1045e-04, 9.9854e-01, 1.2994e-05, 9.1791e-06],\n",
       "          [1.7426e-02, 2.5902e-03, 9.6680e-01, 8.7662e-03, 4.5471e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  156.196518    9.031921  326.654877  330.443817    0.937173      0  person\n",
       "  1  101.371124   21.247774  225.985046  325.872620    0.936256      0  person\n",
       "  2  312.462677   96.199242  499.603760  326.583618    0.933353     57   couch\n",
       "  3  101.763214   82.384560  118.604645  102.812431    0.753705     65  remote\n",
       "  4   28.190559   80.551079  134.537766  225.387650    0.691593      0  person\n",
       "  5  161.388428  101.571892  180.334274  120.555260    0.612023     65  remote\n",
       "  6  213.594986  178.364807  228.027893  194.541641    0.529193     65  remote\n",
       "  7  148.412247  146.617126  161.324982  157.606171    0.472843     65  remote\n",
       "  8  109.116806   84.690361  157.395996  206.467178    0.300857      0  person,\n",
       "  'caption': ['a recliner on which a man in white shorts is sitting',\n",
       "   'the chair that the man is sitting in'],\n",
       "  'bbox_target': [26.34, 90.85, 115.11, 115.1]},\n",
       " 693: {'image_emb': tensor([[ 0.0057, -0.0656,  0.2935,  ...,  0.4707,  0.0806,  0.1636],\n",
       "          [-0.3921, -0.0553,  0.0848,  ...,  1.0801, -0.0263, -0.0773],\n",
       "          [-0.0453, -0.1387,  0.1813,  ...,  1.2100,  0.0033,  0.0893],\n",
       "          [ 0.2834, -0.3015,  0.3516,  ...,  0.3848,  0.0402,  0.0462]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3657, -0.2695, -0.1942,  ..., -0.1193, -0.0215, -0.1470],\n",
       "          [-0.1821, -0.1388, -0.2903,  ..., -0.0567,  0.2345, -0.4065]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0470, 0.2749, 0.0776, 0.6006],\n",
       "          [0.0018, 0.6172, 0.3687, 0.0124]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    0.182297    0.506454  474.617249  472.413574    0.949568      0  person\n",
       "  1  412.741791  169.247879  639.599609  473.332275    0.913539     18   sheep\n",
       "  2  272.848328  259.217590  521.411011  476.772644    0.912308     18   sheep,\n",
       "  'caption': ['A black sheep.',\n",
       "   'The black goat is on the right side of the white goat.'],\n",
       "  'bbox_target': [409.5, 171.84, 230.5, 302.72]},\n",
       " 694: {'image_emb': tensor([[ 0.1543,  0.2329, -0.2651,  ...,  0.4954, -0.0464, -0.2330],\n",
       "          [ 0.1460, -0.3625, -0.5576,  ...,  0.0210, -0.2747,  0.1843],\n",
       "          [ 0.2722,  0.3813, -0.0930,  ...,  1.0186,  0.1007,  0.4219],\n",
       "          ...,\n",
       "          [ 0.1202,  0.0120, -0.2661,  ...,  0.3962,  0.0359,  0.1407],\n",
       "          [ 0.0508,  0.1735, -0.1560,  ...,  1.2402,  0.0767, -0.0981],\n",
       "          [ 0.2227, -0.3818, -0.4065,  ...,  0.2018, -0.2629,  0.1523]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2026, -0.3018, -0.3267,  ...,  0.1707, -0.5830, -0.0965],\n",
       "          [ 0.1244, -0.3533, -0.4014,  ...,  0.4600, -0.2473,  0.2522]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.3844e-02, 6.7969e-01, 1.2189e-01, 6.2990e-04, 8.5083e-02, 5.5909e-05,\n",
       "           7.8674e-02],\n",
       "          [1.4519e-02, 8.8428e-01, 1.0357e-03, 3.8326e-05, 6.8176e-02, 2.9802e-07,\n",
       "           3.1708e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.000000  132.158066  143.032318  231.974243    0.894091      2   \n",
       "  1   89.592438   43.123459  603.794678  416.157959    0.887052      2   \n",
       "  2  309.598877  140.544128  368.410889  210.934692    0.882705     77   \n",
       "  3  237.349030   33.330933  308.844635  163.941406    0.879862      0   \n",
       "  4   39.942825  121.268661  228.648911  252.725525    0.787186      2   \n",
       "  5  596.198181  156.841675  638.673279  239.374817    0.730404      0   \n",
       "  6  598.571777  213.746033  637.720581  264.603699    0.644585     56   \n",
       "  7  315.688934   40.640335  398.033661   99.160233    0.559907      0   \n",
       "  \n",
       "           name  \n",
       "  0         car  \n",
       "  1         car  \n",
       "  2  teddy bear  \n",
       "  3      person  \n",
       "  4         car  \n",
       "  5      person  \n",
       "  6       chair  \n",
       "  7      person  ,\n",
       "  'caption': ['A blue car with a teddy bear on the front',\n",
       "   'An old green car with a bear in the grill'],\n",
       "  'bbox_target': [89.99, 47.71, 515.98, 366.64]},\n",
       " 695: {'image_emb': tensor([[-0.2512, -0.1454,  0.2269,  ...,  0.6890,  0.0253, -0.0741],\n",
       "          [-0.1835, -0.3059,  0.1660,  ...,  1.0039,  0.2198, -0.2893],\n",
       "          [-0.2274, -0.0872, -0.1327,  ...,  0.9790, -0.0717, -0.1300],\n",
       "          [ 0.0975, -0.0298, -0.1917,  ...,  1.2686,  0.2791, -0.3726],\n",
       "          [-0.0457, -0.2686,  0.2681,  ...,  0.7451,  0.0823, -0.1656]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0786, -0.3411,  0.0278,  ...,  0.2595, -0.0486, -0.0934],\n",
       "          [-0.0279, -0.0126, -0.1216,  ...,  0.1165, -0.2700, -0.2284]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1930, 0.1503, 0.5410, 0.0732, 0.0424],\n",
       "          [0.0179, 0.4902, 0.4062, 0.0740, 0.0117]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0   17.200089   75.854935  395.391968  476.630432    0.950669     17  horse\n",
       "  1  407.283020  102.245422  579.757935  229.180115    0.913460     17  horse\n",
       "  2  552.999512  132.828888  639.557007  311.895355    0.907218     17  horse\n",
       "  3  544.731995   99.846390  578.591248  131.446579    0.805229     17  horse\n",
       "  4  480.156525   95.949707  503.886749  119.234039    0.348662     17  horse,\n",
       "  'caption': ['Horse with mange facing the meadow.',\n",
       "   'small brown horse facing left'],\n",
       "  'bbox_target': [408.62, 103.39, 172.98, 125.34]},\n",
       " 696: {'image_emb': tensor([[-0.3181,  0.1857, -0.3877,  ...,  1.3027,  0.0580, -0.1403],\n",
       "          [-0.1135,  0.0224, -0.0307,  ...,  1.1270, -0.4158,  0.0917],\n",
       "          [-0.1621,  0.2834, -0.4258,  ...,  1.3018, -0.0547, -0.2625],\n",
       "          [-0.2769,  0.1058,  0.0032,  ...,  0.4795, -0.0891, -0.0304]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1923, -0.0867, -0.1237,  ...,  0.0446, -0.1346, -0.4368],\n",
       "          [-0.1373, -0.1049, -0.2438,  ...,  0.0850, -0.2729, -0.2325]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0486e-01, 8.1778e-05, 8.7842e-01, 1.6861e-02],\n",
       "          [3.1812e-01, 1.4663e-05, 6.7334e-01, 8.3466e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  260.159180  332.018799  467.722046  425.277161    0.925455     28  suitcase\n",
       "  1  130.321686  245.639923  274.892487  372.396454    0.922334     62        tv\n",
       "  2  261.065613  404.446198  454.968384  481.145172    0.859954     28  suitcase,\n",
       "  'caption': ['A brown suitcase on top of the box and other suitcase.',\n",
       "   'suitcase that is on top of other suitcase'],\n",
       "  'bbox_target': [262.91, 332.8, 205.9, 98.68]},\n",
       " 697: {'image_emb': tensor([[-0.0156, -0.3721, -0.2052,  ...,  0.8770, -0.1171, -0.0555],\n",
       "          [-0.2460, -0.2218, -0.1956,  ...,  1.2725, -0.0084,  0.0759],\n",
       "          [-0.3108, -0.4629, -0.2910,  ...,  1.0049, -0.0574,  0.1514],\n",
       "          [-0.1025, -0.2045, -0.2583,  ...,  0.5127, -0.4253, -0.0220]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1216, -0.1591, -0.3015,  ..., -0.4143, -0.0699, -0.0462],\n",
       "          [-0.0535, -0.1715, -0.0603,  ...,  0.3870, -0.3691,  0.0572]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3269, 0.0416, 0.1406, 0.4907],\n",
       "          [0.0498, 0.0316, 0.0221, 0.8965]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0   19.161148   22.775314  179.224701  374.342834    0.916426     23  giraffe\n",
       "  1  305.747437  185.932007  453.026489  385.602478    0.914190     23  giraffe\n",
       "  2  262.850342  234.716797  376.085999  391.735229    0.805930     23  giraffe\n",
       "  3  616.729614  220.537872  637.005493  235.167084    0.528569      0   person\n",
       "  4  590.704041  220.990845  610.177063  261.029236    0.455274      0   person\n",
       "  5  592.512268  220.815186  609.798279  236.286194    0.297239      0   person,\n",
       "  'caption': ['The giraffe all the way in the back.',\n",
       "   'giraffe standing farthest away in the shade'],\n",
       "  'bbox_target': [258.7, 234.18, 119.25, 159.64]},\n",
       " 698: {'image_emb': tensor([[-4.7998e-01,  2.6978e-01,  1.2433e-01,  ...,  1.0303e+00,\n",
       "           -4.9591e-04, -4.2310e-01],\n",
       "          [-5.2344e-01,  2.9565e-01, -1.6260e-01,  ...,  1.1455e+00,\n",
       "           -5.2986e-03, -2.7759e-01],\n",
       "          [-1.3184e-01,  2.3376e-01,  4.9866e-02,  ...,  7.3486e-01,\n",
       "            3.3508e-02,  7.7454e-02],\n",
       "          [-4.6362e-01,  1.7896e-01,  1.4490e-01,  ...,  9.9854e-01,\n",
       "           -1.8909e-01, -2.4744e-01],\n",
       "          [-2.2424e-01,  3.5889e-01,  2.4216e-02,  ...,  5.3467e-01,\n",
       "           -9.4727e-02, -2.5366e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0456,  0.0786,  0.1265,  ..., -0.0154, -0.1411,  0.3779],\n",
       "          [ 0.1678,  0.0245,  0.1965,  ...,  0.2202, -0.2244,  0.2805]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.1075e-04, 2.0862e-06, 9.1162e-01, 1.0185e-03, 8.7463e-02],\n",
       "          [1.1754e-04, 5.3644e-07, 7.4170e-01, 1.8387e-03, 2.5635e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0    4.096097  103.698708  170.863983  235.575699    0.923699     29  frisbee\n",
       "  1  150.689728   15.067381  313.154907  118.470978    0.915448     29  frisbee\n",
       "  2   63.909466  176.681946  459.039886  470.502869    0.904449     46   banana\n",
       "  3  103.626465  155.143677  352.022003  355.993988    0.889474     29  frisbee,\n",
       "  'caption': ['A blue color plate kept near the banana',\n",
       "   'a blue plate by a banana'],\n",
       "  'bbox_target': [105.86, 155.97, 247.74, 201.58]},\n",
       " 699: {'image_emb': tensor([[-3.4106e-01,  1.3330e-01, -7.2449e-02,  ...,  6.4111e-01,\n",
       "            7.0312e-02,  1.5271e-01],\n",
       "          [-6.8367e-05,  2.4536e-01, -3.4058e-02,  ...,  6.6846e-01,\n",
       "            2.8946e-02, -1.1926e-01],\n",
       "          [-2.6901e-02,  3.6548e-01, -3.4729e-02,  ...,  7.2119e-01,\n",
       "            1.7236e-01,  6.1218e-02],\n",
       "          ...,\n",
       "          [-2.7295e-01,  5.9912e-01,  4.8462e-02,  ...,  7.8076e-01,\n",
       "            4.9927e-01, -3.9276e-02],\n",
       "          [ 2.1790e-01, -7.5569e-03, -8.8623e-02,  ...,  9.3066e-01,\n",
       "            1.7908e-01, -7.3145e-01],\n",
       "          [ 3.3875e-02,  2.5024e-01,  6.5041e-03,  ...,  4.9219e-01,\n",
       "            3.6938e-01,  5.2582e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0705,  0.0682, -0.3147,  ..., -0.5723, -0.1643,  0.2394],\n",
       "          [ 0.1284, -0.0050,  0.1449,  ...,  0.2749,  0.2333, -0.2134]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.0409e-03, 5.3125e-01, 1.6211e-01, 1.2042e-01, 7.8201e-03, 1.0424e-03,\n",
       "           2.0935e-02, 5.9009e-06, 3.6240e-05, 3.5248e-03, 6.3181e-06, 1.6918e-03,\n",
       "           2.3861e-03, 7.6538e-02, 1.6651e-03, 4.2938e-02, 2.4857e-02, 2.1994e-05,\n",
       "           3.5453e-04, 1.0812e-04],\n",
       "          [9.2697e-04, 2.3401e-01, 4.5349e-02, 2.4277e-02, 1.0948e-02, 9.5654e-04,\n",
       "           7.9590e-02, 2.3437e-04, 2.7676e-03, 1.0401e-04, 4.8022e-01, 4.4212e-03,\n",
       "           5.7983e-04, 1.3832e-02, 4.7546e-02, 1.8435e-03, 8.2626e-03, 7.6866e-04,\n",
       "           4.3274e-02, 1.1420e-04]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   462.678467  258.717407  541.629456  407.060242    0.933359     56   \n",
       "  1   166.766632  209.502869  311.565979  478.921692    0.925599      0   \n",
       "  2     0.094086  239.345917  180.684082  436.346313    0.907329      0   \n",
       "  3   311.418274  252.618866  479.745239  474.988647    0.891255      0   \n",
       "  4   164.243469  307.481232  260.509827  474.090088    0.878010     56   \n",
       "  5   314.423889  391.680908  482.087585  478.450806    0.864142     56   \n",
       "  6   283.023743  197.802155  379.516724  361.189423    0.823677      0   \n",
       "  7   262.258911  189.330750  295.847961  249.094482    0.809358     56   \n",
       "  8     0.000000  417.122131   37.088943  450.067810    0.800506     53   \n",
       "  9   452.270874  190.573730  488.854919  251.422668    0.793833     56   \n",
       "  10   32.660698  387.774902   65.213814  451.429382    0.792634     39   \n",
       "  11  335.231018  202.393707  457.917603  388.372162    0.790775      0   \n",
       "  12  210.914154  188.770081  251.351379  247.299133    0.780060     56   \n",
       "  13  148.501770  163.147827  209.577759  300.713623    0.749967      0   \n",
       "  14    1.185707  387.294434  186.267120  476.502258    0.741967     60   \n",
       "  15  100.947876  219.527496  159.896545  317.357208    0.728897     56   \n",
       "  16  293.848389  151.880569  326.315979  213.728882    0.710956      0   \n",
       "  17    3.191963  170.541168   89.961716  256.221161    0.706941      0   \n",
       "  18    0.100293  335.459564   19.183372  358.134064    0.705045     53   \n",
       "  19  261.643433  232.256348  316.815491  332.036377    0.686113     56   \n",
       "  20  246.149109  328.679260  277.241394  346.407715    0.676201     53   \n",
       "  21  288.097748  357.735992  339.728912  386.331268    0.639378     53   \n",
       "  22   84.839035  215.160370  126.202469  281.479706    0.600739     56   \n",
       "  23  255.675781  153.283005  290.204407  193.915100    0.579051      0   \n",
       "  24  431.985626  150.477249  478.827606  195.856415    0.571622      0   \n",
       "  25  373.986877  187.035034  384.976013  241.484314    0.569928     56   \n",
       "  26  484.025269  184.542358  501.958923  241.015503    0.554006     56   \n",
       "  27   13.603699  228.284393   46.198944  259.163361    0.547417     56   \n",
       "  28  336.689941  173.567734  348.164246  199.377960    0.540465     56   \n",
       "  29  223.457184  153.893616  268.220215  244.738159    0.506577      0   \n",
       "  30  294.579834  181.081848  325.380737  232.716125    0.502760     56   \n",
       "  31  599.098999  164.544144  639.673706  220.562286    0.481188     62   \n",
       "  32   79.600227  231.681519   87.258583  257.219177    0.476410     39   \n",
       "  33  210.228424  156.922241  230.933929  190.648682    0.471194      0   \n",
       "  34  389.110840  152.899796  437.656128  245.912537    0.456596      0   \n",
       "  35  294.324890  284.558777  342.400391  295.119080    0.439761     53   \n",
       "  36    0.066448  257.450500   25.794956  336.038330    0.428308     56   \n",
       "  37    0.688210  176.758820   46.118355  223.246613    0.409860      0   \n",
       "  38  603.226440  237.503876  639.724487  262.170319    0.373565     66   \n",
       "  39  284.414398  146.244019  303.514862  184.092957    0.361811      0   \n",
       "  40  403.419434  188.045837  482.926819  248.025391    0.311056     60   \n",
       "  41  254.728271  155.740952  294.815430  241.302734    0.309535      0   \n",
       "  42   84.758560  214.395905   95.168427  252.830231    0.305561     56   \n",
       "  43  296.893768  232.199768  318.051483  277.681763    0.288676     56   \n",
       "  44  315.534332  148.507538  337.208344  183.272949    0.274346      0   \n",
       "  45  307.364594  178.315002  337.926788  231.791687    0.273076     56   \n",
       "  46  405.029449  183.851379  481.823151  207.955444    0.264405     60   \n",
       "  47  389.232788  152.040863  422.789551  204.735077    0.263959      0   \n",
       "  \n",
       "              name  \n",
       "  0          chair  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3         person  \n",
       "  4          chair  \n",
       "  5          chair  \n",
       "  6         person  \n",
       "  7          chair  \n",
       "  8          pizza  \n",
       "  9          chair  \n",
       "  10        bottle  \n",
       "  11        person  \n",
       "  12         chair  \n",
       "  13        person  \n",
       "  14  dining table  \n",
       "  15         chair  \n",
       "  16        person  \n",
       "  17        person  \n",
       "  18         pizza  \n",
       "  19         chair  \n",
       "  20         pizza  \n",
       "  21         pizza  \n",
       "  22         chair  \n",
       "  23        person  \n",
       "  24        person  \n",
       "  25         chair  \n",
       "  26         chair  \n",
       "  27         chair  \n",
       "  28         chair  \n",
       "  29        person  \n",
       "  30         chair  \n",
       "  31            tv  \n",
       "  32        bottle  \n",
       "  33        person  \n",
       "  34        person  \n",
       "  35         pizza  \n",
       "  36         chair  \n",
       "  37        person  \n",
       "  38      keyboard  \n",
       "  39        person  \n",
       "  40  dining table  \n",
       "  41        person  \n",
       "  42         chair  \n",
       "  43         chair  \n",
       "  44        person  \n",
       "  45         chair  \n",
       "  46  dining table  \n",
       "  47        person  ,\n",
       "  'caption': ['A table that is nearest to the person taking the picture, where a girl is eating a slice of pizza and we can see her shirt logo.',\n",
       "   'slice of pepperoni pizza, dasani bottled water'],\n",
       "  'bbox_target': [1.12, 383.13, 185.05, 89.72]},\n",
       " 700: {'image_emb': tensor([[-0.3345,  0.0986,  0.0321,  ...,  0.8691, -0.1611,  0.0067],\n",
       "          [-0.1322,  0.6411,  0.0773,  ...,  0.6978, -0.0589, -0.3533],\n",
       "          [-0.7627,  0.4001, -0.0119,  ...,  0.6934, -0.3042, -0.2478],\n",
       "          [-0.1241,  0.2810, -0.2422,  ...,  0.4333,  0.0228, -0.0560]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1746,  0.1711, -0.2974,  ..., -0.4785,  0.0126, -0.2610],\n",
       "          [-0.0853, -0.0761, -0.6738,  ..., -0.3525, -0.1273, -0.0017]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0067, 0.1213, 0.0681, 0.8037],\n",
       "          [0.0009, 0.8105, 0.0021, 0.1865]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   454.557892  203.281372  580.528381  308.758301    0.907584      0   \n",
       "  1   107.634506   87.259796  522.594727  282.212128    0.893836     57   \n",
       "  2   277.422852  204.176910  639.071228  413.592712    0.848918     57   \n",
       "  3   162.297638  212.179382  229.392120  257.104614    0.644668     45   \n",
       "  4   151.228699  283.150299  222.803955  314.124054    0.580966     73   \n",
       "  5   207.434448  203.742676  230.871521  227.158569    0.559830     49   \n",
       "  6    95.648659   95.379883  151.928802  111.799408    0.505316     73   \n",
       "  7   160.139374  214.412018  181.567596  235.524567    0.459419     47   \n",
       "  8   123.448326  236.307739  153.992508  276.909668    0.403890     75   \n",
       "  9     0.042295  221.932739   14.785583  278.507935    0.377008     41   \n",
       "  10  162.843811  214.506805  230.631561  251.528900    0.329589     47   \n",
       "  11   31.120319  159.141632   44.225704  208.650543    0.290879     73   \n",
       "  12   86.336395  215.767456  117.558105  266.368896    0.256789     75   \n",
       "  13   75.416771  106.208359  154.309753  127.398972    0.255320     73   \n",
       "  14    0.003250  268.113312  164.162857  396.814850    0.254205     24   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1      couch  \n",
       "  2      couch  \n",
       "  3       bowl  \n",
       "  4       book  \n",
       "  5     orange  \n",
       "  6       book  \n",
       "  7      apple  \n",
       "  8       vase  \n",
       "  9        cup  \n",
       "  10     apple  \n",
       "  11      book  \n",
       "  12      vase  \n",
       "  13      book  \n",
       "  14  backpack  ,\n",
       "  'caption': ['the person underneath the white blankets not smiling',\n",
       "   'The couch the man in the back is sleeping on.'],\n",
       "  'bbox_target': [105.21, 87.16, 418.08, 191.24]},\n",
       " 701: {'image_emb': tensor([[ 0.1772,  0.6562,  0.0157,  ...,  0.4541, -0.6284, -0.4141],\n",
       "          [-0.0825,  0.6274,  0.2196,  ...,  0.6743, -0.2649, -0.0183],\n",
       "          [ 0.1748, -0.0620, -0.2600,  ...,  0.7002, -0.0980, -0.0784],\n",
       "          [ 0.1095,  0.1825, -0.1715,  ...,  1.1416,  0.0994,  0.0926],\n",
       "          [ 0.0451,  0.5469,  0.3408,  ...,  0.2542, -0.2661, -0.0825]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0400,  0.3047, -0.0562,  ...,  0.4050, -0.4248, -0.4707],\n",
       "          [-0.1782,  0.3872, -0.0259,  ...,  0.6074, -0.5366, -0.3374]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.5986e-01, 4.2145e-02, 4.5657e-05, 6.5565e-06, 9.7961e-02],\n",
       "          [6.2842e-01, 3.5431e-02, 5.3644e-07, 1.1921e-07, 3.3618e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  253.642853  112.396805  461.893097  420.599121    0.952281      0   \n",
       "  1  426.077393  276.024963  639.328979  443.679382    0.926250      0   \n",
       "  2  179.892792   54.869110  197.868774   76.409119    0.877900     32   \n",
       "  3  426.234772  278.896698  479.390717  328.980499    0.715029     35   \n",
       "  4  142.878372  115.786713  285.997375  142.915344    0.679020     34   \n",
       "  \n",
       "               name  \n",
       "  0          person  \n",
       "  1          person  \n",
       "  2     sports ball  \n",
       "  3  baseball glove  \n",
       "  4    baseball bat  ,\n",
       "  'caption': ['A BASEBALL PLAYER IN A WHITE JERSEY HITTING THE BALL.',\n",
       "   'A baseball player in blue and white hitting a baseball.'],\n",
       "  'bbox_target': [252.7, 109.74, 209.4, 314.1]},\n",
       " 702: {'image_emb': tensor([[-0.4602,  0.0801, -0.2795,  ...,  0.3616, -0.0974,  0.3008],\n",
       "          [ 0.0166,  0.3284, -0.5200,  ...,  0.7788, -0.2766,  0.3552],\n",
       "          [ 0.2251,  0.7896, -0.6582,  ...,  1.1240, -0.0224,  0.1965],\n",
       "          ...,\n",
       "          [ 0.2793,  0.0523, -0.1838,  ...,  0.8828, -0.0058,  0.0605],\n",
       "          [-0.0529,  0.4170, -0.4690,  ...,  1.0586,  0.0636,  0.0505],\n",
       "          [ 0.0517,  0.1439, -0.6641,  ...,  0.4114, -0.1521,  0.2175]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2485,  0.2649, -0.3091,  ...,  0.1630, -0.3684,  0.0354],\n",
       "          [ 0.2710, -0.0313, -0.4290,  ..., -0.0594, -0.6729, -0.0963]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.4520e-04, 1.7212e-01, 2.3666e-02, 5.3027e-01, 4.3106e-03, 2.5439e-01,\n",
       "           1.5045e-02],\n",
       "          [3.3975e-05, 4.2877e-02, 7.0000e-03, 6.2012e-01, 3.1233e-04, 3.2178e-01,\n",
       "           7.6866e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  286.619324    8.206558  500.000000  349.812500    0.959139     41   \n",
       "  1  116.794930   34.919811  294.568695  148.777740    0.924809     54   \n",
       "  2   24.461210  109.900131  229.515747  217.983002    0.894892     54   \n",
       "  3    0.000000   16.463066  218.360092  108.065483    0.872875     56   \n",
       "  4  227.789688  129.650696  297.686340  183.165985    0.747322     54   \n",
       "  5   28.172577   41.863609  152.659409  127.032043    0.708368     54   \n",
       "  6    2.077413   31.094992  497.910889  367.532928    0.435052     60   \n",
       "  7   25.986416   40.796970  156.890747  125.355576    0.397924     56   \n",
       "  \n",
       "             name  \n",
       "  0           cup  \n",
       "  1         donut  \n",
       "  2         donut  \n",
       "  3         chair  \n",
       "  4         donut  \n",
       "  5         donut  \n",
       "  6  dining table  \n",
       "  7         chair  ,\n",
       "  'caption': ['A brown color donut extreme to the class',\n",
       "   'A chocolate donut.'],\n",
       "  'bbox_target': [26.1, 39.56, 122.5, 91.91]},\n",
       " 703: {'image_emb': tensor([[-0.4844,  0.4583, -0.0739,  ...,  0.8984,  0.0355, -0.1467],\n",
       "          [ 0.0601,  0.4868,  0.0743,  ...,  1.0420,  0.7129,  0.1854],\n",
       "          [ 0.0458,  0.4272, -0.1680,  ...,  0.6646,  0.3010, -0.0077],\n",
       "          [ 0.0383,  0.4385, -0.3777,  ...,  1.1260,  0.3042, -0.0911],\n",
       "          [-0.4165,  0.2595, -0.2847,  ...,  0.9971,  0.3171, -0.2279],\n",
       "          [-0.0167,  0.1874,  0.1636,  ...,  0.7114,  0.2479, -0.4104]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0613, -0.1353,  0.0198,  ..., -0.1085,  0.2229, -0.0244],\n",
       "          [-0.2222,  0.4832, -0.1432,  ..., -0.2944,  0.2471,  0.0388]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.5806e-03, 3.9124e-02, 5.4004e-01, 2.3861e-03, 1.2238e-01, 2.9346e-01],\n",
       "          [7.2193e-04, 1.1721e-03, 5.9766e-01, 1.2040e-05, 3.7402e-01, 2.6260e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  195.308014  219.197662  335.821472  333.906494    0.937343     66  keyboard\n",
       "  1   32.006420  103.484467  140.635498  233.972290    0.915963     63    laptop\n",
       "  2  262.289429   57.935394  446.702850  288.390167    0.915613     62        tv\n",
       "  3  245.160797  340.866760  309.056061  373.765381    0.896232     64     mouse\n",
       "  4  144.315521   61.714409  273.373169  187.699127    0.864544     62        tv\n",
       "  5   41.126038  181.829971  137.216614  212.596710    0.481508     66  keyboard\n",
       "  6    1.111913  175.828201   27.207045  219.324661    0.269895     41       cup,\n",
       "  'caption': ['The computer that has a flat screen and is in the niddle of the other two.',\n",
       "   \"the computer labeled with 'the itunes'\"],\n",
       "  'bbox_target': [144.62, 63.02, 132.85, 129.48]},\n",
       " 704: {'image_emb': tensor([[ 0.3464,  0.0497, -0.3660,  ...,  0.6221, -0.3723,  0.3313],\n",
       "          [-0.0806,  0.0476, -0.3098,  ...,  0.5620, -0.0976,  0.1343],\n",
       "          [ 0.1270,  0.2734, -0.2335,  ...,  0.5669, -0.4873,  0.3381]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 6.8665e-02, -1.7676e-01, -1.1853e-01,  1.7102e-01,  1.4307e-01,\n",
       "           -3.2166e-02, -2.6489e-01, -6.0107e-01, -2.9150e-01, -4.5361e-01,\n",
       "            3.6914e-01, -3.1323e-01, -5.1611e-01, -2.8833e-01,  1.7480e-01,\n",
       "            2.4756e-01,  4.9225e-02,  1.9958e-01, -1.8408e-01,  2.5415e-01,\n",
       "            2.3694e-01,  4.1724e-01,  4.2261e-01, -1.8518e-01,  1.9385e-01,\n",
       "           -2.6245e-01, -2.1570e-01,  1.3123e-01, -2.0459e-01, -1.6394e-01,\n",
       "           -9.8083e-02,  4.9835e-02, -1.3504e-02, -2.3962e-01, -6.2305e-01,\n",
       "            3.7866e-01,  7.7698e-02,  1.8909e-01, -6.8420e-02,  1.6956e-01,\n",
       "           -2.0483e-01,  6.7810e-02,  1.3831e-01,  8.8806e-02,  3.7695e-01,\n",
       "           -5.1147e-02,  6.5137e-01,  1.2091e-01, -2.1497e-01, -2.9346e-01,\n",
       "           -4.0552e-01, -3.9185e-01,  9.0759e-02, -1.2610e-01, -5.7373e-01,\n",
       "           -3.0713e-01,  3.1860e-01, -1.3660e-01,  2.0874e-01,  4.2432e-01,\n",
       "            2.3083e-01,  3.5010e-01, -1.1560e-01, -2.5708e-01,  3.3521e-01,\n",
       "           -3.5303e-01, -2.0557e-01,  2.2070e-01,  4.1473e-02,  6.8787e-02,\n",
       "            1.5808e-01, -3.6206e-01, -2.5284e-02,  1.0266e-01,  4.8657e-01,\n",
       "           -4.1650e-01, -9.0271e-02, -2.6147e-01,  3.7018e-02, -4.7070e-01,\n",
       "           -1.9629e-01,  2.4536e-01, -4.5776e-02, -1.1932e-01,  2.2766e-02,\n",
       "           -1.6113e-01,  9.5825e-02,  1.1450e-01,  3.2715e-01, -3.0411e-02,\n",
       "           -1.9006e-01, -4.3018e-01, -1.0156e+00,  2.1130e-01, -4.1675e-01,\n",
       "            6.9702e-02,  7.3395e-03, -1.2560e-03,  9.9426e-02, -3.3252e-01,\n",
       "           -1.2622e-01, -7.6660e-02,  3.3545e-01,  1.7236e-01, -5.6305e-02,\n",
       "            2.6245e-01, -1.6284e-01,  5.7715e-01,  6.8726e-02, -1.5295e-01,\n",
       "           -8.1848e-02, -2.3242e-01, -1.5186e-01,  9.5215e-02, -1.2476e-01,\n",
       "           -8.9539e-02, -5.6732e-02,  5.0781e-01,  1.1493e-01,  2.8687e-01,\n",
       "            5.6549e-02, -4.1357e-01, -1.0597e-02, -1.7627e-01,  4.3579e-01,\n",
       "           -1.6833e-01,  6.7444e-02, -2.5684e-01,  4.0869e-01,  4.4800e-01,\n",
       "            1.8250e-01,  2.1619e-01,  2.8503e-02,  4.0039e+00, -1.2891e-01,\n",
       "           -2.3499e-01,  5.3906e-01, -1.2299e-01, -1.1487e-01, -2.5098e-01,\n",
       "           -1.5839e-02,  7.4707e-02, -5.0098e-01, -9.7229e-02,  1.0315e-01,\n",
       "           -2.0920e-02,  1.9519e-01, -2.4207e-01, -1.5063e-01, -1.0443e-01,\n",
       "           -7.2021e-02, -1.8982e-01,  1.8982e-01,  2.2363e-01, -1.0216e-02,\n",
       "            3.0420e-01,  9.2285e-02,  7.1487e-03,  5.5811e-01, -7.9895e-02,\n",
       "           -1.0657e-01,  1.6434e-02, -4.1504e-01, -3.0615e-01, -3.4888e-01,\n",
       "           -3.6499e-01,  5.9766e-01,  5.5939e-02,  3.9581e-02, -1.4490e-01,\n",
       "            1.8591e-01, -1.5210e-01, -5.9717e-01,  3.9868e-01, -5.0842e-02,\n",
       "           -1.5820e-01, -2.1899e-01, -2.7808e-01,  3.2935e-01,  1.8958e-01,\n",
       "            5.2757e-03,  4.3506e-01, -5.5322e-01, -2.2961e-01,  5.6122e-02,\n",
       "           -1.7517e-01,  2.2607e-01, -3.7476e-01, -4.0771e-01, -1.3635e-01,\n",
       "            3.3423e-01,  1.6125e-01, -8.2031e-02, -5.9113e-02, -3.3643e-01,\n",
       "            9.6375e-02, -4.7339e-01,  4.9048e-01,  9.1553e-02, -1.8604e-01,\n",
       "            1.3062e-01, -5.9814e-01,  1.0675e-01, -1.0376e-01,  8.5815e-02,\n",
       "           -2.4963e-01,  1.1957e-01, -2.8711e-01, -3.5132e-01, -7.5745e-02,\n",
       "            4.2798e-01, -2.7612e-01,  3.8116e-02,  3.0811e-01, -1.7334e-01,\n",
       "           -3.5547e-01, -2.8882e-01, -2.2964e-02,  1.6968e-01,  2.1631e-01,\n",
       "            1.3229e-02,  5.8789e-01, -3.5858e-02, -4.7949e-01, -1.4880e-01,\n",
       "            1.5955e-01,  4.9121e-01, -3.4399e-01, -8.2214e-02, -9.4971e-02,\n",
       "           -2.1350e-01, -1.1926e-01,  2.5903e-01,  1.1456e-01, -2.4585e-01,\n",
       "            4.7827e-01, -1.3237e-02,  4.1284e-01, -2.2034e-01, -2.2937e-01,\n",
       "           -1.9800e-01,  3.6523e-01,  1.1169e-01,  1.0199e-01, -1.9568e-01,\n",
       "           -4.2480e-01,  1.7932e-01,  7.2144e-02,  1.0431e-01, -1.0278e-01,\n",
       "           -3.5693e-01,  1.7114e-01,  6.5820e-01, -1.4832e-01,  2.7588e-01,\n",
       "           -2.4933e-02,  2.4622e-01, -2.0886e-01,  2.3645e-01,  4.5264e-01,\n",
       "           -4.6094e-01,  1.1053e-01,  7.2144e-02, -5.2582e-02, -1.7871e-01,\n",
       "            6.5039e-01,  2.2546e-01, -3.7280e-01, -1.6565e-01,  2.6514e-01,\n",
       "           -1.2976e-01,  2.0337e-01, -1.7957e-01,  3.1348e-01, -8.3862e-02,\n",
       "           -1.7993e-01,  4.3652e-01,  1.8542e-01,  3.3667e-01, -3.6285e-02,\n",
       "            1.3428e-03, -9.2651e-02,  4.0918e-01,  3.9526e-01,  1.2100e-02,\n",
       "            9.8267e-02,  1.5213e-02,  2.2131e-01,  1.2012e-01,  3.6102e-02,\n",
       "           -6.6064e-01,  1.0541e-01,  2.3096e-01, -1.1041e-01, -3.8208e-01,\n",
       "            4.0332e-01, -4.4482e-01,  3.8605e-02,  7.3120e-02,  1.1749e-01,\n",
       "            6.6040e-02, -3.4888e-01, -1.5723e-01, -2.3840e-01,  2.0190e-01,\n",
       "           -3.9185e-01,  6.0272e-02, -2.3254e-01,  1.4685e-01,  7.0801e-02,\n",
       "            6.3110e-02,  3.9526e-01,  3.9980e+00, -8.2581e-02, -1.9653e-01,\n",
       "            1.4954e-01,  4.2188e-01, -3.3813e-02,  1.3538e-01,  2.3254e-01,\n",
       "            8.4045e-02,  2.6099e-01,  7.3181e-02, -6.1572e-01, -3.1128e-01,\n",
       "            7.8796e-02,  3.4448e-01, -1.0468e-01,  9.0881e-02, -1.1523e+00,\n",
       "            2.3132e-01, -2.3474e-01, -1.1438e-01, -1.7578e-01, -1.2964e-01,\n",
       "           -3.5742e-01, -1.6760e-01,  6.3416e-02,  1.7899e-02, -9.1553e-02,\n",
       "           -5.2637e-01, -3.4717e-01,  1.4905e-01, -1.5039e-01, -1.9946e-01,\n",
       "           -4.5128e-03,  3.7549e-01,  1.9641e-01,  1.6333e-01,  3.1201e-01,\n",
       "           -5.3467e-01,  1.5900e-02,  4.0698e-01,  3.8892e-01, -3.9551e-01,\n",
       "           -2.8296e-01, -1.2537e-01, -1.8689e-01,  2.6340e-03, -1.1267e-01,\n",
       "            3.6523e-01, -4.0869e-01, -1.1121e-01,  8.7256e-01, -1.6443e-01,\n",
       "           -1.6016e-01, -3.3741e-03,  3.4741e-01, -1.8274e-01, -1.5259e-01,\n",
       "           -5.4810e-02, -4.0845e-01,  2.5903e-01,  1.4856e-01, -5.8655e-02,\n",
       "            2.1704e-01, -1.2366e-01, -4.5509e-03,  4.8999e-01,  3.1982e-01,\n",
       "            5.3680e-02,  2.0032e-01,  2.3145e-01,  6.4697e-02, -8.5266e-02,\n",
       "            1.2524e-01, -2.3590e-02,  3.1543e-01, -2.6172e-01,  5.0049e-03,\n",
       "           -7.4902e-01,  2.8613e-01,  1.3191e-02, -1.5552e-01, -5.4779e-02,\n",
       "            2.0874e-02,  2.3938e-01,  5.2100e-01, -3.7231e-01,  3.5461e-02,\n",
       "           -1.1328e-01, -3.5376e-01, -1.1261e-01,  1.5186e-01,  2.3926e-01,\n",
       "            3.4619e-01,  8.3557e-02, -6.7688e-02,  4.2334e-01, -7.4646e-02,\n",
       "            2.9126e-01,  1.3458e-02, -7.8491e-02,  1.9519e-01,  3.5059e-01,\n",
       "           -2.0654e-01,  1.8539e-03, -2.1851e-01,  4.2542e-02, -1.6772e-01,\n",
       "            5.1172e-01, -2.3682e-01,  2.7148e-01, -2.1716e-01,  8.8867e-02,\n",
       "           -3.2520e-01, -2.8784e-01,  2.4597e-01,  3.6084e-01, -2.4255e-01,\n",
       "            1.0687e-01,  2.8366e-02, -6.7432e-01,  4.6692e-02,  1.1774e-01,\n",
       "            1.2390e-01,  1.2335e-01,  3.1769e-02, -1.8066e-01,  3.0533e-02,\n",
       "           -3.2202e-01, -3.5980e-02,  6.3416e-02, -2.5879e-01,  5.9723e-02,\n",
       "            2.6904e-01, -2.3572e-01, -6.6064e-01, -1.2732e-01,  1.3721e-01,\n",
       "            3.0420e-01,  1.3062e-01,  3.0807e-02, -2.5195e-01,  5.2197e-01,\n",
       "           -3.1372e-01,  5.1855e-01,  1.4514e-01,  7.2632e-02,  3.0347e-01,\n",
       "           -1.6724e-01,  1.0913e-01,  1.5289e-02,  4.1699e-01,  4.9829e-01,\n",
       "            3.7451e-01,  3.0176e-01, -3.8770e-01, -1.2732e-01, -1.3818e-01,\n",
       "            7.6477e-02, -2.3328e-01,  2.2498e-01, -7.9285e-02,  4.0283e-02,\n",
       "           -2.8979e-01, -3.1921e-02, -2.9541e-01,  2.0667e-01,  4.0845e-01,\n",
       "            1.2903e-01, -2.1765e-01,  3.4521e-01,  1.9446e-01, -8.8501e-03,\n",
       "            3.4271e-02,  1.0541e-01,  3.2129e-01,  8.1253e-03, -2.6367e-01,\n",
       "            5.0781e-01,  2.9639e-01, -2.2314e-01,  1.0967e+00,  5.6934e-01,\n",
       "            3.0396e-01, -8.5022e-02, -1.3745e-01, -2.8394e-01,  3.2928e-02,\n",
       "            3.9429e-02,  1.6724e-01, -3.9032e-02,  3.1714e-01,  2.8247e-01,\n",
       "            2.3178e-02,  6.0577e-02, -1.2922e-03,  1.5015e-01, -7.3303e-02,\n",
       "            3.2153e-01,  5.6396e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.4167, 0.1792, 0.4041]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0    1.583153  219.848175  206.999786  344.399139    0.859457      6  train\n",
       "  1  195.161377  187.885712  350.097290  275.812042    0.800881      6  train,\n",
       "  'caption': ['The front of a red train, making its way through the forest.'],\n",
       "  'bbox_target': [192.52, 187.94, 158.49, 89.49]},\n",
       " 705: {'image_emb': tensor([[-0.1931,  0.4077, -0.2147,  ...,  1.0869,  0.0036, -0.3142],\n",
       "          [-0.0033,  0.7227, -0.2942,  ...,  0.9897,  0.0897, -0.1520],\n",
       "          [ 0.0912,  0.4419, -0.1749,  ...,  0.8169, -0.0848, -0.3540],\n",
       "          [ 0.4644,  0.5342, -0.1371,  ...,  1.0166, -0.0778, -0.1833],\n",
       "          [ 0.5352,  0.3301, -0.2010,  ...,  1.1445, -0.1128, -0.0699]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0158, -0.2681, -0.2360,  ...,  0.3789, -0.1996, -0.1732],\n",
       "          [-0.0375,  0.1262, -0.1442,  ...,  0.1873, -0.0920, -0.0776]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.3027e-01, 6.1401e-02, 3.6450e-01, 2.2583e-02, 2.1210e-02],\n",
       "          [1.5015e-01, 8.1201e-01, 3.7964e-02, 1.4424e-05, 4.5180e-05]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  453.569641  114.284622  615.422424  353.576050    0.934335     28  suitcase\n",
       "  1  364.770508  201.098541  509.203064  409.161163    0.924769     28  suitcase\n",
       "  2   98.042801  236.079529  393.816406  426.949158    0.897644     28  suitcase\n",
       "  3    0.000000    0.100571  638.608948  467.552063    0.770734      2       car\n",
       "  4   25.851265   96.422821  256.088562  274.066193    0.542560     24  backpack\n",
       "  5  383.291687  152.855560  461.114624  205.742523    0.451568     24  backpack\n",
       "  6   26.767517   97.790619  250.647491  268.508881    0.450488     28  suitcase\n",
       "  7  242.683395  122.465546  379.611877  247.364960    0.404997     26   handbag\n",
       "  8   29.097401  250.479034  115.484756  434.465698    0.253927     26   handbag,\n",
       "  'caption': ['second luggage closest to right',\n",
       "   'A greyish-black suitcase that is laying on its side with an orange ribbon tied around the handle'],\n",
       "  'bbox_target': [371.06, 194.16, 139.14, 213.57]},\n",
       " 706: {'image_emb': tensor([[-0.3655,  0.3176,  0.0610,  ...,  1.1943, -0.0899, -0.0057],\n",
       "          [-0.0728,  0.4751,  0.0946,  ...,  0.8247, -0.1129,  0.0989],\n",
       "          [-0.4133,  0.3191, -0.0626,  ...,  0.4663, -0.3467,  0.0143],\n",
       "          [-0.3618,  0.2402, -0.2693,  ...,  0.8184,  0.0120,  0.4658],\n",
       "          [-0.1245,  0.1860,  0.3398,  ...,  0.5068, -0.1136,  0.0099]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0528, -0.0787, -0.1893,  ...,  0.3892, -0.3687,  0.3242],\n",
       "          [ 0.1722, -0.0094, -0.2317,  ...,  0.0733, -0.2898, -0.0726]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.0054e-05, 1.3340e-04, 1.0000e+00, 8.2850e-06, 5.9605e-08],\n",
       "          [2.0035e-02, 1.4206e-02, 9.6533e-01, 2.5630e-04, 2.1696e-05]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  135.798798  258.757477  205.582672  327.986359    0.911597     56   \n",
       "  1  360.029633  261.775269  418.981781  347.151245    0.905308     56   \n",
       "  2    0.752197  305.332520  167.716461  422.301514    0.840723     57   \n",
       "  3  478.787598  224.407471  549.614990  289.717651    0.837577     62   \n",
       "  4  315.377075  332.610016  567.722168  423.034668    0.647722     57   \n",
       "  5  246.483047  267.712433  301.840576  346.667755    0.580979     58   \n",
       "  \n",
       "             name  \n",
       "  0         chair  \n",
       "  1         chair  \n",
       "  2         couch  \n",
       "  3            tv  \n",
       "  4         couch  \n",
       "  5  potted plant  ,\n",
       "  'caption': ['the back of a green plaid sofa chair', 'single seater sofa'],\n",
       "  'bbox_target': [315.05, 333.46, 253.16, 92.54]},\n",
       " 707: {'image_emb': tensor([[-0.4216,  0.3057,  0.0888,  ...,  0.9370, -0.0699, -0.0259],\n",
       "          [ 0.0439,  0.3049, -0.2291,  ...,  0.8989,  0.2505, -0.2778],\n",
       "          [-0.2395,  0.0848,  0.0154,  ...,  0.9111, -0.0157,  0.1068],\n",
       "          [ 0.2544,  0.1342, -0.1816,  ...,  1.0391,  0.1401, -0.0036],\n",
       "          [-0.0940,  0.2136, -0.0206,  ...,  0.7983, -0.1556, -0.1229]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0846, -0.0907, -0.1233,  ...,  0.4851,  0.4580,  0.0133],\n",
       "          [ 0.0970, -0.0481, -0.0289,  ...,  0.3589,  0.3357, -0.1377]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0490e-04, 9.3359e-01, 5.6946e-02, 8.9407e-07, 9.1553e-03],\n",
       "          [5.9271e-04, 6.2061e-01, 3.0713e-01, 3.5167e-06, 7.1838e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  247.823730  159.150391  454.361877  420.491577    0.949431      0   \n",
       "  1  442.934601   31.381622  638.926636  418.051605    0.936456      1   \n",
       "  2    0.282822    0.131271  281.972107  474.665955    0.925991      1   \n",
       "  3  316.649963  244.705750  356.621033  291.922913    0.703446     67   \n",
       "  4  582.639893  154.302719  639.861084  200.270691    0.435899     13   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1     bicycle  \n",
       "  2     bicycle  \n",
       "  3  cell phone  \n",
       "  4       bench  ,\n",
       "  'caption': ['The yellow bicycle leaning against the curb.',\n",
       "   'A yellow bicycle.'],\n",
       "  'bbox_target': [442.84, 26.67, 197.16, 396.38]},\n",
       " 708: {'image_emb': tensor([[ 0.1030,  0.3274, -0.1669,  ...,  1.2178, -0.1600, -0.3584],\n",
       "          [-0.1746,  0.3362,  0.1255,  ...,  1.3369,  0.0536, -0.1648],\n",
       "          [-0.1436,  0.4060, -0.1064,  ...,  1.1777,  0.0886, -0.2391],\n",
       "          [-0.2969,  0.1331,  0.1748,  ...,  1.4111,  0.0055, -0.2367],\n",
       "          [-0.1564, -0.0776, -0.2275,  ...,  1.0635,  0.0215, -0.4988]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.5015,  0.2847,  0.2325,  ...,  0.4202, -0.3596, -0.4260],\n",
       "          [-0.4858,  0.1692, -0.1372,  ...,  0.2169, -0.2859, -0.4438]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0017, 0.3174, 0.4204, 0.1752, 0.0854],\n",
       "          [0.0727, 0.2499, 0.1091, 0.0225, 0.5459]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    30.341171  227.357635   90.962540  401.014191    0.911060      0   \n",
       "  1    55.943604  406.242950  141.276245  456.630646    0.885739     45   \n",
       "  2    31.757530  547.025452  122.659058  589.705750    0.803789     45   \n",
       "  3    32.260033  520.815491  122.323151  563.784729    0.769200     45   \n",
       "  4   132.323517  342.863525  152.673218  399.467957    0.688654     39   \n",
       "  5     6.064201  247.064941   39.366127  406.230896    0.673297      0   \n",
       "  6     0.000000  315.883148  376.745544  639.171631    0.668682     60   \n",
       "  7   241.871124  438.784302  335.368378  501.577209    0.606172     45   \n",
       "  8   401.068176  228.436401  438.286621  400.821838    0.553144     69   \n",
       "  9   300.543945  423.543457  343.255615  463.832336    0.550172     44   \n",
       "  10  435.471191  221.767303  480.000000  440.998322    0.459438     69   \n",
       "  11  184.974091  323.884399  200.607452  346.423035    0.458334     39   \n",
       "  12    0.000000  445.480499   64.574997  501.685822    0.411423     45   \n",
       "  13  458.826477  367.075134  479.700623  402.607300    0.408312      0   \n",
       "  14  227.034760  397.056183  252.766510  426.119293    0.375599     39   \n",
       "  15  301.498657  244.985535  338.224182  275.602600    0.358720      0   \n",
       "  16  366.976562  229.313599  403.695557  394.316956    0.354959     69   \n",
       "  17    0.441757  542.254028   34.054062  591.726685    0.353782     45   \n",
       "  18  143.472610  326.706299  161.956070  385.575806    0.312577     39   \n",
       "  19  266.687408  395.747742  287.957062  418.620300    0.310028     41   \n",
       "  20    0.297668  516.484009   34.910721  565.066040    0.281255     45   \n",
       "  21  257.516479  292.982849  276.591919  316.489624    0.261228     62   \n",
       "  22  225.125031  358.465912  258.679901  377.257477    0.255823     44   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1           bowl  \n",
       "  2           bowl  \n",
       "  3           bowl  \n",
       "  4         bottle  \n",
       "  5         person  \n",
       "  6   dining table  \n",
       "  7           bowl  \n",
       "  8           oven  \n",
       "  9          spoon  \n",
       "  10          oven  \n",
       "  11        bottle  \n",
       "  12          bowl  \n",
       "  13        person  \n",
       "  14        bottle  \n",
       "  15        person  \n",
       "  16          oven  \n",
       "  17          bowl  \n",
       "  18        bottle  \n",
       "  19           cup  \n",
       "  20          bowl  \n",
       "  21            tv  \n",
       "  22         spoon  ,\n",
       "  'caption': ['This is a white table containing assorted cookware',\n",
       "   'A table full of kitchen utensils'],\n",
       "  'bbox_target': [0.0, 322.16, 376.81, 310.65]},\n",
       " 709: {'image_emb': tensor([[-0.2571, -0.0673, -0.2299,  ...,  1.1211, -0.3254,  0.2417],\n",
       "          [ 0.1074,  0.1379, -0.0415,  ...,  0.2058, -0.2428,  0.2986],\n",
       "          [ 0.1427,  0.1935, -0.0514,  ...,  0.6558, -0.2627, -0.0374],\n",
       "          [-0.3545,  0.0665,  0.1015,  ...,  0.6777, -0.1880, -0.0588],\n",
       "          [ 0.0630, -0.1326, -0.2446,  ...,  0.8696, -0.0112, -0.5122],\n",
       "          [-0.1052, -0.0872,  0.1736,  ..., -0.1066, -0.2345,  0.3777]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0315,  0.2103, -0.5425,  ..., -0.0475, -0.1561,  0.0394],\n",
       "          [-0.0822, -0.0978, -0.6738,  ...,  0.2930,  0.1003,  0.0457]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.3184e-01, 1.9989e-02, 4.5044e-02, 3.0322e-01, 4.3511e-06, 5.7936e-05],\n",
       "          [1.8079e-01, 3.2597e-03, 5.1308e-03, 8.1055e-01, 4.3440e-04, 6.6638e-05]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   539.295837   50.811462  639.696594  239.265686    0.941397      0   \n",
       "  1   271.695618   57.715179  385.894592  249.014252    0.935791      0   \n",
       "  2   397.590668  105.800186  489.147736  239.702576    0.914746      0   \n",
       "  3    33.658207   50.557098  129.095795  202.249573    0.907294      0   \n",
       "  4   407.712708  148.006653  421.522339  160.817566    0.805267     32   \n",
       "  5   417.141083  152.504272  444.935333  175.456055    0.537008     32   \n",
       "  6   416.758118  150.928436  446.945923  175.531403    0.517812     35   \n",
       "  7   162.308594  188.384796  185.395447  200.093903    0.328674     24   \n",
       "  8   301.834473   84.851501  310.331055  122.156616    0.287550     34   \n",
       "  9   107.130760  173.595154  134.371140  199.030334    0.273543     24   \n",
       "  10  523.302063  173.823364  579.741760  201.382507    0.263376     24   \n",
       "  11  577.903259   50.034515  640.000000  107.251373    0.251289      2   \n",
       "  \n",
       "                name  \n",
       "  0           person  \n",
       "  1           person  \n",
       "  2           person  \n",
       "  3           person  \n",
       "  4      sports ball  \n",
       "  5      sports ball  \n",
       "  6   baseball glove  \n",
       "  7         backpack  \n",
       "  8     baseball bat  \n",
       "  9         backpack  \n",
       "  10        backpack  \n",
       "  11             car  ,\n",
       "  'caption': ['A man sitting with a yellow hat and black shorts outside the fence.',\n",
       "   'Man with yellow hat sitting on the rail behind the fence.'],\n",
       "  'bbox_target': [37.25, 48.55, 106.96, 153.76]},\n",
       " 710: {'image_emb': tensor([[-0.2181,  0.4431,  0.4216,  ...,  0.3223,  0.0575,  0.1385],\n",
       "          [-0.3918,  0.0391,  0.0339,  ...,  0.1017, -0.1043,  0.0352],\n",
       "          [ 0.0153,  0.0261, -0.2498,  ...,  0.6636,  0.0494,  0.3447],\n",
       "          ...,\n",
       "          [-0.4446,  0.2668, -0.1401,  ...,  0.3809,  0.2474,  0.0345],\n",
       "          [-0.1232,  0.2379, -0.1409,  ...,  0.8579,  0.3799, -0.0607],\n",
       "          [-0.1016, -0.3757,  0.0713,  ...,  0.0898, -0.0560,  0.2023]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1877,  0.1578, -0.0339,  ..., -0.1852, -0.3848, -0.0691],\n",
       "          [-0.1841,  0.1615, -0.0043,  ..., -0.2361, -0.3845, -0.1031]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.8659e-03, 3.2910e-01, 3.7360e-04, 2.6083e-04, 5.6885e-01, 1.4236e-02,\n",
       "           7.9407e-02],\n",
       "          [5.1727e-03, 6.1719e-01, 4.7374e-04, 1.7703e-04, 2.5708e-01, 2.3926e-02,\n",
       "           9.6130e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  216.129303  201.341492  326.303467  379.935577    0.919597     56  chair\n",
       "  1    0.696683  120.133499  265.134674  378.350098    0.918824     59    bed\n",
       "  2  387.928406  138.370850  423.842346  198.254684    0.869241     56  chair\n",
       "  3  351.704163  154.571075  405.193481  240.276505    0.867513     56  chair\n",
       "  4  192.352707  115.597702  363.094269  239.609879    0.856218     59    bed\n",
       "  5  268.877228  112.129852  392.508118  185.105103    0.731825     59    bed,\n",
       "  'caption': ['the bed closest to the back wall',\n",
       "   'last bed closest to the wall in the back'],\n",
       "  'bbox_target': [268.82, 111.42, 125.72, 75.27]},\n",
       " 711: {'image_emb': tensor([[-0.2404,  0.2556, -0.1741,  ...,  1.4404,  0.1669, -0.2124],\n",
       "          [-0.1425,  0.6724, -0.1108,  ...,  1.3701,  0.2954,  0.1378],\n",
       "          [ 0.2102,  0.5762, -0.1887,  ...,  1.0273,  0.1389, -0.1237],\n",
       "          [-0.1796, -0.0750, -0.3306,  ...,  1.4121,  0.1432,  0.0964],\n",
       "          [ 0.2720,  0.3071, -0.1864,  ...,  0.9238,  0.1874,  0.2429]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3508, -0.3335, -0.1153,  ...,  0.3149,  0.0110,  0.3552],\n",
       "          [-0.2856, -0.0944, -0.1787,  ...,  0.4016,  0.1807,  0.0684]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.1040e-01, 3.1647e-02, 2.8973e-03, 2.8366e-02, 5.2686e-01],\n",
       "          [7.4707e-01, 1.0330e-02, 2.2471e-04, 9.9548e-02, 1.4258e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':         xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   0.412977  171.581345  276.838501  245.343811    0.917738     42   \n",
       "  1   0.000000   74.035789  254.379135  162.845749    0.869042     43   \n",
       "  2   2.770877    2.079463  429.624756  235.819504    0.867630     53   \n",
       "  3  42.893028  235.751938  497.207916  315.211700    0.815591     43   \n",
       "  4   1.295209    0.000000  500.000000  364.891479    0.526568     60   \n",
       "  \n",
       "             name  \n",
       "  0          fork  \n",
       "  1         knife  \n",
       "  2         pizza  \n",
       "  3         knife  \n",
       "  4  dining table  ,\n",
       "  'caption': ['A serrated knife.', 'a steak knife with a black handle'],\n",
       "  'bbox_target': [41.2, 236.22, 458.8, 79.04]},\n",
       " 712: {'image_emb': tensor([[ 0.3362,  0.0205, -0.1885,  ...,  0.8716,  0.3020,  0.0592],\n",
       "          [-0.0968,  0.2332, -0.5000,  ...,  0.8315,  0.0861, -0.0445],\n",
       "          [ 0.2285, -0.0858,  0.2642,  ...,  0.5166,  0.1447, -0.2148],\n",
       "          [ 0.2605, -0.1376, -0.0765,  ...,  0.6143,  0.1473,  0.0684]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-2.0691e-01,  4.8877e-01, -2.4292e-02, -1.0065e-01,  2.8027e-01,\n",
       "            3.4277e-01,  2.4399e-02, -4.1821e-01,  5.5908e-02,  6.4392e-02,\n",
       "            3.7329e-01,  9.0149e-02,  4.4482e-01,  2.8491e-01, -3.0322e-01,\n",
       "           -1.6040e-01,  4.2456e-01,  2.2168e-01, -2.4060e-01, -5.3772e-02,\n",
       "            6.3184e-01, -7.2937e-02,  1.0742e-01, -5.2539e-01, -4.5227e-02,\n",
       "            4.2456e-01, -2.6831e-01,  8.1665e-02, -1.6272e-01, -5.5603e-02,\n",
       "            5.8014e-02, -6.1722e-03, -9.4788e-02,  3.5522e-01, -3.2471e-02,\n",
       "            2.5171e-01, -1.5747e-01,  3.3716e-01, -2.4707e-01,  4.5459e-01,\n",
       "           -1.8518e-01,  1.1597e-01, -3.7939e-01,  1.9128e-01,  2.6025e-01,\n",
       "            3.9258e-01,  1.5833e-01,  9.7656e-02, -4.6948e-01, -4.4971e-01,\n",
       "            3.4277e-01, -2.9199e-01, -3.5547e-01, -4.3018e-01, -3.0176e-01,\n",
       "           -8.0200e-02,  1.5784e-01,  2.0349e-01,  2.3376e-01, -1.5417e-01,\n",
       "            4.6826e-01, -1.6394e-01,  2.6367e-01,  4.1797e-01,  2.0178e-01,\n",
       "            1.2219e-01, -4.4507e-01,  8.7256e-01, -4.4019e-01, -3.2764e-01,\n",
       "           -2.3608e-01,  5.9998e-02,  2.1594e-01,  4.5435e-01,  3.8721e-01,\n",
       "           -5.4138e-02,  4.2358e-02,  1.4977e-02, -9.0881e-02, -2.5854e-01,\n",
       "           -1.5259e-01,  9.8755e-02, -5.0507e-02, -4.0088e-01,  1.9141e-01,\n",
       "            2.2522e-01, -1.3489e-01, -2.7863e-02,  2.0105e-01,  5.4535e-02,\n",
       "           -2.9565e-01, -8.4106e-02, -8.9111e-01, -3.5522e-01,  4.9268e-01,\n",
       "           -9.1675e-02, -7.3242e-02,  2.7847e-02,  1.5540e-01, -2.9810e-01,\n",
       "           -7.8674e-02,  1.6797e-01,  1.4514e-01, -9.3506e-02, -6.7200e-02,\n",
       "           -3.1274e-01, -4.5703e-01,  7.0947e-01,  3.1348e-01, -1.7761e-01,\n",
       "           -1.9922e-01, -5.4297e-01, -6.2683e-02,  8.3557e-02, -1.7456e-01,\n",
       "            8.3496e-02, -4.3243e-02,  3.4375e-01, -1.9788e-01, -3.6530e-02,\n",
       "            4.1321e-02, -5.0244e-01, -2.3254e-01, -4.2358e-01, -9.9670e-02,\n",
       "           -7.9895e-02, -1.6528e-01, -2.1265e-01, -6.9641e-02,  5.9570e-01,\n",
       "            3.9087e-01, -9.0271e-02, -7.0679e-02,  3.7168e+00,  4.4830e-02,\n",
       "           -2.6733e-02, -2.2778e-01, -2.7466e-01,  7.4463e-02, -8.0688e-02,\n",
       "           -6.4964e-03, -5.0244e-01, -8.6035e-01,  4.2749e-01,  1.5735e-01,\n",
       "           -1.1285e-01, -4.3182e-02,  2.1912e-01, -1.7090e-01, -1.4697e-01,\n",
       "           -2.7490e-01,  1.7334e-02,  3.9380e-01,  2.9053e-01,  1.3782e-01,\n",
       "            1.2262e-01,  2.5098e-01, -4.4189e-01,  1.8567e-01,  4.6021e-01,\n",
       "            1.4539e-01,  2.7881e-01, -1.7566e-01, -2.3804e-01, -2.6978e-01,\n",
       "            2.3413e-01,  3.3374e-01, -1.0522e-01, -1.8323e-01,  4.2458e-03,\n",
       "            1.0187e-01, -7.5134e-02, -1.3367e-01,  1.0034e-01, -7.0850e-01,\n",
       "            2.6172e-01,  5.9229e-01, -2.9639e-01,  1.2488e-01, -3.9185e-01,\n",
       "           -1.2146e-02,  1.0339e-01,  1.1230e-02,  2.9938e-02,  3.4668e-02,\n",
       "           -1.9299e-01,  2.8336e-02, -2.9272e-01, -1.9678e-01,  4.0063e-01,\n",
       "            1.7542e-01,  2.7612e-01, -1.4612e-01, -1.8811e-01, -5.0781e-01,\n",
       "            6.3232e-02, -2.2925e-01,  2.2009e-01, -2.5818e-02, -3.1738e-02,\n",
       "           -8.0078e-02, -7.0984e-02,  8.5876e-02,  2.8296e-01,  1.6638e-01,\n",
       "           -2.6074e-01,  8.8930e-05, -1.2646e-01, -3.6694e-01,  2.8125e-01,\n",
       "            3.0957e-01,  1.6357e-01, -2.6025e-01,  7.6025e-01, -8.3557e-02,\n",
       "            9.6130e-02,  4.2700e-01, -2.6172e-01, -5.7709e-02, -1.0541e-01,\n",
       "            1.9226e-01,  1.1041e-01, -1.1365e-01, -1.3977e-01, -2.6294e-01,\n",
       "            8.1970e-02, -7.4463e-02, -2.1973e-01, -1.9824e-01,  1.6528e-01,\n",
       "           -4.7534e-01,  1.2903e-01, -2.8046e-02,  4.0820e-01, -5.1025e-02,\n",
       "           -2.1143e-01,  3.2983e-01, -1.8835e-01, -1.8823e-01, -1.4795e-01,\n",
       "            1.4122e-02,  2.1899e-01,  4.3262e-01,  3.4271e-02, -2.2168e-01,\n",
       "           -4.4098e-02,  1.0431e-01,  1.0956e-01,  3.5797e-02, -1.7139e-01,\n",
       "           -2.3914e-01, -1.0736e-01,  2.8613e-01, -1.6342e-02, -5.1544e-02,\n",
       "           -1.4954e-01, -2.1729e-01,  3.4973e-02,  2.4451e-01, -8.1604e-02,\n",
       "            2.0520e-01,  1.4917e-01,  2.8564e-01, -5.1855e-01,  3.3838e-01,\n",
       "           -7.0984e-02,  2.1045e-01, -3.8916e-01,  1.1731e-01, -2.3022e-01,\n",
       "            5.8441e-02, -8.0994e-02, -3.0640e-02, -2.8857e-01, -1.6418e-01,\n",
       "           -7.4890e-02, -8.4839e-02, -2.3267e-01, -1.3382e-02, -2.6782e-01,\n",
       "           -1.2091e-01, -3.6279e-01,  1.7249e-01, -3.1067e-02,  4.6661e-02,\n",
       "            3.1909e-01, -1.1902e-01,  9.6313e-02,  5.5664e-01,  1.6907e-02,\n",
       "            1.7566e-01, -2.2858e-02,  3.4790e-02,  2.5928e-01, -3.5614e-02,\n",
       "            1.6516e-01, -3.2300e-01, -1.4221e-01, -2.8369e-01, -6.6748e-01,\n",
       "            3.7256e-01,  1.7212e-01,  4.4067e-02,  2.1988e-02, -2.0703e-01,\n",
       "           -9.8047e-01,  2.7319e-01,  8.9172e-02,  2.2192e-01, -1.7444e-01,\n",
       "           -1.2091e-01,  3.4485e-02,  3.7168e+00,  2.0740e-01,  5.1636e-02,\n",
       "            4.3066e-01,  1.8420e-01, -9.4238e-02,  3.8013e-01, -2.6538e-01,\n",
       "            4.6387e-02, -2.4341e-01,  2.1594e-01,  4.2053e-02, -2.8711e-01,\n",
       "            6.4583e-03, -2.5122e-01, -2.7069e-02, -1.6711e-01, -9.0820e-01,\n",
       "            2.2839e-01, -3.7872e-02, -5.1544e-02, -2.4976e-01, -1.0223e-01,\n",
       "            1.0126e-01, -2.6465e-01, -1.7969e-01,  5.9473e-01, -4.5074e-02,\n",
       "           -1.4563e-01, -4.3799e-01,  1.1261e-01, -4.9500e-02,  1.2659e-01,\n",
       "           -1.5002e-01,  5.7666e-01, -1.7960e-02, -5.9479e-02, -5.0098e-01,\n",
       "            2.6196e-01,  7.5256e-02,  2.9419e-01,  2.3132e-01, -2.1301e-01,\n",
       "            6.0699e-02, -4.2749e-01,  7.9346e-02, -6.8703e-03, -8.1726e-02,\n",
       "            7.9529e-02, -5.3857e-01, -5.8899e-02,  2.9883e-01,  4.6729e-01,\n",
       "            1.5112e-01, -3.2373e-01,  1.1450e-01,  3.7817e-01,  1.5466e-01,\n",
       "           -5.9387e-02,  7.4463e-02,  3.2666e-01,  3.9258e-01, -1.3098e-01,\n",
       "            6.0059e-02,  3.0591e-01, -3.3008e-01, -3.6914e-01,  2.5488e-01,\n",
       "           -6.4331e-02,  7.7100e-01,  2.6929e-01, -1.4062e-01, -1.5649e-01,\n",
       "            2.7246e-01, -1.4307e-01, -1.4734e-01, -9.1492e-02,  1.1731e-01,\n",
       "           -1.8274e-01, -3.3594e-01, -2.6050e-01,  1.2054e-01,  3.5425e-01,\n",
       "            6.0596e-01,  2.4536e-01, -1.1017e-01,  1.9974e-02, -8.5449e-02,\n",
       "            3.2867e-02,  1.6455e-01, -1.9153e-01,  5.0146e-01, -7.4097e-02,\n",
       "           -4.4287e-01,  4.0619e-02, -4.6448e-02,  4.4629e-01,  3.4204e-01,\n",
       "           -3.5736e-02,  1.7883e-01, -5.8655e-02,  4.5441e-02, -4.0970e-03,\n",
       "           -2.6172e-01,  1.7471e-02, -1.1346e-01,  5.5725e-02, -3.9136e-01,\n",
       "            1.2561e-01,  2.0874e-02,  3.9136e-01, -5.5273e-01, -4.8187e-02,\n",
       "           -2.7051e-01,  5.0262e-02,  3.0273e-01,  1.5088e-01, -3.1647e-02,\n",
       "            1.1182e-01,  4.2651e-01, -5.0146e-01, -4.4531e-01, -5.5273e-01,\n",
       "            8.8684e-02, -1.3782e-01,  1.2842e-01, -4.5227e-02,  3.0664e-01,\n",
       "           -2.1155e-01,  4.2847e-01, -2.1484e-02, -2.4390e-01,  3.1958e-01,\n",
       "            1.3342e-01, -3.2642e-01, -2.2034e-01, -1.4062e-01,  3.5181e-01,\n",
       "           -5.4395e-01,  7.9773e-02, -6.2073e-02,  3.5400e-01, -6.5430e-02,\n",
       "           -1.2177e-01,  2.6611e-01,  2.4002e-02, -3.2739e-01, -1.3428e-01,\n",
       "            2.7084e-02, -7.4402e-02, -5.0171e-02,  1.9141e-01,  3.0933e-01,\n",
       "            1.5637e-01, -8.6670e-02, -1.2671e-01, -1.2866e-01,  3.9185e-01,\n",
       "            8.8501e-02,  8.9355e-02,  1.7480e-01, -9.6741e-02, -5.3253e-02,\n",
       "            2.2290e-01,  2.5073e-01, -4.1389e-03,  2.9392e-03,  1.2549e-01,\n",
       "           -1.2457e-01, -6.7627e-01,  4.7363e-01, -3.7933e-02, -1.0443e-01,\n",
       "            9.1614e-02,  1.5186e-01,  2.0984e-01, -1.1469e-01, -2.0947e-01,\n",
       "            1.1032e-02, -2.0416e-02, -1.2054e-01,  1.2314e+00, -9.2102e-02,\n",
       "           -1.1145e-01,  1.2952e-01, -1.3269e-01, -1.2671e-01,  3.4570e-01,\n",
       "           -5.7343e-02,  2.5244e-01,  1.1395e-01,  5.5615e-01,  4.4775e-01,\n",
       "            3.4351e-01, -1.9116e-01, -1.5930e-01, -1.1584e-01, -4.4238e-01,\n",
       "            2.3422e-02, -2.6169e-02]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3784, 0.2993, 0.0134, 0.3088]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    0.000000   40.706848  639.259033  476.374390    0.907465      7   truck\n",
       "  1  456.590088  278.966431  612.138184  385.120422    0.867177      0  person\n",
       "  2    0.367203  156.527374  244.325775  358.193817    0.819185     19     cow\n",
       "  3  567.846863  253.654266  639.771179  346.580841    0.567728      0  person\n",
       "  4  504.520782  289.369690  546.566162  342.337708    0.422371      0  person\n",
       "  5   49.270386  101.809174  365.451752  359.530731    0.270909     19     cow,\n",
       "  'caption': ['A man leaning out of a truck facing the camera'],\n",
       "  'bbox_target': [457.33, 277.47, 156.07, 105.45]},\n",
       " 713: {'image_emb': tensor([[-0.1331,  0.1721, -0.1152,  ...,  0.9341, -0.2751,  0.0124],\n",
       "          [-0.1104,  0.3247, -0.1013,  ...,  0.8350, -0.3174,  0.1035],\n",
       "          [-0.4443,  0.2489,  0.0528,  ...,  0.7363, -0.2803,  0.0366]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-7.3059e-02,  1.7297e-01, -4.8096e-02, -2.2253e-01, -2.7847e-02,\n",
       "            2.1118e-01, -3.3423e-01, -1.1133e+00,  6.3110e-02, -1.2683e-01,\n",
       "            2.5806e-01, -8.9407e-04,  2.3317e-04, -3.3550e-03, -3.6670e-01,\n",
       "            1.7786e-01,  1.8335e-01, -1.6516e-01, -2.7637e-01,  6.2866e-02,\n",
       "           -1.5100e-01, -9.7168e-02, -1.2238e-01, -1.4612e-01, -2.3694e-01,\n",
       "           -8.8440e-02, -2.3499e-01,  1.5991e-01, -1.1053e-01,  1.4954e-01,\n",
       "           -3.4741e-01,  2.4377e-01, -2.8906e-01,  1.0706e-01, -7.1973e-01,\n",
       "            3.9032e-02, -8.9783e-02, -2.2705e-01, -3.3569e-01, -1.8103e-01,\n",
       "            2.7490e-01,  2.3422e-03, -2.4573e-01, -3.7671e-01,  2.5464e-01,\n",
       "            7.7515e-03, -3.6621e-02,  2.9565e-01, -2.3514e-02, -9.1492e-02,\n",
       "           -2.3232e-03, -2.2742e-01,  2.6904e-01,  3.9941e-01, -3.0078e-01,\n",
       "            1.4722e-01,  2.9114e-02, -1.7542e-01, -5.1221e-01,  1.6541e-01,\n",
       "            1.6309e-01, -8.6853e-02, -1.1340e-01, -2.9343e-02,  1.7163e-01,\n",
       "            2.4194e-01,  4.7437e-01,  6.4941e-02, -1.6235e-01, -1.9177e-01,\n",
       "            5.4657e-02, -1.8152e-01, -5.9448e-02,  1.8616e-01,  2.4878e-01,\n",
       "            1.6748e-01,  2.5171e-01, -2.4426e-01,  4.8676e-02, -3.3789e-01,\n",
       "           -3.7573e-01, -3.2300e-01, -1.2927e-01, -2.1460e-01, -7.7820e-02,\n",
       "            1.0211e-01,  2.4194e-01, -1.7493e-01, -5.2295e-01,  1.0223e-02,\n",
       "           -4.9194e-02, -2.4854e-01, -1.2119e+00,  3.8818e-01, -1.1255e-01,\n",
       "           -1.8530e-03,  3.8428e-01,  2.0239e-01, -5.1819e-02, -1.2329e-01,\n",
       "           -6.8016e-03,  3.2349e-01,  6.2256e-02,  1.7578e-01, -1.1383e-01,\n",
       "            4.4531e-01,  9.8816e-02,  1.4624e-01,  2.1347e-02, -3.4424e-01,\n",
       "           -4.1260e-02,  1.1237e-01,  1.0834e-01,  1.7480e-01,  2.4780e-01,\n",
       "           -1.2671e-01,  4.7119e-01,  8.1299e-02, -9.3231e-03,  2.9321e-01,\n",
       "           -1.5762e-02, -7.0898e-01, -2.3047e-01,  3.9844e-01, -1.4734e-01,\n",
       "           -3.0737e-01,  9.8328e-02, -2.2888e-01,  1.9922e-01,  3.4692e-01,\n",
       "           -1.4755e-02,  3.3228e-01, -6.3110e-02,  4.8828e+00, -1.0248e-01,\n",
       "            3.7476e-02, -2.3669e-01,  3.5791e-01,  1.5332e-01, -4.0552e-01,\n",
       "           -2.5635e-01, -1.0199e-01,  7.8917e-04,  5.5176e-01, -3.6255e-01,\n",
       "           -9.8145e-02, -1.2708e-01, -2.9858e-01,  2.6294e-01, -1.8506e-01,\n",
       "           -3.9844e-01, -5.6976e-02, -8.7708e-02,  2.9639e-01, -1.3374e-02,\n",
       "           -1.3586e-01,  2.0813e-01, -2.1460e-01,  3.5596e-01,  3.0054e-01,\n",
       "            2.7319e-01, -1.0510e-01,  4.7363e-02,  5.7098e-02, -1.8201e-01,\n",
       "            2.4939e-01, -3.9032e-02, -9.5291e-03, -1.3269e-01,  2.3972e-02,\n",
       "            1.2036e-01, -1.3562e-01,  4.8633e-01,  3.7964e-01, -4.0552e-01,\n",
       "            7.6172e-02,  2.6099e-01, -7.7637e-02, -1.3965e-01, -8.4290e-02,\n",
       "           -2.5854e-01,  9.9121e-02, -7.4097e-02, -2.5610e-01, -2.2229e-01,\n",
       "            2.7393e-01,  2.9639e-01, -1.8433e-01,  4.2139e-01, -5.2948e-02,\n",
       "           -5.0293e-01,  4.5508e-01,  2.4384e-02, -4.6118e-01, -5.9265e-02,\n",
       "            9.1370e-02,  4.9414e-01,  2.1942e-02, -4.0625e-01, -3.0615e-01,\n",
       "           -1.8384e-01,  4.1840e-02,  1.3611e-01, -1.2802e-02, -4.2542e-02,\n",
       "            3.3228e-01, -4.1187e-01,  3.3472e-01,  3.6835e-02, -2.0813e-01,\n",
       "            2.5171e-01,  2.3425e-01,  2.6636e-01, -2.3438e-01, -1.4136e-01,\n",
       "            5.2490e-02, -2.9526e-02,  2.2278e-01,  5.6152e-01, -7.8186e-02,\n",
       "            7.9163e-02,  4.4336e-01, -4.1772e-01,  1.5007e-02,  2.2510e-01,\n",
       "           -9.1553e-02,  1.6846e-01,  3.7781e-02,  2.9639e-01, -3.1372e-01,\n",
       "           -4.7583e-01, -2.9694e-02, -1.2927e-01,  3.8525e-01, -7.9590e-02,\n",
       "            2.8442e-01,  1.3098e-01, -1.5393e-01, -4.3915e-02, -2.8955e-01,\n",
       "           -2.0276e-01,  1.2756e-01,  8.1848e-02,  1.0992e-01,  1.6693e-02,\n",
       "            4.1528e-01,  4.0112e-01,  1.5366e-02,  3.9453e-01,  1.4014e-01,\n",
       "           -2.2473e-01,  7.7026e-02,  2.2021e-01, -2.8247e-01,  1.2866e-01,\n",
       "           -6.1150e-03,  6.6040e-02, -2.1289e-01, -1.9165e-01,  3.5675e-02,\n",
       "           -1.6846e-02,  2.6758e-01,  3.6597e-01,  1.8823e-01,  1.1060e-01,\n",
       "           -1.1224e-01, -1.6724e-02, -3.6426e-01,  2.4967e-03,  9.4788e-02,\n",
       "            1.5100e-01, -4.9561e-02, -2.9150e-01, -1.6467e-01, -4.5068e-01,\n",
       "           -1.7615e-01,  5.3027e-01,  1.6223e-01,  4.6173e-02,  5.4871e-02,\n",
       "            7.1143e-01, -6.7236e-01, -7.0374e-02,  2.9199e-01,  4.8999e-01,\n",
       "           -1.5808e-01,  1.9495e-01, -2.3059e-01,  4.9194e-01,  3.6035e-01,\n",
       "           -1.3965e-01,  7.6233e-02,  1.1041e-01,  9.7900e-02,  6.6589e-02,\n",
       "            1.4429e-01, -1.2329e-01, -3.8086e-02, -4.3115e-01, -9.5032e-02,\n",
       "           -1.2610e-01, -1.1249e-01, -1.9373e-01,  2.9907e-01,  5.0244e-01,\n",
       "           -2.0584e-02,  1.6199e-01,  3.5547e-01, -2.3157e-01, -2.8833e-01,\n",
       "           -1.6565e-01,  6.0938e-01,  4.8711e+00,  1.2585e-01, -4.6875e-01,\n",
       "           -1.3293e-01,  2.4280e-01, -4.3793e-03,  2.8174e-01, -1.1757e-02,\n",
       "           -2.9590e-01,  1.1676e-01, -2.6904e-01, -2.7075e-01,  4.5197e-02,\n",
       "           -5.3589e-02,  2.4438e-01,  3.9453e-01,  2.3157e-01, -1.8672e+00,\n",
       "           -1.3330e-01,  4.5557e-01,  2.2314e-01, -5.8154e-01, -1.3840e-04,\n",
       "            3.9746e-01,  1.4783e-01, -4.5319e-02,  2.6172e-01,  1.2598e-01,\n",
       "           -3.7689e-02, -5.9326e-02,  4.7900e-01, -3.9600e-01,  1.5430e-01,\n",
       "            3.0664e-01,  2.2949e-01,  4.3457e-01,  4.1695e-03, -4.2236e-01,\n",
       "           -5.9875e-02, -2.8955e-01, -1.9189e-01,  3.4119e-02, -1.7578e-01,\n",
       "           -1.4758e-01, -4.2896e-01, -7.6111e-02, -3.9749e-03,  1.0754e-01,\n",
       "           -4.0674e-01, -1.3538e-01, -1.7004e-01, -2.0142e-02, -1.9189e-01,\n",
       "           -2.4854e-01,  2.2913e-01,  3.7720e-02, -1.6919e-01,  2.0007e-01,\n",
       "            9.8206e-02, -7.0679e-02,  9.8694e-02,  3.0869e-02, -4.8340e-01,\n",
       "            4.4775e-01,  2.7979e-01, -2.1277e-01,  4.7394e-02,  2.1957e-02,\n",
       "            1.4046e-02,  1.0339e-01,  2.3938e-01, -1.3501e-01,  2.6074e-01,\n",
       "            8.5815e-02,  9.0637e-03, -2.1497e-01, -2.0309e-02, -2.2766e-01,\n",
       "           -1.1194e-01,  2.8369e-01,  3.0762e-02,  7.4768e-02,  2.0294e-02,\n",
       "            3.5229e-01,  2.9346e-01, -2.6001e-01, -1.7981e-01,  1.0864e-01,\n",
       "            6.4258e-01,  1.4069e-02,  2.1631e-01, -3.3423e-01, -5.7373e-01,\n",
       "           -1.3367e-01,  1.6333e-01,  5.4834e-01, -2.2144e-01, -3.0060e-02,\n",
       "            3.5278e-01,  3.2178e-01,  4.6844e-02,  5.0684e-01,  4.0869e-01,\n",
       "            1.0364e-01, -1.8726e-01,  9.2773e-02, -8.0017e-02, -6.9702e-02,\n",
       "            1.7166e-02, -4.7333e-02, -1.0138e-01, -2.7002e-01, -1.5332e-01,\n",
       "           -4.1260e-01,  1.6589e-01, -1.8469e-01,  2.3035e-01,  1.3171e-01,\n",
       "           -2.2812e-02, -9.1125e-02, -9.5276e-02,  4.4495e-02,  1.3969e-02,\n",
       "           -1.9226e-02,  2.1191e-01, -5.8533e-02,  4.7363e-02,  2.5244e-01,\n",
       "            4.6753e-02, -1.5161e-01, -3.6157e-01, -1.9250e-01,  1.3049e-01,\n",
       "            2.3877e-01, -1.1682e-01, -4.2212e-01,  1.2866e-01,  3.7451e-01,\n",
       "           -1.4929e-01,  5.6519e-02,  3.4546e-01,  3.1738e-01,  2.1411e-01,\n",
       "           -1.7737e-01,  2.4976e-01, -1.4648e-01,  1.4270e-01,  5.7910e-01,\n",
       "           -1.6418e-01,  1.6687e-01,  3.4326e-01, -9.7595e-02,  1.0095e-01,\n",
       "            1.9226e-01, -1.1322e-01, -3.1006e-01,  1.5247e-01,  3.1348e-01,\n",
       "           -3.2410e-02, -6.1218e-02, -1.5088e-01,  1.7249e-01, -8.6609e-02,\n",
       "           -4.9286e-02,  2.9102e-01,  2.1533e-01, -2.4377e-01, -1.5564e-01,\n",
       "           -8.5022e-02, -1.6089e-01,  2.8711e-01,  3.3984e-01,  2.7588e-01,\n",
       "            1.7786e-01, -3.2056e-01, -7.5054e-04,  1.3977e-01,  5.3215e-03,\n",
       "            1.3623e-01,  1.0254e-01,  1.4801e-02,  7.9004e-01,  4.1479e-01,\n",
       "           -3.7036e-01,  1.0986e-01, -1.5344e-01,  2.4170e-01, -2.6416e-01,\n",
       "           -4.5825e-01,  1.0193e-01, -1.7517e-02,  3.8281e-01, -3.4082e-01,\n",
       "            1.5454e-01, -3.0899e-02,  2.4060e-01,  3.2739e-01, -1.1406e-02,\n",
       "           -5.0928e-01, -4.3994e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5488, 0.3601, 0.0910]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class name\n",
       "  0  271.197815  202.713959  428.000000  373.081696    0.846934     59  bed\n",
       "  1    1.219345  212.350266  273.649963  637.796082    0.823169     59  bed\n",
       "  2    1.069679  457.944489  171.573395  638.301392    0.393409     59  bed,\n",
       "  'caption': [\"childen's double bed\"],\n",
       "  'bbox_target': [272.62, 204.19, 155.38, 172.88]},\n",
       " 714: {'image_emb': tensor([[ 0.0085, -0.0127, -0.3787,  ...,  0.3647,  0.2468,  0.0878],\n",
       "          [-0.1814,  0.5381,  0.0233,  ...,  0.9775,  0.0065, -0.2395],\n",
       "          [-0.0484, -0.0487, -0.0430,  ...,  1.0576, -0.0840, -0.6699],\n",
       "          ...,\n",
       "          [ 0.1801, -0.0307, -0.1738,  ...,  0.9307,  0.0840, -0.4294],\n",
       "          [-0.2021,  0.1108, -0.2749,  ...,  0.9863, -0.0646, -0.0417],\n",
       "          [ 0.0957,  0.0237, -0.2308,  ...,  0.3496,  0.2634,  0.0031]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1522,  0.0575, -0.3186,  ...,  0.4287, -0.0870, -0.1263],\n",
       "          [ 0.0851, -0.0364,  0.1932,  ..., -0.0386,  0.2366, -0.2029]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0000e+00, 7.4902e-01, 0.0000e+00, 1.7881e-07, 2.5098e-01, 0.0000e+00,\n",
       "           5.9605e-08, 1.7881e-07, 1.1325e-06],\n",
       "          [1.8001e-05, 4.0161e-02, 2.2054e-05, 6.9141e-06, 9.5801e-01, 5.5432e-05,\n",
       "           5.1260e-05, 1.2131e-03, 2.7919e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  269.180908  128.325241  414.153381  423.990479    0.936163      0   \n",
       "  1  372.828918   56.863571  639.771790  174.302216    0.924793      2   \n",
       "  2  207.531723  276.024200  232.295517  300.236786    0.922568     32   \n",
       "  3    0.465549    0.660278   72.283890  243.152191    0.916419      0   \n",
       "  4    3.848297   63.741730  330.315002  211.003784    0.905419      2   \n",
       "  5  147.541077  295.740082  163.755127  311.701874    0.893325     32   \n",
       "  6  562.492065  401.972229  584.595825  422.523132    0.874923     32   \n",
       "  7    0.131130  245.116516   56.324551  424.964417    0.784617      0   \n",
       "  8  176.501038  262.182709  312.789093  283.960419    0.617175     34   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1           car  \n",
       "  2   sports ball  \n",
       "  3        person  \n",
       "  4           car  \n",
       "  5   sports ball  \n",
       "  6   sports ball  \n",
       "  7        person  \n",
       "  8  baseball bat  ,\n",
       "  'caption': ['A dark four door vehicle behind a black fence with a bicycle on top.',\n",
       "   'A silver van with a bike on it'],\n",
       "  'bbox_target': [375.18, 50.56, 264.82, 124.74]},\n",
       " 715: {'image_emb': tensor([[ 0.0432,  0.6157, -0.2372,  ...,  0.9297,  0.0587,  0.0906],\n",
       "          [ 0.2644,  0.6221, -0.1945,  ...,  0.9072,  0.2017,  0.1857],\n",
       "          [ 0.3135,  0.4590, -0.3230,  ...,  0.7720,  0.0840,  0.1920]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1392,  0.0380, -0.0407,  ...,  0.5781,  0.2856,  0.2288],\n",
       "          [ 0.0407, -0.1902,  0.2053,  ...,  0.3330, -0.3071,  0.2891]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0388, 0.8838, 0.0772],\n",
       "          [0.1772, 0.7231, 0.0994]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin      ymin        xmax        ymax  confidence  class        name\n",
       "  0  269.382324  0.870346  639.201416  420.480774    0.921870      3  motorcycle\n",
       "  1   55.586014  0.117661  351.269653  321.517822    0.903107      3  motorcycle\n",
       "  2  610.832336  0.737915  639.718323   90.771942    0.323363      0      person,\n",
       "  'caption': ['Green and yellow motorcycle on the left of an identical one.',\n",
       "   'motorcycle closest to the grass'],\n",
       "  'bbox_target': [56.03, 1.29, 304.28, 318.77]},\n",
       " 716: {'image_emb': tensor([[-0.0975,  0.0712, -0.2595,  ...,  1.3311, -0.1377,  0.2639],\n",
       "          [-0.2457,  0.4150, -0.0455,  ...,  0.8506, -0.4380,  0.2322],\n",
       "          [-0.0545,  0.0443, -0.3101,  ...,  0.9165, -0.2007, -0.3257],\n",
       "          [-0.1678,  0.1997, -0.3008,  ...,  1.1250, -0.0188, -0.1128],\n",
       "          [-0.0867,  0.0237, -0.0609,  ...,  0.8691,  0.0153,  0.1241]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0574, -0.0361, -0.0825,  ...,  0.4072, -0.2913, -0.1520],\n",
       "          [-0.1544,  0.0720, -0.1226,  ...,  0.2396, -0.4199, -0.0887]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.1914e-04, 9.5752e-01, 4.3917e-04, 3.7720e-02, 3.6774e-03],\n",
       "          [7.3433e-04, 8.4424e-01, 5.1260e-04, 1.2952e-01, 2.5101e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  370.591248  169.427429  524.138000  291.627136    0.918714     62      tv\n",
       "  1  212.921753  191.110687  331.496643  298.167450    0.867183     56   chair\n",
       "  2  188.963425  327.285156  219.207504  350.807617    0.847652     65  remote\n",
       "  3   57.188271  240.302094  248.927429  446.177429    0.769574     57   couch\n",
       "  4  157.074265  353.923889  548.690796  473.712769    0.571935     57   couch\n",
       "  5  613.323181  304.160645  639.991272  439.519897    0.551495     57   couch\n",
       "  6  228.209335  267.766449  253.651199  282.928925    0.342514     65  remote\n",
       "  7  249.540909  280.019745  293.633545  296.947296    0.278246     73    book,\n",
       "  'caption': ['brown leather chair back', 'The brown leather chair'],\n",
       "  'bbox_target': [151.74, 354.36, 398.45, 120.77]},\n",
       " 717: {'image_emb': tensor([[ 0.2394,  0.1404,  0.2349,  ...,  0.9434, -0.0234,  0.5879],\n",
       "          [-0.1288,  0.2260, -0.1099,  ...,  1.1807,  0.1841,  0.0765],\n",
       "          [ 0.0406,  0.1840, -0.1367,  ...,  1.0459,  0.4373,  0.0367],\n",
       "          [ 0.1338,  0.1714,  0.1112,  ...,  0.5938,  0.1279,  0.0674]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2925,  0.1664, -0.4683,  ...,  0.0745, -0.3936,  0.2185],\n",
       "          [ 0.1179,  0.2039, -0.4219,  ..., -0.0494, -0.1304, -0.1583]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.1057e-01, 6.6943e-01, 4.8578e-05, 1.2000e-01],\n",
       "          [5.1331e-02, 3.0005e-01, 3.2825e-03, 6.4551e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  281.483948   73.185501  618.379395  478.025574    0.879310      0  person\n",
       "  1    0.489143  158.788971  133.513214  460.152100    0.834054     57   couch\n",
       "  2  218.775467  356.336060  247.850937  402.911499    0.767453     41     cup\n",
       "  3  246.021973  378.555908  270.247986  394.392883    0.618672     65  remote\n",
       "  4  108.019043  148.282990  631.615723  477.171387    0.617995     57   couch\n",
       "  5    0.278326  309.225769   37.158005  323.805481    0.615814     65  remote,\n",
       "  'caption': ['A man in a blue shirt lying on a sofa',\n",
       "   'A man laying down on the couch with a drink in his hand.'],\n",
       "  'bbox_target': [126.2, 142.38, 487.55, 331.15]},\n",
       " 718: {'image_emb': tensor([[ 0.1089, -0.0269, -0.4070,  ...,  0.8032,  0.3047, -0.2749],\n",
       "          [-0.1497,  0.0827, -0.1791,  ...,  0.9653,  0.1691, -0.0416],\n",
       "          [-0.1968,  0.1357, -0.3342,  ...,  0.9072, -0.1088,  0.0082],\n",
       "          [-0.1906, -0.0174, -0.1486,  ...,  0.6768,  0.0394, -0.4360]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1324, -0.7212,  0.0769,  ..., -0.1328, -0.1783,  0.2020],\n",
       "          [-0.0632,  0.1885, -0.1138,  ...,  0.3787,  0.3118, -0.2413]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7147e-03, 8.8818e-01, 5.5647e-04, 1.0944e-01],\n",
       "          [4.8248e-02, 6.5576e-01, 1.9995e-01, 9.5947e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  186.178604  188.343811  524.245483  371.595337    0.915674      6  train\n",
       "  1  252.408859  150.953217  638.938416  265.509247    0.879308      6  train\n",
       "  2   18.158150  162.409393  111.405739  232.473907    0.823686      6  train,\n",
       "  'caption': ['Five red crates.',\n",
       "   'The train with the black and orange engine'],\n",
       "  'bbox_target': [253.52, 156.64, 386.48, 107.0]},\n",
       " 719: {'image_emb': tensor([[-0.2585,  0.2280, -0.2301,  ...,  0.7607,  0.0732, -0.2581],\n",
       "          [ 0.2144, -0.2983, -0.0978,  ...,  0.0659, -0.0342, -0.2103],\n",
       "          [-0.1980,  0.0515, -0.3079,  ...,  0.7202,  0.1893, -0.1694],\n",
       "          [ 0.0083, -0.2063, -0.0916,  ...,  0.5562,  0.0102, -0.2225],\n",
       "          [-0.2198, -0.0949,  0.0241,  ...,  0.5474, -0.2649, -0.0568]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-2.3450e-01, -3.4644e-01, -1.8604e-01, -1.6370e-01, -3.5248e-02,\n",
       "            2.9126e-01, -2.2156e-01, -1.1543e+00, -1.6248e-01, -1.1719e-01,\n",
       "            3.1177e-01, -2.4170e-01, -4.4281e-02, -7.6843e-02,  2.6538e-01,\n",
       "           -6.9092e-02,  9.5654e-04,  1.3367e-01, -3.0298e-01,  6.0693e-01,\n",
       "            3.2568e-01,  2.2717e-01, -1.1261e-02, -2.5122e-01, -9.4055e-02,\n",
       "           -4.5380e-02, -1.1017e-01,  1.1749e-01, -6.2500e-01, -1.4107e-02,\n",
       "           -2.2812e-02, -1.1768e-01, -7.1045e-02,  4.0894e-01, -6.1584e-02,\n",
       "           -4.5532e-02,  1.8884e-01,  4.9121e-01, -1.5234e-01,  1.6321e-01,\n",
       "           -2.9199e-01, -1.7883e-01,  2.3779e-01,  2.4316e-01, -5.0049e-01,\n",
       "            4.5380e-02,  1.2830e-01,  8.8501e-02,  4.1162e-01,  5.8258e-02,\n",
       "           -2.7271e-01, -1.0883e-01,  1.0490e-02,  2.2815e-01, -4.9292e-01,\n",
       "            1.3049e-01,  2.1460e-01, -3.3008e-01,  1.5732e-02,  2.1326e-01,\n",
       "           -4.4312e-02,  6.0997e-03,  1.0948e-02,  8.9661e-02,  4.5807e-02,\n",
       "            1.1078e-01,  5.9586e-03, -1.1279e-01,  5.5078e-01,  5.6976e-02,\n",
       "            1.0834e-01, -2.2668e-01,  2.5024e-01, -1.7822e-01,  4.0869e-01,\n",
       "           -8.0127e-01, -1.8665e-01,  1.5175e-02, -8.1604e-02, -2.9272e-01,\n",
       "            1.3781e-03,  4.2822e-01, -3.2397e-01,  3.4424e-01, -2.3059e-01,\n",
       "            2.9465e-02, -2.7481e-02, -2.8247e-01, -1.4795e-01, -2.3083e-01,\n",
       "            2.0923e-01, -1.3405e-02, -1.5391e+00,  1.2537e-01, -1.5601e-01,\n",
       "            1.8079e-01,  3.4313e-03, -4.7632e-01,  9.4910e-02, -1.3440e-01,\n",
       "           -9.5581e-02, -3.5425e-01, -1.6418e-01,  2.7237e-02,  8.9569e-03,\n",
       "           -2.7197e-01,  1.3818e-01,  3.5571e-01,  6.0425e-02, -2.9968e-02,\n",
       "            4.7217e-01,  7.4829e-02, -2.0850e-01, -2.4622e-01,  2.4487e-01,\n",
       "           -7.0312e-02, -4.4116e-01,  3.9825e-02,  2.0789e-01,  2.3291e-01,\n",
       "           -7.0435e-02, -6.2500e-01,  3.3112e-02, -7.0312e-02,  3.4424e-01,\n",
       "           -2.2354e-02,  1.8225e-01,  2.7197e-01,  1.7053e-01,  2.4658e-01,\n",
       "           -1.9482e-01,  1.8982e-02, -5.8319e-02,  5.2227e+00, -2.8979e-01,\n",
       "           -3.2983e-01, -4.5166e-01, -2.6855e-01, -3.1006e-01, -4.9194e-02,\n",
       "           -2.9492e-01,  1.0193e-01,  4.5288e-02,  2.7734e-01,  1.0059e-01,\n",
       "            4.5215e-01, -2.8091e-02, -2.3596e-01, -2.0557e-01, -2.5162e-02,\n",
       "            3.1934e-01, -4.1699e-01,  1.0443e-01, -1.7078e-01,  3.3105e-01,\n",
       "           -3.7573e-01,  2.8320e-01,  3.1055e-01, -2.9419e-01,  2.1753e-01,\n",
       "            3.8727e-02, -1.7322e-01, -1.2549e-01, -1.1871e-01,  2.1777e-01,\n",
       "            2.4487e-01,  2.2937e-01, -7.0996e-01,  1.3489e-01,  5.2368e-02,\n",
       "            9.2590e-02, -2.0337e-01,  9.8572e-02,  2.4072e-01, -8.6487e-02,\n",
       "            1.4087e-01, -4.5563e-02,  5.9357e-03, -3.8525e-01,  3.0566e-01,\n",
       "           -3.9795e-02, -1.7603e-01,  1.9946e-01, -6.7688e-02, -2.9761e-01,\n",
       "           -1.2230e-02, -5.2148e-01, -1.4526e-01,  7.0862e-02,  1.1255e-01,\n",
       "           -3.4155e-01,  8.2703e-02, -1.2769e-01, -5.4901e-02, -2.1301e-01,\n",
       "            2.6074e-01, -5.8624e-02,  1.7285e-01, -4.2651e-01, -2.6807e-01,\n",
       "            1.1688e-01, -3.4814e-01,  5.3253e-02, -2.2571e-01,  1.8408e-01,\n",
       "            2.3718e-01, -3.2446e-01, -9.7351e-02,  2.7002e-01, -2.9883e-01,\n",
       "            8.5449e-03,  5.3320e-01,  9.3628e-02, -6.2622e-02,  2.9224e-01,\n",
       "           -2.8671e-02, -4.0161e-02,  5.6824e-02,  5.7129e-02,  2.6978e-01,\n",
       "           -3.9703e-02, -9.6924e-02,  2.4121e-01, -1.2769e-01,  1.7822e-02,\n",
       "            5.4749e-02, -3.7891e-01, -5.1807e-01, -1.7310e-01, -4.4220e-02,\n",
       "            3.9307e-01, -3.5553e-02,  1.8896e-01,  3.3521e-01,  1.4880e-01,\n",
       "           -2.4915e-01, -1.3306e-01,  5.3223e-02, -5.1025e-01, -9.3262e-02,\n",
       "            9.1248e-02,  6.8298e-02, -1.6760e-01,  5.5511e-02, -1.7676e-01,\n",
       "           -1.8433e-01, -1.2903e-01, -4.9829e-01,  1.0431e-01,  8.1482e-02,\n",
       "           -2.1680e-01,  4.2480e-01, -2.2595e-01,  4.8657e-01, -6.4990e-01,\n",
       "           -3.3887e-01,  4.4653e-01,  2.4377e-01,  2.7637e-01,  4.1260e-01,\n",
       "           -6.4575e-02,  2.1875e-01, -4.2542e-02,  2.1619e-01,  2.6733e-01,\n",
       "            2.3608e-01, -9.2087e-03, -6.1523e-02,  3.0054e-01, -6.4331e-02,\n",
       "           -3.1274e-01, -7.4707e-02, -2.8174e-01, -3.3356e-02,  8.1787e-03,\n",
       "            2.5781e-01, -2.2681e-01,  1.4539e-01,  3.4790e-01, -2.4933e-02,\n",
       "            1.9336e-01,  4.6045e-01,  1.5161e-01, -2.8491e-01, -2.6474e-02,\n",
       "            1.4807e-01,  1.1224e-01,  3.4637e-02,  2.0312e-01,  8.1238e-02,\n",
       "           -2.4231e-01, -2.1545e-01,  4.2534e-04,  2.0032e-01, -2.7661e-01,\n",
       "            1.8164e-01,  2.1130e-01, -6.1157e-02,  1.4258e-01, -9.9243e-02,\n",
       "           -9.8755e-02, -9.9182e-02,  2.8979e-01,  2.9858e-01, -2.2424e-01,\n",
       "            3.0005e-01, -2.7515e-01, -3.6719e-01,  8.1787e-02, -1.7053e-01,\n",
       "            4.8462e-01, -4.0405e-02,  5.2109e+00,  1.2323e-01,  2.8122e-02,\n",
       "            2.9102e-01,  1.9495e-01, -1.9556e-01,  9.2268e-04,  1.0583e-01,\n",
       "            3.9136e-01,  1.3892e-01,  2.6929e-01,  2.8516e-01,  2.7661e-01,\n",
       "            9.7351e-02, -1.1066e-01, -5.5725e-02,  6.4404e-01, -2.4023e+00,\n",
       "           -3.3984e-01,  3.0371e-01,  3.0579e-02, -1.6876e-02, -6.1798e-02,\n",
       "           -3.3960e-01,  4.9744e-03, -2.1973e-02,  7.1899e-02, -1.7578e-01,\n",
       "           -9.3506e-02,  6.5117e-03, -2.7271e-01, -6.7688e-02,  9.8267e-02,\n",
       "           -1.8115e-01, -4.3610e-02, -2.2607e-01,  6.1829e-02, -1.1060e-01,\n",
       "           -5.5908e-02, -4.0863e-02,  1.3147e-01,  1.6003e-01,  6.2042e-02,\n",
       "           -3.6682e-02,  2.2815e-01,  5.7220e-02,  4.0503e-01, -1.2085e-01,\n",
       "            4.6899e-01,  2.4390e-01,  7.8186e-02,  1.2671e-01,  2.7417e-01,\n",
       "           -4.8315e-01,  1.8524e-02, -2.6440e-01,  2.4658e-01,  1.2488e-01,\n",
       "            1.9331e-03, -2.1826e-01,  6.5727e-03,  2.3132e-01,  3.9966e-01,\n",
       "           -8.8562e-02, -2.4536e-01,  1.4294e-01, -6.8848e-02,  2.7368e-01,\n",
       "           -9.8083e-02, -1.4111e-01,  1.3403e-01,  2.8979e-01,  1.0181e-01,\n",
       "            1.1040e-02,  4.8431e-02,  1.2781e-01, -5.5518e-01, -1.2610e-01,\n",
       "           -4.7168e-01, -8.6182e-02, -6.5381e-01, -1.1456e-01,  5.6732e-02,\n",
       "           -3.8745e-01,  2.5684e-01,  1.8762e-01, -1.1475e-02,  5.6494e-01,\n",
       "            7.8247e-02, -2.3267e-01,  2.0984e-01,  1.1365e-01, -1.2213e-01,\n",
       "            4.8145e-01, -8.1238e-02,  2.0752e-02,  3.9160e-01, -1.6406e-01,\n",
       "            4.0527e-01,  3.3301e-01,  2.1265e-01,  2.5781e-01,  1.9653e-01,\n",
       "            7.5989e-02,  2.6685e-01,  2.4597e-02,  1.3538e-01, -1.1894e-02,\n",
       "           -3.9490e-02, -2.8763e-02, -3.6768e-01, -3.6206e-01,  4.8071e-01,\n",
       "            1.8286e-01, -1.2079e-01, -5.1465e-01,  2.8198e-01, -1.2134e-01,\n",
       "           -1.9592e-01,  1.4221e-01, -2.9053e-01,  1.8994e-01,  4.6777e-01,\n",
       "           -2.9810e-01, -3.1299e-01,  1.0048e-02, -4.3793e-02, -9.3506e-02,\n",
       "            1.9922e-01, -2.5732e-01, -1.1121e-01, -1.7250e-04,  1.0376e-01,\n",
       "            3.8647e-01,  1.7139e-01,  8.8730e-03,  2.6367e-01,  1.3512e-02,\n",
       "           -3.9111e-01,  4.9854e-01,  3.1769e-02, -1.0872e-02, -1.7725e-01,\n",
       "            3.2837e-02,  5.2295e-01, -3.4546e-02,  3.4009e-01,  3.6328e-01,\n",
       "           -2.5244e-01, -3.4302e-02, -4.9591e-02,  3.0566e-01,  1.0422e-02,\n",
       "           -3.7903e-02,  1.1096e-01,  4.2822e-01,  1.9312e-01,  9.6497e-02,\n",
       "            2.3218e-01,  4.1199e-02,  2.8027e-01, -8.1421e-02,  4.7583e-01,\n",
       "           -1.5161e-01, -9.4452e-03, -2.7466e-02, -1.6479e-01,  3.6572e-01,\n",
       "            6.3843e-02, -4.3311e-01, -2.6001e-01,  1.2842e-01,  3.3752e-02,\n",
       "           -2.7075e-01,  5.5481e-02, -1.4233e-01, -2.4414e-03,  4.2725e-02,\n",
       "            1.7664e-01, -3.0945e-02,  8.5999e-02,  3.4741e-01,  2.7002e-01,\n",
       "            1.5942e-01,  4.1455e-01,  9.1919e-02, -5.4169e-02, -5.4150e-01,\n",
       "           -4.9896e-02,  1.2476e-01,  2.6929e-01, -8.2031e-02, -4.5197e-02,\n",
       "           -1.9507e-01,  9.2163e-02, -2.7597e-05, -3.5913e-01, -2.1744e-03,\n",
       "            1.2488e-01,  4.6484e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.0225e-01, 3.2104e-02, 1.3940e-01, 5.7888e-04, 2.5391e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  357.210480  308.158112  580.489807  455.384033    0.939808     22    zebra\n",
       "  1  134.469788  135.122208  441.039551  404.909729    0.930961     23  giraffe\n",
       "  2  227.271805  291.549164  426.732361  435.869019    0.926783     22    zebra\n",
       "  3  131.172943    0.342499  262.599579  151.546112    0.904518     23  giraffe\n",
       "  4  483.385010  399.569122  508.253174  421.397064    0.691592     14     bird,\n",
       "  'caption': ['The nearest zebra.'],\n",
       "  'bbox_target': [359.72, 308.76, 220.33, 145.01]},\n",
       " 720: {'image_emb': tensor([[-0.0612,  0.2852, -0.2051,  ...,  0.7681,  0.2469, -0.0535],\n",
       "          [ 0.0159,  0.2499, -0.2219,  ...,  0.8540,  0.1179, -0.0627],\n",
       "          [-0.2395, -0.0728, -0.1361,  ...,  0.3953,  0.4160, -0.2004],\n",
       "          [-0.3379, -0.0555, -0.0817,  ...,  0.2230,  0.3228,  0.3572]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2434,  0.0733, -0.1237,  ...,  0.1864,  0.2949, -0.1995],\n",
       "          [-0.0698, -0.0509, -0.0930,  ..., -0.3367, -0.3262,  0.1642]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.2827e-02, 4.2915e-06, 6.4392e-03, 9.7070e-01],\n",
       "          [9.2773e-01, 6.2164e-02, 9.9869e-03, 1.1098e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  238.484222   77.242676  322.748749  294.397644    0.903600      0   person\n",
       "  1  311.586365  152.321899  372.510742  294.446289    0.887668      0   person\n",
       "  2  238.435410  239.454620  346.551575  340.589691    0.778504     30     skis\n",
       "  3  324.761871  275.640778  381.814850  309.183197    0.600050     30     skis\n",
       "  4  252.251007  163.382751  295.108734  193.494324    0.268072     26  handbag,\n",
       "  'caption': ['A person in a blue jacket skiing with a child.',\n",
       "   'a man wearing blue.'],\n",
       "  'bbox_target': [238.41, 80.9, 81.76, 209.15]},\n",
       " 721: {'image_emb': tensor([[-0.1589,  0.3228, -0.2666,  ...,  1.1172, -0.0229, -0.0020],\n",
       "          [-0.4907,  0.3970,  0.1660,  ...,  1.4717,  0.0737, -0.5103],\n",
       "          [-0.3716,  0.6279,  0.1177,  ...,  1.2832,  0.0856, -0.2698],\n",
       "          [-0.6333,  0.4419,  0.2468,  ...,  0.9951,  0.1675, -0.3767],\n",
       "          [-0.5869,  0.6377, -0.0742,  ...,  0.3779,  0.1877, -0.3604]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-3.0371e-01,  3.2617e-01, -6.8726e-02, -1.5442e-01,  1.5955e-01,\n",
       "            2.0898e-01, -2.4915e-01, -8.1970e-02, -8.3130e-02, -1.8091e-01,\n",
       "           -2.9517e-01, -3.1421e-01, -7.6111e-02,  9.5337e-02,  3.3521e-01,\n",
       "            1.5967e-01,  2.3514e-02,  3.3154e-01, -2.3975e-01, -1.0199e-01,\n",
       "            9.1919e-02,  6.3477e-02,  8.0420e-01,  2.1155e-01, -1.0292e-02,\n",
       "            1.7975e-02,  9.7168e-02, -6.4331e-02,  3.0518e-01, -3.4155e-01,\n",
       "           -6.9618e-03,  6.0516e-02,  2.5073e-01,  1.0048e-02, -7.0850e-01,\n",
       "            1.0999e-01,  6.5625e-01, -3.1445e-01,  5.4993e-02, -3.6682e-02,\n",
       "            1.7175e-01,  7.0251e-02,  1.6663e-01, -1.1719e-01,  1.1421e-02,\n",
       "            3.1104e-01,  7.8076e-01,  3.3960e-01, -1.8835e-01,  7.9895e-02,\n",
       "           -7.2144e-02,  1.7017e-01,  1.4429e-01, -2.7124e-01, -2.6685e-01,\n",
       "            2.6855e-01, -4.8193e-01, -1.2524e-01,  1.1310e-01, -1.0201e-02,\n",
       "            7.6111e-02,  6.3248e-03, -2.4622e-01,  3.0444e-01, -2.7759e-01,\n",
       "           -2.5732e-01,  4.1138e-01,  1.9116e-01, -1.3184e-01,  6.1371e-02,\n",
       "           -1.7944e-02, -3.4644e-01, -6.2683e-02, -3.7500e-01,  1.9189e-01,\n",
       "           -3.8696e-01, -3.4521e-01, -2.6514e-01,  4.5502e-02, -1.6602e-01,\n",
       "           -3.7048e-02,  4.3311e-01,  1.5121e-02, -2.7295e-01,  1.0870e-01,\n",
       "           -3.6328e-01,  4.1089e-01, -1.8799e-01, -4.4971e-01,  3.1128e-01,\n",
       "           -1.0284e-01,  5.3809e-01, -7.3633e-01,  2.8271e-01, -3.8477e-01,\n",
       "           -2.4815e-03, -1.5247e-01, -1.0168e-01,  7.6953e-01,  1.0883e-01,\n",
       "            2.0239e-01, -2.7661e-01,  1.6895e-01,  1.0303e-01,  1.6821e-01,\n",
       "           -2.4902e-01,  1.6467e-01, -2.8687e-01, -2.9272e-01, -1.0565e-01,\n",
       "           -1.1902e-01,  5.6885e-01,  2.3267e-01, -5.7678e-02, -1.4917e-01,\n",
       "            1.9348e-01,  4.6704e-01,  3.3350e-01, -5.3223e-01,  1.9482e-01,\n",
       "           -2.5073e-01, -4.2944e-01, -7.9834e-02,  2.3413e-01,  5.1074e-01,\n",
       "            3.0664e-01,  2.4072e-01, -1.3074e-01, -2.9492e-01, -5.6183e-02,\n",
       "           -6.3477e-02, -8.8074e-02, -1.2817e-01,  2.3711e+00, -4.8779e-01,\n",
       "           -1.3428e-01,  3.0981e-01, -1.5186e-01, -3.4180e-02, -1.7712e-01,\n",
       "            2.2614e-02,  1.0846e-01, -4.2554e-01, -3.0225e-01, -4.6143e-01,\n",
       "           -6.2500e-01, -1.3647e-01,  7.2510e-02,  6.6956e-02, -3.5010e-01,\n",
       "            8.7158e-02, -4.0308e-01,  2.7075e-01,  7.9803e-03, -4.3579e-01,\n",
       "            2.9492e-01, -9.0576e-02, -6.4270e-02,  3.1641e-01, -6.4697e-02,\n",
       "           -5.7251e-02, -4.1821e-01,  1.3147e-01,  5.5962e-03,  2.6703e-02,\n",
       "            2.3518e-03,  2.9956e-01,  9.7168e-02,  6.0400e-01,  9.1980e-02,\n",
       "           -1.0986e-01, -3.5547e-01, -2.9810e-01,  1.8079e-01, -4.0649e-01,\n",
       "            6.1989e-04,  2.2180e-01,  2.5854e-01,  6.5479e-01, -5.4053e-01,\n",
       "            1.4717e-02, -1.7322e-01, -4.6631e-01,  2.8296e-01, -5.5957e-01,\n",
       "           -2.4609e-01, -6.4026e-02,  1.1654e-03, -3.3875e-02,  1.1511e-01,\n",
       "            2.7856e-01, -2.5073e-01,  2.3889e-01,  2.2659e-02,  3.3545e-01,\n",
       "            3.0533e-02, -1.4233e-01, -3.1519e-04,  8.7128e-03, -3.2642e-01,\n",
       "           -3.3960e-01,  8.4656e-02, -1.6284e-01, -2.5269e-01, -3.1885e-01,\n",
       "            1.5588e-01,  5.0781e-01, -5.6335e-02,  3.5474e-01,  1.0925e-01,\n",
       "            4.4897e-01,  4.6906e-02,  1.3176e-02,  7.4646e-02,  4.1919e-01,\n",
       "           -9.7595e-02,  6.6162e-02, -4.3286e-01,  6.4819e-02, -1.1926e-01,\n",
       "            3.0859e-01,  6.0645e-01,  9.6008e-02, -8.7097e-02, -2.7075e-01,\n",
       "           -1.6272e-01,  2.0288e-01, -1.4392e-01,  1.0480e-01,  1.2976e-01,\n",
       "           -3.4607e-02, -5.4150e-01,  3.7994e-02, -1.0193e-02,  9.6802e-02,\n",
       "            4.3213e-01,  3.0981e-01, -2.9224e-01, -4.6295e-02,  8.3801e-02,\n",
       "           -3.4863e-01,  1.8787e-01,  1.4685e-01, -2.6465e-01, -3.1958e-01,\n",
       "           -4.8853e-01, -3.0347e-01,  2.3572e-01,  4.5312e-01, -6.9580e-02,\n",
       "           -2.8662e-01, -9.1705e-03,  2.8418e-01, -7.6965e-02, -7.8369e-02,\n",
       "            1.1426e-01,  2.4500e-01, -1.2842e-01, -2.5366e-01,  1.6711e-01,\n",
       "           -6.9031e-02,  9.5154e-02, -3.0151e-01,  9.4910e-02,  5.2881e-01,\n",
       "            3.7207e-01,  1.0217e-01,  5.0323e-02, -1.1627e-01,  3.8379e-01,\n",
       "           -5.5518e-01,  3.5303e-01,  3.8696e-01,  2.9321e-01, -1.9678e-01,\n",
       "           -2.5635e-01,  1.9714e-01,  6.0486e-02, -3.9087e-01,  2.5269e-01,\n",
       "           -3.4424e-01, -2.3047e-01,  3.2690e-01,  2.3572e-01,  2.9761e-01,\n",
       "           -1.3623e-01, -2.5732e-01,  4.9683e-01,  7.6904e-02, -1.8884e-01,\n",
       "            7.6416e-02,  3.4937e-01,  2.0483e-01,  1.2903e-01,  2.7075e-01,\n",
       "            2.1692e-01, -3.4741e-01,  7.1777e-02, -2.2278e-01, -1.7676e-01,\n",
       "           -3.2544e-01,  2.0340e-02,  2.5879e-01,  1.7960e-02, -6.2485e-03,\n",
       "           -4.1919e-01,  6.8054e-02,  3.6224e-02, -7.7393e-02,  7.2510e-02,\n",
       "           -6.2683e-02,  5.3711e-01,  2.3789e+00, -3.7378e-01, -1.2152e-01,\n",
       "            3.6011e-01, -1.8402e-02, -4.1602e-01,  1.4844e-01, -1.6602e-01,\n",
       "            4.6143e-01,  3.3374e-01, -2.2607e-01, -8.9661e-02, -4.2822e-01,\n",
       "            3.8574e-01,  9.0088e-02, -2.4292e-01,  1.8335e-01, -7.2949e-01,\n",
       "            2.5928e-01, -6.3037e-01, -3.6768e-01, -6.9153e-02,  1.0095e-01,\n",
       "           -1.1070e-02,  1.4209e-01, -1.9373e-01,  4.8065e-02, -2.3926e-01,\n",
       "           -1.5063e-01, -2.4329e-01,  2.5513e-01,  1.6956e-01,  1.1218e-01,\n",
       "           -1.0979e-02, -8.0933e-02,  1.7872e-03,  1.5088e-01,  1.0223e-01,\n",
       "            1.4633e-02, -1.4862e-02, -2.6978e-01,  2.7759e-01, -5.2881e-01,\n",
       "           -4.2285e-01, -3.5614e-02,  3.7769e-01,  5.9424e-01, -1.4331e-01,\n",
       "           -8.2092e-02, -2.5488e-01,  1.7896e-01,  3.5229e-01,  5.1086e-02,\n",
       "            1.4181e-03,  1.3985e-02,  9.5764e-02, -3.9380e-01, -2.4506e-02,\n",
       "           -3.2593e-01,  1.9153e-01,  2.5977e-01,  5.8746e-03, -1.0124e-02,\n",
       "           -8.8684e-02, -3.9575e-01, -1.8823e-01, -4.0497e-02, -2.4097e-01,\n",
       "            6.3525e-01,  1.9312e-01,  2.0154e-01,  9.2010e-03,  2.4243e-01,\n",
       "           -2.3608e-01,  1.4368e-01,  8.1543e-02, -1.2817e-01, -1.1597e-01,\n",
       "           -8.6279e-01,  1.4319e-01,  4.8853e-01, -1.0950e-01,  1.6693e-02,\n",
       "           -1.6406e-01,  1.0211e-01,  2.0776e-01, -4.3066e-01, -5.2930e-01,\n",
       "           -5.0684e-01, -2.3899e-03,  4.1724e-01, -1.1328e-01, -4.7314e-01,\n",
       "            5.2686e-01,  2.3145e-01,  6.0028e-02,  3.1738e-01,  3.7207e-01,\n",
       "           -2.6112e-03, -2.7319e-01, -4.0576e-01,  1.7517e-01,  4.7913e-02,\n",
       "           -3.3887e-01, -1.8176e-01,  9.0981e-04, -3.9551e-01, -2.1826e-01,\n",
       "           -2.8137e-02,  2.3877e-01,  3.6597e-01, -3.2324e-01,  2.5195e-01,\n",
       "            2.0789e-01, -3.2080e-01, -1.3781e-03,  1.9333e-02,  1.1157e-01,\n",
       "            1.3000e-01,  2.6025e-01, -4.3628e-01,  7.2900e-01,  2.8052e-01,\n",
       "           -1.1224e-01,  6.4026e-02,  1.7249e-01, -1.6467e-01,  1.2079e-01,\n",
       "            1.0034e-01, -3.5425e-01,  1.1304e-01, -7.8659e-03, -6.2744e-01,\n",
       "            1.8036e-02, -2.3694e-01, -6.0059e-01,  6.1371e-02,  4.6045e-01,\n",
       "            1.0410e+00,  3.4180e-02,  2.6505e-02, -5.4492e-01, -1.2457e-01,\n",
       "           -2.0554e-02,  5.0439e-01,  2.4487e-01, -6.1615e-02, -4.9634e-01,\n",
       "            5.0415e-02, -3.6450e-01,  3.1250e-01,  6.0596e-01,  1.8884e-01,\n",
       "            4.2432e-01,  2.9565e-01, -4.6313e-01, -1.0065e-01,  4.0283e-01,\n",
       "           -2.1289e-01,  1.6858e-01,  4.0210e-01,  2.0459e-01, -1.8884e-01,\n",
       "            3.3838e-01, -5.7275e-01,  9.6817e-03,  1.8701e-01, -2.4536e-02,\n",
       "           -7.0190e-02,  2.0569e-01,  8.4839e-02,  5.0488e-01, -4.7699e-02,\n",
       "            3.5126e-02,  2.8125e-01, -1.3725e-02, -2.4399e-02,  2.5122e-01,\n",
       "           -2.0142e-01, -8.9172e-02, -1.5039e-01,  9.5410e-01,  1.2085e-01,\n",
       "           -8.6304e-02, -9.4849e-02, -2.1057e-01, -4.7241e-02, -3.4106e-01,\n",
       "           -1.2840e-02, -4.9530e-02,  4.0802e-02,  2.2815e-01,  5.3125e-01,\n",
       "           -3.6255e-02, -1.2744e-01,  1.3074e-01, -2.9602e-02,  1.9519e-01,\n",
       "            1.3391e-01,  1.1029e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.7088e-05, 1.3391e-01, 7.1289e-01, 1.5173e-01, 1.3132e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  106.154068   57.850250  206.416916  268.949615    0.924129      0   \n",
       "  1  456.771820  286.500214  639.467529  341.468781    0.876738     71   \n",
       "  2  244.305023  292.823517  447.267975  365.131195    0.866395     71   \n",
       "  3    0.000000  315.992188  222.805130  393.949158    0.854846     71   \n",
       "  4  169.155762  107.089386  206.150787  187.485687    0.524952     26   \n",
       "  5  155.099380   93.897644  174.325668  107.214478    0.311585     67   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1        sink  \n",
       "  2        sink  \n",
       "  3        sink  \n",
       "  4     handbag  \n",
       "  5  cell phone  ,\n",
       "  'caption': ['this is a sink below a soap dispenser with a red background'],\n",
       "  'bbox_target': [0.63, 315.43, 221.59, 78.37]},\n",
       " 722: {'image_emb': tensor([[-0.0610,  0.4390, -0.0982,  ...,  0.9688, -0.0067, -0.0063],\n",
       "          [-0.0881,  0.6484, -0.0987,  ...,  1.1338, -0.1975,  0.1255],\n",
       "          [-0.1898,  0.1678, -0.1418,  ...,  1.3789,  0.1225, -0.0947],\n",
       "          ...,\n",
       "          [-0.1501,  0.0917, -0.3337,  ...,  1.0566, -0.1180, -0.1764],\n",
       "          [-0.0594,  0.1505, -0.2339,  ...,  1.2910,  0.0150,  0.2021],\n",
       "          [-0.0359,  0.2441,  0.1200,  ...,  0.4734, -0.2052,  0.2191]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1832,  0.2637, -0.1537,  ..., -0.3945, -0.0545, -0.1494],\n",
       "          [-0.3308, -0.2544,  0.0357,  ..., -0.1545,  0.3271, -0.4470]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.4778e-02, 8.9746e-01, 2.0256e-03, 1.2093e-03, 1.3363e-04, 1.1539e-03,\n",
       "           6.7444e-03, 6.4392e-03],\n",
       "          [3.2935e-01, 5.3467e-01, 1.1444e-02, 2.0847e-03, 4.8950e-02, 4.6997e-03,\n",
       "           8.9645e-04, 6.7993e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   83.008858   32.832993  204.223816  421.296967    0.934247      0   \n",
       "  1  238.723114  235.261292  323.891541  445.245605    0.903770      0   \n",
       "  2  164.692642  240.906265  221.940521  400.054840    0.895157     36   \n",
       "  3  210.523865  230.715591  305.098175  274.924957    0.780315      2   \n",
       "  4  241.564468  434.426971  320.961273  455.062683    0.751802     36   \n",
       "  5   56.173904  234.044128  183.914810  280.392792    0.728480      2   \n",
       "  6  232.071594  245.717575  310.797546  292.439087    0.719852      2   \n",
       "  7  300.030457  125.797096  333.000000  496.394623    0.687708      0   \n",
       "  8   54.703205  234.360626  111.763741  278.840576    0.609742      2   \n",
       "  9    0.361466  248.725479   10.771465  271.869507    0.437199      2   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2  skateboard  \n",
       "  3         car  \n",
       "  4  skateboard  \n",
       "  5         car  \n",
       "  6         car  \n",
       "  7      person  \n",
       "  8         car  \n",
       "  9         car  ,\n",
       "  'caption': ['A man in brwon t-shirt standing in skeate board',\n",
       "   'A tiny tiny man on a skateboard'],\n",
       "  'bbox_target': [240.74, 233.78, 82.91, 209.94]},\n",
       " 723: {'image_emb': tensor([[-0.1008,  0.5708, -0.2385,  ...,  0.9346,  0.0327,  0.0986],\n",
       "          [-0.1788,  0.4021, -0.1967,  ...,  1.0225, -0.0491,  0.1340],\n",
       "          [-0.2435,  0.4011, -0.5747,  ...,  1.2568, -0.0750,  0.0322],\n",
       "          [-0.1088,  0.2054, -0.2166,  ...,  1.5410, -0.2255,  0.0198],\n",
       "          [-0.0327,  0.4490, -0.0731,  ...,  1.5039, -0.3562, -0.1707],\n",
       "          [-0.1013,  0.3157, -0.1884,  ...,  0.9697,  0.0623,  0.1070]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1199,  0.1398, -0.1384,  ...,  0.2375, -0.1786, -0.3948],\n",
       "          [-0.1975,  0.0110,  0.0180,  ...,  0.1633,  0.0316, -0.2505]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.2163e-02, 2.2149e-04, 9.0234e-01, 9.3758e-05, 5.0392e-03, 1.1855e-04],\n",
       "          [4.8798e-02, 6.9618e-04, 9.5020e-01, 1.5497e-06, 2.8312e-05, 3.3402e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  417.101959  276.326233  582.402832  447.179260    0.802341     28  suitcase\n",
       "  1    0.000000    2.318680  640.000000  473.124329    0.786090      2       car\n",
       "  2  139.096130  264.523560  401.468323  396.589966    0.767826     24  backpack\n",
       "  3  216.893341  170.766586  347.505585  244.587494    0.765170     24  backpack\n",
       "  4   80.894272  171.199493  165.169739  254.146820    0.725045     24  backpack\n",
       "  5   14.844784  306.911224  107.046417  474.539795    0.544670     28  suitcase\n",
       "  6    0.917212   14.085541   33.876892   42.438934    0.506173      2       car\n",
       "  7  529.217285  329.991089  639.492554  476.917908    0.423002     28  suitcase\n",
       "  8  388.775635  181.321594  575.213928  309.271729    0.314672     28  suitcase,\n",
       "  'caption': ['The orange and tan bag in the tote',\n",
       "   'a tan and orange duffle bag inside a pink plastic tote box'],\n",
       "  'bbox_target': [153.41, 264.77, 248.57, 127.31]},\n",
       " 724: {'image_emb': tensor([[-1.6577e-01,  8.0713e-01, -1.6589e-01,  ...,  9.6973e-01,\n",
       "            2.7588e-01,  1.4209e-01],\n",
       "          [-6.5613e-02, -1.0994e-02, -1.3542e-02,  ...,  1.0264e+00,\n",
       "            2.1277e-01,  8.8440e-02],\n",
       "          [-1.0345e-02,  5.1855e-01, -2.1204e-01,  ...,  7.6904e-01,\n",
       "            4.3872e-01, -2.5610e-01],\n",
       "          ...,\n",
       "          [-1.8359e-01,  1.7224e-01, -2.5464e-01,  ...,  1.3662e+00,\n",
       "            3.0933e-01, -2.6147e-01],\n",
       "          [ 3.0685e-02,  2.1716e-01, -9.3689e-02,  ...,  1.1973e+00,\n",
       "            2.2430e-02, -2.5098e-01],\n",
       "          [ 4.9686e-04,  3.3179e-01,  1.6650e-01,  ...,  3.2129e-01,\n",
       "            5.4736e-01,  2.8591e-03]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1140, -0.2505,  0.1835,  ..., -0.1776, -0.0735, -0.2267],\n",
       "          [ 0.0280,  0.0070,  0.1873,  ...,  0.2196,  0.1106, -0.2642]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.5963e-06, 1.4267e-03, 8.0383e-02, 9.5398e-02, 5.2393e-01, 9.5546e-05,\n",
       "           1.5032e-04, 2.9858e-01],\n",
       "          [1.2517e-06, 1.1420e-04, 1.4185e-01, 1.0773e-02, 4.3274e-02, 3.9935e-06,\n",
       "           9.5367e-07, 8.0371e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  176.350967   94.557831  271.153320  241.285583    0.936355     41   \n",
       "  1  329.519287    1.714066  638.759460  472.680603    0.934286      0   \n",
       "  2   91.511337  238.317810  337.663696  361.147095    0.809624     53   \n",
       "  3    0.267632  159.679199  185.950317  224.115967    0.798822     53   \n",
       "  4    2.723145  115.199326  421.001617  477.805237    0.754199     60   \n",
       "  5  153.661957    0.000000  294.277771  139.281235    0.750911      0   \n",
       "  6    1.025036  113.448944   57.697861  164.586609    0.728348     39   \n",
       "  7  261.829010  185.679749  343.655792  228.709351    0.462739     43   \n",
       "  \n",
       "             name  \n",
       "  0           cup  \n",
       "  1        person  \n",
       "  2         pizza  \n",
       "  3         pizza  \n",
       "  4  dining table  \n",
       "  5        person  \n",
       "  6        bottle  \n",
       "  7         knife  ,\n",
       "  'caption': ['pizza on the plate',\n",
       "   \"Two slices of cheese and ham piza on a young boy's plate.\"],\n",
       "  'bbox_target': [87.75, 237.59, 264.25, 127.32]},\n",
       " 725: {'image_emb': tensor([[ 0.0863,  0.4246,  0.0428,  ...,  0.3960,  0.6890, -0.0263],\n",
       "          [-0.4258,  0.4924,  0.1611,  ...,  0.2739,  0.3293, -0.0341],\n",
       "          [-0.3101,  0.5938, -0.0293,  ...,  0.4626,  0.5015, -0.1149],\n",
       "          ...,\n",
       "          [-0.1533,  0.7598, -0.2214,  ...,  0.5713,  0.3684, -0.2727],\n",
       "          [-0.7451,  0.2705,  0.0329,  ...,  0.3589,  0.5015,  0.1042],\n",
       "          [-0.2053,  0.1237,  0.1132,  ..., -0.1807, -0.2242,  0.2141]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-3.6572e-01,  6.9275e-02, -1.4209e-01,  1.5662e-01, -1.9446e-01,\n",
       "           -1.0779e-01, -3.6865e-01, -9.4580e-01,  5.5161e-03,  1.5430e-01,\n",
       "           -1.2573e-01, -3.8818e-01,  1.2494e-01,  2.7075e-01,  2.8613e-01,\n",
       "           -3.1079e-01, -1.6528e-01,  2.8519e-02, -3.0737e-01,  3.8428e-01,\n",
       "            1.6272e-01,  8.0811e-02, -2.9761e-01, -3.6438e-02, -2.8931e-01,\n",
       "            1.6052e-01, -3.5645e-01,  4.0869e-01,  2.3792e-01, -5.4657e-02,\n",
       "           -1.8347e-01, -1.5735e-01,  1.4624e-01,  2.6807e-01, -7.1631e-01,\n",
       "            1.4270e-01,  1.3733e-01,  2.1216e-01, -4.2969e-01,  1.7563e-02,\n",
       "           -2.1387e-01, -4.1168e-02,  4.8492e-02,  4.0918e-01, -1.1920e-01,\n",
       "           -5.3009e-02, -2.8702e-02, -6.7078e-02,  1.7685e-02,  2.3181e-01,\n",
       "            4.5319e-02,  8.1177e-02,  8.5999e-02,  4.0649e-01, -2.4573e-01,\n",
       "            1.3123e-01, -2.9126e-01,  1.3428e-01,  1.9580e-01, -6.3354e-02,\n",
       "           -3.2666e-01, -2.4207e-01,  1.8188e-01, -3.2153e-01,  4.0314e-02,\n",
       "            1.9913e-02,  7.8125e-02, -7.2998e-02,  3.1055e-01,  3.4790e-01,\n",
       "           -1.7859e-01, -2.2815e-01,  1.9324e-01,  3.8330e-01,  2.7979e-01,\n",
       "            1.9592e-01,  1.0956e-01,  3.9258e-01, -3.7964e-01, -2.9526e-02,\n",
       "            3.7445e-02,  5.8563e-02,  4.0430e-01,  4.4952e-02, -2.1509e-01,\n",
       "            2.2180e-01, -6.9971e-01,  2.3315e-02, -2.0068e-01, -1.7590e-01,\n",
       "           -1.4648e-01, -9.5764e-02, -1.0508e+00,  6.9287e-01, -1.6919e-01,\n",
       "            6.9031e-02, -2.0798e-02, -4.1321e-02,  7.4158e-02,  4.9866e-02,\n",
       "           -2.5635e-03, -2.0862e-01, -1.3721e-01, -3.7659e-02, -1.8036e-02,\n",
       "           -1.2292e-01, -5.7715e-01,  2.2302e-01, -1.1951e-01, -7.8430e-03,\n",
       "           -3.5034e-01, -1.9092e-01, -1.4062e-01, -4.6875e-02, -1.8262e-01,\n",
       "            2.3218e-01, -1.1481e-01, -9.4849e-02,  2.0416e-02, -1.1432e-01,\n",
       "            6.3049e-02, -5.6592e-01,  1.0181e-01,  1.9653e-02,  8.9417e-02,\n",
       "            7.4036e-02, -3.4497e-01,  1.0002e-02,  6.5369e-02, -1.4307e-01,\n",
       "            4.3311e-01, -5.0391e-01, -3.3765e-01,  4.4883e+00, -3.5522e-01,\n",
       "            6.5155e-03, -4.5435e-01,  3.3740e-01, -5.5225e-01,  4.4678e-02,\n",
       "           -1.2396e-01,  2.3486e-01, -4.2456e-01,  4.0747e-01, -3.6426e-01,\n",
       "            1.2854e-01,  5.0537e-02, -6.0596e-01, -5.6787e-01,  3.6768e-01,\n",
       "           -1.4941e-01, -2.2986e-01,  1.3977e-01,  3.6987e-02,  8.8928e-02,\n",
       "           -7.3291e-01,  2.5342e-01, -3.2056e-01,  6.7444e-02,  2.4438e-01,\n",
       "            2.2009e-01,  2.9492e-01, -4.6509e-01, -4.1779e-02,  1.2335e-01,\n",
       "            6.9629e-01,  2.6025e-01, -5.1270e-02,  4.3701e-01, -4.7656e-01,\n",
       "            6.0852e-02, -2.9373e-02,  2.3267e-01,  3.5327e-01, -1.6785e-01,\n",
       "            2.0178e-01, -1.3184e-01,  2.0203e-01, -1.9006e-01,  2.7563e-01,\n",
       "           -3.8013e-01, -3.2166e-02, -1.8396e-01,  2.7979e-01, -1.0394e-01,\n",
       "           -1.1670e-01, -1.3245e-01,  5.7220e-02, -2.2998e-01,  1.4307e-01,\n",
       "           -4.4531e-01,  4.8804e-01, -1.5332e-01, -4.6112e-02, -3.3789e-01,\n",
       "            3.8428e-01, -5.1239e-02,  4.0942e-01, -2.0532e-01, -8.1543e-02,\n",
       "            3.3130e-01, -2.0178e-01, -1.6541e-02,  1.3074e-01,  4.1626e-02,\n",
       "            9.9976e-02,  1.2561e-01, -4.4403e-02, -2.8101e-01, -6.0791e-02,\n",
       "           -1.1255e-01,  7.1533e-01,  6.4621e-03,  2.3816e-01,  1.9653e-01,\n",
       "           -1.1493e-01, -1.5588e-01,  5.5542e-02, -4.1809e-02,  1.3159e-01,\n",
       "           -4.3945e-01, -1.0345e-01,  1.3867e-01, -5.8594e-01, -3.6597e-01,\n",
       "           -2.2316e-03, -2.1863e-01, -2.8247e-01,  7.9468e-02, -2.0584e-02,\n",
       "           -4.5898e-01,  4.6051e-02, -4.9530e-02,  4.7217e-01,  1.9702e-01,\n",
       "            5.1904e-01,  1.7456e-01, -6.7139e-02, -8.3191e-02,  4.1016e-02,\n",
       "            2.1191e-01, -4.4214e-01,  2.1484e-01,  2.2598e-02, -4.7211e-02,\n",
       "           -1.0406e-01, -8.0338e-03,  6.1279e-01, -1.1359e-01, -1.4844e-01,\n",
       "           -1.0706e-01,  5.2441e-01, -2.0294e-02, -7.4524e-02, -2.7197e-01,\n",
       "           -5.1025e-02, -1.2622e-01,  5.6458e-03,  3.7646e-01,  4.1968e-01,\n",
       "            1.7981e-01, -6.6895e-02,  3.4180e-02, -3.4424e-02,  3.1592e-01,\n",
       "           -1.7456e-01, -1.1505e-01, -3.9722e-01, -1.4832e-01,  2.3206e-01,\n",
       "           -1.1713e-01,  3.5986e-01, -3.5309e-02, -2.7954e-01,  1.1145e-01,\n",
       "            1.0010e-01, -1.1572e-01,  1.9897e-01,  6.3354e-02, -8.3069e-02,\n",
       "           -1.0699e-01, -2.6392e-01, -3.9276e-02,  2.0554e-02,  2.0386e-01,\n",
       "            5.2002e-01,  6.1798e-02, -1.7688e-01,  1.7868e-02, -2.6807e-01,\n",
       "           -2.1301e-01,  4.9286e-02,  2.9251e-02, -1.6289e-03, -1.6541e-01,\n",
       "            4.0723e-01, -6.9885e-02,  7.7637e-02, -1.1987e-01,  1.6565e-01,\n",
       "           -3.7659e-02,  1.8298e-01,  2.8003e-01,  2.8052e-01, -2.6392e-01,\n",
       "           -6.8555e-01, -3.7292e-02,  4.4586e-02,  3.3887e-01, -2.6416e-01,\n",
       "            2.3328e-01,  6.4844e-01,  4.4766e+00, -7.6294e-02,  1.5649e-01,\n",
       "            3.3789e-01,  1.1243e-01, -1.2671e-01, -1.3245e-01,  5.8740e-01,\n",
       "           -1.2915e-01, -4.0375e-02,  6.9946e-02, -2.1326e-01, -1.0785e-01,\n",
       "            9.1003e-02,  5.8838e-02,  5.3497e-02,  3.6646e-01, -1.6768e+00,\n",
       "           -3.4882e-02, -3.3667e-01,  4.4971e-01,  5.5313e-03, -5.6061e-02,\n",
       "           -8.1543e-02, -4.7760e-02,  1.4001e-01,  2.7051e-01,  1.8567e-01,\n",
       "            2.9102e-01,  2.5781e-01,  2.6538e-01,  1.3208e-01,  3.6133e-02,\n",
       "            4.1168e-02,  4.1211e-01, -7.6141e-03,  5.0079e-02,  2.5122e-01,\n",
       "            3.2373e-01,  3.1470e-01,  4.1901e-02, -2.4231e-01, -1.1084e-01,\n",
       "            9.2224e-02, -3.1592e-01, -2.8784e-01,  1.6089e-01,  3.5919e-02,\n",
       "            3.0713e-01, -2.5488e-01,  3.9087e-01, -3.2959e-01, -2.4109e-01,\n",
       "           -2.5806e-01, -2.7100e-01,  2.3376e-02, -2.7588e-01,  2.3230e-01,\n",
       "            8.7891e-03, -3.7378e-01,  4.5630e-01,  2.5610e-01,  7.5626e-04,\n",
       "           -1.4502e-01, -2.2620e-01, -1.5259e-01,  7.2266e-02,  8.9417e-02,\n",
       "           -6.2622e-02,  2.6782e-01,  1.3879e-01, -8.0750e-02, -2.4246e-02,\n",
       "            5.3009e-02,  9.3384e-02,  2.5955e-02, -2.8394e-01,  2.3010e-02,\n",
       "           -6.3721e-01, -1.7493e-01, -1.5784e-01,  2.8397e-02, -1.8030e-01,\n",
       "           -5.3436e-02, -1.4246e-01, -1.6211e-01, -2.9297e-01,  7.5537e-01,\n",
       "           -1.3562e-01, -9.7900e-02,  2.2253e-01, -4.2090e-01, -2.9678e-02,\n",
       "           -7.1777e-02, -3.7933e-02, -1.1414e-01,  5.6641e-01,  4.6158e-03,\n",
       "            3.1128e-01, -4.8779e-01,  1.8384e-01,  3.2251e-01,  9.1629e-03,\n",
       "           -3.7451e-01, -6.2347e-02, -3.4888e-01,  9.9915e-02, -5.2881e-01,\n",
       "            8.7036e-02, -3.1396e-01,  5.6885e-02, -9.2529e-02,  7.4036e-02,\n",
       "           -3.8721e-01,  1.8359e-01, -3.9966e-01,  4.4312e-01,  1.2195e-01,\n",
       "            4.5929e-02,  2.1326e-01,  7.6172e-02,  1.3831e-01,  5.6335e-02,\n",
       "            8.6731e-02, -3.9111e-01,  1.4526e-01, -1.3779e-02, -2.9663e-02,\n",
       "           -4.0137e-01,  1.2915e-01, -7.5989e-02, -2.0044e-01, -1.0352e-01,\n",
       "           -1.0571e-01,  3.8422e-02, -2.8320e-01, -1.0260e-01, -2.5269e-01,\n",
       "           -5.5762e-01,  1.1917e-02,  2.0801e-01, -3.8757e-02,  2.9224e-01,\n",
       "            3.2013e-02,  4.5190e-01,  6.1859e-02, -1.5442e-01, -3.0908e-01,\n",
       "            1.5613e-01,  1.1749e-02, -1.7810e-01,  3.4790e-01,  3.2471e-01,\n",
       "           -2.0264e-01,  2.1057e-02,  1.2622e-01, -3.7384e-02,  6.3538e-02,\n",
       "            3.9087e-01,  6.5247e-02, -2.5665e-02,  1.5381e-01,  4.7485e-02,\n",
       "            1.1711e-03,  1.4258e-01, -1.5271e-01,  1.2610e-01,  4.8730e-01,\n",
       "           -1.4001e-01, -2.5488e-01,  2.3132e-01,  1.9910e-01, -2.6989e-03,\n",
       "           -4.1870e-01, -1.4099e-01, -2.0679e-01, -3.1665e-01,  1.2720e-01,\n",
       "            2.5879e-02, -7.2754e-02, -3.6792e-01,  8.6768e-01,  4.2456e-01,\n",
       "           -9.9854e-02,  1.0773e-01, -9.6130e-02, -1.7041e-01,  1.6248e-01,\n",
       "           -1.3123e-01, -6.9946e-02,  5.6946e-02,  2.2693e-01, -1.8457e-01,\n",
       "            1.6064e-01, -6.3171e-02,  7.3425e-02,  1.3757e-01,  9.4299e-02,\n",
       "            4.8828e-04, -2.4365e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.6663e-02, 1.0653e-03, 1.3466e-03, 2.6379e-03, 4.6539e-04, 7.2823e-03,\n",
       "           1.4791e-03, 3.1018e-04, 9.6875e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  438.316803   23.681213  565.456543  309.785583    0.913412      0  person\n",
       "  1  390.793274   88.850937  580.654297  451.265137    0.912842     17   horse\n",
       "  2  320.755493   53.363449  419.173767  303.609985    0.876252      0  person\n",
       "  3  229.682327  111.322693  415.706543  472.256592    0.866682     17   horse\n",
       "  4  160.218582  120.808167  304.046692  450.818481    0.865116     17   horse\n",
       "  5   44.215958  114.421936  214.844223  446.708679    0.832452     17   horse\n",
       "  6  101.235985   73.019409  202.346375  327.888000    0.826081      0  person\n",
       "  7  227.496506   60.684128  313.635803  194.057983    0.748500      0  person,\n",
       "  'caption': ['The second soldier from right is on an all white horse.'],\n",
       "  'bbox_target': [318.85, 54.07, 99.22, 251.96]},\n",
       " 726: {'image_emb': tensor([[-0.3496,  0.2766,  0.3074,  ...,  0.4841,  0.2012,  0.1512],\n",
       "          [-0.3882,  0.3733,  0.1235,  ...,  0.0676,  0.5620,  0.5107],\n",
       "          [-0.0960,  0.5181,  0.0151,  ...,  0.8438, -0.1517,  0.3372],\n",
       "          [-0.3628,  0.2266,  0.2412,  ...,  0.9434,  0.0571, -0.0496]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1522,  0.1683, -0.3044,  ...,  0.3181, -0.1648, -0.1593],\n",
       "          [ 0.2773, -0.0110, -0.4304,  ...,  0.2812, -0.1031, -0.1094]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0016, 0.6592, 0.3367, 0.0027],\n",
       "          [0.0092, 0.0413, 0.9102, 0.0394]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  334.772217   33.975647  638.795654  413.655090    0.950946      0  person\n",
       "  1    0.388924  117.411926  234.428741  331.757935    0.943824      0  person\n",
       "  2  180.917633  145.767700  326.334106  319.786011    0.896792      0  person\n",
       "  3  594.362793  148.248108  639.805176  226.082275    0.464998      0  person\n",
       "  4  336.752594  116.082428  415.052765  272.630463    0.371629     65  remote,\n",
       "  'caption': ['a man with guitar wearing black t-shirt',\n",
       "   'Reflection of man pointing at guitar.'],\n",
       "  'bbox_target': [0.0, 114.45, 236.21, 219.34]},\n",
       " 727: {'image_emb': tensor([[-0.0991,  0.4148, -0.4346,  ...,  1.0244,  0.5156,  0.0186],\n",
       "          [ 0.0544,  0.3574, -0.1989,  ...,  1.2812,  0.5308,  0.0209],\n",
       "          [-0.0400,  0.1201, -0.3757,  ...,  0.7700,  0.4702, -0.3467],\n",
       "          [ 0.1777,  0.0747, -0.1798,  ...,  1.1074,  0.3555,  0.2712]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1520,  0.1978, -0.0781,  ...,  0.3479,  0.3372, -0.2646],\n",
       "          [ 0.0284,  0.3538, -0.1112,  ...,  0.1758,  0.1628, -0.4348]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0181, 0.5449, 0.3865, 0.0507],\n",
       "          [0.0193, 0.9312, 0.0478, 0.0018]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  113.482002  231.020721  284.788269  495.921875    0.923043     75   \n",
       "  1  432.064148  336.653687  568.424377  520.137573    0.900547     75   \n",
       "  2  301.651733  344.821259  410.129272  527.743774    0.862811     75   \n",
       "  3  395.625732  343.004425  418.356567  442.695709    0.375299     75   \n",
       "  4   13.133438    0.000000  368.537781  506.337769    0.271602     58   \n",
       "  \n",
       "             name  \n",
       "  0          vase  \n",
       "  1          vase  \n",
       "  2          vase  \n",
       "  3          vase  \n",
       "  4  potted plant  ,\n",
       "  'caption': ['clear vase with arrangement of flowers including one sunflower.',\n",
       "   'A medium sized vase holding a small boquet of flowers between two other vases.'],\n",
       "  'bbox_target': [302.88, 346.17, 103.98, 181.09]},\n",
       " 728: {'image_emb': tensor([[-0.0687, -0.0767,  0.0750,  ...,  0.8652, -0.0165, -0.0501],\n",
       "          [-0.2031, -0.1598, -0.0046,  ...,  0.7744,  0.1758,  0.2490],\n",
       "          [-0.3164, -0.1099,  0.2563,  ...,  0.4856, -0.1041, -0.0093]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0676,  0.0157, -0.0132,  ...,  0.1968,  0.0558, -0.0508],\n",
       "          [-0.0984, -0.1475,  0.0376,  ...,  0.0643,  0.3779, -0.1218]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1600, 0.1117, 0.7285],\n",
       "          [0.2908, 0.0538, 0.6553]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  142.324631   83.409348  475.273254  375.457275    0.946609     20  elephant\n",
       "  1  120.495926  248.419312  275.533020  371.674744    0.885066     20  elephant\n",
       "  2  565.748779  183.339355  584.388306  199.037231    0.663093     20  elephant\n",
       "  3   42.556541  178.538452   57.908394  195.891174    0.551778     20  elephant\n",
       "  4  616.888062  186.451721  632.785400  202.914307    0.365143     20  elephant\n",
       "  5  628.359070  194.744263  639.789734  211.976624    0.304209     20  elephant\n",
       "  6  624.297791  166.794098  637.723572  176.156036    0.301920     20  elephant\n",
       "  7  621.962646  188.521484  639.323853  207.969788    0.251947     20  elephant,\n",
       "  'caption': ['A baby elephant under a big elephant.',\n",
       "   'A baby elephant walking with its mother.'],\n",
       "  'bbox_target': [112.86, 247.85, 164.87, 125.04]},\n",
       " 729: {'image_emb': tensor([[-0.4412,  0.2067,  0.0838,  ...,  0.9448,  0.0224, -0.1500],\n",
       "          [-0.7056,  0.3442, -0.0404,  ...,  0.9897,  0.2578, -0.2076],\n",
       "          [-0.6401,  0.2583,  0.0809,  ...,  1.1875, -0.1248, -0.2296],\n",
       "          [-0.4575,  0.2375,  0.1064,  ...,  1.3691, -0.0303,  0.0468],\n",
       "          [-0.5923, -0.0371,  0.1202,  ...,  0.7271, -0.0541,  0.0990]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3354,  0.0070, -0.0370,  ...,  0.3582, -0.1682, -0.3374],\n",
       "          [-0.4104, -0.1638, -0.3223,  ...,  0.3845, -0.0942, -0.4573]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6885, 0.0030, 0.1215, 0.0101, 0.1768],\n",
       "          [0.7300, 0.0347, 0.0177, 0.0018, 0.2158]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  190.560638  160.784378  423.018005  638.265869    0.940521      0    person\n",
       "  1  235.744904   45.044952  422.268311  478.334381    0.843814      0    person\n",
       "  2   68.780426  512.973389  153.268158  587.955811    0.800071     45      bowl\n",
       "  3  226.145111  374.944885  295.971100  437.887817    0.745730     45      bowl\n",
       "  4   59.367950  602.853821  115.233429  639.900940    0.688940     76  scissors\n",
       "  5    2.911537  526.754639   49.476898  615.632568    0.520979     44     spoon\n",
       "  6  113.434479  373.125031  139.457977  399.469513    0.443295     44     spoon\n",
       "  7   12.907181  511.011047   99.186996  562.973328    0.363406     45      bowl\n",
       "  8    5.002121  486.916443  269.042419  638.671570    0.292579     69      oven,\n",
       "  'caption': ['A master chef guides in kitchen',\n",
       "   'A chef with a smiled face starring at the kitchen'],\n",
       "  'bbox_target': [234.67, 41.51, 189.33, 442.23]},\n",
       " 730: {'image_emb': tensor([[-0.1870,  0.4885, -0.4661,  ...,  0.7749, -0.3811, -0.0788],\n",
       "          [ 0.2605,  0.5083,  0.0460,  ...,  1.0107, -0.5249, -0.5503],\n",
       "          [ 0.0505,  0.2544, -0.1142,  ...,  0.9971, -0.0771, -0.4146],\n",
       "          ...,\n",
       "          [-0.0549,  0.3076, -0.1619,  ...,  1.0303, -0.0508, -0.1132],\n",
       "          [ 0.1179,  0.2878, -0.1779,  ...,  1.3320, -0.1315,  0.0765],\n",
       "          [ 0.2432,  0.7046, -0.0438,  ...,  0.8438, -0.2084, -0.2391]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2131,  0.5078, -0.3347,  ..., -0.2266,  0.1289, -0.4412],\n",
       "          [-0.0915, -0.0720, -0.2217,  ..., -0.0624,  0.0043, -0.0875]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.8944e-04, 2.4857e-02, 1.2231e-01, 3.7518e-03, 2.5650e-02, 5.3644e-06,\n",
       "           4.0412e-05, 4.5776e-05, 7.1526e-07, 8.2324e-01],\n",
       "          [1.2042e-01, 1.4381e-02, 1.2427e-01, 3.4904e-04, 3.0273e-01, 7.3471e-03,\n",
       "           1.1139e-01, 1.3725e-02, 2.7027e-03, 3.0273e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0   60.279526  192.535583  203.112015  476.366333    0.933615      0    person\n",
       "  1  461.777954   47.296974  639.387207  235.608032    0.933088     25  umbrella\n",
       "  2  349.375061   48.635208  549.256165  278.451782    0.931595     25  umbrella\n",
       "  3    0.000000   28.838219  273.648438  234.591827    0.930261     25  umbrella\n",
       "  4  536.310913  208.279053  639.395386  474.885925    0.927521      0    person\n",
       "  5  331.257233  144.059464  407.869110  448.568054    0.876191      0    person\n",
       "  6  382.329010  204.395935  522.648376  476.895020    0.859094      0    person\n",
       "  7  264.384705   51.487350  380.386780  140.027237    0.847615     25  umbrella\n",
       "  8  386.736084  244.154602  458.729187  312.685669    0.797647     26   handbag\n",
       "  9  137.217255  193.987946  193.976898  301.098419    0.363303     26   handbag,\n",
       "  'caption': ['The blue umbrella being carried by the person dressed in the long red coat.',\n",
       "   'The blue color umberalla in the hands of the lady with red dress.'],\n",
       "  'bbox_target': [471.15, 49.17, 168.85, 183.37]},\n",
       " 731: {'image_emb': tensor([[ 0.1522, -0.2571, -0.7432,  ...,  0.4690,  0.2084,  0.1686],\n",
       "          [ 0.0163,  0.0333, -0.2059,  ...,  1.1455,  0.3181, -0.0088],\n",
       "          [-0.0364, -0.2290, -0.5640,  ...,  0.5625,  0.0784,  0.0375]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1606, -0.0764, -0.2852,  ...,  0.3904,  0.3188,  0.1863],\n",
       "          [-0.0443, -0.0870,  0.1420,  ...,  0.4778, -0.1220,  0.0944]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2544, 0.5913, 0.1543],\n",
       "          [0.8135, 0.0238, 0.1627]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0    2.226624   89.276978  576.065796  386.335266    0.941558      6  train\n",
       "  1  553.634094  194.110352  639.396423  320.091919    0.780895      6  train,\n",
       "  'caption': ['Blue and yellow train on tracks', 'A blue train no. 510'],\n",
       "  'bbox_target': [1.08, 88.45, 599.73, 304.18]},\n",
       " 732: {'image_emb': tensor([[ 0.2874,  0.2294,  0.2366,  ...,  0.8770,  0.3423,  0.0269],\n",
       "          [-0.0564,  0.6553,  0.1613,  ...,  1.2314, -0.1425,  0.1964],\n",
       "          [-0.1732,  0.6260, -0.0534,  ...,  1.0410, -0.1302,  0.5122],\n",
       "          [ 0.0753,  0.7153, -0.0454,  ...,  1.0225, -0.0645,  0.1897],\n",
       "          [ 0.1105,  0.5845,  0.0869,  ...,  1.1074,  0.3025,  0.3389],\n",
       "          [-0.0619,  0.0661,  0.1708,  ...,  0.8242,  0.2959,  0.0881]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2175,  0.3796,  0.1071,  ..., -0.1248, -0.0510,  0.0629],\n",
       "          [ 0.1093,  0.0925,  0.2109,  ..., -0.1908, -0.0435,  0.3025]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.1921e-07, 3.4644e-01, 1.7969e-01, 2.8271e-01, 1.9128e-01, 2.1815e-05],\n",
       "          [9.4873e-01, 1.9684e-02, 1.5823e-02, 1.2007e-03, 8.2092e-03, 6.4926e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  256.548401   76.385681  411.244446  406.345886    0.943665     19   \n",
       "  1  388.760162  118.099091  552.387451  270.836761    0.896074     58   \n",
       "  2   92.193977  129.514755  198.445648  231.551941    0.878808     58   \n",
       "  3   24.609800  120.860107   84.159973  202.597168    0.839382     58   \n",
       "  4  533.927856  151.873596  612.227905  305.489136    0.754102     58   \n",
       "  \n",
       "             name  \n",
       "  0           cow  \n",
       "  1  potted plant  \n",
       "  2  potted plant  \n",
       "  3  potted plant  \n",
       "  4  potted plant  ,\n",
       "  'caption': ['A large leafed plant in front of a white wall.',\n",
       "   'A bush that looks like it is touching the side of the cow.'],\n",
       "  'bbox_target': [393.88, 119.27, 161.36, 152.34]},\n",
       " 733: {'image_emb': tensor([[ 0.2180, -0.0407, -0.2661,  ...,  1.1934,  0.0895,  0.0659],\n",
       "          [-0.0540,  0.5142,  0.0316,  ...,  1.2002,  0.2473,  0.0383],\n",
       "          [ 0.1005,  0.0272,  0.0431,  ...,  0.7339,  0.1179,  0.1871],\n",
       "          ...,\n",
       "          [-0.2002,  0.3020, -0.2097,  ...,  1.1387, -0.1110,  0.2581],\n",
       "          [ 0.1083, -0.0028, -0.4092,  ...,  1.0762,  0.1487,  0.1913],\n",
       "          [ 0.3594,  0.3054,  0.0081,  ...,  0.5942,  0.4321,  0.4133]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.4062,  0.1013, -0.3730,  ..., -0.0633, -0.2024,  0.1372],\n",
       "          [-0.0895,  0.1526, -0.4209,  ...,  0.2732,  0.0648,  0.2803]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.2129e-04, 0.0000e+00, 1.3660e-01, 7.1533e-01, 1.9586e-04, 2.0733e-03,\n",
       "           1.4539e-01],\n",
       "          [1.5450e-04, 5.9605e-07, 8.8770e-01, 1.3423e-04, 2.6810e-02, 2.6131e-03,\n",
       "           8.2581e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   376.937012    0.089676  639.998352  293.686462    0.945831      0   \n",
       "  1   101.713760  285.752808  201.810074  440.967896    0.944444     41   \n",
       "  2   115.536606    0.773499  430.494995  341.421021    0.941698      0   \n",
       "  3   340.209137  155.549179  516.858704  341.294495    0.937029     63   \n",
       "  4     0.933792    0.608292  149.688889  423.550232    0.912008      0   \n",
       "  5   459.572144  147.583679  639.626099  471.676392    0.880859      0   \n",
       "  6    30.869797  372.429138  288.324280  478.094299    0.699471     63   \n",
       "  7   404.576019  333.343811  596.278870  417.437073    0.368884     29   \n",
       "  8    33.096928  461.883362  115.025513  479.667542    0.331135     67   \n",
       "  9     0.000000  261.777039  605.897644  480.000000    0.298155     60   \n",
       "  10  218.041534  340.804077  401.435638  433.458984    0.274327     73   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1            cup  \n",
       "  2         person  \n",
       "  3         laptop  \n",
       "  4         person  \n",
       "  5         person  \n",
       "  6         laptop  \n",
       "  7        frisbee  \n",
       "  8     cell phone  \n",
       "  9   dining table  \n",
       "  10          book  ,\n",
       "  'caption': ['A little girl using a green and white device.',\n",
       "   'A small blond girl is wearing a striped shirt.'],\n",
       "  'bbox_target': [133.88, 2.9, 287.34, 335.24]},\n",
       " 734: {'image_emb': tensor([[-0.2469,  0.1335,  0.0615,  ...,  0.5552, -0.5488, -0.1212],\n",
       "          [-0.1049,  0.2211, -0.2649,  ...,  0.8271,  0.3916, -0.0099],\n",
       "          [-0.6348,  0.1104, -0.0772,  ...,  0.3992, -0.2062, -0.0153]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0698, -0.2612, -0.0226,  ...,  0.1085,  0.2029, -0.4624],\n",
       "          [-0.2162, -0.1499, -0.0952,  ..., -0.0781,  0.0534, -0.0972]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.4055, 0.2136, 0.3809],\n",
       "          [0.0239, 0.0045, 0.9717]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  156.956253  343.860352  463.412659  595.661865    0.909882      0   person\n",
       "  1  133.500366  425.174194  173.044037  447.123169    0.849667     29  frisbee\n",
       "  2  157.653488  361.001373  304.289856  596.704956    0.456670      0   person,\n",
       "  'caption': ['2 boys playing with a frisbee',\n",
       "   'A man with blue short trousers trying to hold the frisbee'],\n",
       "  'bbox_target': [166.46, 348.41, 292.73, 246.82]},\n",
       " 735: {'image_emb': tensor([[-0.0508,  0.8115, -0.0564,  ...,  0.8037,  0.1881,  0.0701],\n",
       "          [-0.0309,  0.3020, -0.2256,  ...,  1.2871,  0.0704, -0.1897],\n",
       "          [-0.0808,  0.6924, -0.0470,  ...,  0.7817,  0.0983, -0.1536],\n",
       "          ...,\n",
       "          [-0.1428,  0.5952, -0.1531,  ...,  1.4199, -0.2185, -0.0923],\n",
       "          [ 0.3835,  0.3052, -0.0531,  ...,  1.0781,  0.2864, -0.2698],\n",
       "          [ 0.2107,  0.4102, -0.0509,  ...,  1.1045,  0.1206,  0.0804]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1021, -0.3972, -0.1825,  ...,  0.7144,  0.0225, -0.2373],\n",
       "          [ 0.1846, -0.2397,  0.2006,  ...,  0.3635, -0.1046, -0.2225]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.2850e-06, 2.7370e-04, 3.8803e-05, 5.9605e-07, 9.9902e-01, 3.5119e-04,\n",
       "           4.2617e-05, 2.1756e-05],\n",
       "          [1.1051e-04, 3.6418e-05, 7.6675e-04, 1.2517e-06, 9.9902e-01, 9.6023e-05,\n",
       "           5.7340e-05, 1.3542e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  303.933563  123.655350  444.186554  369.017334    0.926698      0   \n",
       "  1    0.000000  140.250931   67.267853  384.889587    0.922881      0   \n",
       "  2  214.685806  166.448975  487.530823  395.388306    0.900004      3   \n",
       "  3   72.525009  134.393982  142.600693  385.208923    0.884497      0   \n",
       "  4  435.858612  167.888733  640.000000  340.579834    0.876798      2   \n",
       "  5  136.006226  112.079681  208.298950  385.987518    0.821626      0   \n",
       "  6    1.470688  137.677933  296.031189  357.103455    0.774346      2   \n",
       "  7  205.621155  164.674591  374.933228  232.807709    0.637520      2   \n",
       "  8  136.998077  120.109085  208.235870  375.711426    0.348569      2   \n",
       "  9   58.840942  130.759583  264.600098  366.054871    0.321725      0   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2  motorcycle  \n",
       "  3      person  \n",
       "  4         car  \n",
       "  5      person  \n",
       "  6         car  \n",
       "  7         car  \n",
       "  8         car  \n",
       "  9      person  ,\n",
       "  'caption': ['front end of gray SUV and back end of white SUV',\n",
       "   'White SUV with its lift gate open.'],\n",
       "  'bbox_target': [439.01, 143.28, 198.47, 199.55]},\n",
       " 736: {'image_emb': tensor([[-0.5654,  0.0736, -0.2368,  ...,  0.6406,  0.0762, -0.5830],\n",
       "          [-0.6191,  0.0475, -0.1544,  ..., -0.0556,  0.1588, -0.5566],\n",
       "          [-0.0938,  0.4731, -0.4790,  ...,  1.0283, -0.1661, -0.2216],\n",
       "          [-0.6362, -0.0596,  0.0725,  ..., -0.0454,  0.2048, -0.5273]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3435, -0.3608,  0.0118,  ...,  0.2490, -0.4302, -0.1531],\n",
       "          [-0.2426, -0.2192, -0.3811,  ...,  0.5195, -0.3364, -0.1398]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.1821e-01, 2.2388e-01, 1.1861e-05, 3.5791e-01],\n",
       "          [1.3660e-01, 7.5000e-01, 3.7556e-03, 1.0974e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  515.612976  162.913040  639.547668  477.406128    0.940456      0   \n",
       "  1  137.585114   46.118286  361.282013  469.587280    0.938138      0   \n",
       "  2  164.383392  290.340210  213.544098  450.794495    0.822287     38   \n",
       "  3  607.038208  369.529449  639.444702  403.697784    0.269665     38   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2  tennis racket  \n",
       "  3  tennis racket  ,\n",
       "  'caption': [\"The female player wearing all white clothes that don't have a blue stripe on the shorts.\",\n",
       "   'A tennis player behind a net.'],\n",
       "  'bbox_target': [136.22, 44.86, 224.86, 425.95]},\n",
       " 737: {'image_emb': tensor([[-0.0571, -0.3184,  0.1005,  ...,  0.7241,  0.1743,  0.0660],\n",
       "          [-0.0535, -0.3528,  0.1847,  ...,  0.7832,  0.2142, -0.0383],\n",
       "          [-0.1556, -0.2756,  0.0456,  ...,  0.5605,  0.1681, -0.0016]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1214,  0.2030, -0.2164,  ..., -0.2241,  0.0543, -0.1578],\n",
       "          [ 0.1043,  0.0463, -0.0744,  ...,  0.2253, -0.2913,  0.0774]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2520, 0.4089, 0.3391],\n",
       "          [0.9761, 0.0107, 0.0131]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  219.921936  187.320923  496.138184  375.336609    0.938011     20  elephant\n",
       "  1  136.120682   52.220139  553.177429  378.673584    0.856776     20  elephant,\n",
       "  'caption': ['An elephant  with its child.', 'a large gray elephant'],\n",
       "  'bbox_target': [142.0, 55.37, 403.13, 319.26]},\n",
       " 738: {'image_emb': tensor([[-0.2440,  0.6011, -0.0455,  ...,  0.9365, -0.2795, -0.0302],\n",
       "          [ 0.0880,  0.4714, -0.0186,  ...,  0.9658,  0.0606,  0.2003],\n",
       "          [ 0.1729,  0.4048, -0.3164,  ...,  0.9961,  0.0175,  0.2437],\n",
       "          [-0.0322,  0.3611, -0.0273,  ...,  1.1631,  0.1324,  0.1802],\n",
       "          [-0.3875,  0.3101,  0.1787,  ...,  0.8521,  0.1191, -0.0050],\n",
       "          [-0.4028,  0.4607,  0.4851,  ...,  0.4993, -0.0558,  0.0684]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0848,  0.3416,  0.1498,  ...,  0.0038, -0.0308, -0.3083],\n",
       "          [ 0.0804,  0.4199,  0.3015,  ...,  0.0617,  0.1191, -0.3386]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0015, 0.4199, 0.3823, 0.0534, 0.0022, 0.1406],\n",
       "          [0.0024, 0.3369, 0.4392, 0.0313, 0.0013, 0.1890]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   190.435837   45.718479  307.848969  441.770233    0.881281     12   \n",
       "  1   110.242935  280.255859  178.104401  426.132477    0.869548     10   \n",
       "  2     0.968289  282.979095   49.990147  396.701233    0.829088     10   \n",
       "  3    54.506294  272.634186  116.616348  414.734985    0.777807     10   \n",
       "  4     0.431967  183.662003   60.494621  249.135468    0.754648      1   \n",
       "  5    64.739571  174.604889  128.589981  229.714386    0.604306      1   \n",
       "  6   110.748604  173.364304  160.065872  219.156265    0.548237      1   \n",
       "  7   317.711395  179.934372  332.671967  254.591843    0.543919      0   \n",
       "  8    40.539749  178.657288  108.276459  236.527542    0.532884      1   \n",
       "  9   128.887268  185.601349  161.707169  219.935684    0.410778      1   \n",
       "  10   63.124554  197.710724  108.312416  238.915604    0.339485      1   \n",
       "  11  155.480652  189.738434  173.244598  216.041428    0.284297      1   \n",
       "  12   96.307510  194.924057  126.853249  230.540771    0.266278      1   \n",
       "  \n",
       "               name  \n",
       "  0   parking meter  \n",
       "  1    fire hydrant  \n",
       "  2    fire hydrant  \n",
       "  3    fire hydrant  \n",
       "  4         bicycle  \n",
       "  5         bicycle  \n",
       "  6         bicycle  \n",
       "  7          person  \n",
       "  8         bicycle  \n",
       "  9         bicycle  \n",
       "  10        bicycle  \n",
       "  11        bicycle  \n",
       "  12        bicycle  ,\n",
       "  'caption': ['A fire hydrant in between two other hydrants.',\n",
       "   'The middle hydrant.'],\n",
       "  'bbox_target': [48.36, 272.83, 65.63, 138.78]},\n",
       " 739: {'image_emb': tensor([[-1.1560e-01,  3.5980e-02, -3.8257e-01,  ..., -4.7668e-02,\n",
       "           -1.0175e-01,  2.3608e-01],\n",
       "          [-1.8799e-01,  3.4082e-01, -1.9763e-01,  ...,  8.2080e-01,\n",
       "           -2.0618e-01, -1.5991e-02],\n",
       "          [-3.5718e-01,  2.4097e-01, -2.5122e-01,  ...,  9.7754e-01,\n",
       "            7.8003e-02, -1.4319e-01],\n",
       "          [-3.5095e-04, -5.0079e-02, -1.9214e-01,  ...,  1.2695e+00,\n",
       "            9.5520e-03, -6.8909e-02],\n",
       "          [-1.8787e-01,  1.6895e-01, -3.3716e-01,  ..., -3.9673e-04,\n",
       "            1.1316e-01,  2.9590e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1173, -0.1777, -0.4868,  ..., -0.0728,  0.0478, -0.2085],\n",
       "          [-0.1104, -0.1936, -0.3955,  ..., -0.4312, -0.0409,  0.0504]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.9608e-03, 6.8701e-01, 1.1041e-01, 5.3596e-04, 1.9995e-01],\n",
       "          [1.8372e-02, 6.1816e-01, 1.6895e-01, 6.2523e-03, 1.8848e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0    1.527202    1.085067  416.968719  225.039932    0.915613      6     train\n",
       "  1  175.371063  153.649368  276.987091  460.558228    0.910954      0    person\n",
       "  2  313.518311  139.221237  388.130981  348.165375    0.878636      0    person\n",
       "  3  140.180740  112.690941  209.169907  310.395142    0.823359      0    person\n",
       "  4  194.007675  224.114441  267.924774  325.784851    0.696201     24  backpack\n",
       "  5  328.403290  184.036331  387.408386  261.826965    0.607492     24  backpack\n",
       "  6  149.882568  139.760406  209.200195  217.341492    0.420764     24  backpack\n",
       "  7  379.083618   67.541107  415.214020  143.290680    0.368483      0    person,\n",
       "  'caption': ['school girl in uniform with backpack standing close to pillar',\n",
       "   'The small girl standing nearest the wall.'],\n",
       "  'bbox_target': [313.48, 137.08, 74.16, 214.61]},\n",
       " 740: {'image_emb': tensor([[-0.3191,  0.4111,  0.0193,  ...,  1.0723,  0.6504,  0.2654],\n",
       "          [-0.1068,  0.2964, -0.1711,  ...,  1.1182,  0.2944,  0.3164],\n",
       "          [ 0.1915,  0.6797, -0.1891,  ...,  1.0361,  0.2375, -0.2328],\n",
       "          ...,\n",
       "          [ 0.3367,  0.6157, -0.4246,  ...,  1.0693,  0.0843, -0.0221],\n",
       "          [-0.2144,  0.3181, -0.2206,  ...,  1.3828,  0.1881, -0.1665],\n",
       "          [-0.0779,  0.3552, -0.2346,  ...,  0.8066,  0.0728,  0.2103]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2808,  0.2747, -0.2566,  ...,  0.3604,  0.0343, -0.1245],\n",
       "          [ 0.3049,  0.0094, -0.1481,  ...,  0.1702, -0.1997,  0.1327]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.1025e-01, 4.2944e-01, 7.2479e-05, 2.6083e-04, 1.0252e-05, 5.9967e-02,\n",
       "           5.8413e-06],\n",
       "          [4.6875e-01, 4.0723e-01, 1.5485e-04, 1.4687e-03, 9.5367e-05, 1.2225e-01,\n",
       "           7.6652e-05]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   226.683624    0.264404  362.906219  135.730652    0.923022     40   \n",
       "  1   451.591064    0.246208  608.518555  201.256042    0.921105     40   \n",
       "  2   183.403015  131.295593  398.855103  323.220886    0.858658     54   \n",
       "  3   548.724609    0.651855  638.969971  151.135132    0.858492     40   \n",
       "  4    15.439125  143.494415  204.997620  297.905914    0.806201     54   \n",
       "  5   304.620941   99.527237  640.000000  243.708984    0.802120     60   \n",
       "  6     3.516815   86.287964  640.000000  475.828552    0.662870     60   \n",
       "  7   359.624878   23.131042  413.430664  106.231506    0.632098     44   \n",
       "  8   367.207489   61.538116  456.519135  148.310394    0.611999     41   \n",
       "  9     0.731064    0.414749  228.369400  120.654465    0.326846      0   \n",
       "  10  584.142639   60.172180  639.780701  140.964417    0.265003      0   \n",
       "  \n",
       "              name  \n",
       "  0     wine glass  \n",
       "  1     wine glass  \n",
       "  2          donut  \n",
       "  3     wine glass  \n",
       "  4          donut  \n",
       "  5   dining table  \n",
       "  6   dining table  \n",
       "  7          spoon  \n",
       "  8            cup  \n",
       "  9         person  \n",
       "  10        person  ,\n",
       "  'caption': ['red wine in glass', 'A glass of red wine.'],\n",
       "  'bbox_target': [227.6, 2.16, 132.67, 130.51]},\n",
       " 741: {'image_emb': tensor([[ 0.0785,  0.2253, -0.2700,  ...,  0.7212, -0.0187,  0.0967],\n",
       "          [ 0.1312,  0.2568, -0.3008,  ...,  0.6133, -0.0410,  0.1072]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 1.2042e-01,  1.2457e-01, -1.8896e-01,  5.7343e-02, -3.1641e-01,\n",
       "           -3.5889e-02, -5.2197e-01, -1.1982e+00, -2.6807e-01,  1.2122e-01,\n",
       "           -3.8721e-01,  1.4075e-01,  4.5441e-02, -1.2793e-01,  3.5571e-01,\n",
       "           -4.3701e-01,  1.7529e-01,  1.4185e-01,  2.2293e-02,  3.2275e-01,\n",
       "            2.2913e-01,  7.2754e-01, -1.8201e-01,  5.4492e-01, -6.1676e-02,\n",
       "            3.3905e-02, -8.6182e-02,  1.6138e-01, -5.4657e-02,  4.4971e-01,\n",
       "            6.4392e-02, -2.2522e-02, -5.1465e-01,  2.8955e-01, -4.7264e-03,\n",
       "           -4.9780e-01, -1.1108e-02, -3.7524e-01, -7.4524e-02,  9.1919e-02,\n",
       "            1.8152e-01,  7.3547e-02,  1.1017e-01,  6.9580e-02, -1.6464e-02,\n",
       "            1.3786e-02,  3.1445e-01,  1.3538e-01, -1.3252e-02, -2.0728e-01,\n",
       "           -2.7637e-01, -7.7979e-01, -1.3879e-01,  5.0690e-02, -2.0044e-01,\n",
       "            3.1372e-01,  6.8420e-02,  1.9165e-01, -1.8738e-01,  3.0664e-01,\n",
       "            4.7241e-01, -2.7466e-01, -8.0505e-02, -5.1807e-01, -5.4504e-02,\n",
       "            1.6003e-01, -3.4448e-01,  4.2554e-01, -1.7419e-01,  6.5125e-02,\n",
       "            1.8921e-01, -2.2571e-01, -3.2257e-02,  2.5806e-01,  4.0259e-01,\n",
       "           -1.0083e-01,  2.2717e-01, -1.6626e-01, -2.6260e-02, -3.7964e-01,\n",
       "            1.3171e-01,  1.0541e-01,  1.4465e-01,  2.1399e-01,  1.3159e-01,\n",
       "            1.3586e-01,  2.4048e-01,  1.9434e-01,  1.6382e-01, -1.0033e-02,\n",
       "            6.4819e-02, -2.7100e-01, -1.4531e+00,  1.1243e-01, -3.6914e-01,\n",
       "            5.4993e-02, -2.3117e-02,  2.8833e-01, -2.0154e-01, -2.7313e-02,\n",
       "            2.4561e-01, -2.8247e-01,  2.6260e-02, -3.8184e-01, -1.1493e-01,\n",
       "            4.0131e-02, -8.3008e-02,  1.6882e-01,  1.6284e-01, -7.3730e-02,\n",
       "            6.0638e-02,  6.1035e-02,  1.6846e-01,  2.7002e-01,  1.5576e-01,\n",
       "           -1.2183e-01,  2.2629e-02, -1.5112e-01,  1.8970e-01,  1.3989e-01,\n",
       "            1.4795e-01, -7.1631e-01, -1.8774e-01, -3.0444e-01,  1.0870e-01,\n",
       "           -4.2297e-02,  8.1299e-02,  1.4087e-01,  1.5503e-01,  1.2891e-01,\n",
       "           -4.4159e-02, -1.9995e-01,  1.8872e-01,  5.0469e+00,  1.1462e-01,\n",
       "           -2.2106e-03, -3.0298e-01, -4.9048e-01, -2.2507e-02, -3.7500e-01,\n",
       "            5.4639e-01,  2.7856e-01, -4.3994e-01,  1.1694e-01, -1.6833e-01,\n",
       "            2.1204e-01, -2.0789e-01, -1.9690e-01, -3.3667e-01, -2.1729e-01,\n",
       "           -2.2974e-01, -7.4097e-02, -4.6814e-02,  6.8115e-02, -6.9809e-03,\n",
       "           -2.6758e-01,  1.0706e-01, -2.0020e-01, -3.5474e-01, -2.2388e-01,\n",
       "            3.2812e-01, -1.2000e-01, -1.1481e-01, -1.1639e-01, -1.6956e-01,\n",
       "           -3.5889e-01,  2.8320e-01, -4.7949e-01,  1.8164e-01, -2.0157e-02,\n",
       "            5.6641e-02,  2.1289e-01, -1.4380e-01,  2.7954e-01, -5.6006e-01,\n",
       "            4.7339e-01, -1.2463e-01, -2.0984e-01,  3.2617e-01,  1.9867e-02,\n",
       "            1.7242e-02,  2.5708e-01, -2.1530e-02, -3.5449e-01, -1.3977e-01,\n",
       "            1.0815e-01, -3.8965e-01, -3.3356e-02,  4.9316e-01,  1.2769e-01,\n",
       "            3.5864e-01,  1.3708e-01,  4.1797e-01, -3.0322e-01,  1.6797e-01,\n",
       "           -2.8671e-02,  4.9469e-02,  1.6443e-01,  1.6651e-03, -2.7417e-01,\n",
       "            2.8992e-02, -2.0325e-01, -1.9397e-01,  1.0400e-01, -8.6823e-03,\n",
       "           -6.0913e-02,  1.9312e-01, -3.4717e-01, -1.7041e-01,  1.1676e-01,\n",
       "           -1.1682e-01,  3.9966e-01,  4.6289e-01, -1.3135e-01, -3.0045e-02,\n",
       "            3.7329e-01, -3.4253e-01, -3.4839e-01, -1.7685e-02, -2.1704e-01,\n",
       "            2.5537e-01,  2.1545e-02,  2.8662e-01,  3.0981e-01, -3.5309e-02,\n",
       "           -2.5317e-01, -2.4768e-01,  2.1912e-02, -1.0931e-01, -2.4194e-01,\n",
       "           -4.3152e-02,  4.2529e-01,  4.6289e-01, -3.1143e-02, -1.9629e-01,\n",
       "           -2.3315e-02,  3.6499e-01,  3.0981e-01, -9.5215e-03, -4.5459e-01,\n",
       "            1.2830e-01,  2.1729e-01,  1.4978e-01, -9.1980e-02,  1.8250e-02,\n",
       "            2.1744e-02,  1.7249e-01, -1.8506e-01,  1.1450e-01,  8.7463e-02,\n",
       "            1.2878e-01,  9.3323e-02,  2.3694e-01,  9.1431e-02, -2.9565e-01,\n",
       "            2.1375e-01, -7.0862e-02, -1.4206e-02,  1.0803e-01,  1.5601e-01,\n",
       "           -5.6055e-01,  8.9417e-03,  1.7993e-01, -1.1255e-01, -1.4758e-01,\n",
       "           -3.6694e-01,  1.9873e-01,  2.5220e-01, -2.1631e-01, -2.5928e-01,\n",
       "           -3.1396e-01, -2.9297e-01, -1.1456e-01,  2.4634e-01, -2.1887e-01,\n",
       "            2.3657e-01,  9.6313e-02,  1.9397e-01, -6.1646e-03, -2.4292e-01,\n",
       "            7.2693e-02, -2.7954e-01,  1.7065e-01,  5.5420e-01, -2.4750e-02,\n",
       "            8.4686e-04,  2.5146e-01,  3.3301e-01,  2.5024e-01, -2.3425e-01,\n",
       "            3.1738e-01,  1.3281e-01,  1.5405e-01,  7.6660e-02, -8.4045e-02,\n",
       "           -3.5303e-01, -1.8933e-01,  2.0300e-01, -1.2646e-01,  2.7515e-01,\n",
       "           -4.3237e-01, -1.9885e-01, -2.1167e-01, -3.5669e-01, -1.0999e-01,\n",
       "           -2.8015e-02, -2.2607e-01, -2.3914e-01,  2.5879e-01, -2.3926e-01,\n",
       "           -7.0557e-02,  3.1921e-02,  5.0391e+00,  4.0430e-01,  1.6708e-02,\n",
       "            1.5038e-02, -1.5579e-02, -1.5576e-01,  3.4180e-01,  2.0020e-01,\n",
       "           -8.6548e-02,  1.3208e-01, -8.6792e-02,  1.4490e-01, -4.5166e-01,\n",
       "            1.8604e-01, -2.4719e-01,  4.9858e-03,  8.2825e-02, -2.4453e+00,\n",
       "           -5.4352e-02, -2.8259e-02,  1.7871e-01, -4.5685e-02, -6.1279e-02,\n",
       "           -2.7710e-01, -3.0371e-01,  1.8408e-01, -1.3928e-01,  3.8379e-01,\n",
       "           -3.6060e-01,  6.3049e-02,  1.3611e-01,  1.0339e-01, -1.5076e-01,\n",
       "            5.0049e-03, -7.1716e-02,  7.2388e-02,  3.7689e-02, -3.7207e-01,\n",
       "            6.3770e-01,  2.3975e-01,  1.4734e-01, -8.0078e-02, -1.4246e-01,\n",
       "           -2.8125e-01,  7.3364e-02, -2.0920e-02, -1.8188e-01,  1.4185e-01,\n",
       "           -5.6076e-03,  1.1243e-01,  4.5258e-02,  1.1255e-01, -3.1311e-02,\n",
       "            1.8213e-01, -7.4646e-02, -1.3947e-02, -1.3892e-01, -7.3181e-02,\n",
       "            7.5928e-02, -1.9006e-01,  3.1433e-02, -1.7834e-01,  1.8799e-01,\n",
       "            2.3132e-01, -2.3285e-02, -2.1277e-01, -3.1006e-01,  4.0479e-01,\n",
       "           -1.2140e-01, -9.7656e-02,  7.1533e-02, -7.7026e-02, -1.0925e-01,\n",
       "            1.9739e-01, -9.9182e-02,  2.4646e-01, -3.7158e-01, -1.9836e-01,\n",
       "           -6.9678e-01,  1.9373e-01, -1.9287e-01, -9.0454e-02,  3.8721e-01,\n",
       "           -9.8877e-02, -2.1265e-01,  5.3955e-02, -2.5195e-01,  5.3809e-01,\n",
       "            1.5343e-02, -1.1914e-01,  1.1395e-01,  1.0797e-01,  6.4049e-03,\n",
       "           -3.4351e-01,  9.6130e-02,  1.9739e-01,  2.7612e-01,  1.2978e-02,\n",
       "           -1.5027e-01, -1.2744e-01, -1.4185e-01,  3.2227e-01, -7.7515e-02,\n",
       "            3.6279e-01, -3.6133e-01,  2.3486e-01, -2.4475e-01, -3.3716e-01,\n",
       "            3.2617e-01, -6.4404e-01,  1.9861e-01, -7.6416e-02,  7.1831e-03,\n",
       "            2.6172e-01, -4.9756e-01, -1.3232e-01,  9.7656e-02, -1.0242e-01,\n",
       "           -1.3635e-01, -5.9357e-02,  1.7212e-01,  3.8892e-01,  2.5049e-01,\n",
       "           -3.5669e-01, -3.6938e-01,  9.8145e-02, -1.3831e-01,  3.4644e-01,\n",
       "           -5.8868e-02,  3.6591e-02,  7.7759e-02, -5.4297e-01, -3.1567e-01,\n",
       "            3.2422e-01,  1.9299e-01, -3.3276e-01,  3.1519e-01,  3.3643e-01,\n",
       "           -2.4731e-01,  2.3499e-01,  4.5471e-02,  3.7750e-02,  3.2349e-01,\n",
       "            3.3203e-01,  2.7441e-01,  7.7332e-02,  1.0590e-01, -1.7615e-01,\n",
       "            4.3144e-03, -1.6345e-01, -5.6543e-01,  8.7280e-02,  6.5625e-01,\n",
       "           -1.9348e-01,  2.1521e-01, -7.0923e-02,  2.5610e-01, -2.4719e-01,\n",
       "            1.8433e-01,  2.8954e-03,  2.8320e-01, -1.3428e-01,  3.5229e-01,\n",
       "            1.1938e-01,  1.8738e-01,  2.3743e-01,  9.3811e-02,  7.4316e-01,\n",
       "           -3.0933e-01, -7.8467e-01,  2.6636e-01, -4.4128e-02, -3.7915e-01,\n",
       "           -3.4937e-01, -1.4807e-01, -2.5854e-01,  1.3672e-01, -2.9712e-01,\n",
       "            2.1130e-01,  1.2476e-01,  4.2358e-02,  7.6074e-01,  1.7920e-01,\n",
       "           -4.0833e-02,  2.3560e-01,  1.3110e-01, -1.3977e-01,  2.5195e-01,\n",
       "            1.5063e-01, -1.8103e-01, -9.0332e-03,  5.1807e-01,  7.1838e-02,\n",
       "           -1.9485e-02, -7.8064e-02, -1.7944e-02,  5.1172e-01, -1.3318e-01,\n",
       "            2.0813e-01,  7.3120e-02]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5117, 0.4883]], dtype=torch.float16),\n",
       "  'df_boxes':         xmin        ymin   xmax        ymax  confidence  class    name\n",
       "  0  22.984909  131.632736  427.0  492.109924    0.890632     46  banana,\n",
       "  'caption': ['the second banana from the front'],\n",
       "  'bbox_target': [101.36, 152.78, 280.29, 220.62]},\n",
       " 742: {'image_emb': tensor([[ 0.0338,  0.3843, -0.2683,  ...,  0.8291, -0.2524, -0.2220],\n",
       "          [ 0.0022,  0.2450,  0.0920,  ...,  1.2344, -0.1096,  0.1109],\n",
       "          [ 0.2979,  0.3318, -0.2064,  ...,  1.5166,  0.0915, -0.1703],\n",
       "          ...,\n",
       "          [ 0.1979,  0.0391, -0.4434,  ...,  1.0596, -0.0715, -0.1538],\n",
       "          [-0.1194,  0.1104, -0.3425,  ...,  0.5972,  0.1203, -0.0455],\n",
       "          [-0.1526,  0.1357,  0.1433,  ...,  0.4717,  0.0351, -0.0966]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0771, -0.2357,  0.1865,  ...,  0.0375, -0.5425, -0.5059],\n",
       "          [-0.0506, -0.0262,  0.1019,  ...,  0.0343, -0.0155, -0.4946]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9072e-01, 8.8096e-05, 7.5698e-06, 7.5645e-03, 2.8872e-04, 2.7561e-04,\n",
       "           1.2517e-05, 9.9182e-04],\n",
       "          [9.8145e-01, 2.9516e-04, 1.0133e-06, 8.3618e-03, 4.7684e-06, 1.0073e-05,\n",
       "           5.9605e-08, 9.7733e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    89.828720  325.544708  386.835266  476.450439    0.929418      2   \n",
       "  1   200.851593  159.332779  395.901520  342.410522    0.905843     17   \n",
       "  2   252.207703  143.999908  297.908630  274.708466    0.875603      0   \n",
       "  3   248.535004  230.579926  569.322632  337.663849    0.835469      2   \n",
       "  4   535.994690   96.086105  575.982239  176.028290    0.795740      9   \n",
       "  5   239.208435   85.095474  274.071472  151.904221    0.744768      9   \n",
       "  6   601.732483  123.501846  640.000000  173.332504    0.707261      9   \n",
       "  7    72.885307   98.764557  215.549927  395.656464    0.651466      5   \n",
       "  8   553.265747  229.325287  614.214111  299.617950    0.630063      2   \n",
       "  9   429.179932  224.747314  570.383179  291.122009    0.623020      2   \n",
       "  10  397.156189  241.549683  424.742432  267.271606    0.591755      0   \n",
       "  11  375.639343  220.186829  392.778137  242.943176    0.471582      0   \n",
       "  12  374.465363  454.373901  637.965942  478.691650    0.433124      2   \n",
       "  13    0.180000    1.640045   97.011589  470.341064    0.287945      5   \n",
       "  14  221.395477  113.222137  239.861359  146.939880    0.275305      9   \n",
       "  \n",
       "               name  \n",
       "  0             car  \n",
       "  1           horse  \n",
       "  2          person  \n",
       "  3             car  \n",
       "  4   traffic light  \n",
       "  5   traffic light  \n",
       "  6   traffic light  \n",
       "  7             bus  \n",
       "  8             car  \n",
       "  9             car  \n",
       "  10         person  \n",
       "  11         person  \n",
       "  12            car  \n",
       "  13            bus  \n",
       "  14  traffic light  ,\n",
       "  'caption': ['A white car.',\n",
       "   'A white car with yellow tags preparing to enter an intersection.'],\n",
       "  'bbox_target': [89.81, 321.03, 300.38, 151.74]},\n",
       " 743: {'image_emb': tensor([[ 0.0636,  0.1582, -0.0968,  ...,  0.5522, -0.0839, -0.1152],\n",
       "          [ 0.2952,  0.2800, -0.2185,  ...,  0.4688,  0.0038, -0.1188],\n",
       "          [ 0.3982,  0.4587, -0.0484,  ...,  0.6187, -0.0467,  0.0798]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1234, -0.0230, -0.6113,  ...,  0.2712, -0.2428,  0.0956],\n",
       "          [ 0.0343,  0.0646, -0.4502,  ...,  0.3184, -0.2446, -0.2505]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.9844, 0.0030, 0.0128],\n",
       "          [0.9780, 0.0200, 0.0021]], dtype=torch.float16),\n",
       "  'df_boxes':        xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  0.676974   81.589981  411.415100  401.618042    0.920772      0  person\n",
       "  1  2.400722  176.985489  609.256165  609.708435    0.729591      0  person\n",
       "  2  2.053480  112.863289  608.934509  608.880676    0.416564     59     bed,\n",
       "  'caption': ['Boy in blue checkered shirt who is sleeping.',\n",
       "   'A sleeping toddler in a blue and white plaid shirt'],\n",
       "  'bbox_target': [0.0, 86.22, 412.58, 317.69]},\n",
       " 744: {'image_emb': tensor([[-4.8828e-01,  1.5881e-01,  1.1237e-01, -9.3933e-02,  7.8430e-03,\n",
       "            7.3425e-02, -3.3667e-01,  4.7073e-03,  2.7075e-01,  4.5898e-01,\n",
       "            2.0105e-01, -3.1323e-01,  5.6183e-02, -2.3941e-02,  1.8616e-01,\n",
       "           -3.4082e-01, -2.7783e-01,  4.7119e-02,  5.5389e-02, -3.6694e-01,\n",
       "            6.1523e-01,  3.5669e-01, -4.7046e-01,  5.9375e-01, -5.9540e-02,\n",
       "           -3.8184e-01, -6.9824e-02, -6.4270e-02, -1.7664e-01,  1.7126e-01,\n",
       "           -4.4238e-01,  4.7314e-01,  8.4229e-02, -2.2131e-01,  2.4170e-01,\n",
       "           -1.2189e-01, -7.3792e-02,  1.0370e-01, -2.1228e-01,  8.2910e-01,\n",
       "            6.7505e-02, -1.6089e-01, -4.7656e-01,  3.3722e-03, -2.2302e-01,\n",
       "           -3.9575e-01,  3.9795e-01,  4.3921e-01, -3.3765e-01, -1.0461e-01,\n",
       "            3.5669e-01, -2.1277e-01, -3.6548e-01, -4.0796e-01, -4.8901e-01,\n",
       "            1.2341e-01,  5.2246e-01,  2.6291e-02,  4.8999e-01, -1.5186e-01,\n",
       "            3.8037e-01,  4.7461e-01, -1.5015e-01, -2.2876e-01,  9.2468e-02,\n",
       "            2.1472e-01,  1.3245e-02,  2.9639e-01, -3.3252e-01,  9.1919e-02,\n",
       "            1.7627e-01,  4.5996e-01,  5.9652e-04,  2.5098e-01, -1.0269e-02,\n",
       "           -2.2961e-01, -1.2433e-01, -8.6609e-02, -2.0679e-01, -9.3384e-02,\n",
       "            3.5718e-01, -2.3950e-01,  4.1748e-01,  6.9922e-01,  8.9111e-02,\n",
       "            4.0625e-01,  3.0859e-01, -5.8936e-01,  1.1436e+00, -2.4597e-01,\n",
       "           -2.4605e-03, -5.7422e-01, -6.8320e+00,  3.7207e-01, -7.1838e-02,\n",
       "            3.7988e-01, -1.1761e-01,  3.1830e-02,  1.3904e-01, -1.4893e+00,\n",
       "            2.4017e-02,  2.0554e-02,  5.4395e-01, -3.4766e-01,  5.4443e-01,\n",
       "           -2.7100e-01, -5.6055e-01,  6.4209e-01,  7.2449e-02, -7.6721e-02,\n",
       "            7.6843e-02, -1.3096e+00,  1.4572e-03,  2.0715e-01,  2.0007e-01,\n",
       "            3.5547e-01, -8.7695e-01, -7.6367e-01, -1.0345e-01, -2.1960e-01,\n",
       "           -5.8746e-02,  2.3914e-01,  2.7612e-01, -2.3816e-01, -1.4941e-01,\n",
       "            3.4375e-01, -2.0618e-01, -1.1987e-01,  1.6321e-01,  6.0254e-01,\n",
       "           -3.1714e-01, -4.2578e-01, -1.4600e-01,  8.9258e-01, -3.1689e-01,\n",
       "            1.8140e-01,  1.3771e-02, -7.0166e-01, -3.5864e-01,  1.4417e-01,\n",
       "           -1.1865e-01, -2.3267e-01,  1.1841e-01, -1.1743e-01,  7.7637e-02,\n",
       "           -2.4316e-01, -3.1689e-01, -1.4417e-01,  6.8787e-02, -2.7649e-02,\n",
       "           -3.0786e-01,  1.0809e-01,  5.5029e-01, -2.7197e-01,  1.2201e-01,\n",
       "            3.5083e-01, -7.2510e-02, -2.7267e-02,  4.4727e-01, -9.0698e-02,\n",
       "           -1.4172e-01, -7.5049e-01,  2.9883e-01,  1.6101e-01, -5.2307e-02,\n",
       "            9.3323e-02,  6.2891e-01,  6.2109e-01, -2.2241e-01, -7.1472e-02,\n",
       "           -2.5192e-02,  2.8296e-01,  1.4099e-01, -1.9434e-01, -2.8320e-01,\n",
       "           -3.2959e-01, -3.6646e-01,  2.0337e-01,  5.2295e-01, -7.0610e-03,\n",
       "           -1.8835e-01, -5.9961e-01, -2.2388e-01, -2.7124e-01,  1.7664e-01,\n",
       "            2.8809e-02,  9.2407e-02,  3.9917e-01, -1.3696e-01, -4.8413e-01,\n",
       "            3.5156e-01, -1.4209e-01,  2.5833e-02, -4.0625e-01,  4.2358e-01,\n",
       "           -2.1033e-01, -4.9170e-01,  7.7832e-01, -1.6589e-01,  1.0742e-02,\n",
       "           -1.6443e-01, -6.5125e-02, -4.4952e-02,  6.9214e-02, -8.3740e-02,\n",
       "            2.2620e-01,  1.2549e-01, -2.1460e-01,  3.7427e-01,  8.6792e-02,\n",
       "           -2.9907e-01,  4.9048e-01, -9.7107e-02,  3.8757e-02, -3.1090e-03,\n",
       "            1.4636e-01, -2.4426e-01, -2.6025e-01,  1.1676e-01,  1.2146e-01,\n",
       "            2.9004e-01, -1.8707e-02, -7.3975e-02,  3.1982e-01, -2.7054e-02,\n",
       "            3.3105e-01, -8.3374e-02, -2.0203e-01,  1.0895e-01,  1.5979e-01,\n",
       "           -6.4600e-01,  2.0337e-01, -9.3384e-02,  3.5327e-01,  3.1152e-01,\n",
       "            2.9639e-01, -7.0267e-03, -4.1040e-01, -5.1074e-01, -2.7686e-01,\n",
       "            1.3428e-02, -2.8091e-02, -6.9043e-01, -2.9224e-01,  1.0913e-01,\n",
       "            1.0022e-01, -3.4570e-01,  5.4639e-01,  7.5623e-02,  4.7021e-01,\n",
       "            2.9077e-01,  1.0730e-01,  1.4746e-01,  1.3306e-01,  2.1411e-01,\n",
       "           -4.3365e-02, -3.1891e-02,  1.8250e-02,  1.6074e+00,  4.0924e-02,\n",
       "            7.6367e-01, -3.8354e-01, -5.2637e-01,  1.8936e+00,  2.5220e-01,\n",
       "           -3.9703e-02,  2.8467e-01,  2.9443e-01,  5.8301e-01, -4.2480e-01,\n",
       "           -1.5161e-01,  1.4038e-01,  1.1572e-01, -3.7573e-01, -1.0565e-01,\n",
       "           -1.4429e-01, -1.7114e-01, -2.8369e-01, -5.5359e-02,  9.8206e-02,\n",
       "           -2.5000e-01, -9.7229e-02, -8.4900e-02,  4.1846e-01, -4.3921e-01,\n",
       "            3.0664e-01,  4.3555e-01, -9.7427e-03,  1.8542e-01,  1.9031e-01,\n",
       "            1.7371e-01,  5.7373e-02, -1.5063e-01,  6.3379e-01, -4.7668e-02,\n",
       "           -1.0614e-01, -5.7178e-01,  1.1353e-01,  5.1331e-02, -4.6558e-01,\n",
       "            2.9199e-01,  7.8552e-02,  1.8445e-01,  6.4014e-01, -1.2280e-01,\n",
       "            2.9810e-01, -2.7417e-01,  3.7939e-01, -3.9062e-02,  3.2764e-01,\n",
       "           -5.6641e-02,  2.2083e-01,  8.9062e-01,  5.9631e-02, -1.4001e-01,\n",
       "            2.5049e-01,  3.7231e-01, -1.2329e-01, -3.6499e-01,  7.7344e-01,\n",
       "            3.2104e-01,  2.3086e+00, -4.4385e-01, -5.8154e-01, -5.2734e-01,\n",
       "            5.3613e-01,  1.5857e-01, -1.6992e-01, -1.3771e-02,  1.3965e-01,\n",
       "           -2.9224e-01,  1.3782e-01,  4.1113e-01,  2.1411e-01,  1.3538e-01,\n",
       "           -2.2058e-01, -2.2046e-01,  2.4146e-01, -1.0605e-02,  4.1577e-01,\n",
       "           -1.6260e-01, -4.5142e-01, -1.0864e-01,  4.6948e-01, -2.3840e-01,\n",
       "           -1.1493e-01, -3.6346e-02,  2.4878e-01, -4.2896e-01, -2.5772e-02,\n",
       "           -2.7441e-01,  2.7847e-02,  1.4526e-01, -5.2032e-02, -2.1130e-01,\n",
       "           -6.7078e-02, -2.5439e-01,  1.1719e+00, -1.0199e-01, -2.5024e-01,\n",
       "           -1.1786e-01,  2.7490e-01,  2.1838e-01,  8.1253e-04, -1.1871e-01,\n",
       "            6.1621e-01, -1.2253e-02,  1.8835e-01, -2.7197e-01,  5.6299e-01,\n",
       "           -2.1606e-01,  4.2700e-01, -1.3901e-02, -1.8201e-01,  1.4697e-01,\n",
       "            2.0862e-01, -3.9697e-01,  3.7451e-01,  5.9784e-02, -4.0314e-02,\n",
       "           -1.0687e-01, -4.2480e-02,  2.8003e-01,  1.1548e-01,  7.4097e-02,\n",
       "           -1.0541e-01, -1.2347e-01, -1.4221e-02,  1.1456e-01,  1.4395e+00,\n",
       "            2.7466e-01,  9.9335e-03, -4.2529e-01,  2.5439e-01,  5.7831e-02,\n",
       "           -7.9773e-02,  5.4443e-01, -4.8218e-01,  5.2441e-01,  9.2407e-02,\n",
       "           -3.4546e-01,  2.0813e-01,  2.4329e-01, -1.6101e-01,  1.6858e-01,\n",
       "            6.7993e-02,  5.7129e-02, -3.5376e-01,  3.6377e-01,  2.2812e-02,\n",
       "           -1.2988e-01, -1.9666e-01,  1.3634e-02,  1.2842e-01, -1.3135e-01,\n",
       "            3.7506e-02, -3.5620e-01,  4.3188e-01, -8.2642e-02, -5.2783e-01,\n",
       "            2.3840e-01,  2.7222e-01,  3.2373e-01, -3.5156e-01,  6.5796e-02,\n",
       "           -4.7388e-01, -1.4687e-02, -1.5594e-02,  1.5674e+00, -4.0479e-01,\n",
       "           -2.0752e-01,  5.7764e-01, -1.1416e+00,  2.5708e-01, -1.9653e-01,\n",
       "            2.5049e-01, -4.4873e-01, -3.9014e-01, -4.8145e-01,  3.5059e-01,\n",
       "           -3.0347e-01, -4.4403e-02, -3.7811e-02, -5.7422e-01,  6.4697e-01,\n",
       "            1.1835e-01,  3.7720e-01, -2.7197e-01, -3.5986e-01, -4.2358e-02,\n",
       "            4.3286e-01, -2.0801e-01, -1.2415e-01,  6.8896e-01,  2.1436e-01,\n",
       "           -4.8340e-01,  2.1741e-01, -1.8372e-01,  8.3313e-02,  4.9011e-02,\n",
       "           -3.0762e-01, -1.6687e-01, -2.4548e-01, -1.6858e-01,  3.7036e-01,\n",
       "           -2.3157e-01,  1.2036e-01,  4.4434e-01, -2.3438e-01,  7.6660e-02,\n",
       "           -1.3928e-01, -6.0616e-03, -6.2195e-02, -5.4541e-01, -1.1810e-01,\n",
       "           -4.0161e-01,  4.8340e-01,  2.2888e-02, -2.0227e-01, -4.0820e-01,\n",
       "           -2.6294e-01, -2.6270e-01,  8.2397e-02,  2.4060e-01,  2.5732e-01,\n",
       "            1.2512e-01,  2.1204e-01, -2.0593e-01,  9.0576e-02, -1.2854e-01,\n",
       "            1.1792e-01, -1.9983e-01,  1.8295e-02, -5.7715e-01, -2.2913e-01,\n",
       "           -2.8223e-01, -2.1594e-01,  1.0120e-01, -5.5511e-02, -2.7905e-01,\n",
       "           -6.5491e-02, -4.2686e-03, -2.6489e-01,  2.3914e-01,  1.0615e+00,\n",
       "           -3.8696e-01, -2.3941e-02,  9.3384e-02, -1.5039e-01,  5.4590e-01,\n",
       "            1.0181e-01, -7.0496e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2759, -0.0740,  0.2104,  ...,  0.0347,  0.3445, -0.4229],\n",
       "          [-0.2571,  0.2091, -0.1399,  ...,  0.2283,  0.4602, -0.2812]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.],\n",
       "          [1.]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  420.918427    0.323135  500.000000  206.725433    0.548422      9   \n",
       "  1  225.925308  350.047241  241.785904  356.345978    0.478979      2   \n",
       "  2  260.795319    2.771038  367.369324  158.206863    0.450451      9   \n",
       "  3  261.531311    0.000000  495.211884  206.650757    0.274979      9   \n",
       "  4    0.242610  344.025879   13.281178  355.502289    0.268973      2   \n",
       "  \n",
       "              name  \n",
       "  0  traffic light  \n",
       "  1            car  \n",
       "  2  traffic light  \n",
       "  3  traffic light  \n",
       "  4            car  ,\n",
       "  'caption': ['red stoplight that is right of the other one.',\n",
       "   'The traffic signal on the right side of the pole.'],\n",
       "  'bbox_target': [401.32, 1.41, 98.68, 209.53]},\n",
       " 745: {'image_emb': tensor([[-0.0738, -0.3953, -0.1087,  ...,  0.5850,  0.1521, -0.4907],\n",
       "          [-0.0778,  0.2139, -0.3215,  ...,  1.2080,  0.0453,  0.3237],\n",
       "          [ 0.0507, -0.5923, -0.1572,  ...,  0.2986, -0.1210,  0.0173]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1451,  0.2263, -0.1636,  ..., -0.2429, -0.0667,  0.0088],\n",
       "          [ 0.0272,  0.0356, -0.4062,  ..., -0.2849, -0.1489,  0.1372]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7464e-05, 9.9854e-01, 1.2064e-03],\n",
       "          [1.4168e-02, 9.0430e-01, 8.1543e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  178.464111  263.308960  389.473145  401.732361    0.911965     23  giraffe\n",
       "  1    0.000000  316.969055   57.203735  631.765320    0.778995      0   person\n",
       "  2   27.429520  194.132660  389.901611  601.323975    0.699232     23  giraffe\n",
       "  3   29.892426  340.734131  114.433075  620.217102    0.676205      0   person\n",
       "  4   27.888382  193.846497  386.825012  333.032349    0.585926     23  giraffe,\n",
       "  'caption': ['The visible right half of the adult wearing a sleeveless pink shirt',\n",
       "   'The arm and side of a person standing directly behind a girl.'],\n",
       "  'bbox_target': [0.0, 330.31, 56.52, 292.57]},\n",
       " 746: {'image_emb': tensor([[-0.5054, -0.0958, -0.1801,  ...,  0.5088,  0.1562,  0.1437],\n",
       "          [-0.4753, -0.2537, -0.1316,  ...,  0.4673,  0.0065,  0.0806],\n",
       "          [-0.6226, -0.2278, -0.1415,  ...,  0.5283,  0.1542,  0.1040],\n",
       "          [-0.6699, -0.2886, -0.0715,  ...,  0.6768,  0.1155,  0.1167],\n",
       "          [-0.4907, -0.4089, -0.0187,  ...,  0.3567, -0.2162,  0.1658]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2742, -0.2976, -0.0814,  ..., -0.0974,  0.0251,  0.3022],\n",
       "          [-0.2498, -0.3281, -0.1534,  ...,  0.0972,  0.0804,  0.3940]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1495, 0.1077, 0.0633, 0.2666, 0.4128],\n",
       "          [0.3508, 0.3508, 0.1037, 0.0807, 0.1139]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  157.569351  147.974731  308.813416  365.218811    0.942629     22  zebra\n",
       "  1   52.616028  152.064819  247.559326  338.126770    0.935033     22  zebra\n",
       "  2  306.480957  154.194855  534.842896  344.391327    0.918155     22  zebra\n",
       "  3  424.256714  197.493805  564.892578  334.387848    0.819211     22  zebra,\n",
       "  'caption': ['Second from the left zebra.', 'the nearest zebra'],\n",
       "  'bbox_target': [155.1, 151.27, 156.05, 216.37]},\n",
       " 747: {'image_emb': tensor([[-0.2976,  0.5356,  0.0070,  ...,  0.9897, -0.1918, -0.1294],\n",
       "          [-0.1060,  0.4829,  0.1876,  ...,  1.0137, -0.0884,  0.0337],\n",
       "          [-0.2402,  0.3284,  0.0301,  ...,  0.7598,  0.0155, -0.1608],\n",
       "          [-0.3391,  0.3193, -0.3032,  ...,  1.1240,  0.3159, -0.1438],\n",
       "          [-0.1103,  0.4539,  0.0306,  ...,  0.8247, -0.1567, -0.0108]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0072, -0.0554, -0.3394,  ..., -0.1429, -0.0494, -0.1083],\n",
       "          [ 0.0743, -0.1216, -0.2808,  ...,  0.0253,  0.0700,  0.2001]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.8877e-01, 6.1393e-05, 4.4403e-03, 7.7152e-04, 5.7907e-03],\n",
       "          [2.7051e-01, 1.0223e-03, 2.3865e-01, 1.5259e-05, 4.8975e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    0.825562    0.746490  325.787231  350.286530    0.909503      0  person\n",
       "  1  243.677811  111.439026  639.386414  475.406433    0.909300      0  person\n",
       "  2    1.199661  197.907959  314.850891  476.042908    0.889228     56   chair\n",
       "  3  182.129395  268.492737  274.018738  311.694824    0.853324     55    cake\n",
       "  4  289.748749  153.142456  322.486664  226.760437    0.544192     42    fork\n",
       "  5  430.800842  412.322968  638.911560  478.409668    0.428327     56   chair\n",
       "  6  289.635071  107.813416  322.714111  226.794495    0.316116     44   spoon,\n",
       "  'caption': ['the hands feeding the baby',\n",
       "   \"a person's hands feeding a young girl in a yellow chair\"],\n",
       "  'bbox_target': [0.0, 0.0, 327.49, 348.56]},\n",
       " 748: {'image_emb': tensor([[-0.3252, -0.0200, -0.0685,  ...,  0.6104,  0.2083, -0.0080],\n",
       "          [-0.3577, -0.1996,  0.0597,  ...,  0.2113,  0.2893,  0.2688]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2869, -0.2568, -0.0763,  ...,  0.2964,  0.4192,  0.6245],\n",
       "          [-0.2450, -0.3210,  0.1252,  ...,  0.4109,  0.3542,  0.3010]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6514, 0.3486],\n",
       "          [0.5273, 0.4727]], dtype=torch.float16),\n",
       "  'df_boxes':         xmin        ymin        xmax       ymax  confidence  class   name\n",
       "  0  87.737595  102.662598  530.093689  404.98877    0.950842     22  zebra,\n",
       "  'caption': ['A zebra eats from the right end of a trough.',\n",
       "   'A zebra leaning down to eat out of a wooden trough.'],\n",
       "  'bbox_target': [96.22, 98.92, 431.35, 297.3]},\n",
       " 749: {'image_emb': tensor([[-0.1301,  0.1520, -0.2788,  ...,  0.3970, -0.0320,  0.2146],\n",
       "          [-0.5576,  0.2229, -0.2329,  ...,  0.6533, -0.3066,  0.1538],\n",
       "          [ 0.2285,  0.1298, -0.3291,  ...,  1.2158,  0.4092, -0.0730],\n",
       "          ...,\n",
       "          [-0.6143, -0.1301,  0.1614,  ...,  0.8926, -0.0038, -0.2241],\n",
       "          [-0.0916, -0.2632, -0.1813,  ...,  1.2441,  0.1611, -0.0754],\n",
       "          [-0.2981, -0.1719,  0.3489,  ...,  0.2061, -0.2147, -0.2181]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2798, -0.2031, -0.2888,  ...,  0.1985, -0.1677,  0.0165],\n",
       "          [-0.1132, -0.3611, -0.8096,  ..., -0.1808,  0.1418, -0.0371]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.4026e-01, 1.1627e-01, 5.9605e-08, 0.0000e+00, 8.5526e-03, 7.1526e-07,\n",
       "           7.3486e-01],\n",
       "          [2.8946e-02, 2.7191e-02, 3.2187e-06, 9.4175e-06, 4.6492e-06, 3.2139e-04,\n",
       "           9.4336e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   299.636475  101.755157  391.753845  303.479706    0.937067      0   \n",
       "  1   222.391693   90.347656  309.609100  337.859619    0.924039      0   \n",
       "  2   374.704376  299.355713  412.444427  359.866516    0.875449     75   \n",
       "  3   552.203491  314.284058  616.820557  359.435608    0.859640     56   \n",
       "  4    97.708923  155.138458  212.328705  298.018036    0.845587     69   \n",
       "  5   259.120117  291.510376  287.667175  358.874512    0.767715     56   \n",
       "  6   226.934937  104.900131  240.561401  125.146942    0.650247     41   \n",
       "  7   144.140427   93.456543  194.954605  126.117584    0.496988     68   \n",
       "  8   337.198273   61.838303  360.677704   81.159866    0.489857     41   \n",
       "  9   594.448303   45.190079  626.949158   64.188950    0.457585      2   \n",
       "  10  337.380066   62.105072  360.948669   81.145630    0.356121     45   \n",
       "  11  198.532715  144.050049  227.371948  160.075867    0.352208     46   \n",
       "  12  264.942749  299.553406  535.823059  358.905579    0.339019     60   \n",
       "  13  386.180664  172.731110  407.283508  196.709198    0.314938     71   \n",
       "  14  205.456573  104.460968  219.321869  125.240662    0.308688     41   \n",
       "  15    0.509659  255.081665   87.402763  356.761169    0.293498     60   \n",
       "  16  537.913635  120.520782  586.821594  177.044769    0.257452     77   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2           vase  \n",
       "  3          chair  \n",
       "  4           oven  \n",
       "  5          chair  \n",
       "  6            cup  \n",
       "  7      microwave  \n",
       "  8            cup  \n",
       "  9            car  \n",
       "  10          bowl  \n",
       "  11        banana  \n",
       "  12  dining table  \n",
       "  13          sink  \n",
       "  14           cup  \n",
       "  15  dining table  \n",
       "  16    teddy bear  ,\n",
       "  'caption': ['A young girl with black hair in a plaid dress playing with a toy kitchen set.',\n",
       "   'The little girl on the left with black hair and wearing a plaid dress.'],\n",
       "  'bbox_target': [220.04, 91.42, 88.99, 243.5]},\n",
       " 750: {'image_emb': tensor([[ 0.2659, -0.1456,  0.1617,  ...,  0.1746,  0.0014, -0.0883],\n",
       "          [-0.0397,  0.1100,  0.1278,  ...,  1.2578, -0.1293, -0.2076],\n",
       "          [ 0.3083, -0.2247,  0.2052,  ...,  0.0299, -0.0594, -0.1454]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1914, -0.0382, -0.3210,  ...,  0.4451, -0.4517,  0.3069],\n",
       "          [ 0.2302,  0.1389, -0.2393,  ...,  0.3101, -0.3799,  0.1073]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.8203, 0.1360, 0.0435],\n",
       "          [0.6777, 0.1399, 0.1824]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class name\n",
       "  0    6.312990  64.485443  439.375458  477.593719    0.937439     15  cat\n",
       "  1  150.165649  64.365974  456.403870  211.791534    0.863515     15  cat,\n",
       "  'caption': ['A cat lying on a deck outdoors looking inside.',\n",
       "   'The cat reclining outside the window.'],\n",
       "  'bbox_target': [150.14, 67.25, 307.53, 149.31]},\n",
       " 751: {'image_emb': tensor([[-0.7275,  0.0825,  0.2067,  ...,  0.9858,  0.0084,  0.0021],\n",
       "          [-0.5693,  0.0702,  0.3140,  ...,  0.7729,  0.2437,  0.2162]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 2.3853e-01, -3.4595e-01, -2.2961e-01, -8.9844e-02,  1.4209e-01,\n",
       "            6.9702e-02, -4.3750e-01, -3.0444e-01,  1.5540e-01, -2.3218e-01,\n",
       "            9.3689e-02, -1.4880e-01,  7.1289e-02, -9.4223e-03, -2.5269e-02,\n",
       "           -1.7041e-01, -1.1176e-01, -2.6636e-01,  1.4160e-01, -6.2622e-02,\n",
       "           -1.1407e-01,  4.0308e-01, -1.1017e-01,  1.4795e-01, -2.1729e-01,\n",
       "           -1.0480e-01,  2.4048e-01,  5.9967e-02,  3.3600e-02, -2.5049e-01,\n",
       "            3.2495e-01, -3.4155e-01, -5.4565e-02, -2.8540e-01, -9.7070e-01,\n",
       "           -6.5979e-02,  3.2812e-01, -2.2632e-01,  8.8196e-02,  1.1646e-01,\n",
       "           -1.2659e-01, -8.8013e-02, -5.9375e-01, -3.7280e-01,  6.2683e-02,\n",
       "           -7.5134e-02, -8.5144e-02,  1.1892e-03, -5.1636e-02, -2.5513e-01,\n",
       "           -1.7664e-01, -2.3743e-01,  1.2756e-01,  1.9751e-01,  1.3342e-01,\n",
       "            2.3047e-01,  2.0996e-01, -2.5977e-01, -1.3220e-01,  1.5369e-01,\n",
       "            5.5176e-02,  4.2261e-01, -3.8815e-04,  7.6050e-02,  4.1724e-01,\n",
       "           -1.6687e-01,  2.5223e-02,  6.8665e-02, -1.7578e-01, -8.2458e-02,\n",
       "            5.3070e-02, -3.8940e-01, -1.5332e-01, -2.1045e-01,  2.2119e-01,\n",
       "           -9.3933e-02,  3.3752e-02, -7.4097e-02, -2.3206e-01,  9.8572e-02,\n",
       "           -3.9600e-01,  7.8613e-02, -3.2080e-01,  4.6570e-02, -2.0361e-01,\n",
       "            3.9032e-02,  1.2323e-01, -2.4475e-02,  3.0322e-01,  1.4026e-01,\n",
       "           -2.1387e-01,  4.3750e-01, -9.1260e-01,  3.6230e-01, -4.2920e-01,\n",
       "            5.5450e-02, -3.3203e-01, -4.8169e-01,  2.2131e-01, -6.0196e-03,\n",
       "           -1.1609e-01,  1.0089e-01, -1.3770e-01,  2.4390e-01,  1.0521e-02,\n",
       "           -1.1755e-01, -3.1055e-01,  2.7051e-01, -1.3428e-01, -3.4106e-01,\n",
       "            1.3000e-01, -5.3406e-02,  8.8562e-02,  1.4307e-01,  3.4375e-01,\n",
       "            3.2690e-01, -9.2773e-02, -1.2927e-01, -6.3525e-01,  5.2832e-01,\n",
       "            1.0559e-01, -1.2457e-01, -2.6465e-01,  7.1338e-01, -9.0820e-02,\n",
       "            1.1322e-01, -4.0894e-02,  1.0388e-01, -6.4355e-01, -7.0068e-02,\n",
       "            2.0471e-01, -1.0704e-02, -5.7190e-02,  3.8770e+00, -7.8491e-02,\n",
       "           -3.8501e-01, -1.2842e-01, -4.7461e-01,  3.7628e-02, -5.5695e-02,\n",
       "           -4.8523e-02,  5.4199e-01,  4.4604e-01, -8.8440e-02,  3.3130e-01,\n",
       "            1.0748e-01,  6.1584e-02, -5.7324e-01,  3.2520e-01, -3.9215e-02,\n",
       "           -1.0101e-01, -2.1716e-01,  5.4382e-02, -2.6904e-01,  2.9761e-01,\n",
       "           -3.2886e-01, -6.9458e-02, -7.0752e-01,  4.0698e-01,  1.5015e-01,\n",
       "            2.1252e-01, -1.0657e-01, -2.7588e-01, -9.8938e-02,  4.6680e-01,\n",
       "           -6.0089e-02,  3.8989e-01,  3.1738e-01,  6.3330e-01,  1.8640e-01,\n",
       "            2.8296e-01, -1.0291e-01,  4.1577e-01,  4.4861e-02, -4.9609e-01,\n",
       "           -6.6504e-01,  3.1104e-01,  2.1606e-01, -4.8676e-02, -1.1978e-02,\n",
       "           -5.3662e-01,  4.3799e-01, -1.6785e-01, -3.2593e-02, -1.6809e-01,\n",
       "           -1.9568e-01,  1.7419e-01,  2.3157e-01,  3.0273e-01, -1.2396e-01,\n",
       "            2.8760e-01,  5.9753e-02, -1.5637e-01,  3.5095e-02, -2.4536e-01,\n",
       "            8.3069e-02, -2.5708e-01,  5.9033e-01,  1.4587e-01,  3.2318e-02,\n",
       "            1.4758e-01, -1.5222e-01,  3.2983e-01,  3.3887e-01, -1.0126e-01,\n",
       "           -4.9976e-01, -4.8492e-02, -5.5389e-02, -7.4768e-03,  2.8052e-01,\n",
       "           -5.4260e-02, -1.3611e-01,  5.1575e-02, -3.0371e-01, -3.7109e-01,\n",
       "           -1.0236e-01, -1.9543e-01, -1.7773e-01,  2.0215e-01, -3.5449e-01,\n",
       "            3.8794e-01,  1.1877e-01,  4.9774e-02,  3.4253e-01, -5.6787e-01,\n",
       "            8.6121e-02, -4.8553e-02,  8.7524e-02,  1.6370e-01, -1.0535e-01,\n",
       "           -3.1403e-02,  1.4001e-01,  1.2337e-02,  2.4689e-02,  6.3171e-02,\n",
       "           -1.6687e-01,  2.2327e-01, -3.3545e-01, -7.9468e-02, -6.8481e-02,\n",
       "            4.3237e-01, -1.1139e-01,  2.7246e-01,  1.9592e-02,  1.1163e-01,\n",
       "           -1.9934e-01, -4.7417e-03,  2.6294e-01,  1.1902e-01,  1.6077e-01,\n",
       "           -2.7783e-01,  8.4717e-02,  1.2140e-01, -2.6782e-01,  3.5522e-01,\n",
       "           -2.8687e-01, -1.9165e-01, -5.8691e-01,  1.1462e-01,  1.7065e-01,\n",
       "            2.1399e-01, -8.4717e-02,  4.0747e-01,  2.1533e-01, -6.7688e-02,\n",
       "            8.1116e-02, -1.8457e-01,  2.5464e-01,  1.6309e-01,  4.5563e-02,\n",
       "           -5.7617e-01, -2.3901e-01,  5.1819e-02,  2.7197e-01, -2.4927e-01,\n",
       "           -3.6572e-01,  7.0068e-02, -4.3921e-01,  3.8623e-01, -4.3018e-01,\n",
       "            2.3340e-01,  5.3809e-01,  6.5234e-01,  5.1074e-01, -7.2403e-03,\n",
       "           -2.5171e-01,  2.6465e-01, -5.0507e-02,  2.4731e-01,  3.1528e-03,\n",
       "           -2.8564e-01,  5.0244e-01,  2.7026e-01,  1.6284e-01, -2.4780e-02,\n",
       "            5.0244e-01, -5.6396e-01,  8.1238e-02,  3.4009e-01,  3.4570e-01,\n",
       "           -2.1875e-01,  7.6294e-02, -2.7180e-03, -4.8767e-02,  2.5854e-01,\n",
       "           -7.0020e-01,  1.8030e-01, -1.9861e-01, -1.6138e-01, -1.8701e-01,\n",
       "           -1.3687e-02,  8.6475e-01,  3.8691e+00,  1.2360e-01, -4.6094e-01,\n",
       "            1.6968e-01, -1.9507e-01, -4.8901e-01,  1.4563e-01,  1.6675e-01,\n",
       "           -6.3660e-02, -1.2154e-02, -7.1228e-02,  2.3743e-01, -4.2480e-01,\n",
       "            5.6592e-01,  4.7827e-01,  8.6426e-02, -4.7699e-02, -8.4326e-01,\n",
       "            3.7402e-01, -2.0532e-01,  3.6716e-03,  1.6919e-01,  9.8572e-02,\n",
       "           -4.1260e-02, -1.4258e-01,  7.0114e-03, -1.2962e-02,  1.7236e-01,\n",
       "            3.9746e-01,  1.9006e-01, -5.8533e-02, -7.1582e-01, -3.7720e-01,\n",
       "            1.4197e-01,  2.6611e-01,  1.8164e-01,  3.4595e-01,  7.2205e-02,\n",
       "            7.2070e-01,  3.1714e-01, -2.4475e-01,  1.8835e-01, -5.0195e-01,\n",
       "           -6.8164e-01,  4.3896e-01,  2.0752e-01, -1.7798e-01,  3.5059e-01,\n",
       "            4.7974e-01, -4.1229e-02,  4.9194e-02, -4.1229e-02,  1.9556e-01,\n",
       "            2.4890e-01, -6.7444e-02, -3.9459e-02, -1.9165e-01,  2.1713e-02,\n",
       "            2.9761e-01,  2.0764e-01, -2.2827e-01, -2.4084e-01, -1.2012e-01,\n",
       "            1.8945e-01, -1.8262e-01, -1.6675e-01,  3.4009e-01,  2.4524e-01,\n",
       "            3.8623e-01, -9.4482e-02,  4.0137e-01, -7.2314e-01,  1.5540e-01,\n",
       "            4.5471e-02, -1.6907e-01,  2.6709e-01,  3.1348e-01, -1.0028e-01,\n",
       "           -9.3555e-01,  5.1855e-01,  1.1908e-01, -4.2065e-01, -6.1554e-02,\n",
       "            2.1191e-01,  2.5854e-01, -1.2585e-01,  2.1179e-01,  2.9004e-01,\n",
       "            2.8833e-01, -1.7847e-01,  4.0527e-01, -2.7417e-01,  1.7471e-02,\n",
       "            1.9946e-01, -1.1823e-01,  3.0420e-01,  4.7632e-01, -2.9190e-02,\n",
       "            3.2129e-01, -2.4487e-01,  2.0007e-01,  5.2948e-02, -1.3680e-02,\n",
       "            3.1982e-01, -1.5088e-01, -7.1533e-01,  1.6663e-01,  3.2898e-02,\n",
       "            8.8623e-02,  6.4880e-02, -1.0506e-02, -1.1700e-01,  7.0984e-02,\n",
       "           -3.4155e-01, -2.4475e-01, -1.1133e-01,  3.3350e-01, -5.0781e-01,\n",
       "           -2.5830e-01,  7.5195e-02, -3.8623e-01,  1.1462e-01, -2.8553e-03,\n",
       "           -2.2827e-01,  1.2756e-01,  1.0614e-01,  2.2278e-01,  2.5659e-01,\n",
       "            6.1249e-02, -1.2903e-01, -3.7305e-01,  1.8997e-02, -3.1342e-02,\n",
       "            4.0674e-01,  7.0496e-02, -4.1113e-01,  4.1187e-01, -4.4409e-01,\n",
       "           -1.5405e-01,  5.9662e-02, -2.4927e-01, -4.1992e-02,  5.1384e-03,\n",
       "           -4.1675e-01, -4.3530e-01,  1.0727e-02, -2.2412e-01, -2.9565e-01,\n",
       "            1.0162e-02, -7.7271e-02, -1.5221e-02, -2.8442e-01,  4.7510e-01,\n",
       "           -4.0381e-01,  1.2329e-01, -2.5952e-01, -3.8843e-01, -1.1572e-01,\n",
       "            1.4490e-01,  3.5675e-02,  2.1313e-01,  2.1667e-01,  3.1787e-01,\n",
       "           -1.2915e-01,  1.1444e-01,  1.7017e-01,  3.7427e-01, -1.5820e-01,\n",
       "           -3.2886e-01, -2.7222e-01,  1.3342e-01, -2.6245e-01, -1.0242e-01,\n",
       "           -1.9080e-01, -4.6436e-01,  3.5352e-01, -1.8433e-01,  1.7505e-01,\n",
       "            5.2539e-01,  9.6741e-02, -1.3733e-01,  5.5713e-01,  2.2644e-01,\n",
       "           -2.1484e-01, -1.9482e-01, -4.5990e-02,  8.6548e-02, -3.3179e-01,\n",
       "           -3.4692e-01, -6.5674e-02, -2.5879e-01,  3.8550e-01,  4.0820e-01,\n",
       "            2.2644e-01, -2.4084e-01, -5.0507e-02, -1.1951e-01,  9.4043e-01,\n",
       "            4.1235e-01, -2.9443e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6074, 0.3923]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   305.109222   46.487427  480.449005  477.494202    0.724285     30   \n",
       "  1   145.963196   63.254166  260.429260  469.240051    0.531474     31   \n",
       "  2   230.961029   15.551666  365.312836  475.079346    0.508797     30   \n",
       "  3   393.923218  345.670013  463.933411  477.546326    0.434094     30   \n",
       "  4   446.724213  214.331940  549.716797  475.706787    0.429243     30   \n",
       "  5   568.490417   64.956451  637.473328  177.574249    0.350851     31   \n",
       "  6   413.241547  125.317337  524.953735  350.759644    0.347725     30   \n",
       "  7   176.486862   25.248245  322.198181  476.881287    0.298762     31   \n",
       "  8   570.261719   63.961121  638.042236  178.762024    0.289219     30   \n",
       "  9    84.522705   40.523621  178.236298  349.761353    0.282407     30   \n",
       "  10  502.972992  123.333984  601.654907  244.195618    0.271742     30   \n",
       "  11   38.561512    0.472969   83.242050   63.367859    0.269185     24   \n",
       "  \n",
       "           name  \n",
       "  0        skis  \n",
       "  1   snowboard  \n",
       "  2        skis  \n",
       "  3        skis  \n",
       "  4        skis  \n",
       "  5   snowboard  \n",
       "  6        skis  \n",
       "  7   snowboard  \n",
       "  8        skis  \n",
       "  9        skis  \n",
       "  10       skis  \n",
       "  11   backpack  ,\n",
       "  'caption': ['The skis that are five spots to the left from the black and red skis that say SALOMON'],\n",
       "  'bbox_target': [139.78, 70.91, 111.29, 378.59]},\n",
       " 752: {'image_emb': tensor([[-0.0900,  0.2893, -0.5171,  ...,  0.7847,  0.2920,  0.4973],\n",
       "          [ 0.1749,  0.2360, -0.3711,  ...,  0.9717,  0.3250,  0.2422],\n",
       "          [-0.2532,  0.1790, -0.2664,  ...,  1.2764, -0.1660,  0.1740],\n",
       "          ...,\n",
       "          [ 0.0385,  0.1819, -0.3499,  ...,  0.9966,  0.3354, -0.1624],\n",
       "          [ 0.0358,  0.0533, -0.2976,  ...,  1.0713,  0.0692, -0.2910],\n",
       "          [ 0.0737,  0.3220, -0.5957,  ...,  0.7891,  0.1959,  0.4397]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-6.9580e-02,  4.0619e-02, -1.0620e-01, -2.0776e-01, -3.5309e-02,\n",
       "           -2.3694e-01,  9.3579e-05, -6.1963e-01,  1.4343e-01,  3.9404e-01,\n",
       "            1.0498e-01,  1.5942e-01,  2.4878e-01,  2.1655e-01,  1.6394e-01,\n",
       "            3.8177e-02, -3.1921e-02,  2.0279e-02, -2.5122e-01,  3.4106e-01,\n",
       "            3.1641e-01, -4.7778e-01, -2.2021e-01,  3.6987e-01,  5.2155e-02,\n",
       "           -2.6688e-02,  5.4901e-02,  1.5210e-01, -2.6709e-01,  3.5217e-02,\n",
       "           -1.0065e-01,  1.6211e-01, -3.3264e-02,  1.5135e-03, -3.9233e-01,\n",
       "           -8.7646e-02, -2.0544e-01,  2.2876e-01,  2.3425e-01,  6.7444e-02,\n",
       "           -7.5562e-02,  2.2937e-01, -3.8135e-01,  1.5344e-01,  1.5479e-01,\n",
       "           -9.5581e-02,  1.2830e-01,  1.4758e-01,  4.2627e-01,  3.8428e-01,\n",
       "            2.2620e-01, -3.4985e-01,  2.4792e-01, -1.8689e-01, -3.8037e-01,\n",
       "           -2.1191e-01,  2.2693e-01,  9.5886e-02, -1.6760e-01,  3.2471e-01,\n",
       "           -2.4170e-01, -2.4048e-01, -9.1309e-02, -7.7051e-01, -2.6465e-01,\n",
       "            9.3323e-02,  2.1716e-01,  9.1003e-02,  2.3267e-01, -4.0015e-01,\n",
       "           -2.2742e-01,  3.4363e-02,  4.2755e-02,  5.5511e-02,  7.0557e-02,\n",
       "            6.3770e-01,  2.1460e-01,  5.8545e-01, -2.4548e-01,  1.9873e-01,\n",
       "           -5.2490e-01,  2.2485e-01,  1.1621e-01,  1.6077e-01, -1.8457e-01,\n",
       "            1.5942e-01,  8.8196e-02,  2.0190e-01, -8.9941e-01,  7.3914e-02,\n",
       "           -2.0386e-01, -5.0488e-01, -8.9355e-01,  1.6589e-01, -3.0957e-01,\n",
       "            3.7012e-01, -1.7712e-01, -1.8994e-01,  1.1957e-01,  9.6313e-02,\n",
       "            1.7419e-01,  4.1656e-03, -3.0565e-04, -1.0413e-01,  1.3550e-01,\n",
       "            3.6316e-02,  2.2034e-01,  1.9482e-01,  2.4609e-01, -1.8665e-01,\n",
       "           -3.4912e-01,  2.8687e-01,  1.3342e-01, -2.9861e-02, -1.3220e-01,\n",
       "           -9.5276e-02, -1.9641e-01, -1.3098e-01, -4.8004e-02,  6.1865e-01,\n",
       "           -1.4417e-01, -5.6494e-01, -4.8218e-01, -1.6638e-01,  2.3865e-02,\n",
       "            1.1859e-01,  6.0156e-01, -1.8713e-01, -1.0187e-01, -1.7383e-01,\n",
       "            1.2360e-01, -3.3472e-01, -2.6318e-01,  3.2129e+00, -1.8164e-01,\n",
       "            4.3384e-01,  4.7821e-02,  1.5417e-01, -2.4200e-02, -2.9736e-01,\n",
       "           -1.5417e-01, -2.1045e-01, -3.0640e-01,  5.3027e-01, -6.8848e-02,\n",
       "           -2.4670e-01,  2.0984e-01, -3.0298e-01, -1.1035e-01,  3.1445e-01,\n",
       "            2.1072e-02, -3.3960e-01,  9.0332e-02, -6.8604e-02, -7.3914e-02,\n",
       "            2.7856e-01,  7.9285e-02,  7.7454e-02, -8.0017e-02, -6.4819e-02,\n",
       "            5.0879e-01, -7.0740e-02, -2.3746e-03, -2.6688e-02,  1.6882e-01,\n",
       "            8.6548e-02,  1.2573e-01, -4.0552e-01,  1.2842e-01, -2.5684e-01,\n",
       "           -2.1167e-01,  2.6459e-02,  6.5820e-01,  2.8296e-01, -1.9128e-01,\n",
       "            4.7925e-01,  2.7197e-01,  5.2765e-02,  1.6663e-01, -1.2842e-01,\n",
       "           -4.8267e-01, -3.4210e-02,  9.7656e-02, -2.5177e-02, -5.1855e-01,\n",
       "           -2.5635e-01,  4.0332e-01,  8.0444e-02,  4.3243e-02,  1.9482e-01,\n",
       "           -1.7853e-02,  1.8750e-01,  2.1317e-02,  3.3997e-02,  1.0574e-02,\n",
       "           -2.1533e-01, -2.7026e-01, -3.1714e-01, -6.9238e-01, -2.1606e-01,\n",
       "            5.2307e-02,  2.5830e-01, -1.1877e-01,  1.6675e-01, -5.5664e-01,\n",
       "            9.5642e-02,  5.5811e-01, -9.2957e-02, -1.0602e-01, -5.6152e-01,\n",
       "            2.0178e-01,  6.9189e-01, -3.3301e-01, -1.4148e-01, -8.1024e-03,\n",
       "           -2.1252e-01, -2.0068e-01,  5.1260e-06,  6.6699e-01,  1.8103e-01,\n",
       "            3.4668e-01,  2.2119e-01, -4.9170e-01,  8.9600e-02,  1.7310e-01,\n",
       "            6.5247e-02, -6.8817e-03, -1.9519e-01, -5.1172e-01,  2.3145e-01,\n",
       "           -4.6600e-02,  3.8184e-01,  4.3262e-01,  1.6443e-01, -2.1118e-01,\n",
       "            1.7044e-02,  1.8616e-01,  1.5625e-01, -2.7637e-01,  6.0272e-02,\n",
       "            1.7993e-01,  1.7065e-01,  1.3000e-01,  4.9146e-01, -2.8271e-01,\n",
       "            2.8711e-01, -5.9174e-02,  8.3740e-02, -2.2156e-01,  1.1528e-02,\n",
       "           -8.9050e-02,  2.0007e-01,  2.5439e-01, -2.7930e-01, -2.8394e-01,\n",
       "            2.5171e-01,  6.2939e-01, -3.2990e-02,  1.0742e-02, -5.5756e-02,\n",
       "           -8.3008e-02,  3.8892e-01, -1.7432e-01,  9.3002e-03, -1.8494e-01,\n",
       "           -1.7395e-01, -4.0601e-01, -2.1094e-01, -1.0284e-01, -2.4872e-02,\n",
       "           -1.2927e-01,  3.4229e-01,  2.6840e-02, -9.8511e-02,  1.2128e-01,\n",
       "           -1.3452e-01, -2.5781e-01,  5.8643e-01,  1.0675e-01, -3.7384e-02,\n",
       "            2.1326e-01,  1.8665e-01,  8.2336e-02,  4.0344e-02,  4.5703e-01,\n",
       "            1.9849e-01, -1.5771e-01,  4.0747e-01, -1.3475e-03, -1.1011e-01,\n",
       "           -1.5637e-01, -3.4766e-01,  1.5039e-01, -1.4844e-01, -2.2974e-01,\n",
       "            7.3096e-01, -4.9487e-01,  5.8044e-02, -9.4666e-02, -7.5623e-02,\n",
       "           -8.1329e-03, -2.7637e-01,  8.9050e-02,  2.1667e-01, -7.3181e-02,\n",
       "           -7.0312e-02,  2.8467e-01,  1.5823e-02, -1.1490e-02, -1.5466e-01,\n",
       "            1.1987e-01,  2.6343e-01,  3.2031e+00, -2.9712e-01, -2.3877e-01,\n",
       "           -6.8298e-02,  6.6528e-02, -1.3184e-01,  1.7285e-01, -2.2083e-01,\n",
       "            2.2754e-01,  2.7515e-01,  3.5620e-01,  1.5845e-01, -1.2433e-01,\n",
       "            5.4199e-02,  2.1057e-01, -2.4927e-01,  2.5830e-01, -1.2168e+00,\n",
       "            5.6787e-01, -1.7871e-01,  1.4246e-01, -4.7095e-01, -1.3525e-01,\n",
       "            1.3098e-01, -3.8916e-01, -1.4539e-01,  2.2461e-01,  2.8882e-01,\n",
       "           -2.5024e-01, -4.9268e-01,  4.2603e-01, -3.1708e-02,  1.2122e-01,\n",
       "           -1.7126e-01, -3.4882e-02,  2.0422e-01,  1.1475e-01, -8.0566e-03,\n",
       "           -4.9512e-01, -1.0339e-01,  3.7085e-01,  1.4404e-01,  2.0004e-02,\n",
       "           -5.0244e-01, -2.1088e-02, -3.5327e-01, -1.9434e-01,  9.5825e-02,\n",
       "           -2.8760e-01,  2.6587e-01, -5.9753e-02,  3.4302e-01, -2.8003e-01,\n",
       "            2.2266e-01,  2.2009e-01, -6.4636e-02, -2.7734e-01, -1.7261e-01,\n",
       "            1.0797e-01, -3.4741e-01, -2.2339e-01, -1.5833e-01, -5.6030e-02,\n",
       "            6.3672e-01, -1.8604e-01,  8.9233e-02,  1.0150e-01, -1.3062e-01,\n",
       "           -1.2549e-01, -2.4976e-01, -1.3635e-01,  4.9609e-01,  2.2873e-02,\n",
       "            1.1084e-01,  1.1511e-01,  1.6382e-01,  1.0321e-01,  8.7585e-02,\n",
       "           -4.4092e-01, -1.2988e-01, -4.3945e-01,  1.3110e-01,  2.0337e-01,\n",
       "           -3.0615e-01,  2.5488e-01,  8.9355e-02, -1.9897e-01,  2.8687e-01,\n",
       "            2.3376e-01, -4.4952e-02,  5.8154e-01, -2.0032e-01, -8.8318e-02,\n",
       "           -6.4111e-01,  2.7319e-01, -1.4270e-01, -4.3262e-01,  1.8835e-01,\n",
       "            3.7903e-02,  1.6614e-01, -5.2979e-02,  6.9702e-02, -3.2617e-01,\n",
       "           -3.7427e-01,  8.0078e-02, -7.3059e-02,  1.9910e-01, -3.6182e-01,\n",
       "            3.3173e-02, -1.5820e-01,  1.6113e-01, -2.4207e-01,  3.5828e-02,\n",
       "           -1.8082e-02, -6.4636e-02, -1.4453e-01,  6.2103e-02,  2.4048e-01,\n",
       "           -5.6787e-01, -3.1665e-01,  3.1787e-01,  2.2876e-01, -1.1548e-01,\n",
       "            1.5918e-01, -6.1279e-01, -2.0703e-01, -1.1005e-01,  8.3557e-02,\n",
       "           -1.5308e-01,  7.7591e-03,  2.5391e-02, -1.1279e-01,  2.0129e-01,\n",
       "           -3.1776e-03, -9.3994e-02, -2.7637e-01, -2.0520e-01,  4.2139e-01,\n",
       "           -6.1182e-01,  3.1812e-01,  2.0544e-01, -2.1021e-01,  5.7959e-01,\n",
       "           -2.2314e-01,  2.9492e-01,  4.1333e-01,  5.2002e-02, -1.3843e-01,\n",
       "           -2.0435e-01,  2.2388e-01,  3.4180e-01,  1.3647e-01,  2.6025e-01,\n",
       "           -1.5295e-01, -3.9868e-01,  1.0730e-01, -2.5854e-01,  2.1729e-01,\n",
       "            5.7422e-01, -3.1714e-01, -3.0225e-01,  1.1810e-01,  7.4219e-02,\n",
       "           -3.1616e-01,  2.8418e-01, -2.1484e-01, -3.7085e-01,  3.0957e-01,\n",
       "            1.5588e-01, -2.1436e-01,  1.5503e-01,  1.0785e-01, -1.4862e-02,\n",
       "           -3.4210e-02, -5.1758e-02, -1.4355e-01, -1.1182e-01, -1.0187e-01,\n",
       "           -5.7373e-02, -2.4658e-01,  4.2773e-01,  8.7891e-01,  2.8979e-01,\n",
       "           -4.4580e-01,  3.7476e-01, -4.1455e-01, -7.9163e-02,  1.2225e-01,\n",
       "            1.1121e-01, -2.9102e-01, -6.0944e-02, -1.6455e-01,  6.6943e-01,\n",
       "           -3.5864e-01, -3.6255e-02, -6.4636e-02,  4.5776e-01,  5.2490e-01,\n",
       "           -3.8428e-01, -1.8140e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0009, 0.0023, 0.8394, 0.1459, 0.0073, 0.0027, 0.0018]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    95.246109   61.812958  487.922424  426.944702    0.947542      0   \n",
       "  1   176.519623  320.673737  340.765411  404.024200    0.928901     63   \n",
       "  2     0.073265  205.251343   93.627716  425.351379    0.868272     56   \n",
       "  3   406.273651  225.221344  499.472870  426.595093    0.839456     56   \n",
       "  4   158.994476  221.217957  181.076172  274.079285    0.750558     40   \n",
       "  5   201.525558  219.364319  225.742416  274.244995    0.712907     40   \n",
       "  6   198.884338  207.725555  249.929321  239.878082    0.695935     56   \n",
       "  7    38.839935  255.450714  236.044037  340.282196    0.589914     60   \n",
       "  8   100.321892  124.958817  115.476906  154.935211    0.532332     40   \n",
       "  9    20.286356  174.258057   40.662964  197.901367    0.479681     40   \n",
       "  10   17.621727  120.673660   34.248238  151.378296    0.414578     40   \n",
       "  11   84.269661  134.898499   96.345802  154.254669    0.371378     40   \n",
       "  12   49.368084   50.671951   68.121872   96.034348    0.364695     40   \n",
       "  13   47.318489  175.126129   63.488350  199.146637    0.307260     40   \n",
       "  14  102.985741    0.000000  195.960907   72.918732    0.296513     58   \n",
       "  15  178.654968  228.636963  196.482910  273.893738    0.277195     40   \n",
       "  16   36.184036  124.051346   51.126450  152.619110    0.275876     40   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         laptop  \n",
       "  2          chair  \n",
       "  3          chair  \n",
       "  4     wine glass  \n",
       "  5     wine glass  \n",
       "  6          chair  \n",
       "  7   dining table  \n",
       "  8     wine glass  \n",
       "  9     wine glass  \n",
       "  10    wine glass  \n",
       "  11    wine glass  \n",
       "  12    wine glass  \n",
       "  13    wine glass  \n",
       "  14  potted plant  \n",
       "  15    wine glass  \n",
       "  16    wine glass  ,\n",
       "  'caption': ['BROWN CHAIR WITH CREAM COLORED CUSHION'],\n",
       "  'bbox_target': [0.0, 205.54, 90.88, 223.46]},\n",
       " 753: {'image_emb': tensor([[-0.3235, -0.3862,  0.2927,  ...,  0.7358,  0.0750,  0.2837],\n",
       "          [-0.1262,  0.2761, -0.0731,  ...,  0.9648,  0.1010,  0.1615],\n",
       "          [-0.4907, -0.2886,  0.1036,  ...,  0.6108,  0.0399,  0.2783]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0823, -0.0096, -0.1071,  ...,  0.0494, -0.0070, -0.0688],\n",
       "          [ 0.0151,  0.3191, -0.2083,  ..., -0.0724, -0.0022, -0.0626]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1343, 0.3823, 0.4834],\n",
       "          [0.1389, 0.2290, 0.6323]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0   47.138214  137.419006  405.197632  397.050354    0.954496     20  elephant\n",
       "  1  508.215912    1.363983  639.852417  452.108398    0.932919     20  elephant,\n",
       "  'caption': ['an elephant with a smaller elephant behind it',\n",
       "   'The large elephant.'],\n",
       "  'bbox_target': [509.75, 0.0, 130.25, 453.36]},\n",
       " 754: {'image_emb': tensor([[-0.2532,  0.1326, -0.0963,  ...,  1.0703,  0.1708,  0.1185],\n",
       "          [-0.4207,  0.3745, -0.3464,  ...,  0.6089,  0.1427, -0.0088],\n",
       "          [-0.1884,  0.4370,  0.0473,  ...,  0.6416,  0.3481,  0.1572]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3447, -0.1433, -0.3564,  ..., -0.2363, -0.3911, -0.1224],\n",
       "          [ 0.1736,  0.1218, -0.4194,  ...,  0.1439, -0.6836,  0.1749]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0997, 0.6206, 0.2798],\n",
       "          [0.0620, 0.3679, 0.5698]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  143.002808  227.262573  594.006592  453.706848    0.944652     16       dog\n",
       "  1   74.652100  336.922668  635.770020  548.705139    0.824447     57     couch\n",
       "  2    0.045025  277.397400   39.926300  445.530334    0.577085      0    person\n",
       "  3   21.356211  177.543915   80.050545  199.208313    0.537088     25  umbrella\n",
       "  4  260.726227  202.044205  295.675446  230.304077    0.513225      0    person\n",
       "  5    0.430275  306.086121  127.420784  550.491638    0.449531      0    person\n",
       "  6   92.910278  434.467041  107.084259  456.238770    0.425218      0    person\n",
       "  7  435.845886  221.530548  484.337708  241.491241    0.402055     25  umbrella,\n",
       "  'caption': ['A cute little pillow for the couch.', 'green pillow'],\n",
       "  'bbox_target': [0.0, 307.78, 127.21, 238.01]},\n",
       " 755: {'image_emb': tensor([[-0.1937, -0.1381, -0.1450,  ...,  1.0244, -0.1687,  0.7080],\n",
       "          [-0.4866,  0.1334, -0.0323,  ...,  0.8027, -0.0785,  0.3335],\n",
       "          [-0.3601,  0.2439, -0.0283,  ...,  0.7510,  0.0598,  0.3447],\n",
       "          [-0.3438,  0.2017, -0.0958,  ...,  0.6724, -0.0463,  0.2771]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0455, -0.1567, -0.0494,  ...,  0.0651,  0.0086,  0.3647],\n",
       "          [ 0.0186, -0.2781, -0.1041,  ..., -0.0656, -0.2705,  0.4739]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6958, 0.2900, 0.0051, 0.0093],\n",
       "          [0.7993, 0.1869, 0.0038, 0.0099]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class      name\n",
       "  0  123.771866  85.602379  189.100540  173.801590    0.922610     76  scissors\n",
       "  1   55.720825  93.487152  145.385101  188.778854    0.919471     76  scissors\n",
       "  2  204.075500  81.271469  262.726074  160.464386    0.917290     76  scissors,\n",
       "  'caption': ['A black pair of scissors.', 'black scissors'],\n",
       "  'bbox_target': [125.14, 85.35, 60.74, 91.63]},\n",
       " 756: {'image_emb': tensor([[ 0.0221,  0.5029, -0.3162,  ...,  1.0371,  0.4727, -0.2151],\n",
       "          [-0.1107,  0.2563,  0.1511,  ...,  1.2871, -0.0994, -0.0670],\n",
       "          [ 0.1321,  0.4282, -0.2360,  ...,  1.2500, -0.0720, -0.0065],\n",
       "          [-0.1364,  0.4077, -0.5093,  ...,  0.9365, -0.0548, -0.2174],\n",
       "          [-0.2417,  0.2808, -0.0729,  ...,  1.1367,  0.1469, -0.1929],\n",
       "          [-0.1302,  0.2028, -0.2114,  ...,  1.1064,  0.1814, -0.1130]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2440, -0.0083, -0.1085,  ..., -0.0259,  0.1638,  0.0552],\n",
       "          [ 0.2568, -0.1272,  0.0403,  ...,  0.0841, -0.1769, -0.1331]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.0746e-03, 9.1791e-06, 2.6166e-05, 9.9658e-01, 1.9491e-05, 1.2696e-04],\n",
       "          [3.0398e-06, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 5.9605e-08]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.465897   41.905228  233.664261  466.485474    0.942153      0   \n",
       "  1  374.536560    2.168396  639.489319  476.299683    0.928562      0   \n",
       "  2  194.285980  211.953125  559.413086  449.904907    0.921412     63   \n",
       "  3  135.091156   35.643280  309.594635  224.878021    0.907532      0   \n",
       "  4  261.507874  424.752625  363.576904  479.412903    0.880423     39   \n",
       "  5  220.025009  171.333633  288.526001  249.269989    0.662977     73   \n",
       "  6  460.298340  422.794983  486.305847  454.070251    0.497339     74   \n",
       "  7  342.408447  232.942444  399.530701  251.802063    0.260358     67   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2      laptop  \n",
       "  3      person  \n",
       "  4      bottle  \n",
       "  5        book  \n",
       "  6       clock  \n",
       "  7  cell phone  ,\n",
       "  'caption': ['A lady in a black suit.',\n",
       "   'A woman in a black suit eating a sandwich.'],\n",
       "  'bbox_target': [136.73, 33.36, 177.43, 193.09]},\n",
       " 757: {'image_emb': tensor([[ 0.1948, -0.2064, -0.1929,  ...,  0.2180, -0.2275,  0.1273],\n",
       "          [ 0.1013,  0.0207,  0.0359,  ...,  0.7393, -0.2986,  0.1324],\n",
       "          [-0.0144, -0.2260, -0.0287,  ...,  0.3167, -0.2087,  0.1808]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1117,  0.1541, -0.0804,  ..., -0.0211, -0.4717, -0.0991],\n",
       "          [ 0.1252, -0.3491, -0.4089,  ..., -0.1185, -0.1417, -0.3472]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2542, 0.0103, 0.7354],\n",
       "          [0.7407, 0.0071, 0.2520]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin       xmax        ymax  confidence  class     name\n",
       "  0  228.283966   63.690094  411.33609  441.262451    0.935354     23  giraffe\n",
       "  1  241.436218  257.516479  299.65387  392.433228    0.728676     23  giraffe,\n",
       "  'caption': ['A tall giraffe standing near some trees.',\n",
       "   'A giraffe in front of the other giraffe.'],\n",
       "  'bbox_target': [230.83, 65.8, 182.29, 378.6]},\n",
       " 758: {'image_emb': tensor([[-0.0084,  0.2522, -0.3193,  ...,  1.0410, -0.0869, -0.1063],\n",
       "          [ 0.1151,  0.5073, -0.0541,  ...,  1.1846, -0.5078, -0.4978],\n",
       "          [ 0.1807,  0.3997, -0.0018,  ...,  1.0322, -0.1990,  0.0500],\n",
       "          [ 0.6665,  0.2520,  0.0798,  ...,  0.5703, -0.2981, -0.3984]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.4377,  0.3049, -0.5601,  ...,  0.2373, -0.1376, -0.1018],\n",
       "          [ 0.2632,  0.1823, -0.3552,  ..., -0.3057, -0.0167, -0.0766]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.3983e-03, 9.9561e-01, 1.5736e-05, 1.1909e-04],\n",
       "          [1.2154e-02, 9.6582e-01, 2.0355e-02, 1.7786e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  378.142120  214.200653  438.515228  364.412689    0.933658      0    person\n",
       "  1  299.860565  227.855530  371.372589  473.500610    0.921032      0    person\n",
       "  2  208.401505  238.264130  270.963745  311.884735    0.882605     33      kite\n",
       "  3  224.395996  426.285614  295.682495  478.951477    0.535917     26   handbag\n",
       "  4  224.254623  426.422546  296.247742  478.790466    0.475221     24  backpack,\n",
       "  'caption': ['Girl wearing black leggings and gray tank top',\n",
       "   'a lady wearing black tights'],\n",
       "  'bbox_target': [302.7, 228.65, 71.35, 243.24]},\n",
       " 759: {'image_emb': tensor([[ 0.0240,  0.4214, -0.0724,  ...,  0.3704, -0.2483,  0.0453],\n",
       "          [ 0.1262,  0.1650, -0.1410,  ...,  0.6587, -0.0331, -0.1071],\n",
       "          [-0.2061,  0.4541, -0.1096,  ...,  0.8369, -0.1793, -0.0437],\n",
       "          [-0.1434,  0.7305,  0.0133,  ...,  1.0498, -0.1285, -0.1338],\n",
       "          [-0.0787,  0.3293,  0.0192,  ...,  0.3516, -0.1252,  0.0128]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1768, -0.4575,  0.1771,  ...,  0.2651, -0.6694,  0.0952],\n",
       "          [-0.3108, -0.0272,  0.2271,  ..., -0.1011, -0.1511, -0.4668]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0231, 0.0239, 0.8555, 0.0564, 0.0413],\n",
       "          [0.1227, 0.0074, 0.6733, 0.1899, 0.0065]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   71.419128    5.879074  371.623199  422.370056    0.946536      0   \n",
       "  1  439.893890  115.225586  594.827759  397.425903    0.942744      0   \n",
       "  2    0.721535  139.107925  142.794159  422.844055    0.918011      0   \n",
       "  3  114.968262    0.340881  220.066895  228.037842    0.848940     34   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2        person  \n",
       "  3  baseball bat  ,\n",
       "  'caption': ['CATCHER KNEELED DOWN READY',\n",
       "   \"A man wearing a catcher's helmet with an orange faceguard.\"],\n",
       "  'bbox_target': [0.0, 140.89, 143.3, 286.11]},\n",
       " 760: {'image_emb': tensor([[ 0.3816,  0.1267, -0.3044,  ...,  0.7041,  0.1541, -0.3115],\n",
       "          [ 0.2365, -0.5029, -0.2067,  ...,  0.9106,  0.0510, -0.1163],\n",
       "          [ 0.1045, -0.3582, -0.2490,  ...,  0.7715, -0.0558, -0.1023]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2532, -0.2358, -0.3718,  ..., -0.0231, -0.2179, -0.4468],\n",
       "          [-0.1704, -0.2952, -0.5303,  ..., -0.2289, -0.1978, -0.2932]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2529, 0.2959, 0.4512],\n",
       "          [0.4121, 0.1886, 0.3994]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class     name\n",
       "  0  360.974976  81.717377  638.670898  475.925415    0.939109     23  giraffe\n",
       "  1   97.360764  56.367142  483.809631  479.250549    0.814259     23  giraffe,\n",
       "  'caption': ['A giraffe to the right of another giraffe',\n",
       "   'A giraffe on the right facing the camera'],\n",
       "  'bbox_target': [362.43, 80.33, 277.57, 392.63]},\n",
       " 761: {'image_emb': tensor([[-0.0241,  0.1433,  0.0854,  ...,  0.4705, -0.1311, -0.0076],\n",
       "          [ 0.2959,  0.4858, -0.1337,  ...,  0.5859,  0.3586,  0.4700],\n",
       "          [ 0.3447,  0.1272, -0.0232,  ...,  0.9219, -0.0352, -0.1521],\n",
       "          ...,\n",
       "          [-0.1197,  0.6646,  0.1156,  ...,  0.5547, -0.1705, -0.1802],\n",
       "          [ 0.4436,  0.7036,  0.1158,  ...,  0.9136, -0.1566,  0.1405],\n",
       "          [ 0.3503,  0.2231,  0.0105,  ...,  0.5581,  0.1237, -0.0878]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2239,  0.0398,  0.0565,  ...,  0.3110,  0.0177, -0.6846],\n",
       "          [ 0.3430,  0.3179, -0.4373,  ...,  0.1917, -0.0668,  0.0476]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.1989e-06, 6.6471e-04, 2.1040e-05, 3.0446e-04, 4.1604e-04, 1.4305e-06,\n",
       "           9.0723e-01, 9.1248e-02],\n",
       "          [5.7650e-04, 1.0004e-01, 1.7654e-02, 1.0321e-01, 1.8902e-03, 7.4005e-04,\n",
       "           6.6260e-01, 1.1334e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    31.551369  288.768921  246.606140  398.190796    0.935671     56   \n",
       "  1   399.332031  500.498108  590.735718  639.249817    0.919150     46   \n",
       "  2   160.984619  283.915405  353.414673  401.477905    0.912741     56   \n",
       "  3     0.330139  299.285767  105.429977  412.590271    0.911040     56   \n",
       "  4   278.150909  380.417908  423.825165  577.923340    0.899614     41   \n",
       "  5    62.311752    0.995499  231.454483  288.090393    0.883692     25   \n",
       "  6     6.487915  383.228943  638.497620  638.854370    0.738999     60   \n",
       "  7   383.771149  328.712036  550.678223  407.049988    0.575901     60   \n",
       "  8   479.890778  320.411407  625.818787  385.266205    0.556471     60   \n",
       "  9   579.723206  270.247223  624.740906  279.243683    0.346738     56   \n",
       "  10   95.421432  470.834015  259.576599  639.477661    0.322362     48   \n",
       "  11  215.795502  380.436157  471.228790  614.322266    0.267950     60   \n",
       "  \n",
       "              name  \n",
       "  0          chair  \n",
       "  1         banana  \n",
       "  2          chair  \n",
       "  3          chair  \n",
       "  4            cup  \n",
       "  5       umbrella  \n",
       "  6   dining table  \n",
       "  7   dining table  \n",
       "  8   dining table  \n",
       "  9          chair  \n",
       "  10      sandwich  \n",
       "  11  dining table  ,\n",
       "  'caption': ['A wooden table holding a muffin, a banana, and a white cup.',\n",
       "   'There is glass beside banana and cake on the table'],\n",
       "  'bbox_target': [7.21, 379.82, 632.79, 250.81]},\n",
       " 762: {'image_emb': tensor([[ 0.3860,  0.8447, -0.1700,  ...,  1.4424, -0.1231,  0.0507],\n",
       "          [ 0.0034,  0.1824, -0.1426,  ...,  1.0352,  0.0134, -0.4294],\n",
       "          [ 0.0090,  0.5552, -0.3606,  ...,  0.7637,  0.0296, -0.0251],\n",
       "          [-0.1086,  0.9736, -0.0950,  ...,  0.6050, -0.1901,  0.0301]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3000,  0.2015, -0.1943,  ...,  0.2917, -0.1799, -0.4617],\n",
       "          [-0.0571,  0.0162, -0.1511,  ...,  0.3442, -0.1180, -0.1879]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9902e-01, 1.2529e-04, 1.6081e-04, 4.5824e-04],\n",
       "          [1.0000e+00, 9.9540e-06, 6.5565e-06, 1.8239e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   17.803253  179.064667  155.400055  373.916962    0.946323      0   \n",
       "  1  186.650177  268.441010  347.098236  327.228363    0.883427     36   \n",
       "  2  255.590515  223.316116  387.635010  337.686981    0.775984      0   \n",
       "  3  335.766663  342.760437  346.969055  367.939026    0.698398      0   \n",
       "  4  310.370697  342.667023  320.281830  364.464081    0.683298      0   \n",
       "  5    0.525833  500.281921   69.323807  639.812683    0.584057      0   \n",
       "  6  394.478760  373.168243  402.322449  384.146271    0.281045      0   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1  skateboard  \n",
       "  2      person  \n",
       "  3      person  \n",
       "  4      person  \n",
       "  5      person  \n",
       "  6      person  ,\n",
       "  'caption': ['A man in a cap standing on the edge of a skateboard ramp',\n",
       "   'A picture of a white male in blue jeans a grey t-shirt standing on a skate ramp.'],\n",
       "  'bbox_target': [15.84, 177.95, 142.07, 195.77]},\n",
       " 763: {'image_emb': tensor([[-0.1985,  0.5332,  0.0206,  ...,  0.4675, -0.1740,  0.3452],\n",
       "          [-0.0141,  0.6167, -0.2440,  ...,  0.6099,  0.0105, -0.0516],\n",
       "          [ 0.3792, -0.0817, -0.2368,  ...,  0.4604, -0.0093,  0.1261],\n",
       "          [ 0.1714,  0.4417, -0.1587,  ...,  0.5298,  0.0378, -0.1443]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0591, -0.1775, -0.0417,  ...,  0.1792, -0.1114, -0.1765],\n",
       "          [ 0.0594,  0.0699,  0.0731,  ...,  0.2496, -0.1410, -0.2561]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.8515e-04, 6.4209e-01, 8.8730e-03, 3.4888e-01],\n",
       "          [9.5367e-07, 9.1602e-01, 9.5963e-06, 8.3862e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  158.177078  161.280853  275.738953  279.805634    0.914512      7     truck\n",
       "  1  226.933105   72.109818  637.578308  278.759827    0.872750      4  airplane\n",
       "  2  521.185852  287.578247  546.713684  364.137085    0.847888      0    person\n",
       "  3  466.396393  186.005127  476.608246  219.463013    0.655957      0    person\n",
       "  4  473.520721  185.124573  485.040131  218.390503    0.496079      0    person\n",
       "  5  507.388245   68.947586  570.007507   97.324387    0.456250      4  airplane\n",
       "  6  358.928345   70.485825  408.107422   92.190445    0.338596      4  airplane,\n",
       "  'caption': ['Airplane',\n",
       "   \"A large white wing of a plane, it's engine, and some men standing on the ground talking.\"],\n",
       "  'bbox_target': [232.3, 74.94, 407.7, 205.31]},\n",
       " 764: {'image_emb': tensor([[-0.4514, -0.5117, -0.1610,  ...,  0.3870,  0.0067,  0.1072],\n",
       "          [-0.6069, -0.1110, -0.1641,  ...,  0.6270, -0.0009,  0.2173],\n",
       "          [-0.5923, -0.3552, -0.2710,  ...,  0.5562, -0.2190,  0.0540]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2289, -0.2213, -0.0559,  ...,  0.3198, -0.0342,  0.2136],\n",
       "          [-0.4343,  0.0642, -0.0110,  ..., -0.1222,  0.2284,  0.6841]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0461, 0.8164, 0.1375],\n",
       "          [0.0130, 0.9702, 0.0167]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0   72.104965   94.630585  380.694458  379.839264    0.954216     22  zebra\n",
       "  1  389.464478  136.701218  571.394714  301.451538    0.928739     22  zebra,\n",
       "  'caption': ['A zebra with its head pointing to the left and standing beyond another zebra',\n",
       "   'The zebra out of focus.'],\n",
       "  'bbox_target': [387.14, 137.29, 184.79, 164.32]},\n",
       " 765: {'image_emb': tensor([[-0.2703,  0.3911, -0.1824,  ...,  1.2129,  0.0487, -0.2235],\n",
       "          [ 0.1152,  0.4863, -0.1511,  ...,  0.9575, -0.2859,  0.1183],\n",
       "          [ 0.1737,  0.4292,  0.0259,  ...,  1.2100, -0.1732,  0.0457],\n",
       "          [-0.0050,  0.3191, -0.2467,  ...,  0.7764, -0.1205,  0.1942],\n",
       "          [-0.4158,  0.2729, -0.1309,  ...,  1.0117, -0.1256,  0.2152],\n",
       "          [ 0.0716,  0.3186, -0.2169,  ...,  0.7529, -0.2722, -0.1764]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0032, -0.2001, -0.2908,  ...,  0.0396, -0.4536, -0.1301],\n",
       "          [-0.4619, -0.0844, -0.3713,  ...,  0.2778, -0.3052,  0.1396]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3206, 0.1024, 0.0726, 0.0407, 0.4519, 0.0117],\n",
       "          [0.2949, 0.0845, 0.0885, 0.0504, 0.4712, 0.0106]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  name\n",
       "  0  434.975159  107.385712  575.655090  425.884155    0.930969     42  fork\n",
       "  1  308.177002  172.697968  522.914307  424.402649    0.895500     55  cake\n",
       "  2   67.734856    0.761139  276.290833  183.551910    0.895322     55  cake\n",
       "  3   52.228012  144.519714  324.084839  406.362488    0.893551     55  cake\n",
       "  4  270.242584    4.642616  534.394165  179.639709    0.888805     55  cake,\n",
       "  'caption': ['piece of cake next to the fork',\n",
       "   'slice of desert next to fork in lower portion of picture'],\n",
       "  'bbox_target': [308.74, 175.05, 213.52, 252.95]},\n",
       " 766: {'image_emb': tensor([[ 0.5005,  0.0509,  0.1017,  ...,  0.6694,  0.0057, -0.2068],\n",
       "          [ 0.0323,  0.3916,  0.2157,  ...,  0.5161,  0.2341, -0.3757],\n",
       "          [-0.0945,  0.2781, -0.3257,  ...,  1.3633,  0.0453,  0.2096],\n",
       "          [ 0.1139, -0.0273,  0.0852,  ...,  0.7246,  0.1097, -0.3467],\n",
       "          [ 0.5649,  0.0531,  0.0141,  ...,  0.5391,  0.0547, -0.0603]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.4978, -0.3882, -0.3586,  ..., -0.1760, -0.3501, -0.3579],\n",
       "          [-0.1146,  0.2627, -0.0122,  ..., -0.3125, -0.0596, -0.3259]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.1372e-02, 2.0065e-03, 3.5763e-07, 2.0264e-02, 9.4629e-01],\n",
       "          [1.4465e-01, 1.6138e-01, 6.7353e-05, 6.8994e-01, 4.1008e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  296.700684  133.571533  611.955200  368.499878    0.930240     14     bird\n",
       "  1   22.257053  145.098572  138.915894  333.486145    0.909801     14     bird\n",
       "  2  408.125916    0.000000  500.275085   56.685959    0.859762      0   person\n",
       "  3  393.429565  114.903214  562.181213  233.693604    0.848253     14     bird\n",
       "  4  281.399841  240.107727  329.078857  327.839417    0.645974     51   carrot\n",
       "  5  618.603271   46.945908  639.849976  122.057144    0.479079     14     bird\n",
       "  6  281.817383  239.022949  330.355896  327.973328    0.392696     52  hot dog\n",
       "  7  617.267700   47.279320  639.903198  207.700867    0.260701     14     bird,\n",
       "  'caption': ['Bird eating bread on the ground while two birds look at the bread falling on the ground',\n",
       "   'pigeon with its head hidden'],\n",
       "  'bbox_target': [396.39, 111.48, 170.32, 115.62]},\n",
       " 767: {'image_emb': tensor([[ 0.2019,  0.4604,  0.2147,  ...,  0.3474, -0.1105, -0.1356],\n",
       "          [-0.0355,  0.5776,  0.2227,  ...,  0.5815, -0.4260, -0.2135],\n",
       "          [ 0.1534,  0.5312,  0.1987,  ...,  0.5498, -0.3113,  0.0356],\n",
       "          [-0.0390,  0.2998, -0.0182,  ...,  1.1816,  0.1831, -0.4595],\n",
       "          [-0.2384,  0.0870, -0.1306,  ...,  0.5527, -0.2499,  0.0861],\n",
       "          [-0.0252, -0.1547,  0.3328,  ...,  0.5889, -0.5088,  0.0746]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0604, -0.1218,  0.0167,  ...,  0.4778, -0.3958, -0.0643],\n",
       "          [-0.0989, -0.2073,  0.0126,  ...,  0.4250, -0.5913,  0.0198]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.5746e-02, 3.5986e-01, 4.0796e-01, 2.6627e-03, 2.4109e-02, 1.5967e-01],\n",
       "          [1.0468e-02, 1.6626e-01, 6.8945e-01, 1.3888e-05, 4.4327e-03, 1.2952e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  219.751770   70.476715  342.147522  224.220428    0.945574      0   \n",
       "  1  357.817230  141.508118  488.248993  243.599976    0.919078      0   \n",
       "  2  466.688660  100.942520  558.849304  237.979309    0.918071      0   \n",
       "  3  356.779236  205.103851  380.722778  231.333832    0.856327     35   \n",
       "  4  158.992950   91.369629  222.000732  129.883240    0.732961     34   \n",
       "  \n",
       "               name  \n",
       "  0          person  \n",
       "  1          person  \n",
       "  2          person  \n",
       "  3  baseball glove  \n",
       "  4    baseball bat  ,\n",
       "  'caption': ['the baseball catcher',\n",
       "   'catcher wearing blue and grey catching a low pitch'],\n",
       "  'bbox_target': [357.08, 142.23, 133.71, 103.03]},\n",
       " 768: {'image_emb': tensor([[-0.0210,  0.1053,  0.1582,  ...,  0.6934, -0.1367, -0.6553],\n",
       "          [ 0.1473,  0.3206,  0.0102,  ...,  1.0674, -0.1094, -0.2739],\n",
       "          [ 0.2278,  0.0560,  0.3308,  ...,  0.9312, -0.1274, -0.1312],\n",
       "          [-0.0519,  0.0961,  0.3989,  ...,  0.8989, -0.3208, -0.1179]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1515, -0.5220,  0.5674,  ...,  0.0684,  0.2216, -0.5015],\n",
       "          [-0.1501,  0.0228,  0.5420,  ..., -0.2891, -0.0903, -0.4929]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.3447e-01, 3.2867e-02, 6.9580e-02, 6.3354e-02],\n",
       "          [9.8584e-01, 2.7695e-03, 1.1299e-02, 2.6155e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   74.503944  163.065170  264.809021  429.533447    0.949774     72   \n",
       "  1  297.520386  114.944794  441.271118  270.788727    0.928199     72   \n",
       "  2  385.428680  222.258667  519.227051  352.409668    0.891229     72   \n",
       "  \n",
       "             name  \n",
       "  0  refrigerator  \n",
       "  1  refrigerator  \n",
       "  2  refrigerator  ,\n",
       "  'caption': ['white fridge on the grass with door open.',\n",
       "   'An open white refrigerator'],\n",
       "  'bbox_target': [77.32, 163.33, 188.99, 267.38]},\n",
       " 769: {'image_emb': tensor([[ 0.3699,  0.1120, -0.3120,  ...,  0.9351,  0.5840, -0.0469],\n",
       "          [-0.2220,  0.1090, -0.3037,  ...,  0.9346, -0.1348, -0.2668],\n",
       "          [-0.2725,  0.0835, -0.0786,  ...,  0.6553,  0.5020,  0.0917],\n",
       "          [ 0.0844, -0.0771,  0.1044,  ...,  0.6890,  0.6421,  0.1187]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3071,  0.0856, -0.0795,  ...,  0.3372,  0.2452, -0.1495],\n",
       "          [-0.3860,  0.5283,  0.1072,  ..., -0.1847,  0.0780, -0.1768]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.6773e-04, 3.8757e-03, 9.9414e-01, 1.8597e-03],\n",
       "          [2.0981e-04, 2.9743e-05, 9.9951e-01, 1.1772e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  369.675659   56.158752  565.883667  424.557495    0.944600      0  person\n",
       "  1  294.025665  199.568115  374.472260  427.244629    0.926869     27     tie\n",
       "  2  172.144150   50.914627  390.591675  420.467712    0.924082      0  person,\n",
       "  'caption': ['Man in beige suit coat.', 'A man wearing a tan jacket.'],\n",
       "  'bbox_target': [171.97, 48.26, 217.38, 374.86]},\n",
       " 770: {'image_emb': tensor([[-0.1544,  0.7949, -0.4570,  ...,  0.6338,  0.0103, -0.4636],\n",
       "          [-0.3057,  0.3042, -0.3733,  ...,  0.8633,  0.2467, -0.2705],\n",
       "          [ 0.3496,  0.6074, -0.4226,  ...,  0.8345,  0.2891, -0.6021],\n",
       "          [ 0.0936,  0.2163, -0.2452,  ...,  1.4082,  0.2052,  0.0610],\n",
       "          [ 0.2108,  0.6006, -0.8540,  ...,  0.5469, -0.1257, -0.1847]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2759,  0.3120, -0.5933,  ..., -0.3936, -0.5376, -0.3528],\n",
       "          [ 0.2661,  0.1715, -0.1907,  ..., -0.3298, -0.5244, -0.7017]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7725e-01, 1.5426e-04, 2.8648e-03, 1.5426e-04, 8.1934e-01],\n",
       "          [4.2297e-02, 1.9989e-02, 7.9834e-01, 2.4490e-02, 1.1499e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  161.844955    0.422211  640.000000  265.801117    0.915112      0   \n",
       "  1   22.892944  184.107452  379.503052  324.611298    0.888461     54   \n",
       "  2   61.996918  102.379257  302.295135  207.712433    0.886502     54   \n",
       "  3    0.000000  194.650574   79.416489  324.171509    0.763550     54   \n",
       "  4    5.514862  231.090393  635.936035  473.554932    0.591129     60   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1         donut  \n",
       "  2         donut  \n",
       "  3         donut  \n",
       "  4  dining table  ,\n",
       "  'caption': ['Donut on top.', 'A small bread on the large bread'],\n",
       "  'bbox_target': [63.78, 101.62, 231.36, 97.3]},\n",
       " 771: {'image_emb': tensor([[-0.1772,  0.2581, -0.2529,  ...,  1.0566,  0.2683,  0.0798],\n",
       "          [-0.0862,  0.2448, -0.3394,  ...,  0.5229, -0.0150, -0.0135],\n",
       "          [-0.1998,  0.1074, -0.3784,  ...,  1.2676, -0.2411,  0.1580],\n",
       "          ...,\n",
       "          [ 0.2067, -0.0118, -0.0403,  ...,  1.1221, -0.1077,  0.0030],\n",
       "          [-0.4875,  0.0519, -0.3794,  ...,  1.1318,  0.2639, -0.1648],\n",
       "          [-0.2598,  0.3616, -0.4978,  ...,  0.3884, -0.0776,  0.0781]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2776,  0.0072, -0.2920,  ...,  0.2067, -0.0097, -0.0721],\n",
       "          [ 0.2771,  0.1306, -0.3125,  ..., -0.1947,  0.1636, -0.1818]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.8535e-01, 9.8114e-03, 1.5974e-05, 1.3485e-03, 2.9802e-05, 1.1325e-05,\n",
       "           5.9605e-07, 6.3705e-04, 2.8553e-03],\n",
       "          [9.6240e-01, 2.3346e-02, 9.5367e-07, 2.3861e-03, 1.7881e-07, 1.8775e-05,\n",
       "           1.1921e-07, 1.1263e-03, 1.0857e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   386.931854    0.000000  598.884155  256.111450    0.944281      0   \n",
       "  1    99.550278   17.190216  409.401611  422.733398    0.924035      0   \n",
       "  2   405.069885  326.664917  628.618103  425.190552    0.883487     77   \n",
       "  3     0.000000  169.240845  118.027176  422.608826    0.879778     56   \n",
       "  4   567.101624   80.082001  640.000000  187.899719    0.865343     77   \n",
       "  5    57.115196  261.379669  126.941002  423.393921    0.832157     56   \n",
       "  6   446.524658  266.295197  639.209473  390.973907    0.741833     77   \n",
       "  7   307.332001    4.881500  424.988251  103.729309    0.730018     56   \n",
       "  8    73.934837   11.430023  159.018341   67.068329    0.678636     77   \n",
       "  9   362.621399  234.489410  471.021790  333.589447    0.677553     77   \n",
       "  10  328.431274  166.053162  397.319885  227.332947    0.645486     56   \n",
       "  11  554.102112  229.324951  640.000000  344.680908    0.636122     77   \n",
       "  12  259.618591  270.537537  343.358398  377.037231    0.593283     77   \n",
       "  13    0.108257   91.210709   27.065781  141.568680    0.466524     77   \n",
       "  14  275.411102  306.583130  430.302948  415.832520    0.459640     77   \n",
       "  15  109.547600    0.000000  254.112976   65.725510    0.422786      0   \n",
       "  16  269.543427  350.907257  370.724091  424.731140    0.394522     77   \n",
       "  17    0.172554  101.561890  130.492249  211.775940    0.364807     60   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2     teddy bear  \n",
       "  3          chair  \n",
       "  4     teddy bear  \n",
       "  5          chair  \n",
       "  6     teddy bear  \n",
       "  7          chair  \n",
       "  8     teddy bear  \n",
       "  9     teddy bear  \n",
       "  10         chair  \n",
       "  11    teddy bear  \n",
       "  12    teddy bear  \n",
       "  13    teddy bear  \n",
       "  14    teddy bear  \n",
       "  15        person  \n",
       "  16    teddy bear  \n",
       "  17  dining table  ,\n",
       "  'caption': ['A white woman with a black t shirt with her hand over her face.',\n",
       "   'A woman in a black t-shirt pushing the hair out of her face.'],\n",
       "  'bbox_target': [386.7, 5.76, 174.64, 243.72]},\n",
       " 772: {'image_emb': tensor([[-0.0617,  0.6714,  0.1165,  ...,  1.1777,  0.2539,  0.2595],\n",
       "          [ 0.3076,  0.5249,  0.1024,  ...,  0.6621,  0.2339, -0.0885],\n",
       "          [-0.5991,  0.3904,  0.1467,  ...,  0.7949, -0.3005, -0.2001],\n",
       "          ...,\n",
       "          [-0.4014,  0.4851,  0.0165,  ...,  0.8164,  0.0060, -0.4309],\n",
       "          [-0.1143,  0.2581, -0.2795,  ...,  1.2910, -0.0416,  0.2056],\n",
       "          [ 0.1708,  0.4202, -0.0208,  ...,  0.7554,  0.0443,  0.1283]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2308,  0.0020, -0.2299,  ..., -0.2666, -0.2625, -0.0482],\n",
       "          [-0.1429, -0.1396,  0.0218,  ...,  0.2649, -0.3462, -0.2460]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.6098e-02, 3.5706e-02, 1.4124e-01, 3.9825e-02, 1.4067e-03, 4.0466e-02,\n",
       "           3.6060e-01, 4.5135e-02, 3.7415e-02, 1.6516e-01, 3.2501e-02, 5.6183e-02,\n",
       "           2.8244e-02],\n",
       "          [9.6703e-04, 1.9531e-03, 5.8899e-02, 4.7333e-02, 2.3329e-04, 5.6534e-03,\n",
       "           6.5332e-01, 1.4893e-02, 4.8828e-02, 1.7242e-03, 4.5166e-02, 1.1719e-01,\n",
       "           3.7079e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   456.181946  250.840057  618.633240  371.822052    0.939067     41   \n",
       "  1     3.122253   79.579285  640.000000  473.291382    0.918220     60   \n",
       "  2   121.171799  303.658722  187.876175  475.914368    0.910376     42   \n",
       "  3   532.827942  348.367828  640.000000  396.777069    0.896667     44   \n",
       "  4   193.517975  104.324844  447.289215  256.913269    0.887768     53   \n",
       "  5   452.030640  187.134125  501.527283  271.149139    0.884889     39   \n",
       "  6   496.994415    1.624878  639.670105  240.225494    0.883094     56   \n",
       "  7    62.013317   48.580399  148.640808  122.416641    0.874375     41   \n",
       "  8   508.628693  165.688263  594.924194  236.514923    0.852769     45   \n",
       "  9     2.132957  253.352600  150.917145  365.406128    0.834411     42   \n",
       "  10  157.065552  305.278992  220.187469  476.476013    0.782541     43   \n",
       "  11  124.547096   90.826263  156.947372  155.745392    0.752849     44   \n",
       "  12  156.684723  304.178925  220.418121  475.054016    0.658336     42   \n",
       "  13  179.258148    0.000000  413.189178   80.153961    0.646647     56   \n",
       "  14    0.072839  351.770691   19.493378  452.681885    0.286761     45   \n",
       "  \n",
       "              name  \n",
       "  0            cup  \n",
       "  1   dining table  \n",
       "  2           fork  \n",
       "  3          spoon  \n",
       "  4          pizza  \n",
       "  5         bottle  \n",
       "  6          chair  \n",
       "  7            cup  \n",
       "  8           bowl  \n",
       "  9           fork  \n",
       "  10         knife  \n",
       "  11         spoon  \n",
       "  12          fork  \n",
       "  13         chair  \n",
       "  14          bowl  ,\n",
       "  'caption': ['A chair to the right of the table.',\n",
       "   'wooden chair with white cushion'],\n",
       "  'bbox_target': [497.29, 0.0, 142.71, 242.22]},\n",
       " 773: {'image_emb': tensor([[ 0.2791,  0.0443, -0.3787,  ...,  0.8345, -0.2126, -0.0257],\n",
       "          [ 0.1484, -0.4121, -0.4900,  ...,  0.7964, -0.1109, -0.0443],\n",
       "          [ 0.1251,  0.3833, -0.1127,  ...,  0.8760, -0.1025, -0.0656],\n",
       "          [-0.2791,  0.2886, -0.2335,  ...,  0.5962,  0.0319,  0.1244]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1768,  0.0191, -0.1783,  ...,  0.5303,  0.1831, -0.4497],\n",
       "          [-0.1763,  0.0024, -0.0239,  ...,  0.0718,  0.3660, -0.3838]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.4240e-06, 4.6492e-06, 1.7881e-06, 1.0000e+00],\n",
       "          [3.6716e-04, 7.9393e-05, 3.0746e-03, 9.9658e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  548.383484  130.177475  579.678162  147.761856    0.846651      2      car\n",
       "  1  515.450745  129.385223  532.677795  139.858856    0.742608      2      car\n",
       "  2  383.981964  117.906433  415.176483  138.402161    0.703948      2      car\n",
       "  3    1.679291    1.441193  390.162689  476.150024    0.664269      0   person\n",
       "  4    0.890602  160.819717  196.283630  262.683746    0.630430     16      dog\n",
       "  5  474.221985  114.386139  514.326111  146.812653    0.606645      7    truck\n",
       "  6  286.843781  202.901978  369.753021  321.689636    0.601932     16      dog\n",
       "  7  412.601318  120.855011  424.275024  131.956207    0.509674      2      car\n",
       "  8  422.010559  120.625275  434.453674  129.491455    0.443579      2      car\n",
       "  9    1.181854  204.169006  348.797180  476.910583    0.317535      1  bicycle,\n",
       "  'caption': ['A peson is riding on a bicycle that holds a skateboard and a dog',\n",
       "   'the bike rider riding the bike.'],\n",
       "  'bbox_target': [1.11, 0.0, 392.79, 480.0]},\n",
       " 774: {'image_emb': tensor([[ 0.1730,  0.4846,  0.0402,  ...,  0.4529, -0.0729,  0.3152],\n",
       "          [-0.2661,  0.0655, -0.4473,  ...,  0.4067,  0.0230,  0.1235],\n",
       "          [-0.2335, -0.3237,  0.0028,  ...,  1.2705,  0.1653,  0.0504],\n",
       "          [ 0.1903,  0.0238,  0.0675,  ...,  0.2798,  0.1792,  0.1643]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3064, -0.1133, -0.0626,  ..., -0.3860,  0.2959, -0.4277],\n",
       "          [-0.1548, -0.4856, -0.1729,  ...,  0.3203,  0.3342, -0.0977]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.1504e-01, 7.0557e-02, 3.2842e-05, 1.4336e-02],\n",
       "          [2.6398e-02, 4.9829e-01, 7.5698e-06, 4.7534e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0   64.900757  101.800774  166.977432  255.418259    0.943929      0   person\n",
       "  1  349.598206  153.921890  409.616577  309.652832    0.906530      0   person\n",
       "  2  397.668030  207.212112  445.155182  227.045258    0.856093     29  frisbee,\n",
       "  'caption': ['A young child holding a frisbee.',\n",
       "   'toddler in striped shirt playing frisbee'],\n",
       "  'bbox_target': [348.91, 155.32, 61.78, 152.35]},\n",
       " 775: {'image_emb': tensor([[ 0.0753, -0.4558,  0.1231,  ...,  0.6948, -0.1857,  0.4226],\n",
       "          [ 0.1799,  0.0090,  0.2722,  ...,  0.8501,  0.2421,  0.1901],\n",
       "          [ 0.0988, -0.2539, -0.0977,  ...,  0.4060,  0.0306,  0.3799]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2347, -0.0906, -0.1587,  ...,  0.5181, -0.1315, -0.0253],\n",
       "          [ 0.2681,  0.0380, -0.0538,  ...,  0.1270, -0.1360, -0.0037]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0283, 0.4707, 0.5010],\n",
       "          [0.0671, 0.0760, 0.8569]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  name\n",
       "  0  261.939667   23.105270  571.073853  413.840027    0.914244     21  bear\n",
       "  1   40.729439  289.076172  260.596863  637.858765    0.820974     21  bear,\n",
       "  'caption': ['The black bear that is on top of the log.', 'bear on a log.'],\n",
       "  'bbox_target': [267.58, 26.61, 300.79, 386.19]},\n",
       " 776: {'image_emb': tensor([[ 0.3237, -0.2253, -0.0241,  ...,  0.3711,  0.2349,  0.2515],\n",
       "          [ 0.2651, -0.0513, -0.0746,  ...,  0.5229,  0.2583,  0.0966],\n",
       "          [ 0.0968, -0.1761, -0.0648,  ...,  0.7197,  0.3154, -0.0415],\n",
       "          [-0.0376,  0.1422,  0.0074,  ...,  0.7461,  0.3899, -0.1592],\n",
       "          [ 0.1508,  0.0221, -0.0690,  ...,  1.1846,  0.1294, -0.2417],\n",
       "          [ 0.2715, -0.2681,  0.0484,  ...,  0.4978,  0.0952,  0.2400]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0616, -0.0294, -0.2159,  ..., -0.1545, -0.1577, -0.0165],\n",
       "          [-0.0090, -0.0379, -0.0931,  ...,  0.7417,  0.0543,  0.2893]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.7505e-02, 3.2715e-01, 1.3220e-01, 2.5635e-02, 3.4571e-05, 4.4727e-01],\n",
       "          [5.1086e-02, 1.6235e-01, 7.0508e-01, 5.8212e-03, 1.6093e-06, 7.5500e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    17.434845    4.161316  307.495667  344.662048    0.951309     20   \n",
       "  1   338.123657   11.652817  640.000000  338.058655    0.948441     20   \n",
       "  2   191.269287  124.988464  368.230286  352.117126    0.943376     20   \n",
       "  3   319.686371   46.876999  441.909271  312.755798    0.903733     20   \n",
       "  4    13.578450  156.720947   40.467030  212.363342    0.730331      0   \n",
       "  5    31.818129   87.881790   65.614410  137.829758    0.688377      0   \n",
       "  6    29.793770  146.716919   45.867653  173.037354    0.588228      0   \n",
       "  7    57.894135   91.204300   79.379822  134.528824    0.554692      0   \n",
       "  8   603.374023  126.093201  629.378662  183.899109    0.495335      0   \n",
       "  9   583.338745  133.125595  604.776123  171.678192    0.378705      0   \n",
       "  10  573.367676  105.151550  588.732910  139.587646    0.339259      0   \n",
       "  11  527.298828   73.471802  545.540527  115.990326    0.309825      0   \n",
       "  12  569.120911   78.086502  595.370789  114.901047    0.299534      0   \n",
       "  13    0.420831  165.294708   13.324531  210.413239    0.296890      0   \n",
       "  14    0.069506  151.462708    9.371645  172.358459    0.287648      0   \n",
       "  15  543.901367   91.859833  574.412842  135.834259    0.283535      0   \n",
       "  \n",
       "          name  \n",
       "  0   elephant  \n",
       "  1   elephant  \n",
       "  2   elephant  \n",
       "  3   elephant  \n",
       "  4     person  \n",
       "  5     person  \n",
       "  6     person  \n",
       "  7     person  \n",
       "  8     person  \n",
       "  9     person  \n",
       "  10    person  \n",
       "  11    person  \n",
       "  12    person  \n",
       "  13    person  \n",
       "  14    person  \n",
       "  15    person  ,\n",
       "  'caption': ['The elephant in the middle standing on the ground.',\n",
       "   'Elephant at a circus propping up 3 other elephants'],\n",
       "  'bbox_target': [189.99, 120.74, 176.56, 234.13]},\n",
       " 777: {'image_emb': tensor([[-0.4973,  0.5444, -0.2050,  ...,  0.8354, -0.2166, -0.0696],\n",
       "          [ 0.0625, -0.1023, -0.3276,  ...,  1.0850,  0.1941, -0.0284],\n",
       "          [-0.3179,  0.0506, -0.0786,  ...,  0.6240,  0.0370, -0.2402]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0024,  0.0848, -0.4082,  ...,  0.2886, -0.4414, -0.0075],\n",
       "          [-0.2629, -0.1626,  0.1736,  ...,  0.6216, -0.0228, -0.0964]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9316e-01, 1.1921e-07, 7.0114e-03],\n",
       "          [9.6289e-01, 1.3819e-03, 3.5614e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  166.765701  222.008881  395.876587  341.243011    0.901579     57   \n",
       "  1  253.498062  329.892639  294.831055  363.189148    0.830883     45   \n",
       "  2  444.888489  272.353943  568.310303  420.877441    0.678122     57   \n",
       "  3  445.025940  272.333801  567.785828  418.868774    0.615858     56   \n",
       "  4  128.401703  164.568390  184.003021  264.124237    0.597959     58   \n",
       "  5   44.179390  337.716064  111.982323  417.922485    0.593368     56   \n",
       "  6  151.373108  229.535522  173.933014  264.549194    0.575846     75   \n",
       "  7   43.715923  337.169861  112.060013  419.657959    0.421966     62   \n",
       "  \n",
       "             name  \n",
       "  0         couch  \n",
       "  1          bowl  \n",
       "  2         couch  \n",
       "  3         chair  \n",
       "  4  potted plant  \n",
       "  5         chair  \n",
       "  6          vase  \n",
       "  7            tv  ,\n",
       "  'caption': ['A striped love seat with pillows facing a set of stairs.',\n",
       "   'light brown couceh with stripes on it next to a sidetable wiht flowers on one side and a sidetable with a lamp on it'],\n",
       "  'bbox_target': [165.99, 223.08, 229.83, 114.0]},\n",
       " 778: {'image_emb': tensor([[-0.0239, -0.0703, -0.0663,  ...,  0.8955,  0.0866, -0.0236],\n",
       "          [ 0.0152, -0.2468,  0.2224,  ...,  0.2839,  0.1528, -0.0755],\n",
       "          [-0.2856, -0.2974, -0.2421,  ...,  1.0361,  0.1901, -0.2269],\n",
       "          [ 0.2046, -0.3906,  0.0654,  ...,  0.6758,  0.0276, -0.1584],\n",
       "          [-0.0458, -0.2264,  0.1965,  ...,  0.3047, -0.1696, -0.3110]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0754, -0.2284, -0.4448,  ..., -0.1670, -0.4922, -0.5034],\n",
       "          [ 0.2971, -0.0864, -0.5146,  ...,  0.0679, -0.2898, -0.3137]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0756, 0.8125, 0.0804, 0.0107, 0.0210],\n",
       "          [0.2637, 0.2808, 0.3494, 0.0536, 0.0527]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  110.290924    0.924528  220.561310  288.013519    0.909173     23  giraffe\n",
       "  1  192.176743  169.346878  312.624542  327.037537    0.892672     23  giraffe\n",
       "  2  289.397339  179.363541  396.978821  252.452896    0.828439     13    bench\n",
       "  3   78.351852   91.110397  331.559265  290.154663    0.828290     23  giraffe,\n",
       "  'caption': ['The adult giraffe with its head bowed',\n",
       "   'The giraffe with the horizontal neck.'],\n",
       "  'bbox_target': [74.83, 86.56, 256.45, 208.52]},\n",
       " 779: {'image_emb': tensor([[-0.1163,  0.1475, -0.0616,  ...,  0.8667,  0.2075,  0.1870],\n",
       "          [ 0.3704,  0.4478, -0.1411,  ...,  0.8218,  0.0704,  0.0629],\n",
       "          [-0.3162,  0.6553, -0.0199,  ...,  0.9790,  0.2084, -0.1504],\n",
       "          ...,\n",
       "          [-0.0819,  0.0960, -0.5298,  ...,  1.0410,  0.1335,  0.1991],\n",
       "          [ 0.0546,  0.2534, -0.1390,  ...,  1.0098,  0.2527,  0.1544],\n",
       "          [-0.1455,  0.4470, -0.0108,  ...,  0.6567,  0.1407,  0.2321]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3625, -0.0256, -0.2308,  ..., -0.2377, -0.3066, -0.0718],\n",
       "          [ 0.0576, -0.0282, -0.4885,  ...,  0.0320,  0.0786,  0.2117]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0111, 0.3440, 0.0014, 0.0010, 0.0015, 0.0016, 0.0064, 0.6328],\n",
       "          [0.0030, 0.0399, 0.0041, 0.0287, 0.0050, 0.0061, 0.8652, 0.0481]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  158.625824  184.948090  274.428802  316.093964    0.931210      0  person\n",
       "  1  126.119019   40.119675  535.889709  419.298889    0.928671      0  person\n",
       "  2    0.371063   87.759155  119.202103  220.455627    0.896314     63  laptop\n",
       "  3    0.000000  265.242798   54.256931  327.160400    0.890610     65  remote\n",
       "  4   70.555344   29.138702  188.428696  196.561798    0.876410      0  person\n",
       "  5   15.817945  239.871063   66.964233  311.176056    0.779148     65  remote\n",
       "  6   90.207954  122.309784  227.807434  302.160797    0.754327     57   couch\n",
       "  7  505.761230  362.043732  540.089355  392.181976    0.544589     65  remote\n",
       "  8    0.424068  174.911682   56.278709  239.519287    0.503474     63  laptop,\n",
       "  'caption': ['A lady is dancing', 'A girl playing Wii'],\n",
       "  'bbox_target': [127.32, 38.13, 401.11, 383.88]},\n",
       " 780: {'image_emb': tensor([[ 0.0111,  0.4805,  0.2401,  ...,  0.4641, -0.4563,  0.0598],\n",
       "          [-0.0506,  0.1898,  0.3511,  ...,  0.2673, -0.5000, -0.1663],\n",
       "          [ 0.0894,  0.3159,  0.4167,  ...,  0.4753, -0.6318, -0.0658],\n",
       "          [ 0.5176, -0.1283, -0.3264,  ...,  1.2529,  0.2234, -0.3347],\n",
       "          [-0.0451, -0.0016, -0.2742,  ...,  0.8057, -0.0954, -0.1105],\n",
       "          [ 0.1423,  0.1317,  0.5542,  ...,  0.4626, -0.2883,  0.2812]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3499,  0.0770,  0.0261,  ...,  0.5161, -0.3967, -0.3784],\n",
       "          [-0.2369, -0.4890,  0.2673,  ...,  0.5186, -0.5098, -0.1192]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.5537e-01, 5.0781e-01, 2.2888e-01, 1.5509e-04, 5.2750e-05, 7.7095e-03],\n",
       "          [5.1270e-02, 8.4033e-01, 7.4585e-02, 2.8181e-04, 2.0444e-05, 3.3600e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   81.324966  168.369873  179.386215  339.954407    0.945808      0   \n",
       "  1  161.160736  204.410339  293.209625  333.175049    0.906656      0   \n",
       "  2  250.989197  116.793121  363.880554  296.171539    0.879650      0   \n",
       "  3  266.033447  258.744995  296.648682  293.073730    0.852185     35   \n",
       "  4  358.294647  120.051178  383.911285  162.193359    0.764173     34   \n",
       "  \n",
       "               name  \n",
       "  0          person  \n",
       "  1          person  \n",
       "  2          person  \n",
       "  3  baseball glove  \n",
       "  4    baseball bat  ,\n",
       "  'caption': ['A baseball catcher with a white uniform on.',\n",
       "   'Catcher squatting on the field.'],\n",
       "  'bbox_target': [164.37, 203.87, 117.09, 130.49]},\n",
       " 781: {'image_emb': tensor([[ 0.4214,  0.0324,  0.4006,  ...,  0.7622,  0.1903, -0.0378],\n",
       "          [-0.6733,  0.5806, -0.1119,  ...,  0.9907,  0.2256, -0.4087],\n",
       "          [-0.0560,  0.1134,  0.0708,  ...,  0.7993,  0.4285,  0.0714],\n",
       "          ...,\n",
       "          [ 0.0138,  0.8472,  0.1442,  ...,  1.1797, -0.0299, -0.0584],\n",
       "          [-0.0839,  0.0749, -0.0968,  ...,  1.5244, -0.0536, -0.1465],\n",
       "          [-0.0258,  0.1910, -0.0560,  ...,  0.9287,  0.2450,  0.0928]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2769,  0.1213,  0.0297,  ...,  0.1481, -0.0334, -0.1453],\n",
       "          [-0.0193,  0.0657, -0.0143,  ...,  0.1914,  0.0812, -0.5317]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.2064e-02, 6.8652e-01, 1.5076e-01, 4.0650e-05, 1.0653e-03, 4.9496e-04,\n",
       "           1.6332e-04, 9.2896e-02, 6.5565e-06, 4.5990e-02],\n",
       "          [9.9540e-05, 3.8696e-01, 2.5528e-02, 8.1658e-06, 6.1393e-06, 6.9499e-05,\n",
       "           1.3113e-06, 6.3553e-03, 8.3447e-07, 5.8105e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   460.908875  111.885742  544.701538  207.056519    0.919794     41   \n",
       "  1   456.017548   47.414276  522.579346  155.654388    0.906453     39   \n",
       "  2   210.716019   76.188538  639.827942  375.125549    0.894303     60   \n",
       "  3   544.017090    0.020981  639.358398   87.954422    0.893206     56   \n",
       "  4   449.479767  209.385315  629.934692  326.590637    0.843633     45   \n",
       "  5     1.342148    0.000000  264.243286  376.875244    0.824750      0   \n",
       "  6   412.914368  149.136993  489.498108  192.686981    0.745829     44   \n",
       "  7   357.588867  161.287994  454.699280  294.361359    0.738718     41   \n",
       "  8   124.015289    0.186874  379.487213  215.983887    0.729543     57   \n",
       "  9   401.951996    0.000000  493.649078  106.386047    0.544743     56   \n",
       "  10  545.621643  219.996063  593.576477  254.128510    0.384522     47   \n",
       "  11  357.628387  160.471375  453.443390  292.969421    0.284565     60   \n",
       "  12  574.745483  236.460480  616.592773  285.020355    0.275946     47   \n",
       "  13  552.781555  251.705322  590.474182  287.162598    0.270134     47   \n",
       "  \n",
       "              name  \n",
       "  0            cup  \n",
       "  1         bottle  \n",
       "  2   dining table  \n",
       "  3          chair  \n",
       "  4           bowl  \n",
       "  5         person  \n",
       "  6          spoon  \n",
       "  7            cup  \n",
       "  8          couch  \n",
       "  9          chair  \n",
       "  10         apple  \n",
       "  11  dining table  \n",
       "  12         apple  \n",
       "  13         apple  ,\n",
       "  'caption': ['a mug of baby food with a green spoon in it',\n",
       "   'A jar with baby food and a spoon sticking out of it.'],\n",
       "  'bbox_target': [359.22, 160.82, 92.67, 136.15]},\n",
       " 782: {'image_emb': tensor([[-0.0103, -0.0094, -0.1076,  ...,  0.5454,  0.0065,  0.6567],\n",
       "          [-0.3049,  0.0831,  0.0637,  ...,  0.3523,  0.2274, -0.2524],\n",
       "          [-0.2720,  0.2155, -0.1804,  ...,  0.9004,  0.2406,  0.2969],\n",
       "          ...,\n",
       "          [-0.0598, -0.3210, -0.2715,  ...,  0.7583,  0.1602,  0.0022],\n",
       "          [ 0.2612,  0.3057,  0.0790,  ...,  1.2568,  0.1804, -0.2168],\n",
       "          [ 0.1049,  0.1929, -0.1882,  ...,  0.3257,  0.3740,  0.2976]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0581, -0.1665, -0.2473,  ...,  0.4224, -0.1694, -0.1384],\n",
       "          [ 0.0150,  0.0438, -0.2047,  ..., -0.0273, -0.2917,  0.1924]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.4639e-06, 6.5491e-02, 1.3180e-03, 4.0412e-05, 1.1921e-06, 1.9670e-06,\n",
       "           9.3311e-01],\n",
       "          [4.3457e-01, 5.5811e-01, 3.4771e-03, 6.8843e-05, 7.1526e-07, 1.0252e-05,\n",
       "           3.8185e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  360.182617  113.653397  639.490845  473.590515    0.954676      0  person\n",
       "  1    0.235191   97.541061  207.473663  459.398926    0.949744      0  person\n",
       "  2  150.415680   19.946411  416.961639  283.307556    0.848988     62      tv\n",
       "  3  139.272812  262.225220  175.538406  286.793030    0.834659     65  remote\n",
       "  4  377.307953  243.120728  398.422333  296.223206    0.804347     65  remote\n",
       "  5  170.088867  280.046204  207.775299  319.037537    0.765331     65  remote\n",
       "  6  451.694519   51.653854  522.477600   75.983627    0.387610     65  remote,\n",
       "  'caption': ['A man is white shirt and blue pant is having white color wii is sitting in the floor.',\n",
       "   'A man wearing blue plaid pajama bottoms.'],\n",
       "  'bbox_target': [1.08, 95.78, 212.01, 365.92]},\n",
       " 783: {'image_emb': tensor([[-0.1587,  0.6274, -0.0456,  ...,  0.5610,  0.2532, -0.1464],\n",
       "          [-0.1868,  0.4834, -0.1678,  ...,  0.6494,  0.3389,  0.0150],\n",
       "          [ 0.1675,  0.5015, -0.3044,  ...,  0.8027, -0.0063, -0.3457],\n",
       "          [-0.2120,  0.0244,  0.0668,  ...,  0.0792,  0.3054,  0.3953]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2534,  0.1212, -0.2896,  ...,  0.9170,  0.0526,  0.5239],\n",
       "          [ 0.0820,  0.1782, -0.6016,  ...,  0.0560,  0.1573,  0.2456]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0121, 0.7354, 0.2278, 0.0248],\n",
       "          [0.3276, 0.5151, 0.0587, 0.0983]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  442.578888   64.806000  587.363770  273.765289    0.950745      0   \n",
       "  1  109.784149   73.784729  228.736084  282.497864    0.946475      0   \n",
       "  2  364.740967   78.619720  449.533813  250.080658    0.941276      0   \n",
       "  3  328.025482  239.881226  477.047699  254.300598    0.576510     31   \n",
       "  4   83.586945  273.183990  255.329254  287.198334    0.487895     31   \n",
       "  5  412.326050  261.465454  629.484863  280.576721    0.387177     31   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2     person  \n",
       "  3  snowboard  \n",
       "  4  snowboard  \n",
       "  5  snowboard  ,\n",
       "  'caption': ['GREEN COLOR PANT WEARING SKIIER READY TO SKIING',\n",
       "   'the kid with yellow pants'],\n",
       "  'bbox_target': [112.75, 74.98, 115.17, 209.39]},\n",
       " 784: {'image_emb': tensor([[ 1.2817e-01,  3.1470e-01, -2.2449e-01,  ...,  6.1816e-01,\n",
       "            1.1420e-01,  3.8013e-01],\n",
       "          [ 8.9417e-02,  3.9819e-01, -1.6418e-01,  ...,  8.5596e-01,\n",
       "           -6.2286e-02,  1.9885e-01],\n",
       "          [-5.2738e-04,  6.9287e-01, -2.6074e-01,  ...,  7.8955e-01,\n",
       "            1.7688e-01,  5.0635e-01],\n",
       "          ...,\n",
       "          [-3.5938e-01,  8.8135e-01, -5.3564e-01,  ...,  8.7305e-01,\n",
       "            4.3511e-04,  1.6296e-01],\n",
       "          [ 2.1619e-01,  4.2578e-01, -1.4111e-01,  ...,  6.0107e-01,\n",
       "           -1.2201e-01, -2.6886e-02],\n",
       "          [ 1.7529e-01,  4.0161e-01, -2.3999e-01,  ...,  6.9873e-01,\n",
       "           -2.3755e-01,  7.2876e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1207,  0.4707,  0.2710,  ...,  0.2590,  0.0759, -0.2300],\n",
       "          [-0.2357,  0.1475,  0.0043,  ...,  0.3337,  0.0508, -0.2615]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.2881e-01, 2.2046e-01, 7.3969e-05, 1.4391e-03, 1.2004e-04, 4.2084e-02,\n",
       "           2.0703e-01],\n",
       "          [2.5195e-01, 8.7097e-02, 2.0921e-04, 1.6617e-02, 4.8661e-04, 1.5771e-01,\n",
       "           4.8584e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   273.566681  213.697769  487.796967  418.933777    0.941836     45   \n",
       "  1    17.416441    9.039823  218.029877  190.835938    0.917357     45   \n",
       "  2    34.588074  179.630890  274.781677  417.635681    0.912612     45   \n",
       "  3   215.721130   10.636410  444.935455  230.170990    0.906290     45   \n",
       "  4   245.142700  128.282013  291.660889  180.971329    0.791176     50   \n",
       "  5     0.000000   10.612973  496.380432  419.096710    0.755400     60   \n",
       "  6   355.461273  135.230438  406.717926  185.079010    0.670185     50   \n",
       "  7   287.090607  165.708847  312.126953  185.399979    0.645329     50   \n",
       "  8   322.952698  135.589371  366.563751  189.671631    0.642789     50   \n",
       "  9   287.184387   24.158903  323.255066   66.566757    0.577572     50   \n",
       "  10  340.627380   43.857136  373.932800   71.227013    0.558013     50   \n",
       "  11  404.287811  109.575180  434.237000  133.396698    0.527410     51   \n",
       "  12  370.402924   58.033360  397.032471   88.076530    0.510462     50   \n",
       "  13  237.487106   62.900352  263.473206   89.126678    0.427627     50   \n",
       "  14  299.487396  132.886749  334.902100  165.170471    0.354072     50   \n",
       "  15  264.217712   52.494144  294.622131   70.362999    0.334532     51   \n",
       "  16  376.345520  110.961914  390.498291  132.658722    0.299472     51   \n",
       "  \n",
       "              name  \n",
       "  0           bowl  \n",
       "  1           bowl  \n",
       "  2           bowl  \n",
       "  3           bowl  \n",
       "  4       broccoli  \n",
       "  5   dining table  \n",
       "  6       broccoli  \n",
       "  7       broccoli  \n",
       "  8       broccoli  \n",
       "  9       broccoli  \n",
       "  10      broccoli  \n",
       "  11        carrot  \n",
       "  12      broccoli  \n",
       "  13      broccoli  \n",
       "  14      broccoli  \n",
       "  15        carrot  \n",
       "  16        carrot  ,\n",
       "  'caption': ['A white bowl with pickles and olives.',\n",
       "   'the bowls with pickles and olives'],\n",
       "  'bbox_target': [273.09, 212.45, 210.95, 207.17]},\n",
       " 785: {'image_emb': tensor([[ 0.5332,  0.3486, -0.5425,  ...,  1.4766, -0.2236,  0.3447],\n",
       "          [ 0.2666,  0.4707, -0.6128,  ...,  1.0938, -0.2642,  0.5034],\n",
       "          [ 0.2788,  0.3423, -0.5400,  ...,  1.8115, -0.0632, -0.0733],\n",
       "          [-0.0315,  0.4077,  0.2812,  ...,  1.3730, -0.0487, -0.2727],\n",
       "          [ 0.1216,  0.2554, -0.0718,  ...,  1.1348, -0.4497,  0.2002]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-2.2314e-01,  2.1521e-01,  1.4046e-02,  1.3535e-02, -1.1664e-01,\n",
       "            6.6589e-02, -2.1594e-01, -1.0850e+00, -2.0081e-01,  4.6216e-01,\n",
       "            2.2131e-01, -1.7139e-01,  3.1201e-01, -2.5806e-01,  7.0801e-02,\n",
       "           -6.3049e-02,  1.9006e-01,  4.7394e-02, -4.8920e-02, -3.1494e-02,\n",
       "            4.1772e-01,  3.6353e-01, -4.3433e-01,  2.8748e-02, -1.3342e-01,\n",
       "           -1.4685e-01,  2.5879e-01,  2.8503e-02, -1.8152e-01, -7.4158e-02,\n",
       "            1.3965e-01,  9.7595e-02, -8.2397e-02,  9.5825e-02, -1.1145e-01,\n",
       "            3.3423e-01,  1.6321e-01,  8.1726e-02,  1.4795e-01,  1.1761e-01,\n",
       "           -1.0425e-01,  2.5659e-01, -1.8298e-01,  1.0968e-01, -2.2095e-02,\n",
       "            4.4727e-01,  5.8350e-02, -1.2793e-01,  1.1731e-01, -9.9670e-02,\n",
       "            3.4882e-02,  9.1003e-02,  1.1469e-01, -9.8816e-02, -3.5797e-02,\n",
       "            1.9272e-02, -3.3032e-01,  3.3325e-02,  6.6772e-02,  1.9006e-01,\n",
       "            3.9062e-01,  3.0746e-02, -1.5051e-01,  2.4097e-01, -7.0435e-02,\n",
       "           -2.5513e-01,  1.1383e-01,  5.5078e-01,  2.3834e-02,  1.2305e-01,\n",
       "           -4.1595e-02, -1.2598e-01,  1.6589e-01, -1.3342e-01,  2.3346e-02,\n",
       "            1.7798e-01,  2.4261e-03,  2.6514e-01, -1.6431e-01, -1.9580e-01,\n",
       "           -9.6924e-02,  1.8274e-01, -1.1597e-03, -3.1113e-02,  5.9509e-02,\n",
       "            3.4790e-01, -2.2131e-01,  5.4810e-02, -2.8345e-01, -3.1958e-01,\n",
       "           -5.8258e-02,  4.8901e-01, -1.5645e+00, -3.6914e-01,  6.9702e-02,\n",
       "            3.6652e-02, -4.7058e-02,  2.1191e-03,  3.0054e-01,  2.0850e-01,\n",
       "            6.4209e-02, -1.6406e-01, -5.2155e-02,  4.8309e-02, -2.2290e-01,\n",
       "            1.9360e-01, -2.1838e-01,  2.2675e-02, -4.2511e-02,  4.0314e-02,\n",
       "           -9.1553e-02,  4.1724e-01, -3.6230e-01,  3.9612e-02, -5.9479e-02,\n",
       "           -3.9380e-01, -7.1777e-02,  2.8442e-01, -3.7158e-01,  1.6663e-01,\n",
       "           -3.5181e-01, -2.8882e-01, -1.0791e-01, -2.8076e-01,  4.7119e-02,\n",
       "           -1.0443e-01,  4.4861e-03,  2.5806e-01,  1.4050e-01,  6.9434e-01,\n",
       "            1.3354e-01, -2.9077e-01, -1.0785e-01,  5.7070e+00, -2.4866e-01,\n",
       "            5.2246e-02, -3.9990e-01, -7.1436e-01,  1.6577e-01,  2.7319e-01,\n",
       "           -4.5093e-01, -4.8096e-01,  1.5527e-01,  1.8701e-01, -3.9355e-01,\n",
       "           -2.8345e-01,  3.4692e-01, -2.2705e-01, -2.9858e-01, -1.9934e-01,\n",
       "            1.4221e-01, -3.4082e-01,  5.8936e-01,  1.2213e-01, -1.2219e-01,\n",
       "           -1.9873e-01,  6.9336e-02, -2.7783e-01, -3.8086e-01, -6.7444e-02,\n",
       "           -3.9886e-02,  9.8389e-02,  2.4817e-01,  6.7688e-02, -4.0039e-01,\n",
       "            1.8799e-01,  1.6098e-02, -1.3391e-01, -2.2729e-01, -2.5342e-01,\n",
       "            8.7219e-02, -1.1115e-01,  1.6724e-02,  5.2063e-02, -2.8872e-04,\n",
       "            7.0129e-02,  1.7380e-02, -2.0911e-01,  3.4119e-02,  2.9648e-02,\n",
       "           -1.3989e-01,  2.5781e-01, -2.9102e-01, -1.0669e-01, -1.7834e-01,\n",
       "           -2.1912e-02, -1.0248e-01, -4.9805e-01, -2.3230e-01,  2.9858e-01,\n",
       "            4.0723e-01, -1.6797e-01, -1.5404e-02,  1.1816e-01,  1.6760e-01,\n",
       "            1.0645e-01,  1.1218e-01,  1.1499e-01, -3.4985e-01,  3.1567e-01,\n",
       "            1.8396e-01,  1.2146e-01, -1.8164e-01,  1.0724e-01, -2.7979e-01,\n",
       "            3.0200e-01,  1.2927e-01, -2.0642e-01, -1.1322e-01,  4.6387e-01,\n",
       "           -2.2491e-02,  2.1826e-01,  1.1969e-01,  7.1793e-03, -2.4695e-01,\n",
       "            8.2245e-03,  2.8711e-01, -1.1212e-01,  1.2268e-01, -4.1333e-01,\n",
       "            3.7891e-01, -2.3572e-01,  1.3647e-01,  9.2102e-02, -2.2327e-01,\n",
       "            1.0567e-02,  9.8083e-02,  7.8552e-02, -1.4258e-01, -7.2815e-02,\n",
       "            3.8770e-01,  5.6787e-01,  2.9465e-02,  9.4604e-02,  4.5923e-01,\n",
       "            1.0663e-01, -2.0020e-01, -4.5532e-01, -3.5742e-01,  2.0178e-01,\n",
       "            3.4521e-01, -9.7961e-02, -2.2192e-01, -2.8519e-02, -1.9324e-01,\n",
       "            5.1788e-02,  1.7761e-01, -1.5515e-01,  5.0684e-01,  1.6797e-01,\n",
       "            2.6465e-01, -2.3413e-01,  4.6661e-02, -4.2725e-01, -4.0747e-01,\n",
       "            3.4241e-02,  2.9834e-01, -2.5513e-01,  1.1090e-01,  2.2774e-03,\n",
       "            4.9042e-02, -5.7129e-02,  1.7654e-02, -1.2109e-01, -1.1102e-01,\n",
       "            3.4106e-01, -1.5656e-02, -2.0178e-01,  7.0801e-02, -2.9541e-01,\n",
       "           -2.9785e-01,  2.2009e-01,  8.9722e-02, -3.3203e-02, -2.6758e-01,\n",
       "           -6.6162e-02,  3.6499e-01,  2.2485e-01,  3.4229e-01, -3.6621e-01,\n",
       "            2.9938e-02,  3.4271e-02,  6.1621e-01,  1.4355e-01, -2.0416e-02,\n",
       "           -3.1055e-01, -1.0504e-01,  2.2491e-02,  3.7061e-01, -8.1604e-02,\n",
       "            1.7627e-01,  1.4343e-03,  5.4665e-03,  1.6406e-01, -1.5869e-01,\n",
       "            1.4978e-01,  1.7261e-01,  8.3984e-02, -1.1360e-02, -1.4746e-01,\n",
       "           -4.4922e-02,  2.5269e-01, -4.0552e-01, -1.4771e-01,  7.4463e-02,\n",
       "           -1.2463e-01,  1.5442e-02,  1.4294e-01, -2.3767e-01,  1.0101e-01,\n",
       "            1.2512e-01,  4.7882e-02,  5.6992e+00, -5.1147e-02, -2.2180e-01,\n",
       "            4.0894e-02, -1.5906e-01, -4.9927e-02, -2.1838e-01,  4.1943e-01,\n",
       "           -1.9043e-01,  3.2501e-02,  2.2009e-01, -1.1249e-01, -2.1399e-01,\n",
       "           -1.9739e-01, -8.0200e-02,  3.0106e-02, -1.1914e-01, -2.6660e+00,\n",
       "           -2.6270e-01,  2.5806e-01,  2.6392e-01, -1.7236e-01, -7.1106e-03,\n",
       "           -4.7217e-01, -3.0518e-01,  1.2311e-01, -4.0308e-01, -1.5479e-01,\n",
       "           -2.7808e-01,  7.3914e-02, -2.1960e-01,  4.5868e-02,  1.8896e-01,\n",
       "            5.1727e-03, -1.1340e-01, -8.8562e-02, -6.6162e-02, -3.5156e-01,\n",
       "            1.0181e-01,  6.4331e-02,  6.1066e-02,  2.9736e-01, -2.3239e-02,\n",
       "            1.2988e-01,  1.1792e-01, -3.0933e-01, -1.2219e-01, -1.5527e-01,\n",
       "            2.5073e-01, -2.0374e-01, -1.6382e-01,  1.5173e-01,  1.9031e-01,\n",
       "           -3.1055e-01,  5.6824e-02,  2.3584e-01, -4.8657e-01,  3.3936e-01,\n",
       "           -1.7224e-01, -2.4402e-01,  9.2163e-02,  2.9785e-01, -2.1167e-01,\n",
       "            5.9204e-02,  1.0455e-01, -2.3633e-01, -3.1934e-01,  3.1909e-01,\n",
       "           -2.3413e-01, -7.0618e-02, -7.2754e-02, -2.5513e-01, -3.7720e-01,\n",
       "            1.5747e-01, -2.1387e-01,  8.1863e-03, -1.6797e-01,  1.1401e-01,\n",
       "            2.2974e-01, -1.4697e-01,  3.8013e-01,  3.3301e-01,  1.8970e-01,\n",
       "           -6.7139e-02, -2.6993e-02, -1.0706e-01, -4.7754e-01,  1.5759e-01,\n",
       "           -2.0093e-01, -1.4636e-01,  3.6230e-01, -9.0210e-02,  9.6985e-02,\n",
       "           -4.8389e-01,  3.5669e-01, -3.4546e-02,  7.3608e-02,  2.4695e-01,\n",
       "           -2.4536e-02, -3.1348e-01, -1.5491e-01,  7.8735e-02,  5.1788e-02,\n",
       "            9.6985e-02, -2.7319e-01, -7.7637e-02, -1.4746e-01, -2.2290e-01,\n",
       "            4.9219e-01, -3.8965e-01,  2.5146e-01, -5.3467e-02,  6.2012e-02,\n",
       "            2.6465e-01,  7.8552e-02, -1.9946e-01,  2.0740e-01,  4.8975e-01,\n",
       "            2.7979e-01, -5.2582e-02, -1.5466e-01, -1.1554e-01,  5.2071e-03,\n",
       "            3.2007e-01, -2.4243e-01, -2.7002e-01, -1.1536e-01, -1.2097e-01,\n",
       "            2.4002e-02,  2.7686e-01, -6.6223e-02,  3.6987e-01,  7.3547e-02,\n",
       "            2.5488e-01, -5.5615e-01, -2.7881e-01, -9.5520e-02,  9.0332e-02,\n",
       "            1.0327e-01, -5.2051e-01,  2.9883e-01,  1.1926e-01,  2.5610e-01,\n",
       "            1.7310e-01,  9.0515e-02, -1.6187e-01, -1.3196e-01,  1.6736e-01,\n",
       "            1.0840e-01, -2.7145e-02,  7.8796e-02, -2.1545e-02, -3.8257e-01,\n",
       "           -1.8860e-01,  1.9482e-01, -5.4346e-01, -3.4741e-01,  2.1558e-01,\n",
       "           -5.2795e-02, -1.2042e-01, -1.9287e-01,  4.0039e-02,  2.0703e-01,\n",
       "            1.9336e-01, -3.6865e-02,  1.1505e-01,  1.0025e-02,  2.9321e-01,\n",
       "            1.7444e-01, -8.4912e-01,  7.8369e-01,  8.3252e-02,  1.4539e-01,\n",
       "           -1.0046e-01, -4.2480e-01, -4.9072e-02,  6.6589e-02, -3.6304e-01,\n",
       "           -2.2070e-01,  1.1407e-01,  1.6370e-01,  6.4502e-01, -3.2959e-01,\n",
       "           -5.5664e-01, -2.1423e-01, -2.3755e-01, -1.2140e-01, -1.9324e-01,\n",
       "            3.5791e-01, -2.2568e-02,  3.9600e-01,  2.3755e-01,  6.5552e-02,\n",
       "            6.0669e-02,  2.2144e-01,  1.6846e-01, -6.6895e-02, -2.0471e-01,\n",
       "            8.1299e-02,  2.2681e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3718, 0.4019, 0.1526, 0.0473, 0.0265]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  367.967590  102.653030  520.074524  420.758606    0.932485      0  person\n",
       "  1  242.053421  109.989929  356.178406  324.015076    0.927208      0  person\n",
       "  2   49.773346  251.273041  637.448242  420.266846    0.901909     17   horse\n",
       "  3   29.490540  196.869934  201.736725  367.123596    0.880388     17   horse,\n",
       "  'caption': ['The man is laughing with open mouth'],\n",
       "  'bbox_target': [367.78, 97.19, 156.26, 321.09]},\n",
       " 786: {'image_emb': tensor([[-1.2817e-01,  4.1895e-01, -2.0203e-01,  ...,  8.3545e-01,\n",
       "            3.3301e-01,  3.3960e-01],\n",
       "          [-2.0370e-02,  2.7881e-01, -1.1859e-01,  ...,  1.2734e+00,\n",
       "            2.0471e-01, -1.1609e-01],\n",
       "          [ 7.6965e-02,  4.2578e-01, -1.8518e-01,  ...,  1.2451e+00,\n",
       "            1.0706e-01, -1.8958e-01],\n",
       "          ...,\n",
       "          [-1.7578e-01, -3.7384e-02, -1.5002e-01,  ...,  8.1787e-01,\n",
       "            3.7842e-01, -2.3937e-03],\n",
       "          [-8.5592e-04,  1.3806e-01, -5.2246e-01,  ...,  1.2686e+00,\n",
       "            3.2812e-01,  1.2329e-01],\n",
       "          [-2.2119e-01,  6.9641e-02, -5.7666e-01,  ...,  9.2920e-01,\n",
       "            1.7090e-01,  1.2001e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-9.2651e-02,  2.4365e-01, -4.3872e-01,  2.4414e-02,  4.6362e-01,\n",
       "           -2.2754e-01, -1.5442e-01, -2.0044e-01, -2.4756e-01,  5.1953e-01,\n",
       "           -1.0376e-01, -3.0835e-01,  5.1514e-01,  8.3923e-02,  1.8591e-01,\n",
       "           -3.2227e-01,  5.0293e-01,  6.0822e-02, -2.1924e-01,  1.6589e-01,\n",
       "            3.6084e-01,  1.4661e-01, -1.0516e-01, -1.2390e-01, -3.4912e-01,\n",
       "           -1.0748e-03,  1.1884e-01, -1.1633e-01, -1.5503e-01, -9.5276e-02,\n",
       "            2.0886e-01,  7.8003e-02, -1.1694e-01, -1.4832e-01, -6.7863e-03,\n",
       "           -7.1045e-01,  2.7393e-01, -2.2754e-01, -1.9263e-01,  3.2104e-01,\n",
       "            2.2119e-01, -8.0078e-02,  1.9556e-01, -4.6112e-02,  2.1763e-03,\n",
       "            1.8115e-01,  8.9600e-02,  3.5596e-01, -5.7709e-02, -1.7383e-01,\n",
       "            2.0569e-01, -1.2006e-01,  1.2140e-01, -7.7783e-01, -4.6289e-01,\n",
       "           -2.7271e-01, -3.1860e-01,  1.3565e-02, -1.5222e-01,  5.1611e-01,\n",
       "            2.1545e-02, -5.9473e-01,  1.3245e-02,  1.6333e-01, -5.0140e-02,\n",
       "           -1.2793e-01, -3.8721e-01,  6.3135e-01, -3.5254e-01,  3.5339e-02,\n",
       "            2.2568e-02, -1.2915e-01,  3.7720e-01,  9.1125e-02,  4.2432e-01,\n",
       "           -1.2073e-01, -8.7952e-02, -1.5417e-01, -1.8884e-01, -1.0675e-01,\n",
       "           -5.0163e-03,  1.6077e-01,  2.1094e-01,  1.2433e-01,  1.1493e-01,\n",
       "            1.2500e-01, -1.7883e-01, -2.7176e-02,  1.1345e-02,  1.4050e-01,\n",
       "           -8.9233e-02, -6.3538e-02, -8.2812e-01, -5.1221e-01, -1.2817e-01,\n",
       "           -3.3875e-02,  1.7969e-01,  1.0994e-02,  6.0645e-01, -6.7558e-03,\n",
       "            3.2544e-01, -7.0915e-03,  2.9370e-01, -1.2744e-01,  3.2910e-01,\n",
       "           -1.6504e-01, -5.2795e-02,  1.2598e-01,  8.7830e-02, -7.6721e-02,\n",
       "           -5.4346e-01, -6.9336e-01,  2.7588e-01,  4.6240e-01, -1.6650e-01,\n",
       "            3.4149e-02, -3.8623e-01, -2.1008e-01, -7.0166e-01,  2.9199e-01,\n",
       "            9.1553e-02, -9.8975e-01, -3.6896e-02, -3.2617e-01,  2.1777e-01,\n",
       "           -3.0200e-01, -3.6743e-02, -3.3817e-03, -1.3110e-01, -1.4502e-01,\n",
       "           -1.0193e-02, -3.9551e-01, -4.2310e-01,  3.6934e+00, -1.3046e-03,\n",
       "            3.1299e-01, -1.8250e-01, -2.0508e-01, -3.3789e-01, -3.8354e-01,\n",
       "           -2.8198e-01,  1.1853e-01, -4.5923e-01,  1.1395e-01, -8.5297e-03,\n",
       "           -4.2676e-01,  1.9275e-01, -5.2002e-01, -2.8369e-01, -1.4038e-01,\n",
       "           -4.1699e-01,  7.8125e-02,  2.9321e-01,  3.6454e-04, -7.1777e-02,\n",
       "            2.1033e-01,  2.7359e-02, -1.8018e-01,  7.1838e-02, -5.5084e-02,\n",
       "            2.6978e-01,  2.6611e-01, -1.0620e-01, -2.9248e-01, -2.6001e-02,\n",
       "           -3.5004e-02,  1.3367e-01, -1.9684e-03,  3.2324e-01,  1.6016e-01,\n",
       "           -8.1482e-02, -6.0181e-02,  6.5527e-01,  2.1179e-01, -5.5127e-01,\n",
       "            3.6084e-01,  4.1443e-02, -1.9727e-01,  4.5117e-01, -3.5938e-01,\n",
       "           -5.2063e-02,  3.4180e-01,  2.8763e-02, -1.2970e-02, -1.9604e-01,\n",
       "            1.2805e-01, -1.2756e-01, -3.8184e-01, -1.7590e-01,  1.7920e-01,\n",
       "            4.9048e-01,  2.2302e-01,  2.8906e-01,  1.3843e-01, -1.5979e-01,\n",
       "           -1.2840e-02, -1.1908e-01,  2.5162e-02,  5.7251e-02,  1.6394e-01,\n",
       "           -2.0721e-02,  9.8755e-02, -1.0657e-01,  1.3062e-01, -5.7459e-04,\n",
       "           -5.9814e-02,  2.6562e-01, -3.6597e-01,  3.2867e-02, -3.1067e-02,\n",
       "            3.2275e-01,  7.1045e-01, -1.5308e-01,  2.0850e-01,  4.9164e-02,\n",
       "           -5.3467e-02,  4.8633e-01, -4.1870e-01,  5.7959e-01,  1.4685e-01,\n",
       "            5.2295e-01,  1.8097e-02, -1.1334e-01, -3.9429e-01, -4.5068e-01,\n",
       "           -3.6475e-01, -8.2458e-02,  2.4426e-01, -5.9906e-02,  4.8447e-03,\n",
       "           -2.2241e-01,  2.6440e-01, -9.9548e-02, -1.4954e-01,  3.1738e-01,\n",
       "           -3.1567e-01,  4.2529e-01,  3.2837e-02, -7.6172e-02,  5.8502e-02,\n",
       "            2.6749e-02, -7.2083e-02,  3.0640e-01, -3.7354e-01, -1.9934e-01,\n",
       "            6.9092e-02, -9.1125e-02,  1.9299e-01, -3.1030e-01, -2.9077e-01,\n",
       "           -4.6533e-01,  8.0444e-02,  1.2964e-01, -4.8853e-01, -9.9060e-02,\n",
       "            1.6418e-01, -1.1670e-01, -3.2837e-01, -1.8933e-01,  3.0835e-01,\n",
       "           -7.6355e-02, -1.6772e-01,  3.5675e-02, -3.8086e-01,  2.4490e-02,\n",
       "           -5.6366e-02,  3.5620e-01, -3.5693e-01, -7.7026e-02, -7.5745e-02,\n",
       "            1.9800e-01,  2.1143e-01,  4.8657e-01, -3.6346e-02, -1.9287e-02,\n",
       "           -1.5906e-01,  3.3386e-02, -9.0332e-02, -3.9258e-01, -5.3174e-01,\n",
       "           -2.1069e-01,  1.6138e-01,  3.1250e-01,  3.4851e-02, -7.2571e-02,\n",
       "            1.9751e-01, -4.1113e-01,  4.2529e-01,  1.6040e-01, -2.9224e-01,\n",
       "            3.7891e-01,  4.4507e-01,  1.6980e-01,  3.7134e-01, -8.4839e-02,\n",
       "            8.1848e-02,  1.1993e-01, -1.6589e-01,  7.1838e-02, -6.2439e-02,\n",
       "           -1.1310e-01, -1.0614e-01,  1.3293e-01,  2.2351e-01, -2.8418e-01,\n",
       "           -4.0356e-01,  2.6099e-01,  9.5581e-02,  3.2007e-01, -2.2925e-01,\n",
       "            1.7163e-01,  4.2944e-01,  3.6914e+00,  4.4141e-01,  3.7964e-01,\n",
       "            2.4719e-01, -4.5044e-02,  2.5513e-02,  2.2644e-01, -1.2781e-01,\n",
       "           -3.2300e-01, -1.7236e-01,  3.9490e-02,  2.3010e-01, -4.8804e-01,\n",
       "            1.6113e-01,  2.0410e-01, -2.0093e-01,  2.2388e-01, -4.8975e-01,\n",
       "            5.7471e-01, -8.0627e-02,  1.7627e-01, -2.2876e-01,  5.3894e-02,\n",
       "           -1.1383e-01, -1.2988e-01, -1.7004e-01, -8.8562e-02,  1.2561e-01,\n",
       "           -3.0835e-01, -5.7373e-02,  3.7939e-01, -1.2561e-01,  3.5718e-01,\n",
       "           -2.1069e-01,  5.7568e-01,  5.1416e-01,  4.4022e-03, -1.4490e-01,\n",
       "            1.8274e-01, -6.7566e-02,  2.3877e-01,  1.3879e-01, -4.3140e-01,\n",
       "           -1.7200e-01,  1.5576e-01, -6.8298e-02, -2.5244e-01, -1.8091e-01,\n",
       "           -2.4255e-01, -2.1106e-01,  2.2168e-01,  1.7578e-01,  2.1875e-01,\n",
       "           -7.1350e-02,  1.0480e-01, -1.1188e-01,  2.3560e-01,  2.1558e-01,\n",
       "           -9.0088e-02, -1.9189e-01, -8.0688e-02,  3.2007e-01,  2.3181e-01,\n",
       "           -3.8013e-01,  3.2495e-01, -3.4912e-01, -6.2842e-01,  2.6489e-01,\n",
       "           -1.9470e-01, -2.5122e-01,  4.1821e-01, -1.4429e-01,  1.0419e-01,\n",
       "           -2.0496e-01,  1.7163e-01,  1.9568e-01, -9.8694e-02,  3.6792e-01,\n",
       "           -1.2720e-01, -1.7593e-02, -2.8320e-01,  4.3457e-02, -1.2952e-01,\n",
       "            2.2034e-02,  4.5319e-02,  2.2681e-01,  5.7404e-02,  2.7939e-02,\n",
       "            8.7219e-02,  8.9539e-02,  4.6851e-01,  3.0127e-01, -3.4985e-01,\n",
       "            1.0889e-01,  4.8169e-01, -5.9479e-02,  6.4600e-01,  9.8877e-02,\n",
       "           -4.9731e-01, -1.2439e-01, -1.0089e-01,  9.0393e-02, -3.3740e-01,\n",
       "           -4.2041e-01,  2.7466e-01, -2.7051e-01,  4.9255e-02, -4.9362e-03,\n",
       "           -1.7102e-01,  1.0699e-01,  5.2393e-01, -3.2227e-01, -8.6136e-03,\n",
       "           -2.5537e-01,  2.0435e-01, -1.2720e-01,  1.1462e-01,  1.2024e-01,\n",
       "            9.2651e-02,  5.9570e-01,  2.7197e-01,  1.7395e-01, -1.0107e-01,\n",
       "           -2.9150e-01, -3.1543e-01,  5.1367e-01, -9.3262e-02,  1.1273e-01,\n",
       "           -3.6084e-01, -4.0015e-01,  2.0471e-01,  8.0347e-04,  5.3680e-02,\n",
       "           -2.6562e-01, -5.9961e-01, -2.7588e-01, -2.8784e-01,  4.3677e-01,\n",
       "            1.6687e-01, -3.1812e-01,  2.3291e-01, -1.6431e-01,  1.5076e-01,\n",
       "            5.4321e-02,  1.8665e-01,  1.3809e-02, -3.7158e-01, -4.5435e-01,\n",
       "            1.6370e-01, -2.5708e-01, -1.1780e-01, -1.8408e-01, -7.1228e-02,\n",
       "            1.6870e-01,  1.7725e-01, -1.6284e-01, -3.0859e-01, -3.1982e-02,\n",
       "           -2.7783e-01, -2.6685e-01,  4.3115e-01, -2.3572e-01,  2.0569e-01,\n",
       "           -8.3618e-02,  2.7344e-01, -5.0244e-01, -2.5952e-01,  5.4688e-01,\n",
       "           -9.0866e-03, -4.9219e-01,  1.4331e-01,  2.0288e-01, -3.0664e-01,\n",
       "           -3.6890e-01,  9.0759e-02,  1.7725e-01,  3.3447e-02, -1.8054e-01,\n",
       "            4.1718e-02, -8.5022e-02,  2.5439e-01,  1.1289e+00,  2.2949e-01,\n",
       "           -1.2772e-02,  1.0052e-01, -1.9641e-01, -5.1416e-01,  2.0203e-01,\n",
       "           -3.5400e-02, -5.6396e-02, -1.4380e-01,  3.2104e-01,  6.9043e-01,\n",
       "           -1.7786e-01,  1.1816e-01,  9.3872e-02, -3.6084e-01, -3.3252e-01,\n",
       "            1.0675e-01, -2.1338e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.0879e-01, 1.9348e-04, 4.3091e-02, 2.6392e-01, 1.9236e-03, 2.4319e-03,\n",
       "           1.5527e-01, 2.4185e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   326.799561    4.948723  464.450745  244.111450    0.919604     40   \n",
       "  1   530.762085    4.024529  626.292114  193.739929    0.914276     41   \n",
       "  2   419.996338    1.054359  638.764771  132.466858    0.860196      0   \n",
       "  3    10.288387   64.689819  126.895599  285.589417    0.849967     40   \n",
       "  4   326.063110   51.716095  397.318420  220.978546    0.844026     41   \n",
       "  5    63.697617    0.521881  406.028137  471.646973    0.807645     39   \n",
       "  6    79.898346   84.796951  206.835449  393.576782    0.749111     39   \n",
       "  7     0.452778    0.674309   96.341583  203.739197    0.684182      0   \n",
       "  8     6.111023   34.162689  636.528992  473.627319    0.627790     60   \n",
       "  9     0.000000  317.269104   31.886545  350.664368    0.526387     42   \n",
       "  10    0.251516  142.864029   37.628227  261.483246    0.479247     41   \n",
       "  11  367.723328    1.335228  458.470764  164.064667    0.272583     40   \n",
       "  \n",
       "              name  \n",
       "  0     wine glass  \n",
       "  1            cup  \n",
       "  2         person  \n",
       "  3     wine glass  \n",
       "  4            cup  \n",
       "  5         bottle  \n",
       "  6         bottle  \n",
       "  7         person  \n",
       "  8   dining table  \n",
       "  9           fork  \n",
       "  10           cup  \n",
       "  11    wine glass  ,\n",
       "  'caption': ['A person sitting behind a wine glass to the left of a large bottle'],\n",
       "  'bbox_target': [4.31, 1.08, 110.03, 193.08]},\n",
       " 787: {'image_emb': tensor([[-0.0580, -0.1639, -0.0417,  ...,  0.3916,  0.2681,  0.3748],\n",
       "          [ 0.1231,  0.1652, -0.0586,  ...,  0.8506, -0.4280,  0.0980],\n",
       "          [ 0.0289, -0.2864, -0.2556,  ...,  0.8477, -0.0303,  0.1516],\n",
       "          ...,\n",
       "          [-0.4543, -0.0141,  0.2566,  ...,  0.4836,  0.1570,  0.3748],\n",
       "          [ 0.0985, -0.0801, -0.2019,  ...,  0.5093,  0.0940,  0.1901],\n",
       "          [-0.1022, -0.0482, -0.0071,  ...,  0.3567,  0.2708, -0.0047]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1742, -0.0736,  0.0399,  ..., -0.4253,  0.1464, -0.3008],\n",
       "          [-0.0077, -0.2438, -0.2351,  ..., -0.0297, -0.2539, -0.2435]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.5176e-01, 5.7888e-04, 1.4114e-03, 2.4628e-02, 8.8684e-02, 3.4924e-03,\n",
       "           3.2935e-01],\n",
       "          [1.1786e-01, 1.6093e-06, 2.1458e-06, 7.1585e-05, 4.1723e-07, 1.1139e-02,\n",
       "           8.7109e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   21.637360  155.295151  225.200989  601.197876    0.941750      0   \n",
       "  1  191.381561  281.676544  306.272186  399.232330    0.871318     55   \n",
       "  2   79.251617  549.035522  371.012604  639.301392    0.792216      0   \n",
       "  3  365.468292  145.326477  426.021484  289.498291    0.740055      0   \n",
       "  4   13.458954  545.507263  158.272339  640.000000    0.726982     56   \n",
       "  5  152.398285  296.942719  426.314331  563.167236    0.711177     60   \n",
       "  6    0.000000  228.495285   93.465118  254.691177    0.658527     71   \n",
       "  7  173.149384   68.785980  306.635284  530.626709    0.554115      0   \n",
       "  8   33.767624  170.991821   52.365463  231.104614    0.384072     39   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1          cake  \n",
       "  2        person  \n",
       "  3        person  \n",
       "  4         chair  \n",
       "  5  dining table  \n",
       "  6          sink  \n",
       "  7        person  \n",
       "  8        bottle  ,\n",
       "  'caption': ['The boy that is standing on the stool.',\n",
       "   'A boy wearing a white shirt and bending over to blow out birthday candles.'],\n",
       "  'bbox_target': [21.62, 155.68, 209.01, 454.05]},\n",
       " 788: {'image_emb': tensor([[-0.5112,  0.3940, -0.0888,  ...,  0.3823,  0.2030,  0.1154],\n",
       "          [-0.5552,  0.2981, -0.1903,  ...,  0.6826,  0.3196, -0.0282],\n",
       "          [-0.0370,  0.3091, -0.4207,  ...,  1.2363,  0.1647, -0.1733],\n",
       "          [-0.6050,  0.1877,  0.1220,  ...,  0.3218,  0.2834,  0.2678]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 4.2816e-02,  1.3135e-01,  7.8064e-02, -5.1483e-02, -2.3462e-01,\n",
       "            1.7847e-01, -2.3682e-01, -5.8807e-02, -1.9470e-01,  1.0382e-01,\n",
       "            1.1377e-01, -4.1406e-01,  5.5328e-02, -3.1421e-01,  5.0684e-01,\n",
       "            1.1060e-01,  1.8774e-01,  5.3925e-02, -6.2042e-02, -2.0520e-01,\n",
       "            1.2030e-01,  5.4590e-01,  3.0078e-01, -3.8757e-03, -2.3572e-01,\n",
       "            4.1333e-01,  3.3618e-01, -7.4585e-02, -1.2512e-01,  1.9495e-01,\n",
       "            1.6809e-01, -5.1318e-01,  1.2622e-01, -8.7769e-02, -3.1152e-01,\n",
       "            2.7783e-01, -5.0201e-02,  4.6509e-01,  1.2585e-01,  1.2329e-01,\n",
       "           -3.3105e-01, -1.5213e-02, -2.0361e-01, -1.0907e-01,  1.1823e-01,\n",
       "            1.8042e-01,  3.8208e-02,  4.6234e-02,  1.8689e-01, -9.7778e-02,\n",
       "           -5.9113e-02,  2.2375e-01, -2.0532e-01,  2.6718e-02,  3.9276e-02,\n",
       "           -3.4766e-01,  1.3947e-02,  5.1636e-02, -5.7861e-02,  3.4424e-01,\n",
       "           -6.3324e-04,  7.6904e-02,  1.0767e-01, -1.4648e-02, -2.4231e-01,\n",
       "           -4.9609e-01, -2.7075e-01,  8.4717e-02, -6.8237e-02, -2.7661e-01,\n",
       "           -1.7322e-01, -1.2842e-01, -1.6321e-01,  3.3667e-01, -1.2054e-01,\n",
       "            1.9653e-01,  1.5027e-01,  2.5977e-01, -1.0931e-01,  3.4027e-02,\n",
       "           -9.6619e-02, -2.4365e-01,  7.9102e-02,  3.0054e-01, -1.2744e-01,\n",
       "            4.6234e-02, -1.5045e-02, -2.5220e-01,  3.9624e-01,  2.8296e-01,\n",
       "            1.4148e-01,  1.8518e-01, -1.3301e+00,  6.2549e-01, -2.9068e-03,\n",
       "            2.4368e-02, -1.4856e-01, -1.9150e-02,  6.4507e-03,  1.1053e-01,\n",
       "            2.3486e-01,  2.7979e-01, -1.0516e-01,  2.3132e-01, -3.3838e-01,\n",
       "            2.6703e-02, -1.7151e-01, -2.4634e-01, -3.1641e-01, -1.2671e-01,\n",
       "           -5.1727e-02, -1.5312e-02,  1.9055e-01, -7.8491e-02,  4.7607e-01,\n",
       "           -5.8014e-02, -2.8244e-02,  3.7305e-01, -2.9663e-01,  6.6284e-02,\n",
       "            3.4399e-01, -1.9751e-01, -4.5593e-02,  3.5065e-02, -1.7029e-01,\n",
       "           -1.4087e-01, -2.4853e-03, -2.7100e-01, -3.9624e-01,  2.3267e-01,\n",
       "            2.5586e-01,  2.6758e-01, -2.0349e-01,  5.2617e+00, -4.6654e-03,\n",
       "           -5.0586e-01,  5.4199e-02, -6.5039e-01, -3.3569e-01,  1.0480e-01,\n",
       "           -1.8860e-01,  3.1104e-01,  1.3696e-01,  3.9893e-01, -8.6975e-02,\n",
       "           -3.2806e-02,  1.4380e-01, -3.5522e-01,  5.4199e-02, -2.0325e-01,\n",
       "            6.7177e-03,  3.4692e-01,  2.0859e-02,  3.1226e-01,  5.0684e-01,\n",
       "           -3.3521e-01, -1.3916e-02, -4.0356e-01,  4.5654e-01, -8.7036e-02,\n",
       "            1.3013e-01,  2.8516e-01,  1.0547e-01, -4.4067e-02,  2.5000e-01,\n",
       "            1.9623e-02,  1.7419e-01,  1.5454e-01,  3.3740e-01,  3.7415e-02,\n",
       "            2.0520e-01, -1.4673e-01,  2.9419e-01,  2.2119e-01, -6.0498e-01,\n",
       "           -3.1348e-01, -2.0239e-01,  1.0626e-01,  1.2988e-01,  1.1353e-01,\n",
       "            2.1204e-01,  2.8906e-01, -3.7524e-01, -4.6753e-01,  4.9219e-01,\n",
       "           -1.0846e-01,  4.8730e-01, -3.4204e-01,  2.7954e-01, -1.2939e-01,\n",
       "            3.4058e-01,  1.6138e-01, -1.9690e-01,  1.7822e-01, -2.1582e-01,\n",
       "            1.0980e-01, -3.7915e-01,  3.3691e-01,  1.7480e-01,  9.1797e-02,\n",
       "            1.5930e-01,  1.4832e-01,  9.1064e-02,  2.3682e-01, -1.8506e-01,\n",
       "           -4.2358e-01,  1.9800e-01, -1.8213e-01,  9.2239e-03,  2.3047e-01,\n",
       "            6.0254e-01,  2.5562e-01,  1.0529e-01, -9.4299e-03, -2.2217e-02,\n",
       "           -1.9363e-02,  1.5149e-01, -1.6687e-01, -3.9624e-01, -3.2568e-01,\n",
       "           -7.6599e-02, -1.6772e-01, -4.1797e-01,  1.9678e-01,  5.9143e-02,\n",
       "           -3.8452e-01, -9.1553e-02, -1.2793e-01,  2.3108e-01, -2.8369e-01,\n",
       "            8.7158e-02,  2.5952e-01,  3.7598e-01, -9.0820e-02, -2.2247e-02,\n",
       "           -6.8970e-02, -8.8745e-02,  1.4880e-01,  9.7290e-02, -1.3403e-01,\n",
       "            5.8691e-01,  8.0383e-02,  3.5522e-02,  1.9702e-01,  6.6895e-02,\n",
       "            6.9153e-02, -1.4575e-01, -1.5125e-01, -5.7556e-02,  9.6558e-02,\n",
       "           -2.8101e-01,  1.3220e-01,  4.1809e-02,  7.5562e-02,  1.6541e-01,\n",
       "           -1.1023e-01, -2.8564e-01, -7.1533e-01, -7.2144e-02,  4.3579e-01,\n",
       "           -6.4850e-03, -1.1829e-01,  4.1821e-01, -9.6985e-02,  1.0895e-02,\n",
       "            8.5831e-03, -2.6709e-01, -2.0996e-01, -1.3098e-01,  2.1500e-02,\n",
       "           -3.2080e-01, -5.1025e-01,  4.3970e-01,  1.0468e-01,  1.0590e-02,\n",
       "           -3.3020e-02,  2.4622e-01, -8.4229e-02,  6.1182e-01, -6.5918e-01,\n",
       "            4.0234e-01,  2.9639e-01,  5.2930e-01,  2.9395e-01, -7.6721e-02,\n",
       "            1.4124e-01,  3.3008e-01, -2.7222e-01,  1.4124e-01, -1.0822e-01,\n",
       "            1.1551e-02, -2.2644e-01,  2.0691e-01,  6.9519e-02, -9.5581e-02,\n",
       "            2.0679e-01, -9.0234e-01,  1.0002e-02, -7.7026e-02, -1.2408e-01,\n",
       "            1.6992e-01, -3.2990e-02, -1.3464e-01, -5.0934e-02, -3.6163e-02,\n",
       "           -2.0667e-01, -1.2158e-01,  9.1553e-02,  9.3460e-03,  2.3242e-01,\n",
       "           -4.2261e-01,  6.5332e-01,  5.2617e+00,  4.7632e-01, -2.5952e-01,\n",
       "            2.0178e-01, -1.9043e-01,  4.1406e-01, -7.1594e-02,  2.9395e-01,\n",
       "           -1.8335e-01,  3.5156e-01, -2.2186e-02,  2.7808e-01, -1.2323e-01,\n",
       "            3.1396e-01,  4.1504e-01,  2.4829e-01,  1.6617e-02, -1.4834e+00,\n",
       "            4.6899e-01,  2.7515e-01, -1.9873e-01,  1.4758e-01,  1.0059e-01,\n",
       "            2.0050e-02,  2.6709e-01, -7.9407e-02, -4.8340e-02,  1.6443e-01,\n",
       "            1.3831e-01,  2.6001e-01,  1.4961e-02, -5.6006e-01, -7.3669e-02,\n",
       "           -3.1714e-01,  2.4072e-01,  8.5693e-02,  5.2704e-02,  4.6631e-02,\n",
       "            6.6211e-01, -1.0791e-01, -2.1277e-01, -8.5754e-02, -2.2900e-01,\n",
       "            2.4402e-01,  2.1289e-01,  7.2998e-02, -2.0325e-01,  2.8320e-02,\n",
       "            2.1472e-01, -1.9641e-01, -1.9836e-01, -1.1688e-01,  1.5930e-02,\n",
       "           -3.6499e-01,  3.0075e-02, -4.8737e-02, -2.1289e-01, -2.4133e-01,\n",
       "            1.6528e-01,  3.6060e-01,  1.8295e-02, -1.6541e-01, -1.4111e-01,\n",
       "           -2.3877e-01, -6.2418e-04,  5.7983e-02,  1.7929e-02, -2.6099e-01,\n",
       "           -1.2561e-01,  2.6917e-02,  1.9434e-01, -5.1904e-01,  2.1265e-01,\n",
       "            1.4185e-01, -3.9111e-01,  1.5527e-01,  2.6416e-01, -2.1155e-01,\n",
       "           -8.4863e-01,  4.0845e-01, -2.9883e-01, -7.4316e-01, -4.6509e-02,\n",
       "            3.7427e-01,  3.4839e-01, -3.8574e-01, -2.6660e-01,  5.8838e-01,\n",
       "            2.0325e-01, -1.8713e-01,  1.5820e-01,  1.8701e-01, -1.6724e-01,\n",
       "            4.5746e-02,  2.5928e-01, -6.7017e-02,  2.3975e-01,  1.1932e-01,\n",
       "            4.1235e-01, -2.5024e-02,  1.1237e-01,  1.1578e-01, -2.6318e-01,\n",
       "            7.7087e-02, -3.2153e-01, -5.1123e-01, -3.0420e-01,  4.1333e-01,\n",
       "           -2.9404e-02, -1.0046e-01, -2.4670e-01, -3.8354e-01,  1.9568e-01,\n",
       "            5.1331e-02, -2.8271e-01, -2.7661e-01,  7.5500e-02, -4.0283e-01,\n",
       "           -9.3506e-02,  5.4047e-02, -8.4534e-03,  6.1920e-02,  6.4331e-02,\n",
       "           -3.3569e-01, -5.3162e-02,  4.6191e-01, -1.3901e-02,  3.6963e-01,\n",
       "           -1.7776e-02, -1.0779e-01, -3.0121e-02,  9.9869e-03, -3.9893e-01,\n",
       "            5.0439e-01, -1.1060e-01, -4.3335e-01,  2.8101e-01, -3.3032e-01,\n",
       "           -3.4180e-02,  1.2219e-01, -2.4084e-01,  2.8271e-01,  3.0258e-02,\n",
       "            8.5083e-02,  1.4954e-01,  2.0117e-01,  1.3763e-02, -1.0852e-01,\n",
       "            1.7432e-01,  2.4268e-01, -2.6904e-01,  5.6519e-02,  4.9878e-01,\n",
       "           -1.1902e-03,  2.3083e-01, -1.3647e-01, -1.1426e-01,  2.0142e-02,\n",
       "           -6.5674e-02, -1.5149e-01, -6.5063e-02,  2.2644e-01,  3.1152e-01,\n",
       "            9.4788e-02,  1.2549e-01,  2.7002e-01,  2.4719e-01, -5.6427e-02,\n",
       "            2.2766e-01, -8.6621e-01,  1.0010e-02, -3.3008e-01,  8.0261e-02,\n",
       "            1.0333e-01, -3.9551e-01,  1.2314e-02, -1.8982e-01, -2.1631e-01,\n",
       "            4.9707e-01,  6.2042e-02, -2.9883e-01,  6.5332e-01,  9.3384e-02,\n",
       "           -2.2815e-01,  1.1896e-01,  1.1377e-01, -1.8628e-01,  1.0132e-01,\n",
       "           -9.4910e-02,  1.8530e-01,  1.4954e-01,  3.6157e-01,  6.3782e-02,\n",
       "            5.0446e-02, -1.8274e-01, -3.0884e-02, -6.0547e-01, -2.9956e-01,\n",
       "            2.5098e-01, -2.2681e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3606, 0.5166, 0.0291, 0.0941]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  185.247192   96.371948  354.284546  328.882019    0.916862      0  person\n",
       "  1  192.401672  290.037933  410.679932  358.591522    0.779674     30    skis\n",
       "  2  168.218475   79.507736  216.533783  261.234985    0.759345      0  person,\n",
       "  'caption': ['skis'],\n",
       "  'bbox_target': [190.13, 292.8, 224.35, 63.69]},\n",
       " 789: {'image_emb': tensor([[ 0.1121, -0.0740,  0.0898,  ...,  0.2539,  0.0721, -0.1985],\n",
       "          [ 0.1038, -0.1110,  0.1188,  ...,  0.8540, -0.2615, -0.5679],\n",
       "          [-0.3269,  0.1722, -0.0904,  ...,  1.0381, -0.1030, -0.2874],\n",
       "          [-0.2087,  0.1857,  0.1740,  ...,  0.4128, -0.0714, -0.2394]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0800,  0.3801, -0.1538,  ..., -0.0507, -0.2622, -0.3486],\n",
       "          [ 0.5605,  0.2499, -0.2698,  ...,  0.0466,  0.1059, -0.4001]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.6458e-03, 9.7949e-01, 1.0551e-02, 4.1313e-03],\n",
       "          [1.3609e-03, 9.9463e-01, 3.7594e-03, 4.4203e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class      name\n",
       "  0  150.278244  25.338165  408.237610  366.461761    0.946072     25  umbrella\n",
       "  1  385.367432   0.000000  639.240234  376.807678    0.942324     25  umbrella\n",
       "  2    6.299606  44.625626  203.527695  343.092834    0.940012     25  umbrella,\n",
       "  'caption': ['a blue and white umbrella', 'umbrella just dance'],\n",
       "  'bbox_target': [385.72, 1.54, 254.28, 378.98]},\n",
       " 790: {'image_emb': tensor([[-0.2722, -0.2405, -0.1196,  ...,  0.3989,  0.0123, -0.0048],\n",
       "          [-0.3833,  0.1290, -0.2500,  ...,  1.2646,  0.1083, -0.0023],\n",
       "          [-0.2213,  0.0338, -0.1688,  ...,  0.4597, -0.1610,  0.0867],\n",
       "          [-0.3469, -0.0021, -0.3000,  ...,  0.7788,  0.0892, -0.0254],\n",
       "          [-0.1635, -0.1849, -0.0835,  ...,  0.2224, -0.1537,  0.0255]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1903, -0.5503,  0.0329,  ...,  0.2947,  0.0809,  0.5083],\n",
       "          [-0.0894, -0.3538, -0.0851,  ...,  0.1991, -0.1212,  0.5771]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0395, 0.0022, 0.0420, 0.8984, 0.0178],\n",
       "          [0.1934, 0.0026, 0.0381, 0.6748, 0.0913]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0    1.248860   52.431065  339.999237  328.820526    0.935579     22  zebra\n",
       "  1    0.618815    0.164562  212.289642   71.585281    0.897955     22  zebra\n",
       "  2  257.027344  106.798767  499.256042  328.858765    0.858398     22  zebra\n",
       "  3    1.208591    0.683206  471.149536  149.877548    0.857570     22  zebra,\n",
       "  'caption': ['the backside of a large zebra behind the head of a younger zebra',\n",
       "   'the backside of a black and white zebra in between two zebra heads'],\n",
       "  'bbox_target': [255.92, 108.38, 242.46, 220.01]},\n",
       " 791: {'image_emb': tensor([[ 0.1312,  0.3777, -0.0478,  ...,  1.1289,  0.0057,  0.0606],\n",
       "          [ 0.2114,  0.3928, -0.0909,  ...,  0.6172,  0.4060,  0.0045],\n",
       "          [ 0.2322,  0.3347, -0.0914,  ...,  0.7939,  0.2225,  0.1798]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0214, -0.0132, -0.1439,  ...,  0.2759, -0.0510,  0.1388],\n",
       "          [-0.1619, -0.1985, -0.1799,  ..., -0.0107,  0.0851,  0.0720]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0000e+00, 5.3644e-07, 7.1526e-07],\n",
       "          [1.0000e+00, 4.1127e-06, 5.0068e-06]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   299.591003    0.000000  477.112122  177.602478    0.903136     56   \n",
       "  1     5.912231    1.630554  423.493530  635.428528    0.858409     60   \n",
       "  2   166.211288  323.707214  229.796783  358.861084    0.674385     48   \n",
       "  3   103.586044  413.923218  385.498657  638.969238    0.670360     45   \n",
       "  4   228.945953  309.715912  318.990326  366.321014    0.669012     48   \n",
       "  5   159.759430  299.298126  237.136505  329.654022    0.658775     48   \n",
       "  6   274.218201  257.459808  329.914856  313.106049    0.566628     48   \n",
       "  7   213.863007  271.097870  276.370453  310.342926    0.557300     48   \n",
       "  8   122.324036  247.147186  201.048462  304.890533    0.544122     48   \n",
       "  9   196.816467  179.978882  284.601013  233.767700    0.510002     48   \n",
       "  10  112.964294  283.517487  167.803680  327.436371    0.483991     48   \n",
       "  11  391.849091   44.100464  477.352905  357.835205    0.477053     56   \n",
       "  12  111.571640  316.715637  172.718277  357.472412    0.469599     48   \n",
       "  13  183.163544   12.337885  214.069855   33.611977    0.437064     48   \n",
       "  14  190.524628    0.583261  216.198090   18.295197    0.434871     48   \n",
       "  15  163.698288  237.074463  234.640320  295.350220    0.423158     48   \n",
       "  16  211.804840   13.611068  243.534393   36.449409    0.415652     48   \n",
       "  17  251.873688  230.444870  305.512970  270.738678    0.359029     48   \n",
       "  18  220.437958    0.000000  248.411041   18.436640    0.351194     48   \n",
       "  19  244.255554    6.906222  273.072571   28.111267    0.316182     48   \n",
       "  20  171.678345  229.277298  260.197693  273.888794    0.260910     48   \n",
       "  \n",
       "              name  \n",
       "  0          chair  \n",
       "  1   dining table  \n",
       "  2       sandwich  \n",
       "  3           bowl  \n",
       "  4       sandwich  \n",
       "  5       sandwich  \n",
       "  6       sandwich  \n",
       "  7       sandwich  \n",
       "  8       sandwich  \n",
       "  9       sandwich  \n",
       "  10      sandwich  \n",
       "  11         chair  \n",
       "  12      sandwich  \n",
       "  13      sandwich  \n",
       "  14      sandwich  \n",
       "  15      sandwich  \n",
       "  16      sandwich  \n",
       "  17      sandwich  \n",
       "  18      sandwich  \n",
       "  19      sandwich  \n",
       "  20      sandwich  ,\n",
       "  'caption': ['A black plastic chair at a table.',\n",
       "   'An empty black plastic chair.'],\n",
       "  'bbox_target': [300.54, 0.0, 166.97, 183.38]},\n",
       " 792: {'image_emb': tensor([[-0.1558,  0.2029, -0.0335,  ...,  1.6299, -0.1213, -0.0614],\n",
       "          [-0.3794,  0.4890,  0.2372,  ...,  0.7578, -0.0579,  0.0420],\n",
       "          [-0.2625,  0.1548, -0.3108,  ...,  1.1299, -0.1881,  0.1960],\n",
       "          ...,\n",
       "          [-0.1207,  0.0359, -0.2905,  ...,  1.0830, -0.2280,  0.0903],\n",
       "          [-0.2678,  0.1931, -0.0641,  ...,  1.9053,  0.0069,  0.1355],\n",
       "          [-0.5127,  0.0365, -0.0135,  ...,  0.9419, -0.1176, -0.2671]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1416,  0.3198,  0.0588,  ..., -0.3108, -0.0315,  0.0026],\n",
       "          [ 0.1813,  0.4414, -0.2119,  ..., -0.2935,  0.1538,  0.1194]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7664e-01, 8.1641e-01, 8.3065e-04, 3.6659e-03, 1.3065e-03, 2.8706e-04,\n",
       "           7.6818e-04],\n",
       "          [7.5439e-01, 2.3010e-01, 1.4305e-06, 3.5496e-03, 3.5882e-05, 4.7088e-06,\n",
       "           1.1642e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  346.179688   56.098572  507.833435  423.342407    0.948695      0   \n",
       "  1   62.244690   84.194565  316.491760  424.330566    0.945452      0   \n",
       "  2  520.226135   92.413422  617.239441  344.344025    0.897120     58   \n",
       "  3  108.159500  221.750610  178.848160  305.928345    0.745074     65   \n",
       "  4    0.188990  365.045929   54.755943  424.341064    0.733153     56   \n",
       "  5   53.517536  354.559814  139.582886  425.462463    0.720014     56   \n",
       "  6  500.118652  323.203125  518.509949  334.664795    0.619019     65   \n",
       "  7  459.882080  340.292267  527.785400  425.099731    0.494519     56   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2  potted plant  \n",
       "  3        remote  \n",
       "  4         chair  \n",
       "  5         chair  \n",
       "  6        remote  \n",
       "  7         chair  ,\n",
       "  'caption': ['A man wearing glasses.',\n",
       "   'A man wearing black T-shirt shaking his hand.'],\n",
       "  'bbox_target': [61.55, 87.03, 254.85, 339.97]},\n",
       " 793: {'image_emb': tensor([[ 0.2656, -0.0903, -0.4014,  ...,  0.7896,  0.2433, -0.1489],\n",
       "          [ 0.0185,  0.2264, -0.3362,  ...,  0.8066, -0.0135, -0.1887],\n",
       "          [ 0.1301, -0.3372, -0.3223,  ...,  0.4189, -0.1091, -0.3447],\n",
       "          [ 0.1162, -0.0810,  0.0625,  ...,  0.6567, -0.0606, -0.4048]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1255, -0.1027,  0.1089,  ...,  0.2903,  0.1071, -0.3794],\n",
       "          [-0.2991, -0.0892,  0.0858,  ...,  0.2020,  0.0752, -0.5093]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0202, 0.0327, 0.4810, 0.4661],\n",
       "          [0.0233, 0.0272, 0.7354, 0.2140]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  430.621277  112.762115  626.814026  277.431976    0.887834     53   \n",
       "  1    0.482658  118.347015  147.234436  284.484344    0.885164     53   \n",
       "  2  151.592667  110.110001  636.894592  389.639465    0.712870     60   \n",
       "  3  106.264244   24.756226  176.930054  148.531433    0.690399     40   \n",
       "  \n",
       "             name  \n",
       "  0         pizza  \n",
       "  1         pizza  \n",
       "  2  dining table  \n",
       "  3    wine glass  ,\n",
       "  'caption': ['The pizza that is resting on the wooden chopping block.',\n",
       "   'A small pizza on a cutting board.'],\n",
       "  'bbox_target': [428.12, 110.5, 200.06, 168.73]},\n",
       " 794: {'image_emb': tensor([[ 0.2001,  0.6602, -0.1115,  ...,  0.8721, -0.0283,  0.4468],\n",
       "          [-0.1213,  0.7847,  0.2520,  ...,  0.9595,  0.0907,  0.2029],\n",
       "          [-0.0433,  0.2246,  0.0348,  ...,  0.8081,  0.1926,  0.3931],\n",
       "          ...,\n",
       "          [ 0.0828,  0.0536,  0.1558,  ...,  1.0830, -0.4529,  0.3513],\n",
       "          [-0.0069,  0.4248,  0.3115,  ...,  1.0635,  0.1521,  0.0410],\n",
       "          [ 0.1074,  0.1086,  0.0172,  ...,  0.7900,  0.0406,  0.2422]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1158,  0.0379, -0.2925,  ...,  0.2086, -0.0054, -0.1873],\n",
       "          [-0.2035, -0.0387,  0.2012,  ...,  0.2148, -0.2185, -0.0983]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.9802e-07, 1.6689e-06, 2.5034e-06, 9.7705e-01, 9.8944e-06, 2.2614e-02,\n",
       "           1.4305e-06, 4.7684e-04],\n",
       "          [1.1921e-07, 3.5763e-07, 5.6624e-06, 5.7715e-01, 4.4703e-06, 4.2236e-01,\n",
       "           1.3113e-06, 1.7357e-04]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     5.304897    4.014069  160.096085  151.573181    0.931465     41   \n",
       "  1     7.248318  174.717148  160.525787  314.835297    0.925072     41   \n",
       "  2   169.302185   40.342884  331.345612  124.066330    0.835723     46   \n",
       "  3   342.034821   17.274929  498.082550  148.188400    0.820005     48   \n",
       "  4    87.457253   27.930933  149.209244   72.425041    0.792346     44   \n",
       "  5   194.225723  192.050278  283.757080  318.219147    0.776166     48   \n",
       "  6   361.030243  174.130432  486.685333  327.754883    0.728788     41   \n",
       "  7   170.926804  352.633301  329.841705  490.291870    0.615162     45   \n",
       "  8   340.597046  360.770782  496.748352  459.269867    0.565109     48   \n",
       "  9   164.717621    1.248515  334.192230  162.037277    0.280073     60   \n",
       "  10  176.337723  168.463058  330.450958  328.597046    0.256655     60   \n",
       "  \n",
       "              name  \n",
       "  0            cup  \n",
       "  1            cup  \n",
       "  2         banana  \n",
       "  3       sandwich  \n",
       "  4          spoon  \n",
       "  5       sandwich  \n",
       "  6            cup  \n",
       "  7           bowl  \n",
       "  8       sandwich  \n",
       "  9   dining table  \n",
       "  10  dining table  ,\n",
       "  'caption': ['The small sandwich with salami on a honey bun.',\n",
       "   'The sandwich made from a roll and piece of salami.'],\n",
       "  'bbox_target': [339.33, 17.79, 160.67, 132.58]},\n",
       " 795: {'image_emb': tensor([[ 0.6069,  0.2097, -0.1453,  ...,  0.6196, -0.1643,  0.4380],\n",
       "          [-0.0930,  0.4807, -0.0469,  ...,  1.2246,  0.3604,  0.2494],\n",
       "          [ 0.4526,  0.0929, -0.3257,  ...,  0.7153, -0.1948,  0.4014],\n",
       "          [-0.3662,  0.1497, -0.1388,  ...,  1.3320, -0.0584,  0.0506],\n",
       "          [-0.3735,  0.4846, -0.2316,  ...,  1.4141, -0.1057, -0.1632],\n",
       "          [ 0.4470, -0.1823, -0.3306,  ...,  0.7578,  0.0817,  0.3347]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3042, -0.0642, -0.3545,  ...,  0.1440, -0.5156, -0.3223],\n",
       "          [-0.3511,  0.1456, -0.3450,  ...,  0.1064, -0.3169, -0.3862]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.6161e-06, 3.7408e-04, 1.1146e-05, 8.6129e-05, 9.9951e-01, 8.4043e-06],\n",
       "          [1.9073e-06, 1.4651e-04, 3.3975e-06, 1.5116e-04, 9.9951e-01, 2.1458e-06]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    51.150467  119.522964  284.018555  312.441284    0.913774     48   \n",
       "  1    97.541313    0.000000  203.549561  121.921417    0.912671     41   \n",
       "  2   182.620544  205.889801  360.628906  356.285004    0.875867     48   \n",
       "  3     0.509609    0.898544   88.038940  221.601318    0.745573     39   \n",
       "  4   483.675537  268.403870  585.700684  382.272644    0.701531     42   \n",
       "  5     8.763611   21.027695  638.685059  419.662659    0.652650     60   \n",
       "  6   560.754333  237.309204  640.000000  408.723328    0.620236     42   \n",
       "  7   550.202820  237.883606  640.000000  423.860596    0.445765     44   \n",
       "  8   544.618835  270.453827  639.352600  397.017914    0.347377     43   \n",
       "  9     9.577129    0.000000   94.496780   52.383636    0.304029      0   \n",
       "  10  614.303955  109.880737  639.814697  174.131012    0.251817     45   \n",
       "  \n",
       "              name  \n",
       "  0       sandwich  \n",
       "  1            cup  \n",
       "  2       sandwich  \n",
       "  3         bottle  \n",
       "  4           fork  \n",
       "  5   dining table  \n",
       "  6           fork  \n",
       "  7          spoon  \n",
       "  8          knife  \n",
       "  9         person  \n",
       "  10          bowl  ,\n",
       "  'caption': ['Two forks and a spoon on a table',\n",
       "   'a fork on top of another and a spoon beside food'],\n",
       "  'bbox_target': [562.3, 236.05, 76.76, 184.23]},\n",
       " 796: {'image_emb': tensor([[-0.2681, -0.0173, -0.2137,  ...,  0.7480,  0.1582,  0.1453],\n",
       "          [-0.3948,  0.1041, -0.0655,  ...,  1.1943, -0.0411,  0.2595],\n",
       "          [-0.3086,  0.1455, -0.1368,  ...,  0.6255, -0.0864,  0.0891],\n",
       "          ...,\n",
       "          [-0.3530,  0.2169, -0.1164,  ...,  0.9966, -0.0573,  0.0843],\n",
       "          [ 0.1414,  0.0665, -0.3054,  ...,  1.1182,  0.0693,  0.4795],\n",
       "          [ 0.0986,  0.3298, -0.4114,  ...,  0.6538, -0.0096,  0.1174]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0872,  0.0036, -0.4209,  ..., -0.5938, -0.4370, -0.3716],\n",
       "          [-0.0872,  0.0036, -0.4209,  ..., -0.5938, -0.4370, -0.3716]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.9518e-05, 5.9605e-08, 3.1519e-01, 1.1237e-01, 3.5763e-07, 1.3704e-03,\n",
       "           4.9639e-04, 3.5763e-06, 5.7080e-01],\n",
       "          [3.9518e-05, 5.9605e-08, 3.1519e-01, 1.1237e-01, 3.5763e-07, 1.3704e-03,\n",
       "           4.9639e-04, 3.5763e-06, 5.7080e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   314.390747   29.395462  501.068054  297.139465    0.944709      0   \n",
       "  1   384.737122  364.796753  433.701660  423.361084    0.900999     41   \n",
       "  2     0.796692    1.647659  302.941376  418.844604    0.900524      0   \n",
       "  3   256.608887  280.785889  395.729919  411.569641    0.900078     55   \n",
       "  4   269.512115   85.217773  351.618500  293.505005    0.898774     56   \n",
       "  5   375.370575  202.495880  638.767578  419.530762    0.865040     60   \n",
       "  6   122.603088    1.445267  303.803894  290.832153    0.844397     56   \n",
       "  7   593.972778  216.545197  639.979980  269.536469    0.783646      0   \n",
       "  8   478.199585  322.780182  525.428406  365.287140    0.692906     41   \n",
       "  9   619.313660  267.779236  640.000000  320.885559    0.685034     41   \n",
       "  10  424.844666  312.695312  497.423828  326.207336    0.338775     42   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1            cup  \n",
       "  2         person  \n",
       "  3           cake  \n",
       "  4          chair  \n",
       "  5   dining table  \n",
       "  6          chair  \n",
       "  7         person  \n",
       "  8            cup  \n",
       "  9            cup  \n",
       "  10          fork  ,\n",
       "  'caption': ['A man holding a birthday cake.',\n",
       "   'A man holding a birthday cake.'],\n",
       "  'bbox_target': [0.95, 1.91, 294.42, 417.33]},\n",
       " 797: {'image_emb': tensor([[-0.3130,  0.3604, -0.0809,  ...,  1.3613, -0.0141, -0.1570],\n",
       "          [-0.1273,  0.2803, -0.2449,  ...,  0.8945, -0.0594, -0.0984],\n",
       "          [-0.0136,  0.2659, -0.2507,  ...,  0.7637,  0.3042, -0.0016]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2037, -0.0374, -0.1317,  ..., -0.0064, -0.3267, -0.1554],\n",
       "          [-0.4280, -0.0502, -0.0114,  ..., -0.6025, -0.0459,  0.0813]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.8828e-01, 2.5034e-04, 1.1330e-02],\n",
       "          [9.2041e-01, 2.2675e-02, 5.7037e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  433.056366  289.857788  601.231201  476.560730    0.893859     56   \n",
       "  1  265.666260  409.891785  548.745300  477.281799    0.742033     56   \n",
       "  2  436.566132  238.468384  467.482452  302.570251    0.580993     75   \n",
       "  3  541.979004  263.803772  597.182129  296.572388    0.525246     56   \n",
       "  4    0.000000    0.000000  606.433472  471.975647    0.435473     58   \n",
       "  5  406.429901  284.844849  542.927612  418.458496    0.275046     60   \n",
       "  \n",
       "             name  \n",
       "  0         chair  \n",
       "  1         chair  \n",
       "  2          vase  \n",
       "  3         chair  \n",
       "  4  potted plant  \n",
       "  5  dining table  ,\n",
       "  'caption': ['A white chair nearest to the entrance of the dining room.',\n",
       "   'The white chair in the distance.'],\n",
       "  'bbox_target': [431.48, 289.03, 174.46, 184.78]},\n",
       " 798: {'image_emb': tensor([[ 1.8823e-01, -2.2546e-01, -2.2253e-01,  ...,  6.6284e-02,\n",
       "           -1.2573e-01, -1.1157e-01],\n",
       "          [ 2.4670e-01,  3.6719e-01, -7.0801e-02,  ...,  8.9502e-01,\n",
       "            2.0020e-01, -1.3354e-01],\n",
       "          [-4.1821e-01,  2.9028e-01, -1.2369e-03,  ...,  3.1616e-01,\n",
       "           -5.4836e-04, -3.8770e-01],\n",
       "          ...,\n",
       "          [-2.9785e-01,  2.3621e-01, -5.1086e-02,  ...,  4.7485e-01,\n",
       "           -2.2021e-01,  3.2928e-02],\n",
       "          [ 2.1289e-01, -4.1046e-03, -3.1738e-01,  ...,  9.9316e-01,\n",
       "            7.9163e-02,  5.9082e-02],\n",
       "          [ 3.1830e-02, -1.1212e-01, -1.0571e-01,  ...,  2.1155e-01,\n",
       "            9.3506e-02, -1.8494e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2242, -0.1225, -0.2007,  ..., -0.4082, -0.0604, -0.0568],\n",
       "          [-0.0350, -0.1671,  0.0287,  ...,  0.1371, -0.1816,  0.0204]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.0723e-01, 4.2081e-04, 4.8904e-03, 2.6047e-05, 6.3777e-06, 1.8463e-02,\n",
       "           7.8082e-06, 1.7810e-04, 3.4676e-03, 6.4492e-05, 5.6543e-01],\n",
       "          [1.7444e-01, 1.7881e-07, 1.8097e-02, 1.1921e-07, 1.1921e-07, 1.0366e-03,\n",
       "           7.6890e-06, 1.7881e-07, 2.7657e-05, 1.0133e-06, 8.0664e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   174.126404   72.313339  535.629150  450.262512    0.948001      5   \n",
       "  1    55.695839  292.285706  189.325867  390.845337    0.923663      2   \n",
       "  2   532.366943  189.130463  640.000000  342.698639    0.917072      5   \n",
       "  3   316.065155  265.019409  368.548248  435.446411    0.884482      0   \n",
       "  4   605.980835  278.942719  636.288452  347.150482    0.843652      0   \n",
       "  5   118.562073  210.968231  188.972351  305.088776    0.803013      5   \n",
       "  6   582.272217  206.086548  601.306641  249.667236    0.775158      9   \n",
       "  7   386.410919  276.655426  424.975006  320.682281    0.774108      0   \n",
       "  8   486.653992  400.834900  639.309265  491.044250    0.721317      2   \n",
       "  9     0.000000  282.103088   25.496658  388.376099    0.719225      0   \n",
       "  10  603.739563  222.065125  619.231018  249.744263    0.673286      9   \n",
       "  11   29.991106  257.321503   40.713463  284.869293    0.567479      9   \n",
       "  12  214.306534  296.120605  228.721420  313.732788    0.550674      0   \n",
       "  13   49.358959  300.972290   63.970028  338.071289    0.455716      0   \n",
       "  14   22.145164  315.471405   43.593979  341.543060    0.376441      2   \n",
       "  15   63.375008  297.910706   74.707756  330.122070    0.312630      0   \n",
       "  \n",
       "               name  \n",
       "  0             bus  \n",
       "  1             car  \n",
       "  2             bus  \n",
       "  3          person  \n",
       "  4          person  \n",
       "  5             bus  \n",
       "  6   traffic light  \n",
       "  7          person  \n",
       "  8             car  \n",
       "  9          person  \n",
       "  10  traffic light  \n",
       "  11  traffic light  \n",
       "  12         person  \n",
       "  13         person  \n",
       "  14            car  \n",
       "  15         person  ,\n",
       "  'caption': ['A back of the bus to the right of the picture.',\n",
       "   'A red double-decker bus behind a street light'],\n",
       "  'bbox_target': [533.96, 187.61, 106.04, 159.86]},\n",
       " 799: {'image_emb': tensor([[-0.1208,  0.4229, -0.3804,  ...,  0.8999, -0.0459, -0.0148],\n",
       "          [ 0.0625,  0.3364, -0.1016,  ...,  0.2874, -0.0681,  0.1309],\n",
       "          [-0.1138,  0.2349, -0.1204,  ...,  1.2939,  0.3225, -0.1240],\n",
       "          [-0.1041,  0.2457, -0.1185,  ...,  0.3535, -0.2151,  0.1733]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1659, -0.3928, -0.0813,  ...,  0.0079, -0.1816, -0.0380],\n",
       "          [ 0.1276, -0.0462,  0.2135,  ...,  0.0726, -0.0041, -0.2268]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.2236e-02, 1.1192e-02, 2.2161e-04, 9.4629e-01],\n",
       "          [9.8926e-01, 6.0081e-04, 1.0610e-04, 1.0010e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class name\n",
       "  0   39.390549  122.963600  164.155914  304.686890    0.922487      5  bus\n",
       "  1  234.620361  118.318184  426.718262  328.209656    0.912220      5  bus\n",
       "  2  179.806030  149.014832  241.540405  247.049652    0.861141      5  bus\n",
       "  3  163.139771  191.851074  185.286377  244.836029    0.292823      5  bus,\n",
       "  'caption': ['THE WHITE AND RED BUS IS STANDING IN FRONT OF A BLUE BUS',\n",
       "   'Red and orange bus.'],\n",
       "  'bbox_target': [37.56, 125.33, 125.69, 173.36]},\n",
       " 800: {'image_emb': tensor([[ 0.1921,  0.1906, -0.3188,  ...,  1.3223, -0.0714, -0.0786],\n",
       "          [-0.1030,  0.0170, -0.3083,  ...,  1.1543,  0.1521, -0.0435],\n",
       "          [ 0.2399,  0.5552, -0.2517,  ...,  1.0176,  0.2561, -0.1284],\n",
       "          [-0.1809,  0.2389, -0.3308,  ...,  1.4756, -0.1543, -0.0933],\n",
       "          [ 0.3621,  0.1459, -0.3005,  ...,  0.5410,  0.1428,  0.0881]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0459,  0.0481, -0.2637,  ..., -0.4937,  0.1508, -0.2365],\n",
       "          [ 0.1288,  0.5347, -0.4604,  ...,  0.1078, -0.0150,  0.0913]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.9170e-01, 1.2048e-01, 1.8478e-02, 1.4771e-01, 2.2168e-01],\n",
       "          [9.2480e-01, 1.6251e-03, 1.4067e-05, 7.3547e-02, 1.8358e-05]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.379539    0.000000  190.449371  293.667694    0.927002      0   \n",
       "  1    0.000000  279.888428  314.343811  474.154358    0.896905     60   \n",
       "  2  172.610031  192.819458  579.573914  473.573486    0.887246     16   \n",
       "  3  390.341675  282.153687  639.779663  474.744446    0.858194      0   \n",
       "  4  550.464294   14.334419  611.717468   53.821899    0.601198     45   \n",
       "  5  524.575256    2.582550  580.008606   41.395386    0.472960     45   \n",
       "  6  475.460938  129.840424  525.836304  183.539459    0.444580     75   \n",
       "  7  617.563782    0.234222  639.547424   67.670090    0.374723     40   \n",
       "  8  630.353210  193.849182  639.941956  221.611633    0.309000     45   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1  dining table  \n",
       "  2           dog  \n",
       "  3        person  \n",
       "  4          bowl  \n",
       "  5          bowl  \n",
       "  6          vase  \n",
       "  7    wine glass  \n",
       "  8          bowl  ,\n",
       "  'caption': ['The man on the left.',\n",
       "   'The elbow of a man in a brown long-sleeve shirt is on the table.'],\n",
       "  'bbox_target': [0.0, 3.34, 190.5, 292.27]},\n",
       " 801: {'image_emb': tensor([[ 0.1571,  0.5933, -0.3052,  ...,  0.6665,  0.1581,  0.2373],\n",
       "          [-0.3213, -0.1370, -0.2217,  ...,  0.2754,  0.4617,  0.5039],\n",
       "          [-0.0283,  0.0261, -0.0072,  ...,  0.4666,  0.1904,  0.1399],\n",
       "          [ 0.0236,  0.3293, -0.2484,  ...,  0.9219,  0.4233,  0.2603],\n",
       "          [-0.0019, -0.2563, -0.1395,  ...,  0.3816,  0.2374,  0.4807]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3652,  0.0231, -0.2382,  ..., -0.2024, -0.0027, -0.2815],\n",
       "          [ 0.1466, -0.2563, -0.1110,  ..., -0.0978, -0.1360, -0.0216]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0334, 0.0595, 0.9019, 0.0025, 0.0030],\n",
       "          [0.0313, 0.0470, 0.7588, 0.1494, 0.0137]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   18.909233   63.003357  201.699371  418.379089    0.944390      0   \n",
       "  1  187.886627   20.823593  585.688354  419.113220    0.849792      0   \n",
       "  2  223.220978  129.572983  470.026215  420.855347    0.767554      0   \n",
       "  3   58.343681   93.252274  275.003082  202.713623    0.760539     78   \n",
       "  4  178.236786  168.514771  247.080383  270.760376    0.655813     56   \n",
       "  5  163.531204  173.705078  639.519836  420.639099    0.626250     57   \n",
       "  6  537.246460  173.658386  639.565552  420.292297    0.349267     57   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2      person  \n",
       "  3  hair drier  \n",
       "  4       chair  \n",
       "  5       couch  \n",
       "  6       couch  ,\n",
       "  'caption': ['A baby wearing pink, with an adult helping her with her hair.',\n",
       "   'girl is getting her hair fixed'],\n",
       "  'bbox_target': [226.35, 129.89, 241.63, 289.38]},\n",
       " 802: {'image_emb': tensor([[-0.1155,  0.2411, -0.0681,  ...,  0.5996, -0.2773, -0.1576],\n",
       "          [-0.1278,  0.2025, -0.0761,  ...,  1.2227,  0.1656,  0.1986],\n",
       "          [-0.0040,  0.5381,  0.1071,  ...,  1.0664, -0.3340, -0.3804],\n",
       "          [-0.1996,  0.0819,  0.2279,  ...,  0.8599,  0.3438,  0.1580],\n",
       "          [ 0.1927,  0.2642,  0.2034,  ...,  0.5942, -0.1509,  0.0479]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1982,  0.1790, -0.3347,  ...,  0.3669, -0.3069,  0.0095],\n",
       "          [-0.2323, -0.1384, -0.2534,  ..., -0.5513,  0.0147, -0.1959]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.0468e-04, 8.9798e-03, 4.1122e-03, 7.4756e-01, 2.3901e-01],\n",
       "          [2.0984e-01, 7.2122e-06, 4.1542e-03, 1.8616e-02, 7.6758e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  119.934631  251.523438  638.676453  475.046692    0.926191     60   \n",
       "  1  158.197922  239.706055  202.515610  302.453125    0.906104     41   \n",
       "  2  270.528137  265.435547  376.893127  330.928589    0.878996     55   \n",
       "  3    0.641762  228.684631  129.827087  476.490540    0.853781      0   \n",
       "  4  185.674393  114.353607  445.146118  272.452728    0.667784      0   \n",
       "  5   93.811020   87.601349  355.262146  288.005890    0.653386     56   \n",
       "  6  203.977753  174.884674  279.145630  218.765411    0.446746     56   \n",
       "  7   95.992279  455.577637  145.068314  479.407715    0.284800     56   \n",
       "  \n",
       "             name  \n",
       "  0  dining table  \n",
       "  1           cup  \n",
       "  2          cake  \n",
       "  3        person  \n",
       "  4        person  \n",
       "  5         chair  \n",
       "  6         chair  \n",
       "  7         chair  ,\n",
       "  'caption': ['a boy violate color t shirt',\n",
       "   'the boy blowing out the candle.'],\n",
       "  'bbox_target': [248.47, 112.43, 204.23, 159.69]},\n",
       " 803: {'image_emb': tensor([[ 0.1013,  0.2700,  0.1070,  ...,  0.6997, -0.1826, -0.0791],\n",
       "          [-0.0097,  0.4727,  0.0681,  ...,  1.0645, -0.0084, -0.1642],\n",
       "          [ 0.3975,  0.1980, -0.0979,  ...,  0.5728, -0.3254,  0.0793]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2766, -0.2262, -0.0116,  ...,  0.0604, -0.1451, -0.0570],\n",
       "          [ 0.0267,  0.2190,  0.0028,  ...,  0.2739,  0.2474,  0.3345]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6899, 0.2137, 0.0963],\n",
       "          [0.5601, 0.4292, 0.0107]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class name\n",
       "  0  258.700775   98.604004  524.631104  344.558167    0.921320      5  bus\n",
       "  1   59.612167  146.113098  211.929367  293.681763    0.907198      5  bus\n",
       "  2  498.813019  154.797852  638.856628  277.665833    0.420207      5  bus\n",
       "  3  225.346375  198.560181  256.229126  219.626282    0.263034      2  car,\n",
       "  'caption': ['PARKED BUS ON LEFT',\n",
       "   'A bus with no destination displayed in its electronic ticker, behind a pole.'],\n",
       "  'bbox_target': [56.61, 143.21, 156.41, 160.25]},\n",
       " 804: {'image_emb': tensor([[-0.2039,  0.4573,  0.0451,  ...,  0.4458,  0.2001, -0.1858],\n",
       "          [-0.5444,  0.0489, -0.0700,  ...,  0.6963, -0.1191,  0.2146],\n",
       "          [-0.1494,  0.0977, -0.0025,  ...,  1.1348,  0.0088, -0.1013],\n",
       "          ...,\n",
       "          [-0.0201,  0.3479, -0.4973,  ...,  0.7969,  0.2277,  0.2678],\n",
       "          [ 0.0017,  0.5977,  0.0467,  ...,  1.4414, -0.0720,  0.0387],\n",
       "          [-0.1019,  0.4595,  0.0221,  ...,  0.3738,  0.2676, -0.0891]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2352,  0.5640, -0.3125,  ..., -0.1309, -0.0654,  0.0366],\n",
       "          [ 0.1938,  0.3577,  0.0471,  ...,  0.5220,  0.1580,  0.2264]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.8014e-06, 1.2517e-06, 5.6386e-05, 1.0610e-05, 1.8663e-03, 4.0364e-04,\n",
       "           9.9756e-01, 5.3644e-07],\n",
       "          [5.6624e-06, 6.8665e-05, 1.1253e-03, 5.6601e-04, 6.0844e-03, 9.0283e-01,\n",
       "           8.9355e-02, 3.3379e-06]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  136.943771   30.474792  490.287048  419.508545    0.917695      0   \n",
       "  1  320.954285  160.018982  365.423157  323.195984    0.880442     27   \n",
       "  2  401.639069    0.000000  640.000000  296.023438    0.842296      0   \n",
       "  3   55.231392  337.135620   87.735619  423.493347    0.833726      0   \n",
       "  4   86.222900  327.337402  126.848221  422.783508    0.745382      0   \n",
       "  5  202.153488  294.276154  247.468521  423.527588    0.739188     26   \n",
       "  6  108.008331  276.232391  216.552002  422.580017    0.708004     26   \n",
       "  7  485.908691  238.553467  639.580811  419.819214    0.688035     77   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1         tie  \n",
       "  2      person  \n",
       "  3      person  \n",
       "  4      person  \n",
       "  5     handbag  \n",
       "  6     handbag  \n",
       "  7  teddy bear  ,\n",
       "  'caption': [\"a brown jacket stuffed into the handbag, which is draped on the person's arm\",\n",
       "   \"The purple coat hanging out of the bag on he man's arm.\"],\n",
       "  'bbox_target': [109.48, 280.21, 107.03, 144.79]},\n",
       " 805: {'image_emb': tensor([[-0.0946,  0.4639, -0.3108,  ...,  0.9375, -0.2460, -0.0272],\n",
       "          [-0.4487,  0.3386, -0.2399,  ...,  0.7744, -0.0322,  0.1115],\n",
       "          [ 0.2893,  0.0720, -0.3323,  ...,  1.1143,  0.2274, -0.1307],\n",
       "          [-0.2303,  0.3489,  0.0181,  ...,  0.3232, -0.3662,  0.1559]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0513, -0.0883, -0.2485,  ...,  0.5889,  0.1322, -0.1326],\n",
       "          [-0.1085,  0.0525, -0.1583,  ...,  0.3828,  0.0549, -0.3035]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.4570e-01, 5.4395e-01, 4.9324e-03, 1.0547e-01],\n",
       "          [2.5830e-01, 5.0586e-01, 5.3930e-04, 2.3523e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  412.058563  127.534760  501.601105  365.734039    0.917327      0  person\n",
       "  1  423.278137   78.204666  497.826050  192.187195    0.812709     30    skis\n",
       "  2  343.446564  214.513885  379.072418  293.341583    0.755141      0  person,\n",
       "  'caption': ['cross country skiier',\n",
       "   'A person cross country skiing behind another person.'],\n",
       "  'bbox_target': [411.73, 129.6, 89.95, 238.12]},\n",
       " 806: {'image_emb': tensor([[ 0.0690,  0.1271,  0.0811,  ...,  0.8560, -0.0818, -0.2739],\n",
       "          [ 0.0145,  0.3369,  0.1272,  ...,  1.1436, -0.0291, -0.1597],\n",
       "          [ 0.0739,  0.1930,  0.1124,  ...,  1.0146, -0.1864, -0.2172]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1393, -0.1484,  0.2192,  ..., -0.2878, -0.2722, -0.3140],\n",
       "          [ 0.0229,  0.1416,  0.1628,  ..., -0.1429, -0.4238, -0.5146]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6694, 0.1589, 0.1719],\n",
       "          [0.5439, 0.1510, 0.3052]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class  \\\n",
       "  0   89.259567  45.091293  534.839783  410.494202    0.833341     53   \n",
       "  1    0.818390   0.022156  287.838531  396.490967    0.731111      0   \n",
       "  2    2.804169  32.712936  640.000000  480.000000    0.561364     60   \n",
       "  3  476.987885   0.000000  556.216675   30.257172    0.499641     45   \n",
       "  \n",
       "             name  \n",
       "  0         pizza  \n",
       "  1        person  \n",
       "  2  dining table  \n",
       "  3          bowl  ,\n",
       "  'caption': ['A pizza.', 'A multi topping pizza.'],\n",
       "  'bbox_target': [92.76, 71.19, 402.34, 325.75]},\n",
       " 807: {'image_emb': tensor([[-0.1526,  0.4941, -0.1566,  ...,  1.2139,  0.1439,  0.0549],\n",
       "          [-0.0448,  0.6543, -0.1622,  ...,  1.0127,  0.0075,  0.0419],\n",
       "          [-0.1980,  0.6870, -0.1600,  ...,  1.0371, -0.0569,  0.0346]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1989,  0.0960,  0.1622,  ...,  0.3037, -0.0930, -0.0519],\n",
       "          [-0.1941,  0.1158,  0.1091,  ...,  0.3340, -0.0661, -0.0499]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.7353e-05, 1.1438e-01, 8.8574e-01],\n",
       "          [5.0068e-05, 9.9487e-02, 9.0039e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  290.189575  214.979706  452.230347  369.754730    0.890733     16   \n",
       "  1  212.006668    0.000000  461.126648  250.305573    0.881177      0   \n",
       "  2    0.000000   61.600418  433.220764  249.582275    0.644435      2   \n",
       "  3   48.869537   61.932510  638.879517  467.407715    0.630185      3   \n",
       "  4  227.366058   43.409645  246.417511   63.194305    0.566262      0   \n",
       "  5  239.928879   43.387642  254.032364   63.374207    0.466054      0   \n",
       "  6  296.145508  212.542358  447.801392  380.424927    0.340209      3   \n",
       "  \n",
       "           name  \n",
       "  0         dog  \n",
       "  1      person  \n",
       "  2         car  \n",
       "  3  motorcycle  \n",
       "  4      person  \n",
       "  5      person  \n",
       "  6  motorcycle  ,\n",
       "  'caption': ['A man on a motorcycle', 'A man on a motorcycle.'],\n",
       "  'bbox_target': [213.57, 2.16, 257.8, 244.85]},\n",
       " 808: {'image_emb': tensor([[-0.3350,  1.0967, -0.5083,  ...,  0.7524, -0.1714,  0.2002],\n",
       "          [-0.4648,  0.9346, -0.1478,  ...,  0.6973, -0.2474,  0.1299],\n",
       "          [ 0.2791,  0.3464,  0.1031,  ...,  0.5273,  0.0646,  0.2118],\n",
       "          [-0.2866,  0.7017, -0.2661,  ...,  0.9453, -0.1044,  0.0758]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0511,  0.3298, -0.3889,  ..., -0.4917, -0.0709, -0.2085],\n",
       "          [-0.1869,  0.4080, -0.2411,  ..., -0.6885, -0.3403, -0.0540]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.6910e-03, 4.1461e-04, 9.9268e-01, 5.8770e-05],\n",
       "          [6.0272e-02, 8.2855e-03, 9.1406e-01, 1.7548e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    1.166382  101.036530  361.920013  470.113403    0.951969      0   \n",
       "  1   91.164062   22.656708  609.559875  395.845978    0.910018     67   \n",
       "  2  255.084930  182.393616  361.404022  337.146301    0.749810      0   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1  cell phone  \n",
       "  2      person  ,\n",
       "  'caption': ['A woman holding a microphone and singing.',\n",
       "   \"A woman's image on a smartphone, with the caption underneath her of Becky McGray\"],\n",
       "  'bbox_target': [253.94, 180.71, 110.86, 161.07]},\n",
       " 809: {'image_emb': tensor([[ 0.0685,  0.4529, -0.1075,  ...,  0.3425,  0.1393,  0.3274],\n",
       "          [ 0.4160,  0.4395, -0.1030,  ...,  0.6577,  0.1327, -0.0803],\n",
       "          [-0.1661,  0.1750,  0.1198,  ...,  1.2568,  0.1213, -0.0845],\n",
       "          ...,\n",
       "          [ 0.0135,  0.1919, -0.0732,  ...,  0.6592,  0.0599, -0.1669],\n",
       "          [-0.1648,  0.3323, -0.0532,  ...,  0.2922, -0.0854,  0.1147],\n",
       "          [-0.0546,  0.1910, -0.1129,  ...,  0.6357,  0.2832,  0.2798]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2045, -0.0622,  0.3215,  ...,  0.0543, -0.0550, -0.1292],\n",
       "          [-0.1201, -0.4038,  0.0867,  ..., -0.4658, -0.0574, -0.2404]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.3032e-01, 1.3770e-01, 5.1953e-01, 4.7874e-03, 1.9145e-04, 5.9891e-04,\n",
       "           4.3154e-04, 6.3400e-03],\n",
       "          [1.5112e-01, 2.5317e-01, 5.4443e-01, 7.5264e-03, 1.1247e-04, 2.9297e-02,\n",
       "           7.1096e-04, 1.3840e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  321.885040    2.267265  499.540527  324.947357    0.952448      0    person\n",
       "  1    0.430948  131.842361  126.527115  327.212006    0.926134      0    person\n",
       "  2  125.441872  117.161835  224.408218  326.945435    0.881701      0    person\n",
       "  3  181.941010  140.112701  234.825775  206.871841    0.817947     62        tv\n",
       "  4  281.214539  116.188011  324.946381  163.367523    0.812081     62        tv\n",
       "  5  326.490845  259.074890  381.736145  312.987762    0.808364     28  suitcase\n",
       "  6   97.124718  161.888565  146.596191  328.067200    0.711770      0    person\n",
       "  7  189.934860  245.446960  299.573517  312.381226    0.596298     28  suitcase\n",
       "  8  207.948761  225.859924  252.354355  250.337601    0.343418     73      book\n",
       "  9  101.748466  161.784683  128.064697  196.921722    0.268552      0    person,\n",
       "  'caption': ['Man in bkack coat reading',\n",
       "   'A man looking at white piece of paper.'],\n",
       "  'bbox_target': [125.15, 116.77, 102.05, 209.32]},\n",
       " 810: {'image_emb': tensor([[-0.3726, -0.0190, -0.0952,  ...,  1.0107,  0.1890, -0.4570],\n",
       "          [ 0.1929,  0.1158, -0.7114,  ...,  1.3262,  0.1984, -0.1600],\n",
       "          [ 0.0709,  0.0332, -0.1686,  ...,  0.9883,  0.0051, -0.3113],\n",
       "          [-0.3298,  0.2712, -0.2163,  ...,  0.4365, -0.0532, -0.1049]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1013, -0.1037, -0.1038,  ...,  0.0036, -0.2325, -0.3677],\n",
       "          [-0.2893, -0.1689, -0.0771,  ...,  0.1109,  0.0713, -0.0866]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.5020e-01, 4.0703e-03, 7.6890e-05, 4.5837e-02],\n",
       "          [2.7686e-01, 6.8909e-02, 3.7909e-04, 6.5381e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  161.629913   63.749359  363.305817  413.869629    0.923693     17    horse\n",
       "  1  324.886627  118.505737  425.650421  416.292419    0.908849      0   person\n",
       "  2   35.881565  130.212921   55.690090  176.417328    0.863469      0   person\n",
       "  3  238.246521  115.075470  263.940918  142.368103    0.609702      0   person\n",
       "  4  194.917236  125.812302  210.236633  174.394287    0.607462     17    horse\n",
       "  5  206.581619  127.950439  220.373032  172.181580    0.570613      0   person\n",
       "  6  110.413269   98.657532  120.358582  109.020477    0.340980      0   person\n",
       "  7   17.366331   86.807419   28.995371   99.407578    0.302122      0   person\n",
       "  8  405.011993  271.163055  428.485260  304.382782    0.292825     26  handbag,\n",
       "  'caption': ['A horse with a red mask on its face',\n",
       "   'The horse walked by #4.'],\n",
       "  'bbox_target': [164.02, 63.78, 200.67, 350.94]},\n",
       " 811: {'image_emb': tensor([[-0.2194,  0.3010, -0.1749,  ...,  0.7539,  0.0643, -0.5264],\n",
       "          [-0.8169,  0.0011, -0.1593,  ...,  0.6533,  0.1096, -0.0704],\n",
       "          [-0.0569,  0.3113, -0.4109,  ...,  1.0947,  0.2512, -0.0726],\n",
       "          [ 0.4148, -0.1818, -0.3804,  ...,  0.7202,  0.0472, -0.0256],\n",
       "          [-0.5435,  0.1165, -0.3030,  ...,  1.0713, -0.1094, -0.0130],\n",
       "          [-0.6826,  0.1954, -0.2056,  ...,  0.7798,  0.0638, -0.3857]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2830, -0.0408, -0.3762,  ...,  0.4998, -0.0995, -0.5122],\n",
       "          [-0.5664, -0.1444, -0.4021,  ...,  0.3853, -0.4409, -0.7969]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7944e-01, 3.3379e-03, 3.2043e-04, 1.7285e-06, 7.2598e-05, 8.1689e-01],\n",
       "          [4.0576e-01, 2.7771e-03, 3.1161e-04, 6.2585e-06, 5.5552e-04, 5.9033e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   53.202225  299.156250  265.682190  638.947693    0.952116      0   \n",
       "  1   55.737030  266.287903  116.174896  367.508057    0.893816     38   \n",
       "  2   87.798203  286.323242  140.293533  465.367676    0.869322      0   \n",
       "  3  148.469650  183.768402  174.637177  228.641144    0.851917      0   \n",
       "  4  283.882568  364.941315  374.369446  431.607574    0.723202     13   \n",
       "  5   23.788437  412.376740   84.442886  438.336884    0.449664     24   \n",
       "  6  410.396729  366.954651  423.648315  395.196167    0.342068     39   \n",
       "  7    3.491173  417.423920   11.057396  439.019318    0.296828     39   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1  tennis racket  \n",
       "  2         person  \n",
       "  3         person  \n",
       "  4          bench  \n",
       "  5       backpack  \n",
       "  6         bottle  \n",
       "  7         bottle  ,\n",
       "  'caption': ['A man in a yellow hat playing tennis.',\n",
       "   'A man in a hard hat playing tennis.'],\n",
       "  'bbox_target': [53.21, 300.58, 217.17, 330.79]},\n",
       " 812: {'image_emb': tensor([[ 0.1700,  1.0146, -0.2864,  ...,  0.5649, -0.2313, -0.0856],\n",
       "          [-0.1339,  1.0615, -0.3762,  ...,  0.4766, -0.2871, -0.2861],\n",
       "          [ 0.1433,  1.0342, -0.1182,  ...,  0.9062, -0.2340, -0.0952],\n",
       "          [ 0.0544,  0.7754, -0.2729,  ...,  0.4546, -0.0230, -0.0122],\n",
       "          [ 0.0701,  0.9087, -0.1262,  ...,  0.5430,  0.0807, -0.1997]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0236,  0.7524, -0.6357,  ..., -0.1158, -0.5737, -0.4429],\n",
       "          [ 0.1494,  0.5698, -0.1708,  ...,  0.2111,  0.0353, -0.0086]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1395, 0.4504, 0.1581, 0.0521, 0.1998],\n",
       "          [0.0190, 0.7026, 0.2751, 0.0009, 0.0023]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class  \\\n",
       "  0   25.144905  32.871639   85.284843  169.278931    0.932281     67   \n",
       "  1  317.161469  62.278721  379.785797  178.271988    0.930094     67   \n",
       "  2  202.339661  15.740480  288.384308  165.702026    0.927083     67   \n",
       "  3  114.665009  36.331444  170.393005  154.995850    0.919110     67   \n",
       "  \n",
       "           name  \n",
       "  0  cell phone  \n",
       "  1  cell phone  \n",
       "  2  cell phone  \n",
       "  3  cell phone  ,\n",
       "  'caption': ['The largest phone out of four.', 'A palm pilot.'],\n",
       "  'bbox_target': [207.0, 18.0, 82.0, 142.5]},\n",
       " 813: {'image_emb': tensor([[ 0.1300,  0.3040, -0.0561,  ...,  0.8809, -0.0791, -0.1936],\n",
       "          [ 0.1220,  0.0512, -0.3750,  ...,  1.1904, -0.1742, -0.0078],\n",
       "          [-0.1271,  0.1116,  0.4089,  ...,  0.6860, -0.0770, -0.1364],\n",
       "          ...,\n",
       "          [ 0.3562,  0.0283, -0.4304,  ...,  1.3545,  0.0477,  0.0781],\n",
       "          [ 0.3271, -0.0025, -0.5225,  ...,  0.7349, -0.0480, -0.1251],\n",
       "          [-0.0222,  0.0768,  0.4124,  ...,  0.4526, -0.1698,  0.0402]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0569,  0.2457,  0.1069,  ...,  0.1919, -0.2449, -0.0673],\n",
       "          [-0.1139, -0.1119,  0.0598,  ..., -0.0126, -0.0388, -0.1274]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.1826e-01, 1.0729e-06, 6.3184e-01, 1.1206e-05, 8.3447e-07, 5.3644e-07,\n",
       "           1.5002e-01],\n",
       "          [7.0068e-02, 2.3842e-07, 2.3328e-01, 2.8181e-04, 3.5763e-06, 5.9605e-07,\n",
       "           6.9629e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    38.343124  173.419876  241.671402  284.263062    0.932914     19   \n",
       "  1     0.000000  102.648026   46.527382  217.405731    0.906739      0   \n",
       "  2   179.138550  171.750122  483.861816  337.428040    0.901812     19   \n",
       "  3    72.682098  155.714706  203.717865  208.743134    0.764899     19   \n",
       "  4    37.497978  104.676544   80.143272  185.764740    0.752218      0   \n",
       "  5   123.409058   96.259201  142.057770  153.775314    0.701168      0   \n",
       "  6   151.736511   98.188705  169.247681  150.558884    0.666718      0   \n",
       "  7   344.692841  105.593063  374.500092  188.363495    0.603718      0   \n",
       "  8    73.514320  151.322418  202.540131  208.277069    0.535092      3   \n",
       "  9   342.761353  136.090317  385.671692  192.062988    0.485361      3   \n",
       "  10  326.271851  100.630829  350.132874  196.067261    0.343990      0   \n",
       "  \n",
       "            name  \n",
       "  0          cow  \n",
       "  1       person  \n",
       "  2          cow  \n",
       "  3          cow  \n",
       "  4       person  \n",
       "  5       person  \n",
       "  6       person  \n",
       "  7       person  \n",
       "  8   motorcycle  \n",
       "  9   motorcycle  \n",
       "  10      person  ,\n",
       "  'caption': ['a brown cow laying down on a pile of hay in front of a pole',\n",
       "   'black cow laying on a pile of garbage'],\n",
       "  'bbox_target': [181.76, 175.96, 301.61, 172.47]},\n",
       " 814: {'image_emb': tensor([[ 0.1265,  0.4001,  0.3081,  ...,  0.7280, -0.2773,  0.1611],\n",
       "          [-0.2705,  0.0366, -0.1656,  ...,  0.4795,  0.0082, -0.0268],\n",
       "          [-0.1774, -0.1252, -0.3042,  ...,  0.4724, -0.1458,  0.1605],\n",
       "          ...,\n",
       "          [ 0.0694,  0.0463,  0.0631,  ...,  0.4241, -0.0594,  0.1576],\n",
       "          [-0.3110, -0.1266,  0.2273,  ..., -0.2751, -0.5083,  0.3274],\n",
       "          [-0.4736, -0.1145,  0.0716,  ..., -0.2021, -0.4072,  0.1925]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.4971,  0.3918,  0.0839,  ..., -0.2487, -0.0396, -0.1434],\n",
       "          [ 0.0831,  0.0121, -0.2257,  ...,  0.0853, -0.1736,  0.0137]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0000e+00, 4.1723e-07, 2.9802e-07, 1.1325e-05, 2.5034e-06, 1.2517e-06,\n",
       "           1.1772e-04, 1.0133e-06],\n",
       "          [3.1555e-02, 1.5144e-02, 2.8290e-02, 1.0633e-03, 2.5809e-05, 1.1384e-04,\n",
       "           5.6836e-01, 3.5547e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  159.276184  160.146667  397.869690  375.361816    0.947724     41   \n",
       "  1  394.416840  159.666290  501.774811  362.658905    0.906362     77   \n",
       "  2  456.422089  136.445526  507.422455  192.519196    0.790477     77   \n",
       "  3  199.031525  125.362823  258.309204  176.348572    0.766083     54   \n",
       "  4  248.504807  122.779663  313.121216  154.239319    0.761551     54   \n",
       "  5  514.607788  151.622009  569.804077  253.709229    0.740239     41   \n",
       "  6    1.713226   90.853409  636.256714  443.727905    0.728437     60   \n",
       "  7  130.034348  165.793488  165.179916  228.065765    0.533673     77   \n",
       "  \n",
       "             name  \n",
       "  0           cup  \n",
       "  1    teddy bear  \n",
       "  2    teddy bear  \n",
       "  3         donut  \n",
       "  4         donut  \n",
       "  5           cup  \n",
       "  6  dining table  \n",
       "  7    teddy bear  ,\n",
       "  'caption': ['A white cup with handle',\n",
       "   \"A coffee cup near the teddy's cookies\"],\n",
       "  'bbox_target': [157.41, 161.95, 244.18, 213.91]},\n",
       " 815: {'image_emb': tensor([[-0.2207,  0.2487, -0.1039,  ...,  1.4121,  0.2571,  0.0087],\n",
       "          [ 0.3564,  0.4397, -0.2036,  ...,  0.9380, -0.1578, -0.0721],\n",
       "          [ 0.2202,  0.3267, -0.1849,  ...,  0.7026,  0.0589, -0.2588]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0462,  0.3484, -0.3408,  ..., -0.0677,  0.0754, -0.3357],\n",
       "          [-0.1011, -0.1191, -0.1250,  ..., -0.2717, -0.0609, -0.0251]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0225, 0.0632, 0.9141],\n",
       "          [0.0092, 0.3179, 0.6729]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  196.879089  218.299210  380.732483  279.984406    0.907403     25  umbrella\n",
       "  1  203.027374  273.406433  318.285614  618.004700    0.831173      0    person\n",
       "  2    0.778748  381.005493  101.164886  450.562500    0.539957      8      boat,\n",
       "  'caption': ['The silhouette of a man holding an umbrella.',\n",
       "   'A MAN WATCHING SUNSET'],\n",
       "  'bbox_target': [207.1, 275.06, 113.62, 333.66]},\n",
       " 816: {'image_emb': tensor([[-0.1242, -0.0313, -0.0362,  ...,  0.5986, -0.0142,  0.4207],\n",
       "          [ 0.1285,  0.1132, -0.1920,  ..., -0.0325,  0.2786,  0.3792],\n",
       "          [-0.0897, -0.1122, -0.0722,  ...,  0.6094, -0.0980,  0.4507]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0026, -0.0098,  0.3081,  ...,  0.2175, -0.2556, -0.0496],\n",
       "          [ 0.0102,  0.0812,  0.3167,  ..., -0.0812, -0.1304, -0.2817]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.4434, 0.2373, 0.3193],\n",
       "          [0.1140, 0.7202, 0.1658]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class  \\\n",
       "  0   15.360465   5.616043  577.642761  610.484741    0.887338      3   \n",
       "  1  361.562805  42.193859  611.602234  501.972626    0.839693      3   \n",
       "  2    0.670685   0.864661  349.528778  136.970886    0.551957      3   \n",
       "  \n",
       "           name  \n",
       "  0  motorcycle  \n",
       "  1  motorcycle  \n",
       "  2  motorcycle  ,\n",
       "  'caption': ['The motorcycle that has the number 42 on it.',\n",
       "   'A bike with number plate 42'],\n",
       "  'bbox_target': [2.74, 13.88, 588.68, 588.67]},\n",
       " 817: {'image_emb': tensor([[-0.1006,  0.2327, -0.3118,  ...,  1.0889,  0.2756,  0.3557],\n",
       "          [ 0.0638, -0.0201,  0.1334,  ...,  1.3154,  0.2406,  0.1304],\n",
       "          [ 0.1324, -0.1819, -0.0408,  ...,  1.1162,  0.1935,  0.2407],\n",
       "          [ 0.0299, -0.6494,  0.1266,  ...,  0.8647,  0.4941, -0.1874]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2620,  0.1151, -0.2861,  ..., -0.1716,  0.3508,  0.2318],\n",
       "          [-0.3970, -0.3718,  0.0975,  ...,  0.0098,  0.3689, -0.0545]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.2531e-03, 5.7030e-03, 6.2891e-01, 3.6401e-01],\n",
       "          [5.3644e-06, 3.8550e-01, 5.7861e-01, 3.5858e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0    0.490860   49.939575  255.845642  637.907959    0.950478      0    person\n",
       "  1  245.185425  187.480682  384.848328  504.451813    0.899968     27       tie\n",
       "  2  213.776886   13.413269  439.050232  638.903137    0.836143      0    person\n",
       "  3  245.733551  180.422653  386.173615  506.359070    0.451173      0    person\n",
       "  4  189.729828  425.038971  266.416595  467.891388    0.387101     76  scissors,\n",
       "  'caption': ['A smiling, balding gentleman standing beside the young woman cutting his red-and-gold tie.',\n",
       "   'The man who is holding money and having his tie cut.'],\n",
       "  'bbox_target': [221.09, 11.64, 217.77, 618.39]},\n",
       " 818: {'image_emb': tensor([[ 1.2817e-01,  3.1470e-01, -2.2449e-01,  ...,  6.1816e-01,\n",
       "            1.1420e-01,  3.8013e-01],\n",
       "          [ 8.9417e-02,  3.9819e-01, -1.6418e-01,  ...,  8.5596e-01,\n",
       "           -6.2286e-02,  1.9885e-01],\n",
       "          [-5.2738e-04,  6.9287e-01, -2.6074e-01,  ...,  7.8955e-01,\n",
       "            1.7688e-01,  5.0635e-01],\n",
       "          ...,\n",
       "          [-3.5938e-01,  8.8135e-01, -5.3564e-01,  ...,  8.7305e-01,\n",
       "            4.3511e-04,  1.6296e-01],\n",
       "          [ 2.1619e-01,  4.2578e-01, -1.4111e-01,  ...,  6.0107e-01,\n",
       "           -1.2201e-01, -2.6886e-02],\n",
       "          [ 1.7529e-01,  4.0161e-01, -2.3999e-01,  ...,  6.9873e-01,\n",
       "           -2.3755e-01,  7.2876e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3027,  0.1015, -0.1288,  ...,  0.1898, -0.1378, -0.1218],\n",
       "          [-0.5234, -0.0115,  0.1133,  ...,  0.3911, -0.5552, -0.0564]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.6591e-02, 8.1301e-05, 3.4213e-04, 9.0039e-01, 5.7556e-02, 3.1967e-03,\n",
       "           2.0313e-03],\n",
       "          [1.8501e-03, 8.2458e-02, 3.4580e-03, 8.8672e-01, 1.5676e-04, 9.5444e-03,\n",
       "           1.5991e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   273.566681  213.697769  487.796967  418.933777    0.941836     45   \n",
       "  1    17.416441    9.039823  218.029877  190.835938    0.917357     45   \n",
       "  2    34.588074  179.630890  274.781677  417.635681    0.912612     45   \n",
       "  3   215.721130   10.636410  444.935455  230.170990    0.906290     45   \n",
       "  4   245.142700  128.282013  291.660889  180.971329    0.791176     50   \n",
       "  5     0.000000   10.612973  496.380432  419.096710    0.755400     60   \n",
       "  6   355.461273  135.230438  406.717926  185.079010    0.670185     50   \n",
       "  7   287.090607  165.708847  312.126953  185.399979    0.645329     50   \n",
       "  8   322.952698  135.589371  366.563751  189.671631    0.642789     50   \n",
       "  9   287.184387   24.158903  323.255066   66.566757    0.577572     50   \n",
       "  10  340.627380   43.857136  373.932800   71.227013    0.558013     50   \n",
       "  11  404.287811  109.575180  434.237000  133.396698    0.527410     51   \n",
       "  12  370.402924   58.033360  397.032471   88.076530    0.510462     50   \n",
       "  13  237.487106   62.900352  263.473206   89.126678    0.427627     50   \n",
       "  14  299.487396  132.886749  334.902100  165.170471    0.354072     50   \n",
       "  15  264.217712   52.494144  294.622131   70.362999    0.334532     51   \n",
       "  16  376.345520  110.961914  390.498291  132.658722    0.299472     51   \n",
       "  \n",
       "              name  \n",
       "  0           bowl  \n",
       "  1           bowl  \n",
       "  2           bowl  \n",
       "  3           bowl  \n",
       "  4       broccoli  \n",
       "  5   dining table  \n",
       "  6       broccoli  \n",
       "  7       broccoli  \n",
       "  8       broccoli  \n",
       "  9       broccoli  \n",
       "  10      broccoli  \n",
       "  11        carrot  \n",
       "  12      broccoli  \n",
       "  13      broccoli  \n",
       "  14      broccoli  \n",
       "  15        carrot  \n",
       "  16        carrot  ,\n",
       "  'caption': ['A container of meat and broccoli.',\n",
       "   'The dish with the flax seeds'],\n",
       "  'bbox_target': [214.57, 7.61, 232.08, 230.3]},\n",
       " 819: {'image_emb': tensor([[ 0.1207, -0.0673, -0.6733,  ...,  0.8115,  0.1301, -0.0122],\n",
       "          [-0.1959,  0.2607, -0.3057,  ...,  1.1328, -0.1758, -0.2534],\n",
       "          [ 0.0163,  0.3098, -0.5557,  ...,  0.7729,  0.0864, -0.1826],\n",
       "          [-0.1528,  0.4297,  0.0348,  ...,  1.3066,  0.0784, -0.2477],\n",
       "          [ 0.0896,  0.1848, -0.7212,  ...,  0.8018,  0.0715,  0.0050]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0441,  0.1213, -0.0966,  ...,  0.3171, -0.2153, -0.1375],\n",
       "          [-0.1204,  0.0670,  0.2593,  ...,  0.1462, -0.2932, -0.0342]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.6226e-05, 8.6719e-01, 6.9082e-05, 1.3293e-01, 1.0788e-05],\n",
       "          [5.5552e-05, 7.9053e-01, 1.4186e-04, 2.0947e-01, 6.1989e-05]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    1.676846   22.839010  380.053497  367.764435    0.915223      0  person\n",
       "  1    6.741858  147.817902  148.769958  268.872833    0.888503     56   chair\n",
       "  2    0.605822  132.610733  347.037323  367.455811    0.856567     15     cat\n",
       "  3  358.575806  167.475311  499.632843  370.334290    0.720179     56   chair,\n",
       "  'caption': ['Over-stuffed chair covered in tan fabric with cushion leaning back.',\n",
       "   'a beige colored chair'],\n",
       "  'bbox_target': [360.39, 167.53, 139.29, 202.6]},\n",
       " 820: {'image_emb': tensor([[-0.4492,  0.5581, -0.0086,  ...,  0.7319, -0.1836,  0.4070],\n",
       "          [-0.2451, -0.0945, -0.2605,  ...,  0.7139, -0.1670,  0.0028],\n",
       "          [-0.2188,  0.2202, -0.3403,  ...,  1.3252, -0.1741,  0.0774],\n",
       "          [ 0.0271,  0.1022, -0.0510,  ...,  1.1475,  0.5723,  0.1980]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3582, -0.2607,  0.1344,  ..., -0.0192, -0.1285, -0.0334],\n",
       "          [-0.0774, -0.1760,  0.0539,  ..., -0.0462, -0.3318,  0.0081]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.6582e-01, 3.9279e-05, 3.4088e-02, 2.5034e-06],\n",
       "          [9.7803e-01, 2.0623e-05, 2.1606e-02, 2.4009e-04]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   269.090088  298.116730  383.892273  462.578857    0.914174     56   \n",
       "  1   477.898560   78.872513  626.578247  172.113174    0.903140     62   \n",
       "  2   584.537476  263.864990  639.994019  430.825012    0.853251     56   \n",
       "  3     0.935097  418.704651  199.672974  478.029480    0.580772     57   \n",
       "  4   448.030548  454.152039  470.791656  479.539001    0.374117     41   \n",
       "  5     2.112410  291.564148   22.824154  360.957214    0.364884     73   \n",
       "  6   313.631226  395.216919  639.325867  477.930664    0.343833     60   \n",
       "  7    18.256870  293.218597   38.586353  357.786774    0.300204     73   \n",
       "  8   120.253876  309.693909  196.096832  335.500549    0.283855     73   \n",
       "  9    11.966325  294.844147   27.470001  360.409515    0.282344     73   \n",
       "  10    0.000000  380.151611   53.019356  413.190491    0.277428     73   \n",
       "  11    0.566006  295.925171   12.144066  362.863220    0.262013     73   \n",
       "  12   37.128517  297.055511   50.603317  355.105255    0.252506     73   \n",
       "  \n",
       "              name  \n",
       "  0          chair  \n",
       "  1             tv  \n",
       "  2          chair  \n",
       "  3          couch  \n",
       "  4            cup  \n",
       "  5           book  \n",
       "  6   dining table  \n",
       "  7           book  \n",
       "  8           book  \n",
       "  9           book  \n",
       "  10          book  \n",
       "  11          book  \n",
       "  12          book  ,\n",
       "  'caption': ['A WOODEN CHAIR FACING THE DESK',\n",
       "   'a wood chair by a desk and window'],\n",
       "  'bbox_target': [267.35, 298.32, 119.75, 165.16]},\n",
       " 821: {'image_emb': tensor([[-0.3645, -0.3069, -0.1288,  ...,  0.4407,  0.2651,  0.1000],\n",
       "          [ 0.2026, -0.1932,  0.0123,  ...,  0.4165,  0.2161, -0.2876],\n",
       "          [-0.1954, -0.4629, -0.1888,  ...,  0.4336,  0.0790,  0.1537]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3657, -0.4314,  0.0439,  ...,  0.2542, -0.1685,  0.4067],\n",
       "          [-0.1647, -0.4822,  0.0433,  ...,  0.3835,  0.0241,  0.5488]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2484, 0.5425, 0.2091],\n",
       "          [0.1455, 0.6523, 0.2021]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class   name\n",
       "  0    0.734711  37.688721  315.894257  473.666016    0.946700     22  zebra\n",
       "  1  426.582306  11.149597  638.505127  339.127899    0.926791     22  zebra,\n",
       "  'caption': ['A zebra eating grass which is in the left',\n",
       "   \"A zebra eating grass not hanging it's head over a log.\"],\n",
       "  'bbox_target': [0.0, 34.52, 319.28, 436.85]},\n",
       " 822: {'image_emb': tensor([[-5.6458e-02,  4.3872e-01, -5.0879e-01,  ...,  1.1074e+00,\n",
       "            1.2939e-01,  1.4355e-01],\n",
       "          [ 2.1667e-02,  1.4868e-01, -4.2798e-01,  ...,  7.4316e-01,\n",
       "            2.7490e-01,  5.3711e-01],\n",
       "          [-1.4685e-01,  2.7490e-01, -4.8755e-01,  ...,  8.0518e-01,\n",
       "            2.8711e-01,  1.7261e-01],\n",
       "          [ 4.7541e-04,  3.6530e-02, -2.1130e-01,  ...,  3.3740e-01,\n",
       "           -3.0640e-02,  1.0852e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.4521,  0.2406, -0.2500,  ...,  0.0887,  0.0111,  0.1836],\n",
       "          [ 0.2717,  0.0044, -0.5156,  ...,  0.0881,  0.1309,  0.5142]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.8215e-04, 9.9854e-01, 2.2650e-06, 7.7868e-04],\n",
       "          [4.7874e-03, 8.8428e-01, 3.6538e-05, 1.1072e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  462.355469   20.078857  640.000000  477.082886    0.955894      0   \n",
       "  1  233.679108  161.217957  437.274994  475.586121    0.940318      0   \n",
       "  2    1.199066  236.686035  307.124725  474.587646    0.906603     57   \n",
       "  3  473.483582  384.549835  535.755920  409.810333    0.623726     73   \n",
       "  4  263.372253  341.650116  287.718933  373.208099    0.595855     65   \n",
       "  5  423.941650  297.823608  495.811890  379.022827    0.581778     58   \n",
       "  6  401.917175  217.814789  621.139771  347.501068    0.491920     57   \n",
       "  7  263.314117  359.912231  275.755585  373.715210    0.433735     65   \n",
       "  8  417.895111  174.946960  444.975494  215.858704    0.305818     75   \n",
       "  9  454.034363  347.471985  484.254822  379.280518    0.283920     75   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2         couch  \n",
       "  3          book  \n",
       "  4        remote  \n",
       "  5  potted plant  \n",
       "  6         couch  \n",
       "  7        remote  \n",
       "  8          vase  \n",
       "  9          vase  ,\n",
       "  'caption': ['A woman in a green shirt and black sweater.',\n",
       "   'Girl in green shirt.'],\n",
       "  'bbox_target': [227.6, 158.83, 203.86, 314.97]},\n",
       " 823: {'image_emb': tensor([[ 0.0399,  0.1855, -0.4534,  ...,  0.8008,  0.2795,  0.1965],\n",
       "          [-0.0862,  0.2399, -0.4226,  ...,  1.0146,  0.3115,  0.2313],\n",
       "          [-0.2964,  0.3630, -0.1675,  ...,  1.3828,  0.2352, -0.0995],\n",
       "          ...,\n",
       "          [ 0.1225,  0.1576, -0.3845,  ...,  0.8760,  0.0043, -0.1643],\n",
       "          [-0.3015,  0.3098, -0.2852,  ...,  0.8838,  0.1342,  0.0155],\n",
       "          [-0.1377,  0.2025, -0.0461,  ...,  0.8271,  0.2234,  0.0364]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0812,  0.0572, -0.1219,  ...,  0.5713, -0.2898,  0.4531],\n",
       "          [-0.1257,  0.1996, -0.2031,  ...,  0.2546, -0.3032,  0.1429]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.7924e-03, 1.5182e-03, 9.9030e-03, 8.1718e-05, 1.8606e-03, 2.5826e-03,\n",
       "           1.2789e-03, 6.7627e-02, 3.8776e-03, 3.2139e-03, 9.0527e-01],\n",
       "          [2.7603e-02, 8.1116e-02, 1.6138e-01, 2.8931e-02, 3.2257e-02, 1.0095e-01,\n",
       "           3.6011e-02, 2.9224e-01, 4.5502e-02, 2.5131e-02, 1.6907e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    30.568764   81.698486   65.899727  164.436478    0.848870     52   \n",
       "  1    64.619598   99.773293  112.094894  147.271606    0.834828     52   \n",
       "  2   278.769775    0.403851  346.060181   87.321442    0.831957     39   \n",
       "  3     5.524175  108.265236   39.617126  164.522522    0.823246     52   \n",
       "  4    70.706490   90.635460  136.852524  140.229980    0.821385     52   \n",
       "  5   114.859840   74.351898  178.321625  133.261200    0.819395     52   \n",
       "  6    79.496834   68.290192  143.783188  113.645386    0.753690     52   \n",
       "  7   129.451050  108.177925  160.661392  140.740463    0.743879     52   \n",
       "  8     0.000000  102.206871   29.031002  152.042694    0.731488     52   \n",
       "  9   464.250946  100.204559  499.651062  181.207657    0.703758     45   \n",
       "  10    2.751207    1.220983  498.553986  327.533722    0.549553     60   \n",
       "  \n",
       "              name  \n",
       "  0        hot dog  \n",
       "  1        hot dog  \n",
       "  2         bottle  \n",
       "  3        hot dog  \n",
       "  4        hot dog  \n",
       "  5        hot dog  \n",
       "  6        hot dog  \n",
       "  7        hot dog  \n",
       "  8        hot dog  \n",
       "  9           bowl  \n",
       "  10  dining table  ,\n",
       "  'caption': ['The lower portion of table between the blue and green plates of burger patties.',\n",
       "   'A blue and white table cloth on a table'],\n",
       "  'bbox_target': [248.25, 254.63, 167.25, 72.75]},\n",
       " 824: {'image_emb': tensor([[-0.3652, -0.1412, -0.0727,  ...,  0.5415, -0.0930,  0.1851],\n",
       "          [ 0.0137,  0.0395, -0.0146,  ...,  0.6992, -0.2859,  0.1545]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0806, -0.0326,  0.1971,  ...,  0.2764, -0.0148,  0.0339],\n",
       "          [ 0.1226, -0.1837,  0.0837,  ...,  0.2004, -0.2529, -0.0385]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.7549, 0.2451],\n",
       "          [0.9307, 0.0695]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   46.424225   39.740097  639.064697  400.317566    0.934460      5     bus\n",
       "  1    0.265270  131.705475   83.237076  310.882660    0.662557      5     bus\n",
       "  2  106.188133  195.200195  140.624023  238.063721    0.522480      0  person,\n",
       "  'caption': ['A Chinese public transportation bus with red and white stripe painted on it',\n",
       "   '12-B379 bus'],\n",
       "  'bbox_target': [57.04, 38.31, 582.96, 361.62]},\n",
       " 825: {'image_emb': tensor([[ 0.1499, -0.1400, -0.0615,  ...,  0.7441, -0.0091, -0.1304],\n",
       "          [ 0.0519, -0.1055,  0.0865,  ...,  0.4324, -0.0363, -0.1471],\n",
       "          [ 0.0929, -0.1263, -0.0498,  ...,  0.2280, -0.2010, -0.0735]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1062, -0.1115, -0.2776,  ..., -0.3506, -0.0816, -0.3740],\n",
       "          [ 0.1481,  0.0025, -0.2368,  ..., -0.2585, -0.1581, -0.5352]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1486, 0.3245, 0.5269],\n",
       "          [0.0902, 0.2737, 0.6362]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  254.372375   11.533646  447.053528  308.866028    0.934373     23  giraffe\n",
       "  1  163.524658  103.763092  427.254272  346.638275    0.921819     23  giraffe,\n",
       "  'caption': ['the giraffe standing all the way up',\n",
       "   'The giraffe standing tall'],\n",
       "  'bbox_target': [257.41, 10.47, 188.45, 301.15]},\n",
       " 826: {'image_emb': tensor([[ 0.0365, -0.1783,  0.2512,  ...,  0.5830,  0.2830, -0.3560],\n",
       "          [ 0.0738, -0.4446, -0.0434,  ...,  0.4871,  0.4143, -0.4805]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-8.8120e-03, -2.0386e-01, -4.2358e-01, -1.6711e-01,  2.8793e-02,\n",
       "            1.6882e-01, -2.2375e-01, -1.3750e+00, -7.9773e-02,  4.8218e-01,\n",
       "            2.0093e-01,  1.5161e-01,  1.8213e-01, -1.4697e-01,  5.5389e-02,\n",
       "           -1.4771e-01, -4.3610e-02, -1.2360e-01,  9.8145e-02,  4.3481e-01,\n",
       "            2.7122e-03,  2.4048e-01,  2.0752e-02, -9.2834e-02, -1.9775e-01,\n",
       "            1.2494e-01, -8.4290e-02, -3.1616e-02, -8.1177e-02, -2.6562e-01,\n",
       "            3.6572e-01, -4.0942e-01, -2.1558e-01,  6.0730e-03, -1.4136e-01,\n",
       "            1.4954e-02,  4.5581e-01,  1.6882e-01,  2.5928e-01,  1.9861e-01,\n",
       "           -1.2535e-02,  1.9287e-01,  1.2842e-01,  4.6682e-04, -4.1107e-02,\n",
       "            7.9834e-02,  1.4099e-01, -5.6519e-02,  1.0284e-01, -5.1660e-01,\n",
       "           -3.2690e-01, -3.0151e-01, -3.2178e-01, -1.5222e-01, -1.6797e-01,\n",
       "            2.3108e-01, -2.3694e-01,  1.9623e-02, -1.0352e-01,  1.3403e-01,\n",
       "            1.9788e-01, -2.5806e-01, -2.6758e-01,  2.2803e-01,  2.5903e-01,\n",
       "           -1.8945e-01,  4.1931e-02,  7.0862e-02,  5.0697e-03,  9.9915e-02,\n",
       "           -2.7054e-02, -2.8345e-01,  1.1725e-01,  1.2708e-01,  4.9866e-02,\n",
       "           -3.0444e-01,  1.0278e-01,  1.6248e-01, -4.0436e-03, -1.4783e-01,\n",
       "            3.4888e-01,  2.8784e-01, -1.0486e-01,  5.5511e-02, -3.8422e-02,\n",
       "            4.7951e-03,  1.3098e-01,  1.1395e-01, -2.9659e-03, -3.1934e-01,\n",
       "            1.0376e-01,  4.3831e-03, -1.7734e+00,  5.1318e-01, -3.0249e-01,\n",
       "           -2.9932e-01,  2.1814e-01, -2.9150e-01, -3.1052e-02,  3.1543e-01,\n",
       "           -1.1725e-01, -1.6809e-01,  4.3530e-01,  1.0388e-01,  2.6025e-01,\n",
       "            2.6443e-02, -1.1597e-01,  1.6980e-01,  2.2925e-01,  3.0273e-01,\n",
       "            3.4912e-01, -4.3411e-03, -3.1174e-02,  2.8882e-01,  1.8713e-01,\n",
       "            1.4490e-01, -1.7798e-01, -4.0308e-01, -1.9928e-02,  2.4695e-01,\n",
       "           -3.8770e-01, -6.1133e-01, -1.6528e-01, -2.0227e-01, -4.8553e-02,\n",
       "           -2.0227e-01,  5.5389e-02,  5.3271e-01,  1.2012e-01, -5.9175e-04,\n",
       "           -5.2948e-03, -2.0935e-01, -1.3489e-01,  6.4688e+00, -1.7639e-01,\n",
       "           -5.2734e-01, -2.6807e-01,  5.8975e-03,  2.2070e-01, -4.8798e-02,\n",
       "           -3.7598e-01, -1.8384e-01, -5.6396e-02,  2.0471e-01, -1.5906e-01,\n",
       "           -1.5533e-02, -4.8370e-02, -5.5420e-02, -3.8525e-01,  4.5624e-02,\n",
       "            1.7383e-01, -2.5439e-01,  9.2834e-02, -5.1331e-02, -1.6821e-01,\n",
       "           -2.6025e-01,  1.3110e-01, -3.4229e-01, -3.6041e-02,  6.6345e-02,\n",
       "            2.3438e-01, -1.1398e-02, -1.6895e-01, -1.7554e-01,  2.4292e-01,\n",
       "            6.8054e-02, -9.2834e-02, -4.8920e-02, -9.6802e-02,  9.7656e-02,\n",
       "            2.2812e-02, -3.0396e-01,  1.4075e-01,  2.0862e-01, -4.4098e-03,\n",
       "            1.9885e-01, -4.8608e-01,  7.1594e-02, -3.0664e-01, -9.1934e-03,\n",
       "           -1.3452e-01,  1.4062e-01, -2.8442e-02,  4.5776e-02, -1.0022e-01,\n",
       "           -1.7078e-01, -2.0129e-01, -4.1748e-02, -1.4429e-01, -6.0310e-03,\n",
       "           -3.1665e-01,  1.3257e-01,  1.8872e-01,  6.0486e-02,  2.7969e-02,\n",
       "            2.2266e-01, -1.5259e-01, -5.6824e-02, -2.8564e-01,  1.5820e-01,\n",
       "            4.0039e-01,  2.6550e-02, -7.7942e-02, -2.4902e-01,  1.1975e-01,\n",
       "           -3.8135e-01, -1.3306e-01, -5.1709e-01,  1.2476e-01, -1.0425e-01,\n",
       "           -4.0131e-02,  5.6982e-01,  8.8135e-02,  2.8418e-01, -5.6854e-02,\n",
       "            1.2244e-01, -6.7993e-02, -1.5991e-01,  7.1729e-01,  2.3840e-01,\n",
       "            6.2561e-02, -2.7695e-03,  7.5188e-03,  4.9683e-01,  1.0266e-01,\n",
       "           -5.3772e-02,  6.7810e-02,  5.4871e-02, -1.3660e-01, -4.5013e-02,\n",
       "            1.7035e-04,  3.1152e-01,  1.1884e-01,  3.9673e-01,  9.6680e-02,\n",
       "           -1.7444e-01,  5.5008e-03, -2.9565e-01, -3.7109e-01,  1.9763e-01,\n",
       "            7.2266e-02,  9.4727e-02,  1.4221e-01, -1.9604e-01, -8.9966e-02,\n",
       "            1.6931e-01, -2.7271e-01,  3.9917e-02, -7.8979e-02,  2.1790e-01,\n",
       "           -4.6326e-02, -1.0605e-02,  3.2935e-01,  3.7964e-01, -1.6455e-01,\n",
       "           -5.8203e-01, -9.0881e-02, -1.4404e-01,  3.0762e-01,  5.1562e-01,\n",
       "           -8.9050e-02, -1.4307e-01, -1.7981e-01, -1.5008e-04,  1.7676e-01,\n",
       "            8.8013e-02,  1.8234e-02, -3.7231e-01,  4.5395e-03, -5.9326e-02,\n",
       "            2.1509e-01,  8.7463e-02, -2.0007e-01,  3.8696e-02, -2.4390e-01,\n",
       "           -8.4991e-03,  1.8457e-01,  9.9945e-03, -2.1667e-01,  1.3464e-01,\n",
       "           -1.8463e-02,  3.7085e-01,  1.2671e-01,  1.6467e-01,  1.1688e-01,\n",
       "            5.3760e-01, -9.9731e-02,  7.4402e-02, -4.7760e-02,  1.5649e-01,\n",
       "           -2.9022e-02,  1.1938e-01,  1.4270e-01,  1.6626e-01,  2.8854e-02,\n",
       "            1.0327e-01,  4.4067e-01,  1.3831e-01,  8.0688e-02,  7.6050e-02,\n",
       "           -1.9946e-01, -2.4109e-02,  2.5928e-01,  4.4342e-02, -1.7957e-01,\n",
       "           -4.9103e-02,  2.8418e-01, -1.5454e-01, -7.6477e-02, -3.6987e-01,\n",
       "            2.2791e-01,  3.0811e-01,  6.4570e+00,  4.7974e-02,  2.3889e-01,\n",
       "            1.1328e-01,  4.6338e-01, -6.7200e-02,  2.6685e-01,  2.4841e-02,\n",
       "            2.8589e-01,  1.7725e-01,  2.5049e-01,  1.5274e-02,  2.9419e-01,\n",
       "           -1.3428e-01,  1.3220e-01, -1.0162e-01,  5.2393e-01, -2.9199e+00,\n",
       "            6.1127e-02, -1.6382e-01,  3.7207e-01, -1.5698e-01, -4.1772e-01,\n",
       "           -4.2285e-01, -1.0254e-01,  3.1433e-02,  1.2428e-02,  1.5979e-01,\n",
       "           -2.3010e-01, -2.8296e-01,  3.4155e-01, -2.0959e-01, -8.5754e-02,\n",
       "           -1.9824e-01,  2.1533e-01, -1.0840e-01, -2.3535e-01,  3.0762e-01,\n",
       "           -1.2976e-01, -1.4343e-01,  1.9104e-01,  1.0553e-01,  1.5640e-02,\n",
       "            2.9053e-01, -4.4342e-02, -8.9355e-02,  1.0643e-02,  2.0483e-01,\n",
       "            3.8428e-01,  2.4500e-01,  2.3767e-01, -3.0594e-02,  1.4258e-01,\n",
       "            1.7480e-01,  5.3223e-02,  3.6865e-02,  9.0210e-02, -6.2439e-02,\n",
       "           -7.3608e-02, -3.6963e-01,  2.6270e-01,  4.6851e-01,  3.3112e-02,\n",
       "            8.0200e-02,  1.2085e-01, -1.1523e-01, -1.6821e-01, -2.2131e-01,\n",
       "           -4.4409e-01,  6.9885e-02,  3.7451e-01,  3.1104e-01,  4.5013e-02,\n",
       "           -4.9561e-02,  1.1993e-01,  1.1951e-01, -1.0394e-01, -1.8921e-01,\n",
       "           -5.3906e-01, -2.4426e-01, -2.4695e-01, -3.6255e-02,  2.1228e-01,\n",
       "           -9.2896e-02, -9.7473e-02,  6.2317e-02, -1.1725e-01,  8.5815e-02,\n",
       "           -6.3660e-02,  3.5614e-02,  1.3599e-01, -1.7700e-01, -7.6233e-02,\n",
       "            3.0664e-01, -5.9082e-02, -1.6138e-01,  3.3447e-01,  2.3730e-01,\n",
       "            2.6807e-01,  3.4149e-02, -1.6931e-01,  3.3716e-01,  2.3132e-01,\n",
       "           -3.2104e-01,  1.9653e-01, -2.3975e-01,  1.3074e-01, -9.7559e-01,\n",
       "           -8.8318e-02, -2.0605e-01, -4.7791e-02, -1.6467e-01, -8.2245e-03,\n",
       "           -2.0703e-01,  2.2583e-01,  1.0663e-01,  3.7183e-01, -1.3574e-01,\n",
       "           -1.8787e-01,  1.3428e-01,  9.6069e-02,  2.5073e-01, -4.7607e-02,\n",
       "           -5.1422e-03, -1.7004e-03, -8.4839e-02,  3.8086e-02,  7.1472e-02,\n",
       "           -2.4744e-01, -1.5343e-02, -2.4048e-01, -2.1106e-01, -1.0718e-01,\n",
       "           -8.2520e-02, -1.4905e-01, -2.9761e-01,  2.8589e-01, -1.7578e-01,\n",
       "            1.7932e-01, -9.7839e-02,  2.0703e-01, -2.2876e-01,  3.7378e-01,\n",
       "           -5.1025e-01,  2.8174e-01, -4.3262e-01, -3.4253e-01,  3.7323e-02,\n",
       "            1.3257e-01, -1.4734e-01,  1.3428e-01,  8.0750e-02,  2.3450e-01,\n",
       "           -3.5248e-02, -5.1758e-01, -4.4525e-02,  8.8745e-02, -1.0999e-01,\n",
       "            8.5022e-02,  3.7384e-02, -1.1243e-01, -3.5400e-01,  2.7686e-01,\n",
       "            5.5023e-02, -9.6436e-02, -2.7881e-01,  1.1194e-01,  4.8657e-01,\n",
       "           -4.7333e-02, -6.8604e-01,  2.4323e-02, -1.0971e-02,  9.5886e-02,\n",
       "           -7.7637e-02,  7.2021e-02,  1.1340e-01, -1.2073e-01,  4.0649e-02,\n",
       "            6.4697e-02, -1.5247e-01, -1.1035e-01,  2.9004e-01,  1.9873e-01,\n",
       "           -1.7700e-01,  3.3154e-01,  7.2632e-02, -9.7473e-02,  8.1604e-02,\n",
       "            3.6133e-02, -1.6028e-01, -1.4624e-01,  4.2847e-02,  1.6041e-03,\n",
       "            4.6289e-01,  8.4106e-02, -9.9609e-02,  9.5154e-02, -7.7087e-02,\n",
       "           -1.1694e-01,  1.3831e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6260, 0.3738]], dtype=torch.float16),\n",
       "  'df_boxes':        xmin        ymin       xmax        ymax  confidence  class   name\n",
       "  0  1.091629  133.387909  326.30127  386.137299    0.922277     18  sheep,\n",
       "  'caption': ['The animal in the back, with its head farther forward than the other animal.'],\n",
       "  'bbox_target': [128.26, 136.5, 264.76, 218.96]},\n",
       " 827: {'image_emb': tensor([[ 0.2366,  0.6411, -0.8955,  ...,  0.6943, -0.2876,  0.5464],\n",
       "          [ 0.2581,  0.5361, -0.5176,  ...,  0.7471, -0.2206, -0.0928],\n",
       "          [ 0.3552,  0.5024, -0.1675,  ...,  0.4138, -0.1965,  0.1836],\n",
       "          [ 0.4158,  0.3455, -0.4966,  ...,  0.4221, -0.3459, -0.1077]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0355, -0.2159,  0.1538,  ..., -0.0631, -0.5498, -0.5420],\n",
       "          [-0.3262, -0.0819, -0.1693,  ..., -0.1643, -0.0390, -0.4504]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0803e-01, 1.7810e-01, 5.1514e-01, 1.9861e-01],\n",
       "          [3.6168e-04, 5.8044e-02, 1.0187e-01, 8.3984e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0   92.305443   68.301025  266.822693  214.625626    0.920809     54  donut\n",
       "  1  189.969391   84.232964  441.319275  319.383301    0.914603     54  donut\n",
       "  2   93.634026  232.286575  259.729523  390.784393    0.901547     54  donut,\n",
       "  'caption': ['a chocolate iced cupcake',\n",
       "   'One of three pastries in a box with a white sqiggley line on top of it.'],\n",
       "  'bbox_target': [90.93, 232.15, 171.8, 156.83]},\n",
       " 828: {'image_emb': tensor([[-0.7729, -0.0347, -0.1469,  ...,  0.0710, -0.1026, -0.3726],\n",
       "          [-0.2656,  0.3916, -0.4937,  ...,  0.9551,  0.0289, -0.0529],\n",
       "          [-0.5737,  0.3472,  0.1710,  ...,  1.4922,  0.1959, -0.0984],\n",
       "          [-0.0116, -0.1888, -0.5898,  ...,  1.2568, -0.1290, -0.3831],\n",
       "          [-0.3044,  0.2107, -0.2625,  ...,  0.9189, -0.1378,  0.1378],\n",
       "          [-0.3206, -0.3164, -0.1642,  ..., -0.0585, -0.0166, -0.3535]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0474,  0.3818,  0.1915,  ..., -0.2255, -0.0461,  0.0310],\n",
       "          [-0.0214, -0.0676, -0.2969,  ...,  0.4199,  0.0797, -0.6079]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9658e-01, 9.8324e-04, 1.4484e-05, 1.3709e-06, 2.8789e-05, 2.1801e-03],\n",
       "          [5.4639e-01, 4.1723e-07, 5.0592e-04, 3.1352e-05, 0.0000e+00, 4.5312e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   65.231094  176.886398  229.390503  578.031555    0.947527      0   \n",
       "  1  255.157806   10.068153  328.414032  157.716248    0.880190      0   \n",
       "  2   84.116074  127.904503  169.320312  187.754730    0.869916     38   \n",
       "  3  213.635773   96.650093  226.530121  109.407982    0.821677     32   \n",
       "  4  246.726074   82.695045  336.867188  156.037537    0.815100     56   \n",
       "  5  292.595581    3.605514  356.860901   29.495327    0.417224      0   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2  tennis racket  \n",
       "  3    sports ball  \n",
       "  4          chair  \n",
       "  5         person  ,\n",
       "  'caption': ['A man in a purple and white outfit.',\n",
       "   'A tennis player jumping up high with his racket in the air.'],\n",
       "  'bbox_target': [66.78, 181.28, 163.91, 395.26]},\n",
       " 829: {'image_emb': tensor([[ 9.5886e-02,  2.8418e-01,  1.2482e-01,  ...,  1.0586e+00,\n",
       "           -4.1382e-02,  1.8787e-01],\n",
       "          [ 4.3640e-03,  2.0581e-01, -4.2554e-01,  ...,  1.3867e+00,\n",
       "           -1.2195e-01, -1.2347e-01],\n",
       "          [ 1.3046e-03,  2.4023e-01, -1.0919e-01,  ...,  1.1846e+00,\n",
       "            3.1934e-01, -1.5234e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 4.4580e-01,  1.2903e-01,  2.0215e-01, -4.3896e-01, -1.3770e-01,\n",
       "            1.2439e-01, -1.5271e-01, -7.2607e-01, -1.7371e-01,  9.0332e-02,\n",
       "            1.2189e-01, -2.2363e-01,  2.7222e-01,  9.3140e-02, -8.7585e-02,\n",
       "            3.4106e-01,  1.3770e-01,  3.1689e-01, -3.2031e-01, -2.0007e-01,\n",
       "            4.0820e-01,  2.4402e-01, -2.8613e-01, -3.1909e-01, -1.1932e-01,\n",
       "            5.4834e-01, -1.3623e-01, -7.6965e-02, -1.2158e-01,  1.7786e-01,\n",
       "            2.1667e-01, -2.2498e-01, -2.8052e-01, -8.3557e-02, -1.5662e-01,\n",
       "            2.5781e-01,  1.0339e-01, -3.1021e-02,  2.1582e-01,  1.3098e-01,\n",
       "            3.3447e-01,  1.2718e-02,  8.6914e-02,  4.1748e-01,  1.9812e-01,\n",
       "            1.3660e-01,  1.2183e-01,  1.3879e-01, -2.7145e-02, -3.1030e-01,\n",
       "            6.7578e-01, -8.0200e-02,  2.1763e-03, -5.3662e-01, -2.1692e-01,\n",
       "            2.7710e-01,  1.4893e-02, -3.7079e-02, -3.0078e-01, -4.6173e-02,\n",
       "            1.4526e-01, -3.3984e-01,  2.3346e-02, -1.7380e-02, -6.1914e-01,\n",
       "           -4.2188e-01, -1.0901e-01,  1.2598e-01, -1.8762e-01, -6.9336e-02,\n",
       "            3.2983e-01, -4.7363e-01, -1.1200e-02, -6.7993e-02,  1.6675e-01,\n",
       "           -3.2764e-01, -3.4729e-02,  5.3741e-02,  3.0792e-02, -9.0576e-02,\n",
       "            5.9296e-02,  3.5583e-02,  1.6699e-01,  1.7126e-01,  8.1726e-02,\n",
       "            4.4670e-03,  2.7759e-01,  1.3220e-01, -1.7004e-01, -1.7746e-02,\n",
       "            4.2358e-02, -5.8398e-01, -1.3838e+00,  6.0156e-01,  1.1421e-02,\n",
       "            4.5581e-01,  5.2582e-02,  3.7817e-01, -9.6741e-02,  2.3596e-01,\n",
       "            5.3314e-02, -1.0626e-01,  1.2286e-01, -4.1016e-01,  2.0776e-01,\n",
       "           -2.7148e-01, -1.6858e-01, -3.4570e-01,  1.1127e-01,  2.8732e-02,\n",
       "           -1.5161e-01,  3.3264e-02,  8.0566e-03,  2.6172e-01,  3.8159e-01,\n",
       "           -1.3208e-01,  2.2253e-01, -1.2732e-01, -2.1130e-01,  1.0815e-01,\n",
       "           -2.3914e-01, -4.6753e-01, -7.4768e-02, -8.9661e-02,  4.5685e-02,\n",
       "            1.2195e-01,  1.9592e-01, -1.6077e-01,  1.0791e-01,  1.6943e-01,\n",
       "           -8.3618e-02,  3.7262e-02, -1.1328e-01,  5.1055e+00, -1.5686e-01,\n",
       "            1.0596e-01, -3.9453e-01, -6.3525e-01, -2.1851e-01, -1.5259e-01,\n",
       "           -2.1887e-01, -5.4871e-02, -1.3660e-01, -3.3325e-01, -1.5747e-01,\n",
       "            4.3945e-02, -9.0149e-02, -1.3635e-01,  2.4902e-02, -1.5198e-01,\n",
       "            3.9093e-02, -6.7725e-01,  1.9507e-01,  3.1396e-01,  5.8807e-02,\n",
       "           -6.0577e-02,  7.4646e-02, -2.1948e-01, -3.3667e-01,  2.5049e-01,\n",
       "           -2.6270e-01,  7.0496e-02, -1.4453e-01,  1.8176e-01,  2.0313e-03,\n",
       "           -1.7114e-01,  1.2115e-01, -3.3173e-02,  4.1675e-01,  1.0858e-01,\n",
       "            3.3862e-01,  3.9764e-02, -1.0693e-01,  2.2034e-01, -1.8066e-01,\n",
       "           -1.3220e-01,  1.5430e-01, -1.8970e-01,  5.3369e-01,  2.4582e-02,\n",
       "            2.0044e-01,  1.3440e-01,  8.3130e-02, -3.4619e-01, -1.9543e-01,\n",
       "            1.2863e-02,  6.8481e-02,  1.7871e-01,  1.6113e-01,  2.0081e-02,\n",
       "           -1.2073e-01,  2.4646e-01, -2.5708e-01,  2.3218e-01, -8.0078e-02,\n",
       "            7.6294e-02, -2.7002e-01,  1.9897e-01, -2.1594e-01, -4.2627e-01,\n",
       "            6.2195e-02,  7.8247e-02,  4.3732e-02,  1.3037e-01,  1.5295e-01,\n",
       "           -1.4490e-01,  1.8933e-01, -1.4417e-01,  1.3550e-01,  2.0312e-01,\n",
       "            4.5557e-01, -7.9407e-02,  2.0081e-01,  2.5635e-01,  3.5864e-01,\n",
       "           -2.3096e-01,  3.0981e-01, -2.2961e-01, -5.2551e-02, -2.7661e-01,\n",
       "            3.7329e-01, -1.4641e-02, -3.2788e-01,  1.3281e-01,  4.0894e-02,\n",
       "            6.2866e-02, -9.8694e-02, -1.1658e-01,  2.6794e-02, -4.0845e-01,\n",
       "           -1.0071e-01, -1.5186e-01,  7.3486e-02,  1.2756e-01,  3.7476e-01,\n",
       "            1.5588e-01,  3.8672e-01,  3.2056e-01, -2.7588e-01, -2.2656e-01,\n",
       "            4.9072e-02,  1.2927e-01,  1.6724e-01,  6.6589e-02, -6.5576e-01,\n",
       "            1.5198e-01, -2.1851e-01, -1.1176e-01,  2.0911e-01, -3.5156e-01,\n",
       "           -2.2171e-02,  3.1030e-01,  2.1558e-01,  5.8008e-01,  3.0981e-01,\n",
       "           -3.1952e-02, -1.1176e-01,  9.9243e-02,  1.1572e-01,  2.5562e-01,\n",
       "            1.5686e-01,  2.0752e-01, -3.8354e-01,  5.1041e-03,  1.4233e-01,\n",
       "           -5.4492e-01, -3.9459e-02,  3.2056e-01,  3.2227e-02,  1.0269e-02,\n",
       "           -5.1172e-01, -1.4722e-01,  2.2046e-01,  2.3889e-01, -2.8223e-01,\n",
       "           -1.9800e-01, -3.1641e-01, -1.9971e-01,  3.3154e-01,  1.7334e-01,\n",
       "            2.6953e-01, -3.7476e-02, -6.6211e-01, -8.1299e-02, -9.4727e-02,\n",
       "           -4.8804e-01,  1.2842e-01,  3.9111e-01,  4.7656e-01, -7.0618e-02,\n",
       "            4.2603e-01,  1.1993e-01,  1.2402e-01,  4.0234e-01,  4.6539e-02,\n",
       "            6.0242e-02, -4.3311e-01, -7.7332e-02, -1.8274e-01,  1.6525e-02,\n",
       "            2.0093e-01, -2.2937e-01, -1.6052e-01,  2.7100e-01, -2.1362e-01,\n",
       "            2.3486e-01, -4.8309e-02,  3.0859e-01, -1.5759e-01,  4.8615e-02,\n",
       "            1.1017e-02, -2.5024e-01,  5.1094e+00,  2.6636e-01, -2.1118e-01,\n",
       "           -5.6982e-01, -2.4426e-01, -7.8271e-01,  1.2036e-01,  4.2084e-02,\n",
       "           -1.1530e-01,  2.3779e-01,  3.0347e-01, -6.0333e-02, -6.0107e-01,\n",
       "           -2.8564e-01,  2.1643e-01,  2.3087e-02, -1.5979e-01, -2.2793e+00,\n",
       "           -1.2042e-01,  7.1960e-02, -7.0862e-02,  1.6821e-01,  2.9321e-01,\n",
       "           -2.5293e-01,  9.8450e-02, -1.0797e-01, -3.7378e-01, -2.0764e-01,\n",
       "            1.4270e-01,  1.6870e-01,  1.6431e-01, -3.4204e-01, -3.6084e-01,\n",
       "           -1.1456e-01,  2.1472e-01, -1.0785e-01,  1.2671e-01, -1.2469e-01,\n",
       "           -4.7461e-01, -3.5352e-01, -2.3267e-01, -3.5400e-01,  2.4304e-01,\n",
       "           -4.2896e-01, -1.6614e-01,  2.2974e-01,  2.6196e-01, -3.4790e-02,\n",
       "            2.0737e-02, -1.4233e-01, -4.0845e-01, -5.7471e-01, -4.5105e-02,\n",
       "           -5.0732e-01,  1.7761e-02,  1.9409e-01, -5.4053e-01,  3.4155e-01,\n",
       "           -5.1562e-01,  2.6196e-01, -3.3472e-01,  1.2622e-01, -5.0232e-02,\n",
       "           -2.5684e-01, -1.1115e-01,  1.5308e-01, -1.6919e-01,  1.2537e-01,\n",
       "           -6.4697e-02,  6.2598e-01,  2.4463e-01, -1.6797e-01,  2.6245e-02,\n",
       "           -2.5830e-01,  7.7209e-02, -9.7595e-02, -1.0352e-01, -3.5352e-01,\n",
       "           -5.2197e-01,  3.1055e-01,  2.9248e-01, -1.1224e-01, -2.5488e-01,\n",
       "            1.1853e-01, -1.6272e-01, -7.6416e-02,  1.3220e-01, -1.1096e-01,\n",
       "            4.0222e-02,  3.6621e-01, -3.8391e-02, -2.6782e-01, -4.7827e-01,\n",
       "           -8.7646e-02,  2.9614e-01,  2.4023e-01,  5.7178e-01, -8.5999e-02,\n",
       "           -7.3792e-02, -1.3977e-01,  4.0869e-01, -1.4978e-01, -5.2307e-02,\n",
       "            1.1475e-01,  2.1948e-01,  1.6205e-02,  3.4180e-02,  7.5378e-02,\n",
       "           -6.1005e-02, -1.0193e-01,  8.3069e-02, -6.9238e-01,  1.1023e-01,\n",
       "            2.3108e-01,  8.6670e-02, -3.1769e-02,  1.2988e-01,  7.5134e-02,\n",
       "           -8.5022e-02,  9.3262e-02, -3.6426e-01,  1.6345e-01, -1.3916e-01,\n",
       "           -1.7590e-01, -5.2832e-01,  4.9048e-01, -7.1533e-02, -3.0542e-01,\n",
       "           -1.8896e-01, -3.7524e-01, -3.5889e-01, -5.6152e-01, -3.5522e-01,\n",
       "           -2.4353e-01, -7.1472e-02,  1.1206e-01, -3.7769e-01,  3.5742e-01,\n",
       "           -4.0771e-01,  1.1414e-01,  1.5173e-01,  3.5571e-01,  1.7700e-01,\n",
       "           -3.4985e-01,  1.0638e-01,  1.1554e-01, -1.7532e-02, -4.1089e-01,\n",
       "            1.3550e-01, -3.2080e-01, -2.7734e-01,  6.2500e-02, -1.1029e-01,\n",
       "           -9.7351e-02,  1.3464e-01, -1.0443e-01, -3.9490e-02, -3.5547e-01,\n",
       "            2.7649e-02,  9.9548e-02,  2.6871e-02,  2.3608e-01,  1.9678e-01,\n",
       "           -4.7485e-02,  2.4158e-01,  2.4109e-01, -2.2812e-02,  2.4185e-02,\n",
       "            1.5576e-01, -4.6899e-01, -1.1070e-02,  2.2168e-01,  1.4551e-01,\n",
       "            1.4539e-01, -2.6764e-02, -6.2927e-02,  1.2103e-01, -2.0129e-01,\n",
       "           -3.0469e-01,  3.2642e-01, -3.1152e-01,  3.1860e-01,  2.8247e-01,\n",
       "           -3.6108e-01, -1.2558e-02, -3.3911e-01,  4.3945e-02,  2.1326e-01,\n",
       "           -5.2881e-01, -1.3123e-01, -2.6562e-01,  1.6077e-01,  5.7422e-01,\n",
       "           -3.3374e-01,  2.2168e-01, -2.8149e-01,  2.2131e-01,  3.8940e-01,\n",
       "            1.6418e-02, -2.6077e-02]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3647, 0.0047, 0.6304]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class  \\\n",
       "  0   80.680481  31.077450  433.706726  327.336060    0.916420     12   \n",
       "  1    0.397947  47.626789   79.630638  326.804779    0.875796     12   \n",
       "  2  375.396454  38.699608  499.391541  321.366608    0.492253      2   \n",
       "  \n",
       "              name  \n",
       "  0  parking meter  \n",
       "  1  parking meter  \n",
       "  2            car  ,\n",
       "  'caption': ['partial parking meter'],\n",
       "  'bbox_target': [1.5, 48.52, 78.57, 281.36]},\n",
       " 830: {'image_emb': tensor([[ 0.2588, -0.1616, -0.1759,  ...,  0.8745, -0.5767,  0.1963],\n",
       "          [-0.1149, -0.1504, -0.0306,  ...,  0.8130, -0.8120,  0.3574],\n",
       "          [ 0.2419, -0.2671, -0.1270,  ...,  0.6870, -0.9678,  0.3577]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2286, -0.1075, -0.1322,  ..., -0.3887,  0.1009, -0.1332],\n",
       "          [-0.1354, -0.5698,  0.1236,  ...,  0.2695,  0.1005,  0.2035]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6611, 0.0168, 0.3223],\n",
       "          [0.5542, 0.2920, 0.1539]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  158.391281  124.553574  562.583069  365.232849    0.944482      7  truck\n",
       "  1    0.796333  121.733307  168.638367  262.301544    0.892963      7  truck,\n",
       "  'caption': ['a truck following another truck',\n",
       "   'firetruck behind firetruck with CDF on it'],\n",
       "  'bbox_target': [0.0, 122.09, 165.41, 143.78]},\n",
       " 831: {'image_emb': tensor([[-1.9788e-01,  4.5947e-01,  2.8149e-01,  ...,  8.2715e-01,\n",
       "           -3.1396e-01, -5.0659e-03],\n",
       "          [-3.5596e-01,  1.0767e-01, -1.0565e-01,  ...,  1.0000e+00,\n",
       "           -1.6895e-01, -1.2274e-03],\n",
       "          [-3.2764e-01,  6.9580e-02, -3.0151e-01,  ...,  1.0957e+00,\n",
       "           -2.7856e-01, -1.4258e-01],\n",
       "          ...,\n",
       "          [-3.7598e-01,  2.1655e-01, -2.9565e-01,  ...,  1.0479e+00,\n",
       "           -3.6865e-01, -5.4216e-04],\n",
       "          [-2.8198e-01, -3.6011e-02, -2.7588e-01,  ...,  1.0762e+00,\n",
       "           -2.1387e-01, -8.4900e-02],\n",
       "          [ 1.6846e-01,  7.8735e-02, -1.4746e-01,  ...,  7.1338e-01,\n",
       "            3.7964e-01, -2.1240e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1390, -0.2495, -0.1505,  ...,  0.1466, -0.2532, -0.3547],\n",
       "          [-0.2610, -0.1351, -0.1074,  ..., -0.3201, -0.0629, -0.1172]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.9639e-01, 2.1136e-04, 4.8375e-04, 5.5695e-04, 4.1771e-03, 2.1687e-03,\n",
       "           8.7595e-04, 1.9519e-01],\n",
       "          [9.8047e-01, 4.1580e-04, 1.2035e-03, 9.8228e-04, 3.2196e-03, 8.0948e-03,\n",
       "           2.6684e-03, 2.9774e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   340.983337  163.745972  448.263458  270.866821    0.932924     56   \n",
       "  1    83.364571  204.569290  207.916885  270.624054    0.916592     56   \n",
       "  2   203.543808  195.705414  321.023285  270.529449    0.904926     56   \n",
       "  3    99.772354  163.744522  173.949219  192.525742    0.811524     56   \n",
       "  4   244.633896  135.252762  290.843994  190.944534    0.807170     56   \n",
       "  5   426.690063  228.585571  499.795959  271.358002    0.800043     56   \n",
       "  6   175.298233  162.908524  247.724014  190.165253    0.755295     56   \n",
       "  7   110.559929  151.833267  159.630768  165.280838    0.660378     56   \n",
       "  8    73.057007  186.696930  289.819489  228.733658    0.649231     60   \n",
       "  9   164.088669  151.085617  211.776276  165.063904    0.611938     56   \n",
       "  10  286.506744  174.896957  351.061859  211.463974    0.532126     56   \n",
       "  11  355.376709  136.586761  387.047943  168.804596    0.531745     58   \n",
       "  12  108.064064  139.792709  153.777634  154.286697    0.503222     56   \n",
       "  13  175.312286   79.098953  216.483688  156.976013    0.386246     72   \n",
       "  14  197.713745   60.820030  208.580063   71.573303    0.330769     74   \n",
       "  15  302.347717  163.278793  398.880646  189.090317    0.286766     60   \n",
       "  16  197.602524   46.211479  209.453888   57.846306    0.273957     74   \n",
       "  17  365.058258  156.909851  375.002411  170.001511    0.259403     75   \n",
       "  \n",
       "              name  \n",
       "  0          chair  \n",
       "  1          chair  \n",
       "  2          chair  \n",
       "  3          chair  \n",
       "  4          chair  \n",
       "  5          chair  \n",
       "  6          chair  \n",
       "  7          chair  \n",
       "  8   dining table  \n",
       "  9          chair  \n",
       "  10         chair  \n",
       "  11  potted plant  \n",
       "  12         chair  \n",
       "  13  refrigerator  \n",
       "  14         clock  \n",
       "  15  dining table  \n",
       "  16         clock  \n",
       "  17          vase  ,\n",
       "  'caption': ['The dining table with 4 chairs.',\n",
       "   'Table with four chairs closest to the photographer.'],\n",
       "  'bbox_target': [73.62, 186.5, 208.58, 34.97]},\n",
       " 832: {'image_emb': tensor([[ 0.1696,  0.1538, -0.2810,  ...,  0.9121, -0.1686,  0.2698],\n",
       "          [ 0.1559,  0.0199, -0.0477,  ...,  0.2491, -0.0205,  0.2471],\n",
       "          [ 0.1846,  0.3850, -0.1730,  ...,  1.2471, -0.0767, -0.0132],\n",
       "          ...,\n",
       "          [ 0.2113,  0.4431, -0.0750,  ...,  0.8750, -0.6006,  0.2095],\n",
       "          [-0.2168,  0.0093, -0.2147,  ...,  0.9326, -0.2368, -0.0167],\n",
       "          [ 0.1017,  0.1949, -0.3538,  ...,  0.9102, -0.2678,  0.0640]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1582,  0.1348, -0.0765,  ..., -0.3970, -0.1632,  0.2484],\n",
       "          [ 0.3621, -0.4524, -0.3408,  ...,  0.4080, -0.3857,  0.1002]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.4801e-02, 6.0986e-01, 1.1659e-04, 2.3523e-01, 2.2217e-02, 8.6548e-02,\n",
       "           2.4929e-03, 9.5596e-03, 1.9012e-02],\n",
       "          [2.2302e-01, 5.6104e-01, 2.9135e-04, 4.2572e-02, 7.5579e-04, 3.6407e-02,\n",
       "           6.1951e-02, 5.5313e-04, 7.3547e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   158.764435   32.702011  572.631104  390.902100    0.948756      0   \n",
       "  1     5.846130    1.226135  307.263702  286.458862    0.940854      0   \n",
       "  2   210.488831  281.045380  301.645264  351.592621    0.885249     55   \n",
       "  3   538.206543  135.587570  640.000000  396.934265    0.878837     56   \n",
       "  4   159.631866  108.732971  272.857452  230.098145    0.877509     42   \n",
       "  5   280.809692   97.624359  368.614990  242.481415    0.830181     56   \n",
       "  6     0.000000  243.246887  638.955200  423.839783    0.786848     60   \n",
       "  7     0.000000  220.407745   54.669182  257.489899    0.729263     55   \n",
       "  8     0.006729   58.806854   47.001396  196.477966    0.644615      0   \n",
       "  9   282.591431   94.536774  377.817505  251.668732    0.330836      0   \n",
       "  10  552.683716  292.957153  570.613647  355.015564    0.279942     56   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2           cake  \n",
       "  3          chair  \n",
       "  4           fork  \n",
       "  5          chair  \n",
       "  6   dining table  \n",
       "  7           cake  \n",
       "  8         person  \n",
       "  9         person  \n",
       "  10         chair  ,\n",
       "  'caption': ['A boy wearing a blue shirt with the number 67 on it.',\n",
       "   'a boy wearin blue tshirt taking brekfast with girl wearing blue dress.'],\n",
       "  'bbox_target': [9.64, 0.0, 292.08, 284.37]},\n",
       " 833: {'image_emb': tensor([[-1.7004e-01,  2.7271e-01,  3.7109e-01,  ...,  6.5869e-01,\n",
       "           -1.9241e-02,  3.5327e-01],\n",
       "          [-2.3193e-01,  5.4102e-01, -5.7831e-02,  ...,  8.7207e-01,\n",
       "           -7.5012e-02,  3.9429e-02],\n",
       "          [-1.0405e-03,  4.3384e-01, -1.9690e-01,  ...,  7.8320e-01,\n",
       "           -3.6816e-01,  1.9971e-01],\n",
       "          [ 2.7115e-02,  5.1953e-01, -2.9565e-01,  ...,  8.1348e-01,\n",
       "           -3.3276e-01,  2.8296e-01],\n",
       "          [-1.7322e-01, -5.7251e-02, -1.6602e-01,  ...,  1.1113e+00,\n",
       "           -2.5146e-01,  9.4116e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0052, -0.4043, -0.3054,  ...,  0.3193, -0.1661,  0.1516],\n",
       "          [-0.2791, -0.0755, -0.2157,  ...,  0.1221, -0.3799,  0.1044]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.1324e-03, 2.2934e-02, 5.1367e-01, 3.0200e-01, 1.5918e-01],\n",
       "          [2.8312e-05, 9.0408e-03, 7.5293e-01, 2.1912e-01, 1.9135e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  449.780029   48.115875  579.695801  262.906097    0.931987      0    person\n",
       "  1  396.519470  124.047913  584.059387  323.294373    0.917053     28  suitcase\n",
       "  2    2.037277  229.625702  475.638611  423.534119    0.898152     28  suitcase\n",
       "  3  232.484787  235.524902  431.426453  402.219727    0.880180      0    person\n",
       "  4   49.970310  338.328522   59.086437  356.242462    0.281736      0    person\n",
       "  5   10.885910  229.047272  468.678284  419.630859    0.258709      0    person,\n",
       "  'caption': ['black suitcase girl is sitting in',\n",
       "   'A suitcase with a girl sitting in it'],\n",
       "  'bbox_target': [397.22, 125.21, 189.01, 200.53]},\n",
       " 834: {'image_emb': tensor([[-2.2693e-01,  2.9175e-01, -1.2457e-01,  ...,  8.8330e-01,\n",
       "           -5.6104e-01,  3.2153e-01],\n",
       "          [-2.2498e-01,  7.2021e-01,  2.1271e-02,  ...,  1.1982e+00,\n",
       "            1.0382e-01, -6.7444e-02],\n",
       "          [ 1.3702e-02,  1.4709e-01,  1.9617e-01,  ...,  1.3545e+00,\n",
       "            2.2412e-01, -3.0933e-01],\n",
       "          ...,\n",
       "          [-1.1168e-03,  5.5206e-02, -1.6833e-01,  ...,  9.9609e-01,\n",
       "           -2.0801e-01, -2.4866e-01],\n",
       "          [-1.0338e-02,  1.5674e-01, -6.4026e-02,  ...,  1.1494e+00,\n",
       "           -2.7197e-01, -9.3323e-02],\n",
       "          [-3.3130e-01,  4.5093e-01, -1.2598e-01,  ...,  1.1133e+00,\n",
       "           -1.8726e-01, -6.7627e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2551,  0.4158, -0.0292,  ...,  0.1410,  0.1025,  0.1388],\n",
       "          [-0.2045,  0.2720, -0.1387,  ...,  0.5830, -0.1398,  0.1118]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.5611e-05, 9.9951e-01, 2.3842e-07, 2.1994e-04, 1.0431e-05, 2.0802e-05,\n",
       "           1.5116e-04],\n",
       "          [4.4556e-03, 8.6279e-01, 1.3888e-04, 1.1925e-02, 1.6556e-02, 5.9967e-03,\n",
       "           9.8328e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     0.000000   23.788712  311.982727  410.397095    0.953666      0   \n",
       "  1   301.057861  215.247375  635.980408  421.039856    0.934830     63   \n",
       "  2   219.813904  340.407471  268.302490  375.220947    0.812573     67   \n",
       "  3   561.883972  256.608826  639.729431  302.389648    0.782471     56   \n",
       "  4   451.395905  185.027069  480.909943  216.376862    0.746477     56   \n",
       "  5   421.882599  183.042633  450.774200  217.562103    0.725586     56   \n",
       "  6   494.787231  189.792236  525.373047  215.401123    0.651797     56   \n",
       "  7   525.150146  199.455872  565.316650  346.615967    0.642051     39   \n",
       "  8   597.763916  127.362213  639.925049  195.802399    0.626324      0   \n",
       "  9   544.230164  191.901184  581.864441  262.218384    0.558903     56   \n",
       "  10  617.317688  195.216309  639.817932  265.401245    0.553870     56   \n",
       "  11  610.332520  365.776398  640.000000  411.547363    0.464455     64   \n",
       "  12  546.588806  178.939087  573.207092  194.765747    0.445562     56   \n",
       "  13  604.069824  168.495270  639.676514  200.458588    0.396410     63   \n",
       "  14  413.790466  148.071152  448.494690  179.869690    0.388691     58   \n",
       "  15  504.270813  179.755890  528.971375  191.095245    0.337631     56   \n",
       "  16    6.868774  322.919769  640.000000  421.649414    0.265427     60   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         laptop  \n",
       "  2     cell phone  \n",
       "  3          chair  \n",
       "  4          chair  \n",
       "  5          chair  \n",
       "  6          chair  \n",
       "  7         bottle  \n",
       "  8         person  \n",
       "  9          chair  \n",
       "  10         chair  \n",
       "  11         mouse  \n",
       "  12         chair  \n",
       "  13        laptop  \n",
       "  14  potted plant  \n",
       "  15         chair  \n",
       "  16  dining table  ,\n",
       "  'caption': ['A silver Dell laptop computer with a visible screen',\n",
       "   'this is a Dell Laptop with a silver keyboard'],\n",
       "  'bbox_target': [301.39, 212.43, 338.61, 214.57]},\n",
       " 835: {'image_emb': tensor([[-0.2251,  0.7578, -0.3250,  ...,  0.7705, -0.1284, -0.2979],\n",
       "          [-0.9712,  0.1621,  0.2261,  ...,  0.2744,  0.2244,  0.1365],\n",
       "          [-0.3811,  0.1188,  0.0872,  ...,  0.2477, -0.2578, -0.0221],\n",
       "          [-0.4070, -0.1807, -0.2527,  ...,  0.3582,  0.1757, -0.3210],\n",
       "          [-0.7373,  0.1019, -0.1378,  ...,  0.6812, -0.0598,  0.0782],\n",
       "          [-0.4536, -0.0296, -0.0911,  ..., -0.1501, -0.2222, -0.0905]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2883, -0.1207, -0.1171,  ...,  0.4133, -0.1849, -0.2781],\n",
       "          [-0.5786, -0.3447,  0.0631,  ...,  0.5176, -0.1575, -0.4128]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.7930e-05, 1.0425e-04, 7.3389e-01, 6.9439e-05, 2.0374e-01, 6.2134e-02],\n",
       "          [6.5327e-05, 2.4021e-05, 7.3486e-01, 2.9206e-06, 2.4609e-01, 1.8982e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  151.244263   93.623062  321.496704  422.991028    0.947527      0   \n",
       "  1   62.413765  156.155945  189.338013  311.012329    0.904169     38   \n",
       "  2  304.387146  139.180222  536.541809  421.935242    0.879687      0   \n",
       "  3  296.347107  277.605225  318.626343  300.367371    0.832893     32   \n",
       "  4  354.635834  178.187500  547.078918  303.159058    0.771072     38   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1  tennis racket  \n",
       "  2         person  \n",
       "  3    sports ball  \n",
       "  4  tennis racket  ,\n",
       "  'caption': ['A lady with fully covered and with tennis racket',\n",
       "   'a muslim woman playing tennis'],\n",
       "  'bbox_target': [307.13, 139.41, 236.93, 287.16]},\n",
       " 836: {'image_emb': tensor([[-0.3787,  0.3301, -0.2296,  ...,  0.8286,  0.0183, -0.1881],\n",
       "          [ 0.1864, -0.0419, -0.1890,  ...,  0.8965,  0.2412, -0.3320],\n",
       "          [-0.0488,  0.3879,  0.0071,  ...,  1.2344, -0.3760, -0.1183],\n",
       "          ...,\n",
       "          [ 0.0322,  0.0416, -0.0527,  ...,  1.0146,  0.0846, -0.2986],\n",
       "          [-0.3826,  0.0902, -0.2302,  ...,  0.9067, -0.0657,  0.0551],\n",
       "          [-0.2324,  0.2595, -0.2456,  ...,  0.7290,  0.1964, -0.0748]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2573,  0.0199, -0.3098,  ..., -0.1093, -0.5312, -0.2847],\n",
       "          [-0.5142,  0.2646, -0.2251,  ...,  0.1943, -0.2656, -0.4805]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.6028e-01, 3.6168e-04, 1.2398e-05, 2.7728e-04, 2.6059e-04, 4.6492e-06,\n",
       "           8.1396e-01, 2.4963e-02],\n",
       "          [6.6211e-01, 1.6937e-03, 7.5912e-03, 8.9264e-04, 1.8132e-04, 6.6698e-05,\n",
       "           1.5979e-01, 1.6748e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0    1.315540   23.546768  272.099823  499.210541    0.935390      0    person\n",
       "  1  255.477234  264.573883  286.385742  340.360626    0.892831     39    bottle\n",
       "  2  166.051346  229.679489  234.486206  287.912415    0.870882     45      bowl\n",
       "  3  269.046051  453.635223  303.059113  500.000000    0.817219     42      fork\n",
       "  4  290.156708  262.456635  334.000000  371.017761    0.794091     39    bottle\n",
       "  5  210.384964  297.204498  239.253204  335.893494    0.780599     41       cup\n",
       "  6   76.851646    6.974554  214.723831  370.666870    0.747729      0    person\n",
       "  7  235.714249  328.491913  290.892212  368.727448    0.319650     44     spoon\n",
       "  8  235.841339  328.017242  289.849701  369.541229    0.306467     76  scissors\n",
       "  9  271.679626  252.321671  287.566925  288.578125    0.266376     39    bottle,\n",
       "  'caption': ['A man with curly hair wearing an apron.',\n",
       "   'A man in a white apron mixes ingredients in a glass bowl with a spoon.'],\n",
       "  'bbox_target': [80.72, 5.16, 139.01, 360.98]},\n",
       " 837: {'image_emb': tensor([[-0.1646,  0.7095,  0.0555,  ...,  0.4016,  0.2881,  0.1803],\n",
       "          [ 0.4365, -0.0499,  0.5522,  ...,  0.7144, -0.3757,  0.4353],\n",
       "          [-0.2871,  0.5747, -0.0323,  ...,  0.6758,  0.1510, -0.2092],\n",
       "          [-0.2012,  0.9922,  0.2815,  ...,  0.8203, -0.0703,  0.0658],\n",
       "          [ 0.3374,  0.1128,  0.2039,  ...,  0.4763, -0.3945,  0.3787],\n",
       "          [ 0.5220,  0.3274,  0.2771,  ...,  0.4524, -0.1621,  0.3174]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2375, -0.0697, -0.7495,  ..., -0.1989,  0.0311, -0.1146],\n",
       "          [-0.0945,  0.2086, -0.2355,  ..., -0.1538,  0.0335, -0.1191]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0435, 0.0470, 0.0949, 0.0864, 0.6191, 0.1093],\n",
       "          [0.2717, 0.0511, 0.2759, 0.1168, 0.1500, 0.1345]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.000000  319.476288  189.054337  557.662964    0.923548     44   \n",
       "  1  327.098938  262.450409  611.488403  525.826294    0.846493     48   \n",
       "  2  199.412048    0.253511  351.145355   95.189285    0.833747     41   \n",
       "  3    0.000000   54.327038  315.433624  387.612915    0.829882     45   \n",
       "  4  366.870605  124.730255  610.506714  417.833893    0.814956     48   \n",
       "  5  594.865906    0.098104  612.000000   62.239243    0.427978     41   \n",
       "  6    0.315798    0.266012   89.737686   85.504234    0.414904     41   \n",
       "  7    0.446608    1.327071  612.000000  604.445251    0.256511     60   \n",
       "  \n",
       "             name  \n",
       "  0         spoon  \n",
       "  1      sandwich  \n",
       "  2           cup  \n",
       "  3          bowl  \n",
       "  4      sandwich  \n",
       "  5           cup  \n",
       "  6           cup  \n",
       "  7  dining table  ,\n",
       "  'caption': ['table behind the menus', 'table'],\n",
       "  'bbox_target': [89.39, 0.0, 522.61, 74.27]},\n",
       " 838: {'image_emb': tensor([[ 0.2157,  0.5161, -0.3538,  ...,  0.9385,  0.1923,  0.2927],\n",
       "          [-0.1655,  0.2240,  0.1249,  ...,  0.8423,  0.0254,  0.0596],\n",
       "          [ 0.6528,  0.7095, -0.4185,  ...,  0.9863,  0.3943,  0.1075],\n",
       "          [-0.4106,  0.2954, -0.0192,  ...,  0.8218,  0.0026,  0.1416],\n",
       "          [-0.1053,  0.0396, -0.2563,  ...,  1.0303,  0.3223,  0.2043],\n",
       "          [ 0.6299,  0.6128, -0.2448,  ...,  0.8013,  0.5396,  0.1414]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1121,  0.0858, -0.2747,  ..., -0.0406,  0.1838,  0.1106],\n",
       "          [-0.1471,  0.2091, -0.3887,  ..., -0.2539,  0.3237,  0.0739]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.4287e-01, 7.9041e-03, 3.6597e-05, 1.8967e-02, 3.0304e-02, 1.2004e-04],\n",
       "          [8.6279e-01, 3.0632e-03, 4.1461e-04, 5.2910e-03, 1.2622e-01, 2.2411e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  497.701996    0.000000  629.684570  138.793671    0.898243     41   \n",
       "  1    0.291801  338.267578   57.307243  400.397827    0.872002     42   \n",
       "  2   68.027634   54.623550  529.371155  455.856934    0.757985     53   \n",
       "  3  491.086975  419.759674  605.900330  476.697266    0.726498     43   \n",
       "  4  420.768219    0.000000  497.721222   60.411163    0.724901     41   \n",
       "  5    7.990112    0.000000  633.176453  474.593384    0.633461     60   \n",
       "  6    0.000000  129.685120  109.009796  210.815247    0.568792     44   \n",
       "  7    0.000000  128.957397  108.946259  218.794983    0.308847     42   \n",
       "  8    0.383257    0.000000   52.131546   17.467186    0.287478      0   \n",
       "  \n",
       "             name  \n",
       "  0           cup  \n",
       "  1          fork  \n",
       "  2         pizza  \n",
       "  3         knife  \n",
       "  4           cup  \n",
       "  5  dining table  \n",
       "  6         spoon  \n",
       "  7          fork  \n",
       "  8        person  ,\n",
       "  'caption': ['coke cup on right side', 'The half full glass'],\n",
       "  'bbox_target': [496.18, 0.1, 131.53, 139.02]},\n",
       " 839: {'image_emb': tensor([[-0.1212,  0.0549,  0.0994,  ...,  1.2617, -0.1210, -0.1854],\n",
       "          [-0.2876,  0.1465, -0.2778,  ...,  0.7651,  0.4153, -0.4062],\n",
       "          [-0.2095,  0.7749,  0.3713,  ...,  0.5415,  0.1445, -0.2688],\n",
       "          [-0.1931,  0.7720,  0.5215,  ...,  0.7041,  0.1766,  0.1321],\n",
       "          [ 0.0685,  0.6167,  0.3206,  ...,  0.5239,  0.4497, -0.2311]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2397,  0.5522,  0.0063,  ...,  0.0874,  0.3948,  0.0948],\n",
       "          [-0.1414,  0.1837,  0.0676,  ...,  0.2568,  0.1282, -0.1008]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.1921e-07, 1.7881e-07, 4.6539e-04, 9.9951e-01, 2.4915e-04],\n",
       "          [4.6849e-05, 8.2195e-05, 1.0277e-02, 9.8438e-01, 5.0087e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  209.384430  388.094635  432.003387  435.484131    0.925421     66   \n",
       "  1  462.794739  396.848969  498.571533  440.398804    0.909472     64   \n",
       "  2  208.428833   75.387222  457.080688  268.445374    0.902144     62   \n",
       "  3   24.329956  138.649841  217.113007  313.489563    0.895572     62   \n",
       "  4  512.129089  302.801605  534.347595  333.127716    0.557304     67   \n",
       "  5   72.588020  329.037262  111.611885  355.185150    0.505893     67   \n",
       "  6   40.503330  328.465973   67.770493  356.675140    0.492640     67   \n",
       "  7  574.691528  222.447845  639.562866  247.085785    0.252052     73   \n",
       "  \n",
       "           name  \n",
       "  0    keyboard  \n",
       "  1       mouse  \n",
       "  2          tv  \n",
       "  3          tv  \n",
       "  4  cell phone  \n",
       "  5  cell phone  \n",
       "  6  cell phone  \n",
       "  7        book  ,\n",
       "  'caption': ['A large mac computer monitor with an apple logo on the front.',\n",
       "   'A white monitor with an apple logo.'],\n",
       "  'bbox_target': [205.65, 75.37, 252.0, 191.61]},\n",
       " 840: {'image_emb': tensor([[ 0.6924,  0.3218, -0.0538,  ...,  1.0449, -0.1484,  0.3621],\n",
       "          [ 0.8198,  0.3430, -0.1085,  ...,  0.9194,  0.0095,  0.6064],\n",
       "          [ 0.8564,  0.3413, -0.1523,  ...,  0.7480, -0.1199,  0.4333]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1742, -0.2444, -0.1346,  ..., -0.2463, -0.2974, -0.1583],\n",
       "          [ 0.2222,  0.1256,  0.0611,  ...,  0.2646, -0.5752, -0.1787]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2896, 0.4849, 0.2255],\n",
       "          [0.3391, 0.4707, 0.1902]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class  \\\n",
       "  0  208.333923  67.766769  578.097290  362.081299    0.904485     48   \n",
       "  1   70.754852  73.646606  341.190155  354.039307    0.840918     48   \n",
       "  2    3.012115   1.682236  637.447876  280.367859    0.380565     60   \n",
       "  \n",
       "             name  \n",
       "  0      sandwich  \n",
       "  1      sandwich  \n",
       "  2  dining table  ,\n",
       "  'caption': ['The piece of the sandwhich on the left.',\n",
       "   'One half of a sandwich arrange behind another half of a sandwich.'],\n",
       "  'bbox_target': [76.24, 74.47, 267.8, 274.47]},\n",
       " 841: {'image_emb': tensor([[-0.0423,  0.0197, -0.0089,  ...,  0.3018,  0.3994,  0.4780],\n",
       "          [ 0.1088,  0.3901, -0.1921,  ...,  0.1002,  0.1558, -0.0102],\n",
       "          [-0.1302,  0.5454, -0.4236,  ...,  0.8662,  0.1163,  0.1250],\n",
       "          ...,\n",
       "          [-0.1110,  0.0504, -0.3215,  ...,  0.5361,  0.2551,  0.0605],\n",
       "          [-0.0466,  0.3101, -0.1686,  ...,  0.8101,  0.2242,  0.4504],\n",
       "          [-0.0471,  0.4441,  0.1263,  ...,  1.2744,  0.0351,  0.0746]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0661,  0.4324, -0.1189,  ..., -0.3621, -0.0553, -0.0120],\n",
       "          [ 0.0305,  0.2964, -0.1086,  ...,  0.0141,  0.1346,  0.0356]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.3628e-02, 5.7106e-03, 8.8818e-01, 6.2466e-05, 9.9182e-04, 1.2741e-03,\n",
       "           1.0233e-03, 4.1366e-04, 2.2483e-04, 1.8072e-04, 5.0688e-04, 4.5891e-03,\n",
       "           3.2711e-04, 1.8644e-04, 1.9848e-04, 1.9121e-03, 7.8678e-06, 7.2575e-04],\n",
       "          [1.0736e-01, 1.7807e-02, 8.4473e-01, 4.1223e-04, 2.1267e-03, 4.3640e-03,\n",
       "           6.9733e-03, 8.7261e-04, 6.1893e-04, 2.5797e-04, 1.9360e-03, 5.3444e-03,\n",
       "           9.1457e-04, 2.5392e-04, 6.7949e-04, 2.5654e-03, 2.7180e-05, 2.9526e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   442.043335  193.212402  639.235718  465.756287    0.922405      0   \n",
       "  1     0.231041  202.963333  148.162476  434.811157    0.918185      0   \n",
       "  2   262.806091  236.790039  442.332764  433.784119    0.901419      0   \n",
       "  3    67.669327  474.468048  184.167648  542.557495    0.893316     45   \n",
       "  4    50.560593  408.839050   92.312881  474.282104    0.880182     41   \n",
       "  5   408.433044  426.578094  455.878174  511.694855    0.878959     41   \n",
       "  6   215.301331  475.844879  278.005310  595.600586    0.869855     41   \n",
       "  7   458.551575  533.132141  542.101624  601.627014    0.868906     45   \n",
       "  8   154.649612  566.293396  178.189743  638.713196    0.845873     42   \n",
       "  9   330.025848  397.432343  368.841095  447.289215    0.839383     41   \n",
       "  10   74.783585  577.589172  157.182266  639.916931    0.799459     43   \n",
       "  11  284.415192  386.825348  319.944427  443.508026    0.786827     41   \n",
       "  12  408.092316  432.663391  639.301270  543.849121    0.766954      0   \n",
       "  13  384.813202  574.212280  454.304291  623.482300    0.764839     45   \n",
       "  14   97.819336  407.495697  218.155151  479.090912    0.748084     45   \n",
       "  15  295.011627  445.152496  357.550934  494.074677    0.735631     45   \n",
       "  16    2.706726  410.058685  638.163025  639.136658    0.728444     60   \n",
       "  17    0.000000  442.015228   27.194401  551.348999    0.694818     41   \n",
       "  18  181.640381  441.729950  313.010712  553.032043    0.690847     45   \n",
       "  19  409.077515  533.308228  454.493042  580.387939    0.618370     41   \n",
       "  20    0.116631  586.822998   53.016533  640.000000    0.510551     43   \n",
       "  21  221.018433  395.400787  265.582306  446.745697    0.320574     41   \n",
       "  22  354.025604  449.858368  411.509247  540.316772    0.254346     41   \n",
       "  23  272.258270  446.248596  311.895844  550.825317    0.251563     41   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3           bowl  \n",
       "  4            cup  \n",
       "  5            cup  \n",
       "  6            cup  \n",
       "  7           bowl  \n",
       "  8           fork  \n",
       "  9            cup  \n",
       "  10         knife  \n",
       "  11           cup  \n",
       "  12        person  \n",
       "  13          bowl  \n",
       "  14          bowl  \n",
       "  15          bowl  \n",
       "  16  dining table  \n",
       "  17           cup  \n",
       "  18          bowl  \n",
       "  19           cup  \n",
       "  20         knife  \n",
       "  21           cup  \n",
       "  22           cup  \n",
       "  23           cup  ,\n",
       "  'caption': ['A man with short hair wearing a black shirt.',\n",
       "   'Man wearing a black polo shirt.'],\n",
       "  'bbox_target': [264.23, 237.22, 178.26, 196.33]},\n",
       " 842: {'image_emb': tensor([[-0.0452,  0.4851, -0.3958,  ...,  0.7637,  0.0279, -0.5293],\n",
       "          [-0.2043,  0.2032, -0.1862,  ...,  1.2402, -0.1547, -0.2546],\n",
       "          [-0.2678,  0.4639, -0.3848,  ...,  1.3496, -0.3494, -0.0213],\n",
       "          [ 0.2578,  0.6040, -0.4724,  ...,  0.7056,  0.1714, -0.4556]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1694,  0.0577, -0.2927,  ..., -0.0362, -0.0334, -0.5464],\n",
       "          [-0.2206, -0.0154, -0.3174,  ...,  0.0323,  0.1117, -0.4773]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.9160e-01, 6.7377e-04, 2.3758e-02, 8.4229e-02],\n",
       "          [5.3223e-02, 4.7517e-04, 2.6512e-03, 9.4385e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   42.767365    4.849457  365.519012  602.375366    0.944525      0   \n",
       "  1  185.117767  584.236755  257.578339  639.029236    0.844859     36   \n",
       "  2  355.358948   21.811218  422.000000  626.051086    0.838927      0   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1  skateboard  \n",
       "  2      person  ,\n",
       "  'caption': ['a woman wearing cooling glass and white cap and cement color dress',\n",
       "   'The woman on the right side of the photo wearing a white cap, she is not standing on a skateboard.'],\n",
       "  'bbox_target': [356.8, 22.19, 65.2, 606.66]},\n",
       " 843: {'image_emb': tensor([[-0.2993,  0.2321, -0.2299,  ...,  0.5615,  0.0237, -0.3889],\n",
       "          [-0.3342,  0.1473, -0.2229,  ...,  0.6045,  0.0143, -0.3354],\n",
       "          [-0.9604,  0.2058, -0.1118,  ...,  0.8501, -0.0795, -0.2708],\n",
       "          [-0.9316,  0.1976, -0.2433,  ...,  0.7959,  0.0017, -0.3018],\n",
       "          [ 0.3293, -0.0280, -0.5459,  ...,  0.5132,  0.1169, -0.2198],\n",
       "          [-0.2339, -0.2646,  0.1194,  ...,  0.2297, -0.0271, -0.0254]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-1.5488e-02,  1.8201e-01, -1.7822e-01,  1.4868e-01,  1.1102e-01,\n",
       "            2.7985e-02,  9.5642e-02, -5.2881e-01,  3.2129e-01,  1.3391e-01,\n",
       "           -5.7678e-02,  2.2205e-01,  1.6870e-01, -2.8149e-01,  3.0322e-01,\n",
       "            9.7412e-02,  1.9669e-02, -2.2803e-01,  2.2974e-01, -1.0339e-01,\n",
       "            3.4497e-01, -9.3994e-02,  6.2408e-02, -1.3220e-01, -3.2837e-02,\n",
       "            1.3696e-01,  1.6455e-01,  3.6646e-01,  2.1204e-01,  2.8418e-01,\n",
       "           -1.0872e-02,  3.3936e-01, -3.0078e-01, -1.7334e-01, -2.0447e-02,\n",
       "           -2.0105e-01, -1.7969e-01,  1.2262e-01, -4.6906e-02,  2.7939e-02,\n",
       "           -1.9312e-01, -3.9215e-02, -3.3936e-02,  1.6992e-01,  5.7312e-02,\n",
       "            1.8140e-01, -1.0388e-01,  5.9692e-02, -9.2224e-02,  2.3544e-02,\n",
       "           -1.1847e-01,  7.1289e-02,  1.3184e-01,  5.5122e-03,  2.2034e-01,\n",
       "            2.3035e-01, -1.8970e-01,  5.7678e-02, -1.5796e-01,  7.0862e-02,\n",
       "            2.7344e-01, -1.9556e-01,  2.2986e-01, -1.8591e-01,  3.4485e-02,\n",
       "           -1.0101e-01,  1.2802e-02, -3.1342e-02,  2.5562e-01, -1.7786e-01,\n",
       "           -3.5132e-01, -3.5864e-01, -3.9282e-01, -1.5149e-01,  9.1064e-02,\n",
       "            1.3892e-01, -3.5980e-02,  1.1261e-01,  1.3574e-01, -6.6650e-02,\n",
       "            1.6797e-01, -2.7124e-01,  1.1511e-01,  2.9938e-02, -1.8567e-01,\n",
       "            4.8706e-01,  2.7808e-01, -1.4868e-01,  2.6709e-01, -1.0468e-01,\n",
       "            1.0211e-01,  9.6741e-02, -1.8760e+00,  6.4893e-01, -3.8232e-01,\n",
       "            2.4402e-01,  6.8909e-02, -9.1125e-02, -7.1838e-02, -1.3806e-01,\n",
       "           -3.0273e-02, -5.0629e-02,  2.6270e-01,  2.1240e-01, -5.1807e-01,\n",
       "           -6.6162e-02, -3.3350e-01, -1.9043e-01, -1.4717e-02, -2.2729e-01,\n",
       "           -2.2742e-01, -3.0930e-02,  1.1884e-01,  1.3904e-01, -8.1482e-02,\n",
       "            1.0333e-01,  1.5918e-01,  1.0236e-01, -1.2646e-01, -9.3079e-02,\n",
       "           -4.9957e-02, -5.1904e-01,  2.4384e-02, -4.3018e-01, -3.2043e-03,\n",
       "            1.4023e-02,  1.0175e-01,  1.6211e-01, -3.1470e-01,  5.9180e-01,\n",
       "            1.9373e-01, -2.4854e-01, -3.9844e-01,  7.4805e+00,  2.9297e-01,\n",
       "           -1.4099e-01, -1.6504e-01, -3.8989e-01, -2.4341e-01,  2.0374e-01,\n",
       "           -3.5327e-01, -7.9773e-02, -4.5227e-02,  9.9243e-02, -4.8584e-01,\n",
       "           -7.0007e-02,  5.0316e-03,  1.2103e-01,  2.0569e-01,  4.8767e-02,\n",
       "            1.4612e-01, -3.0298e-01,  2.9810e-01,  5.1819e-02, -4.1229e-02,\n",
       "           -5.5469e-01,  3.1372e-01, -3.9368e-02, -3.6987e-02,  5.0732e-01,\n",
       "           -6.6101e-02,  5.3596e-04,  1.4539e-01,  9.9609e-02,  2.3413e-01,\n",
       "            1.7419e-01,  4.2749e-01, -1.8262e-01, -1.7899e-02,  1.5967e-01,\n",
       "            8.5083e-02, -2.9883e-01,  1.9519e-01, -6.2073e-02, -4.5557e-01,\n",
       "           -4.3610e-02, -1.4319e-01,  2.5879e-01,  2.4048e-01, -1.4807e-01,\n",
       "           -1.1969e-01,  2.4182e-01, -4.3848e-01,  2.9907e-01, -2.3657e-01,\n",
       "            2.3975e-01,  2.2571e-01, -2.7490e-01, -6.0913e-02, -1.1945e-01,\n",
       "           -1.5100e-01, -3.6597e-01, -1.7853e-02,  8.5205e-02,  1.7224e-01,\n",
       "           -8.4656e-02, -1.3824e-02, -3.2861e-01, -1.6565e-01,  2.5854e-01,\n",
       "            1.2408e-01, -2.5757e-01, -3.5583e-02,  1.4539e-01, -6.5918e-02,\n",
       "           -2.9443e-01,  5.3406e-02,  1.1090e-01,  9.3323e-02,  1.1615e-01,\n",
       "            3.0811e-01,  2.6978e-01, -1.0889e-01,  1.1154e-02, -3.2654e-02,\n",
       "           -1.2311e-01, -2.2705e-01, -2.0703e-01,  2.6782e-01, -1.2610e-01,\n",
       "            8.0933e-02, -2.7124e-01,  2.4756e-01,  1.9958e-01,  1.0760e-01,\n",
       "           -2.0532e-01,  2.4365e-01,  3.0005e-01, -3.0664e-01,  3.6591e-02,\n",
       "            4.1284e-01,  5.4382e-02, -1.9519e-01, -4.1016e-01,  9.5337e-02,\n",
       "           -1.1145e-01, -9.8877e-02,  1.3293e-01, -1.9946e-01, -1.3623e-01,\n",
       "           -8.5327e-02,  3.3618e-01,  1.3843e-01, -2.6489e-01,  1.3245e-01,\n",
       "            1.8835e-01,  2.4185e-02, -9.4116e-02, -1.7285e-01,  2.5116e-02,\n",
       "            2.0007e-01, -2.7679e-02, -1.0724e-01,  4.7144e-01,  9.2651e-02,\n",
       "           -2.0862e-01, -1.4612e-01, -4.0497e-02,  1.6089e-01, -6.7871e-02,\n",
       "           -1.7615e-01, -3.4692e-01,  3.3051e-02,  6.9580e-03, -3.5614e-02,\n",
       "           -5.1758e-01,  2.1729e-01, -3.6597e-01, -4.1724e-01,  1.7615e-01,\n",
       "           -3.2495e-01,  3.6401e-01,  1.9092e-01, -4.8193e-01, -3.1934e-01,\n",
       "           -8.4076e-03, -6.5857e-02,  1.2805e-01,  1.3489e-01,  1.2611e-02,\n",
       "           -2.4805e-01, -6.3538e-02,  2.7930e-01, -7.0923e-02, -1.0443e-01,\n",
       "            4.5898e-02,  5.1086e-02,  9.7412e-02,  4.7290e-01, -2.7771e-02,\n",
       "            3.7598e-01,  2.8760e-01,  8.5266e-02, -6.8436e-03, -8.0811e-02,\n",
       "           -2.5391e-01, -2.6929e-01,  2.7405e-02,  2.3438e-02, -2.1008e-01,\n",
       "           -1.9730e-02, -9.2346e-02, -1.9006e-01, -5.1416e-01,  2.3682e-01,\n",
       "           -3.8379e-01, -1.3281e-01, -2.3108e-01, -3.9404e-01, -4.7729e-01,\n",
       "           -8.0688e-02,  1.8005e-01,  7.4805e+00, -1.0455e-01,  1.9189e-01,\n",
       "           -4.8279e-02,  7.6782e-02,  1.9751e-01,  1.2018e-01,  1.7834e-01,\n",
       "           -1.0376e-02,  5.1562e-01,  1.9324e-01, -9.5947e-02, -7.3547e-02,\n",
       "            2.5314e-02,  4.5441e-02, -7.2632e-02, -4.9225e-02, -2.5469e+00,\n",
       "            1.4294e-01,  5.9448e-02, -2.0776e-01, -1.3098e-01,  1.1505e-01,\n",
       "           -1.7175e-01,  3.2764e-01,  1.6711e-01, -1.1848e-02,  7.7637e-02,\n",
       "           -1.6174e-01, -1.5930e-01, -2.6196e-01, -1.4661e-01, -1.5015e-01,\n",
       "            1.4502e-01,  2.1606e-01,  1.9971e-01,  3.5547e-01,  9.4360e-02,\n",
       "            1.7432e-01, -1.5173e-01,  9.8633e-02,  6.5234e-01, -1.3391e-01,\n",
       "            1.0535e-01,  4.5502e-02, -3.2471e-02, -9.7778e-02,  4.5433e-03,\n",
       "            4.5715e-02, -3.5547e-01,  5.4810e-02,  5.0049e-01, -4.2358e-01,\n",
       "           -2.5586e-01, -1.9019e-01,  3.7598e-01, -1.4417e-01, -1.8323e-01,\n",
       "            2.8735e-01,  2.6428e-02, -1.2817e-01,  2.0996e-01,  1.8298e-01,\n",
       "           -3.0029e-01,  1.2781e-01,  1.7566e-01, -5.3271e-01, -1.7017e-01,\n",
       "           -1.8884e-01,  5.1221e-01, -1.3281e-01,  1.5833e-01,  1.7090e-01,\n",
       "           -1.6586e-02, -5.1758e-02, -6.9641e-02, -1.8286e-01, -6.0760e-02,\n",
       "           -3.3032e-01,  1.5991e-01,  1.5527e-01,  3.0591e-01, -3.3276e-01,\n",
       "            3.6523e-01, -1.3867e-01, -2.0178e-01, -2.5098e-01, -1.2372e-01,\n",
       "           -2.5464e-01,  2.5269e-02,  1.2152e-01, -2.1143e-01,  5.0964e-02,\n",
       "            1.7273e-01, -2.5903e-01,  1.2372e-01,  6.6650e-02, -3.8891e-03,\n",
       "           -1.2230e-02, -3.1470e-01,  6.5247e-02, -7.0984e-02, -1.0309e-01,\n",
       "           -2.2473e-01, -8.9966e-02, -1.3245e-01, -3.6475e-01,  2.5162e-02,\n",
       "            1.9531e-01, -2.5073e-01, -6.4392e-02, -3.7573e-01, -1.1060e-01,\n",
       "            1.3635e-01, -2.0300e-01, -1.2964e-01, -6.4392e-02,  2.1350e-01,\n",
       "            7.2823e-03,  1.8188e-01, -3.8159e-01, -1.3818e-01, -5.2612e-02,\n",
       "           -4.9438e-01, -9.2773e-02,  3.0167e-02,  4.5258e-02, -2.2876e-01,\n",
       "           -3.4454e-02, -6.2103e-02, -7.8491e-02, -3.2495e-01, -9.1797e-02,\n",
       "           -8.8318e-02, -3.6182e-01, -6.7139e-02,  1.0017e-02,  1.7798e-01,\n",
       "           -2.0667e-01,  5.3345e-02, -6.9763e-02,  1.3123e-01,  1.5295e-01,\n",
       "            2.1887e-01, -1.0260e-01,  7.4036e-02, -8.0750e-02, -1.2708e-01,\n",
       "           -8.2336e-02,  1.3782e-01, -1.3660e-01, -1.6650e-01, -2.5055e-02,\n",
       "            6.8848e-02,  1.5869e-01,  2.5928e-01, -2.7409e-03,  3.9429e-01,\n",
       "            3.8379e-01,  1.2109e-01,  8.8013e-02, -6.5674e-02, -2.2705e-01,\n",
       "            1.7014e-02,  2.9541e-01, -8.9645e-03, -9.7168e-02,  1.1365e-01,\n",
       "            1.3696e-01, -1.3975e+00,  3.4863e-01,  9.6863e-02,  3.0045e-02,\n",
       "            1.2585e-01,  1.8542e-01, -9.3323e-02, -1.6467e-01, -4.9048e-01,\n",
       "            1.9730e-02, -1.2927e-01, -1.3757e-01,  5.9326e-01, -4.8340e-02,\n",
       "           -3.1677e-02,  1.6675e-01,  1.1011e-01, -1.8481e-01,  3.8037e-01,\n",
       "            2.5293e-01,  1.8933e-01,  7.3120e-02, -1.2756e-01,  3.2043e-02,\n",
       "            6.0693e-01, -2.7856e-01,  3.0542e-01,  1.4412e-02, -8.4521e-01,\n",
       "            3.6597e-01,  1.7346e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.4785e-03, 2.8477e-03, 1.8280e-02, 2.1362e-02, 9.5215e-01, 7.5436e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  367.003906   17.210266  505.576538  309.676270    0.940170      0   \n",
       "  1   54.071632   17.371262  192.751160  308.505615    0.934011      0   \n",
       "  2  169.202560   72.357422  288.474091  153.811523    0.881494     38   \n",
       "  3  473.716675   72.330826  598.675171  153.755554    0.881093     38   \n",
       "  4  461.763733  141.688873  475.366943  156.352081    0.783971     32   \n",
       "  5  149.329773  141.415436  163.333801  156.175812    0.636909     32   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2  tennis racket  \n",
       "  3  tennis racket  \n",
       "  4    sports ball  \n",
       "  5    sports ball  ,\n",
       "  'caption': ['Image did not load'],\n",
       "  'bbox_target': [55.37, 19.42, 140.23, 288.36]},\n",
       " 844: {'image_emb': tensor([[ 0.1221,  0.3315,  0.1499,  ...,  0.9492,  0.2323,  0.5415],\n",
       "          [-0.0504,  0.1877,  0.1896,  ...,  0.7178, -0.2751,  0.2615],\n",
       "          [ 0.0959,  0.1963,  0.0246,  ...,  0.6616,  0.0794,  0.5972],\n",
       "          [ 0.1117,  0.0480,  0.2479,  ...,  0.3159, -0.1004,  0.1802]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 1.3855e-01,  3.9368e-02,  3.7048e-02,  4.2212e-01, -1.6760e-01,\n",
       "           -1.0211e-01, -3.9966e-01, -1.0713e+00, -2.9248e-01,  5.0262e-02,\n",
       "           -2.5244e-01,  1.4832e-01, -4.4617e-02, -1.0583e-01,  5.3760e-01,\n",
       "           -5.3174e-01, -2.9102e-01, -1.2317e-01,  2.7271e-01,  5.0293e-01,\n",
       "            2.0966e-02,  5.8203e-01, -5.2637e-01,  2.8906e-01,  1.9421e-01,\n",
       "           -2.3041e-02,  4.2755e-02, -2.7847e-03, -1.6101e-01,  3.8721e-01,\n",
       "            1.9495e-01,  1.0437e-02, -1.5393e-01,  6.6016e-01, -2.3486e-01,\n",
       "           -5.5078e-01,  5.1117e-02, -2.2290e-01, -1.7542e-01, -4.6967e-02,\n",
       "            2.8305e-02, -1.3452e-01,  2.2949e-01,  1.1568e-03, -2.0117e-01,\n",
       "            1.2199e-02,  1.1591e-01,  2.6392e-01,  2.2083e-01, -1.0971e-02,\n",
       "           -2.0435e-01, -1.5637e-01, -8.8074e-02,  2.1765e-01, -1.9238e-01,\n",
       "            6.0156e-01, -1.3770e-01, -1.3538e-01,  1.9043e-01,  2.4695e-01,\n",
       "           -2.5586e-01,  7.1472e-02, -7.1838e-02, -4.5898e-01, -1.1957e-01,\n",
       "            9.4147e-03, -3.4302e-01,  2.5439e-01, -5.1086e-02,  4.5557e-01,\n",
       "            3.8745e-01,  2.0035e-02,  1.3306e-01,  3.3765e-01,  2.9346e-01,\n",
       "            2.1362e-01, -3.4515e-02, -2.4060e-01,  7.7271e-02, -2.1057e-01,\n",
       "            1.4122e-02, -1.7126e-01,  3.2104e-01, -2.1362e-01, -1.0278e-01,\n",
       "           -2.3697e-02,  1.9604e-01,  2.0752e-01,  1.0449e-01, -1.8750e-01,\n",
       "            2.7954e-02,  2.7905e-01, -1.4209e+00,  4.4336e-01, -1.7529e-01,\n",
       "           -5.7831e-02, -3.2715e-01, -1.3786e-02, -2.1484e-01,  1.9592e-01,\n",
       "            1.6907e-01, -2.6807e-01, -3.2471e-01, -5.4541e-01, -6.6650e-02,\n",
       "            2.3474e-01, -1.3391e-01,  2.0972e-01,  3.1104e-01,  8.1909e-02,\n",
       "           -1.4534e-02,  3.1204e-02,  1.1389e-01,  1.8286e-01,  3.1299e-01,\n",
       "           -2.2498e-01,  3.9825e-02, -7.2571e-02, -9.4604e-02,  1.6785e-01,\n",
       "           -1.8079e-01, -3.1934e-01, -3.6816e-01,  9.4116e-02, -8.9478e-02,\n",
       "           -4.2450e-02,  2.5684e-01,  2.9712e-01,  3.0225e-01,  1.8219e-02,\n",
       "            1.9604e-01,  1.2177e-01,  2.4060e-01,  4.7578e+00,  1.2646e-01,\n",
       "            8.3862e-02, -4.7607e-01, -4.3091e-01, -2.4994e-02, -8.2886e-02,\n",
       "           -4.1687e-02,  2.0801e-01, -2.9736e-01,  1.3171e-01, -2.3926e-01,\n",
       "            9.0210e-02, -2.7197e-01, -2.9858e-01, -7.0374e-02,  4.1901e-02,\n",
       "           -2.6245e-01, -2.6440e-01,  1.2903e-01, -5.5634e-02,  3.1958e-01,\n",
       "            6.9885e-02,  5.5359e-02, -1.0004e-01, -1.8152e-01,  3.8605e-02,\n",
       "            2.5757e-01, -1.9495e-01, -1.5686e-01,  1.7426e-02,  5.7434e-02,\n",
       "           -1.1212e-01,  2.2925e-01, -2.6489e-01,  1.0309e-01, -3.5583e-02,\n",
       "            1.1835e-01,  4.3854e-02,  1.9397e-01,  3.5327e-01, -1.5649e-01,\n",
       "            4.0601e-01, -3.7695e-01,  2.1194e-02, -6.1188e-02,  7.2754e-02,\n",
       "            7.3242e-02,  1.0414e-02,  7.1228e-02, -1.3599e-01, -5.9296e-02,\n",
       "           -4.1162e-01, -5.2734e-01,  2.4036e-01,  5.0049e-01,  1.3977e-01,\n",
       "            1.0666e-02,  2.3828e-01,  4.8999e-01, -2.3697e-02,  7.1533e-02,\n",
       "           -9.9731e-02,  2.4268e-01,  2.8101e-01, -1.0034e-01, -4.5410e-01,\n",
       "            1.6382e-01, -1.8140e-01, -9.4849e-02, -1.2073e-01,  1.6528e-01,\n",
       "           -2.2156e-01,  5.5481e-02, -2.2949e-01, -3.7994e-02, -3.7323e-02,\n",
       "           -5.3418e-01,  1.1639e-01,  1.9629e-01, -1.0162e-01, -2.0032e-01,\n",
       "            4.6021e-01, -6.2988e-01, -3.0371e-01, -2.3267e-01, -4.0222e-02,\n",
       "            1.8478e-02, -9.3079e-02,  2.5659e-01,  5.0684e-01, -5.5511e-02,\n",
       "           -1.6431e-01, -4.9023e-01,  9.7656e-03, -1.3538e-01, -2.2913e-01,\n",
       "            2.7441e-01,  4.6460e-01,  4.0112e-01,  2.6367e-01,  1.5649e-01,\n",
       "           -1.2415e-01,  2.9272e-01,  2.0737e-02,  2.7930e-01, -3.2031e-01,\n",
       "           -1.7859e-01,  5.1611e-01,  1.4148e-01, -3.1104e-01, -7.7576e-02,\n",
       "            2.1875e-01, -9.3323e-02,  1.9153e-01,  2.7441e-01,  3.1152e-01,\n",
       "            2.0129e-01,  1.0193e-01, -7.5684e-02, -2.7490e-01, -4.8730e-01,\n",
       "           -3.9258e-01, -3.6407e-02,  6.2225e-02,  2.0874e-01,  2.8442e-01,\n",
       "           -1.0742e-01, -1.5247e-01,  1.9812e-01,  1.6833e-01, -8.9111e-02,\n",
       "           -2.4158e-01,  2.3779e-01,  7.1472e-02, -7.3914e-02,  3.1445e-01,\n",
       "           -2.8052e-01, -6.9962e-03, -2.5024e-02,  1.5771e-01, -3.7134e-01,\n",
       "            5.6738e-01,  2.8076e-01, -2.1759e-02, -2.1179e-02,  1.2109e-01,\n",
       "            3.1396e-01,  1.2585e-01,  2.6050e-01,  2.6880e-01, -4.0430e-01,\n",
       "           -2.3364e-01,  1.7883e-01, -6.7627e-02,  3.2764e-01, -1.2103e-01,\n",
       "            2.2302e-01, -1.1548e-01,  1.9092e-01, -5.4169e-02, -2.0401e-02,\n",
       "            2.1399e-01, -1.1737e-01,  6.9275e-02, -1.2469e-01,  6.7261e-02,\n",
       "           -4.8169e-01, -1.1658e-01, -4.4312e-02, -3.8599e-01, -3.0151e-01,\n",
       "            1.5979e-01, -3.0045e-02, -2.7417e-01,  1.8677e-01, -3.3350e-01,\n",
       "           -3.7781e-02,  1.5784e-01,  4.7500e+00,  4.5752e-01,  1.8689e-01,\n",
       "            3.1494e-01,  1.2012e-01,  2.6147e-01,  4.1089e-01,  4.8193e-01,\n",
       "           -1.1078e-01, -6.2561e-02, -3.5840e-01,  2.1411e-01, -3.9722e-01,\n",
       "            4.3994e-01, -3.8788e-02, -4.0161e-02, -1.7502e-02, -2.2188e+00,\n",
       "           -2.6514e-01, -3.5938e-01,  2.6245e-01, -7.9468e-02, -2.2552e-02,\n",
       "           -5.1904e-01,  2.0361e-01,  4.2188e-01, -6.0028e-02,  7.6953e-01,\n",
       "           -3.6597e-01, -4.8370e-02, -1.2317e-01,  1.1749e-01, -1.9604e-01,\n",
       "           -7.9712e-02,  3.6407e-02, -7.3700e-03,  9.4910e-02,  4.6936e-02,\n",
       "            2.3523e-01,  1.7407e-01,  2.0654e-01, -1.8335e-01,  1.7090e-01,\n",
       "            8.3923e-03, -2.4658e-01,  7.6675e-03,  1.6931e-01,  3.1665e-01,\n",
       "           -1.4001e-01,  1.9568e-01,  3.5864e-01, -8.3801e-02, -1.5381e-01,\n",
       "            3.7329e-01, -1.4697e-01,  1.5063e-01, -7.8674e-02, -1.3879e-01,\n",
       "            1.6565e-01,  1.0620e-01,  2.6514e-01,  3.5309e-02,  3.4668e-01,\n",
       "            1.9043e-01, -2.9028e-01, -5.5939e-02,  1.6101e-01,  2.2974e-01,\n",
       "           -2.9614e-01, -1.8823e-01,  1.0657e-01,  1.0388e-01, -1.9055e-01,\n",
       "            2.4805e-01, -2.3303e-01,  1.3098e-01, -3.7500e-01, -2.1216e-01,\n",
       "           -9.8584e-01,  2.0093e-01, -1.9116e-01,  2.0032e-01,  2.7466e-01,\n",
       "           -8.1421e-02, -1.8958e-01, -1.0712e-01, -6.1646e-02,  4.0576e-01,\n",
       "            7.6294e-02, -1.5393e-01, -1.5796e-01, -1.2439e-01,  2.8442e-01,\n",
       "           -2.2510e-01, -1.2708e-01, -7.0251e-02,  2.3483e-02, -9.2346e-02,\n",
       "            1.6162e-01,  9.3872e-02, -2.5049e-01, -4.9561e-02,  5.7495e-02,\n",
       "            2.5757e-01, -5.7959e-01,  1.6797e-01, -5.6267e-03, -1.2305e-01,\n",
       "           -1.0461e-01, -5.4834e-01, -3.7646e-01, -3.7451e-01,  8.9844e-02,\n",
       "            4.9585e-01, -3.1885e-01, -3.5498e-01,  2.9224e-01,  1.0291e-01,\n",
       "           -2.9712e-01, -1.6736e-01,  4.3945e-02,  4.6753e-01,  4.6753e-01,\n",
       "           -3.0365e-02, -3.2056e-01,  2.9572e-02,  9.4910e-02,  2.9590e-01,\n",
       "           -3.7933e-02,  2.6465e-01, -3.0731e-02,  7.7332e-02, -2.2815e-01,\n",
       "            3.5864e-01,  2.2449e-01,  5.5786e-02,  1.0156e-01, -2.7100e-01,\n",
       "            4.0863e-02,  4.3384e-01, -1.2976e-01, -4.4403e-02,  4.9316e-02,\n",
       "            1.7590e-01,  2.7368e-01, -4.7485e-01, -8.4961e-02, -2.0825e-01,\n",
       "            1.7468e-01, -1.0559e-01, -5.8252e-01, -1.4233e-01,  6.2256e-01,\n",
       "           -1.9211e-02, -2.1497e-01,  9.5398e-02,  1.5662e-01, -1.7627e-01,\n",
       "            2.1021e-01, -1.0544e-02,  3.5693e-01, -3.5352e-01,  1.0071e-01,\n",
       "           -1.0065e-01, -2.4878e-01,  5.3177e-03,  1.0162e-01,  7.8809e-01,\n",
       "           -1.4389e-02, -3.5498e-01,  1.8707e-02, -2.9327e-02, -6.6797e-01,\n",
       "           -7.1631e-01, -8.8501e-02,  7.9590e-02, -2.0239e-01, -1.9348e-01,\n",
       "            3.5010e-01,  4.6753e-02,  2.3438e-01,  7.5928e-01,  5.1758e-01,\n",
       "           -1.6943e-01, -1.5356e-01,  3.6774e-02,  2.0386e-01, -1.3359e-02,\n",
       "            1.1597e-01, -3.3691e-01, -2.3117e-02,  1.8762e-01, -1.7175e-01,\n",
       "           -7.9407e-02,  8.5815e-02,  6.5979e-02,  6.9542e-03, -1.0388e-01,\n",
       "            2.1313e-01,  8.1360e-02]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2881, 0.3999, 0.3115, 0.0007]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   37.279579   81.735123  298.213074  388.118561    0.883169     46  banana\n",
       "  1  348.162506   19.507084  587.513245  299.390717    0.823447     46  banana\n",
       "  2  295.445160  123.541817  510.623810  327.088623    0.801099     46  banana\n",
       "  3  450.822723   62.483624  602.240173  263.720001    0.688440     46  banana,\n",
       "  'caption': ['The farthest right banana.'],\n",
       "  'bbox_target': [449.52, 58.88, 152.8, 211.66]},\n",
       " 845: {'image_emb': tensor([[-0.2756,  0.3284,  0.2144,  ...,  0.7476, -0.1837, -0.0458],\n",
       "          [-0.2211,  0.3228,  0.2437,  ...,  0.5474, -0.1421,  0.1223]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0042,  0.2007,  0.0581,  ..., -0.0160, -0.3503, -0.2578],\n",
       "          [-0.1525, -0.0383, -0.2014,  ..., -0.0622, -0.1437, -0.2308]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.8706, 0.1294],\n",
       "          [0.9497, 0.0503]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class name\n",
       "  0   84.238068  145.014954  598.000854  440.928345    0.909321     59  bed\n",
       "  1  359.544495  348.117188  600.000000  446.929810    0.283150     59  bed,\n",
       "  'caption': ['The mattress on the white framed bed.',\n",
       "   'KIDS BED ON THE CRADLE'],\n",
       "  'bbox_target': [358.64, 351.96, 241.36, 98.04]},\n",
       " 846: {'image_emb': tensor([[-0.3291,  0.7290, -0.2113,  ...,  0.4016,  0.1142, -0.0245],\n",
       "          [-0.1664,  0.2462,  0.0742,  ...,  1.1523,  0.3384, -0.0535],\n",
       "          [ 0.1598,  0.2507, -0.2803,  ...,  0.7827,  0.3511,  0.3662],\n",
       "          ...,\n",
       "          [-0.1384,  0.7295, -0.0648,  ...,  1.2148,  0.3235, -0.3274],\n",
       "          [ 0.2288, -0.1593, -0.5698,  ...,  0.6191, -0.1305,  0.1732],\n",
       "          [-0.2793,  0.6875, -0.2812,  ...,  0.3289, -0.0034,  0.1952]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0481,  0.2646, -0.0161,  ...,  0.3101,  0.3347, -0.4700],\n",
       "          [ 0.1729, -0.1335, -0.0265,  ...,  0.5381,  0.3506, -0.3110]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.7744e-01, 5.3644e-07, 0.0000e+00, 2.3842e-07, 5.9605e-07, 8.9407e-07,\n",
       "           1.2250e-01],\n",
       "          [5.2637e-01, 7.4387e-04, 1.1951e-04, 9.3102e-05, 2.9111e-04, 2.4533e-04,\n",
       "           4.7192e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   12.152939  154.118652  331.685547  501.081543    0.911380      0   \n",
       "  1  358.799469   78.790825  426.697021  130.689301    0.792445      1   \n",
       "  2  290.476105   83.252075  354.842499  136.277008    0.787327      1   \n",
       "  3  375.596100   16.779980  403.789215  125.266144    0.765229      0   \n",
       "  4   11.187492  448.205994  150.348755  497.991028    0.749772     36   \n",
       "  5  312.274994   18.020466  343.881195  133.494553    0.745770      0   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1     bicycle  \n",
       "  2     bicycle  \n",
       "  3      person  \n",
       "  4  skateboard  \n",
       "  5      person  ,\n",
       "  'caption': ['A man in black clothes riding a skate board.',\n",
       "   'the skateboarder doing a grind'],\n",
       "  'bbox_target': [93.48, 158.56, 235.87, 342.29]},\n",
       " 847: {'image_emb': tensor([[-0.9688, -0.2188, -0.0645,  ...,  0.8042,  0.0244, -0.2150],\n",
       "          [-0.5913,  0.1490,  0.2164,  ...,  0.3694, -0.0793,  0.0119],\n",
       "          [-0.5938,  0.2148,  0.0308,  ...,  1.0615,  0.0586, -0.1991],\n",
       "          [-0.6216,  0.0071,  0.0731,  ...,  0.6392,  0.1351, -0.0838],\n",
       "          [-0.2935, -0.3452,  0.0748,  ...,  0.3743,  0.0812, -0.1433]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.5361, -0.1065, -0.3901,  ...,  0.4248, -0.2913, -0.1742],\n",
       "          [-0.6094, -0.3276, -0.2925,  ...,  0.4609, -0.2783, -0.3652]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.0485e-03, 7.5293e-01, 7.5388e-04, 8.1956e-05, 2.4438e-01],\n",
       "          [4.4518e-03, 5.7373e-01, 1.6890e-03, 6.7592e-05, 4.1992e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  120.573853  247.089767  190.675842  335.065247    0.879310     56   \n",
       "  1  130.739212  212.091812  426.282166  552.572021    0.834837      0   \n",
       "  2  300.347900  410.180786  425.880920  462.366089    0.735913     38   \n",
       "  3  149.257294  163.069138  207.367462  330.635986    0.731888      0   \n",
       "  4  201.944366  186.930969  248.422150  249.744232    0.553607      0   \n",
       "  5   80.178055  298.061005  122.848892  339.363312    0.533662     24   \n",
       "  6  117.442947  310.850922  127.675247  337.193207    0.520819     39   \n",
       "  7  197.347198  185.655579  248.012238  337.203430    0.312211      0   \n",
       "  \n",
       "              name  \n",
       "  0          chair  \n",
       "  1         person  \n",
       "  2  tennis racket  \n",
       "  3         person  \n",
       "  4         person  \n",
       "  5       backpack  \n",
       "  6         bottle  \n",
       "  7         person  ,\n",
       "  'caption': ['Guy leaning down with a tennis racket in red and white shorts.',\n",
       "   'Tennis player straddling white line wearing red shorts with white stripe.'],\n",
       "  'bbox_target': [134.36, 209.12, 292.64, 335.17]},\n",
       " 848: {'image_emb': tensor([[ 0.4116, -0.0769, -0.1467,  ...,  0.7817, -0.1973,  0.0754],\n",
       "          [ 0.3894,  0.0516, -0.0399,  ...,  0.3450,  0.0941, -0.0423],\n",
       "          [ 0.2720, -0.1814, -0.1342,  ...,  1.1514,  0.1315,  0.1074],\n",
       "          [-0.1501, -0.3850,  0.0481,  ...,  0.2285, -0.0279,  0.1525]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1216, -0.3181, -0.4094,  ..., -0.0068,  0.0618, -0.0812],\n",
       "          [ 0.1940, -0.1174, -0.2739,  ...,  0.0997, -0.3013, -0.2457]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.8256e-03, 3.0975e-02, 1.5335e-02, 9.4873e-01],\n",
       "          [6.8998e-04, 8.8477e-01, 1.8759e-03, 1.1249e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  386.229340   61.680420  600.595459  420.549133    0.927038      0   person\n",
       "  1   28.861465  172.717407  357.813599  322.726440    0.914449     23  giraffe\n",
       "  2  424.480103   26.608032  589.734802  240.191895    0.780377     23  giraffe,\n",
       "  'caption': [\"A giraffe directly behind a woman with it's head up.\",\n",
       "   'The giraffe standing closest to the tree.'],\n",
       "  'bbox_target': [425.67, 39.25, 166.94, 199.56]},\n",
       " 849: {'image_emb': tensor([[-0.6270,  0.0432,  0.3184,  ...,  0.8164,  0.2112, -0.1390],\n",
       "          [-0.0316, -0.1050, -0.2690,  ...,  1.2695,  0.0397, -0.1146],\n",
       "          [-0.0368, -0.0074, -0.3262,  ...,  0.5391,  0.0953,  0.1083],\n",
       "          [-0.1863,  0.2133,  0.1137,  ...,  0.9492,  0.1227, -0.0687]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0960,  0.2196, -0.3376,  ..., -0.1897, -0.2207,  0.0582],\n",
       "          [-0.0385, -0.0758, -0.1122,  ..., -0.4036,  0.1501, -0.0248]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.7402e-02, 8.3374e-02, 2.9802e-07, 8.2910e-01],\n",
       "          [2.7008e-02, 8.6670e-01, 3.1586e-02, 7.4585e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  312.515320    3.014160  638.631958  475.656372    0.956957      0   \n",
       "  1    0.369236  135.435181  116.511414  473.188599    0.895831      0   \n",
       "  2  329.808594  324.618225  491.730347  358.993225    0.794283     79   \n",
       "  3  432.780731  333.499847  492.758087  359.042816    0.368225     79   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2  toothbrush  \n",
       "  3  toothbrush  ,\n",
       "  'caption': ['A man brushing his teeth, not facing the camera, looking at his reflection in the mirror.',\n",
       "   'The face of the man not seen in the mirror'],\n",
       "  'bbox_target': [316.04, 0.63, 323.96, 479.37]},\n",
       " 850: {'image_emb': tensor([[ 0.3186, -0.0103, -0.0715,  ...,  0.3130,  0.0026, -0.2747],\n",
       "          [ 0.1215,  0.0640, -0.2720,  ...,  0.8228, -0.0273, -0.1901],\n",
       "          [-0.0995, -0.4087,  0.0157,  ...,  0.8433,  0.0369, -0.1383],\n",
       "          [-0.1375, -0.4294,  0.2100,  ...,  0.4866, -0.1532, -0.1718]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2047, -0.0143, -0.2439,  ..., -0.2466, -0.2103, -0.0010],\n",
       "          [ 0.3081, -0.4121, -0.3293,  ...,  0.0252, -0.2930, -0.2957]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5522, 0.1849, 0.2000, 0.0629],\n",
       "          [0.2417, 0.0947, 0.1356, 0.5278]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  259.078888   81.278610  375.886932  390.314941    0.934470     23  giraffe\n",
       "  1  129.428391  169.807861  180.882980  375.022827    0.865502     23  giraffe\n",
       "  2  210.757736  156.796082  308.776917  387.358459    0.832055     23  giraffe\n",
       "  3  230.875183  140.826309  301.019531  266.079407    0.433473     23  giraffe,\n",
       "  'caption': ['The closest giraffe.',\n",
       "   'A giraffe looking to its right with three other giraffes behind it.'],\n",
       "  'bbox_target': [267.02, 81.94, 107.96, 314.25]},\n",
       " 851: {'image_emb': tensor([[-0.1825,  0.5244,  0.3008,  ...,  0.7783, -0.1499,  0.0896],\n",
       "          [ 0.1750,  0.2457,  0.2386,  ...,  1.0723,  0.2893,  0.1241],\n",
       "          [-0.0437,  0.3560, -0.1183,  ...,  1.3213,  0.0970, -0.1333],\n",
       "          [-0.0536,  0.1281, -0.1814,  ...,  1.2979,  0.0866, -0.1273],\n",
       "          [-0.3142,  0.2292,  0.1654,  ...,  0.8384, -0.0654, -0.1624]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0531,  0.3350, -0.3743,  ...,  0.3875, -0.2644, -0.3757],\n",
       "          [ 0.0687, -0.1793, -0.0667,  ..., -0.3647, -0.2917,  0.2637]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.4353, 0.2810, 0.0076, 0.0037, 0.2725],\n",
       "          [0.7427, 0.0094, 0.0015, 0.0013, 0.2450]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  161.248001  161.927261  486.536743  458.107422    0.934873      0   \n",
       "  1  391.253479   98.839706  530.703247  225.759155    0.882507     34   \n",
       "  2   76.259583  274.047485  122.546097  404.687744    0.874713      0   \n",
       "  3   30.597588  281.076782   72.756447  383.430847    0.826502      0   \n",
       "  4    0.000000  263.614716   23.895958  410.145905    0.477729      0   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1  baseball bat  \n",
       "  2        person  \n",
       "  3        person  \n",
       "  4        person  ,\n",
       "  'caption': ['The baseball player holding the bat', 'A man in blue.'],\n",
       "  'bbox_target': [161.27, 153.99, 325.66, 303.81]},\n",
       " 852: {'image_emb': tensor([[-0.1997,  0.7505,  0.0613,  ...,  1.2480,  0.2065, -0.3306],\n",
       "          [ 0.0883,  0.6343, -0.5166,  ...,  0.9541, -0.2664,  0.0981],\n",
       "          [-0.0316,  0.4907, -0.2480,  ...,  0.8286,  0.2161, -0.1707],\n",
       "          ...,\n",
       "          [ 0.0961,  0.3250, -0.0353,  ...,  0.9463, -0.1569, -0.2671],\n",
       "          [ 0.2751,  0.8184, -0.5698,  ...,  0.7251, -0.1610, -0.2581],\n",
       "          [ 0.3689,  0.3901, -0.1431,  ...,  0.6377,  0.1754, -0.1682]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1752, -0.1263, -0.1857,  ..., -0.6675,  0.0963, -0.2124],\n",
       "          [ 0.3342,  0.1473, -0.4714,  ..., -0.0578,  0.0594,  0.0200]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.5062e-03, 1.0445e-02, 7.4043e-03, 9.5508e-01, 1.7319e-03, 5.7678e-03,\n",
       "           1.0284e-02],\n",
       "          [1.3709e-06, 3.3932e-03, 5.5084e-03, 2.9945e-03, 1.1921e-07, 1.8740e-03,\n",
       "           9.8633e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  160.260269  253.478943  252.831314  400.581665    0.935186     41   \n",
       "  1  288.580475  277.039673  375.664032  349.427063    0.898423     54   \n",
       "  2   40.884338   44.447418  290.431335  248.778625    0.874632      0   \n",
       "  3  259.651611   89.532440  418.337524  244.337250    0.868142      0   \n",
       "  4  542.104736  199.872162  639.596680  238.779694    0.835762      2   \n",
       "  5    0.255341  305.795868  640.000000  471.985596    0.747803     60   \n",
       "  6  192.419037  180.478455  233.440674  220.702637    0.622998     67   \n",
       "  7  524.179688  217.621460  533.059448  237.925964    0.530863     12   \n",
       "  8  231.302002  170.368164  259.426849  204.610962    0.491656     67   \n",
       "  \n",
       "              name  \n",
       "  0            cup  \n",
       "  1          donut  \n",
       "  2         person  \n",
       "  3         person  \n",
       "  4            car  \n",
       "  5   dining table  \n",
       "  6     cell phone  \n",
       "  7  parking meter  \n",
       "  8     cell phone  ,\n",
       "  'caption': ['A face of a man.',\n",
       "   'A bald man in sunglesses standing behind a lady taking a picture of a donut.'],\n",
       "  'bbox_target': [258.09, 86.29, 137.49, 162.82]},\n",
       " 853: {'image_emb': tensor([[ 3.9124e-02,  5.4053e-01, -2.8882e-01,  ...,  1.3418e+00,\n",
       "            9.7595e-02,  5.5725e-02],\n",
       "          [ 3.1519e-01,  4.1968e-01, -6.1951e-02,  ...,  1.3545e+00,\n",
       "            1.0077e-01, -9.1858e-03],\n",
       "          [ 2.6147e-01,  3.2983e-01,  2.3657e-01,  ...,  4.3457e-01,\n",
       "            5.3253e-02, -1.4626e-02],\n",
       "          [ 5.6839e-04,  6.6797e-01, -4.2847e-02,  ...,  9.0820e-01,\n",
       "            1.0571e-01, -2.2827e-01],\n",
       "          [ 5.3406e-02,  3.0029e-01, -1.2108e-02,  ...,  9.6289e-01,\n",
       "            9.7534e-02,  1.0277e-02],\n",
       "          [ 3.2178e-01,  8.1421e-02,  1.8298e-01,  ...,  9.8877e-01,\n",
       "            2.9648e-02, -5.4053e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-2.5833e-02,  1.3147e-01, -2.4194e-01,  1.4807e-01,  1.0297e-01,\n",
       "           -2.5757e-01, -3.3789e-01, -5.6787e-01,  4.4556e-01, -1.9836e-02,\n",
       "            2.5928e-01,  1.4087e-01,  5.0928e-01,  4.7180e-02, -2.5610e-01,\n",
       "            1.9373e-01,  1.5247e-01, -2.1472e-01, -1.9608e-02,  4.2236e-01,\n",
       "            2.4329e-01,  7.7539e-01,  2.3328e-01, -2.9755e-02, -1.2476e-01,\n",
       "            4.7949e-01, -3.7036e-01, -9.2346e-02, -7.4829e-02, -1.0204e-03,\n",
       "           -5.1392e-02, -1.6272e-01,  3.7500e-01, -8.0750e-02, -1.1365e-01,\n",
       "           -1.5540e-01,  3.8843e-01,  3.2446e-01, -8.6594e-03, -3.8971e-02,\n",
       "           -2.9541e-01,  5.8203e-01,  2.2095e-02,  2.9083e-02,  2.6245e-01,\n",
       "            3.1885e-01,  2.6611e-01,  3.7598e-01,  1.3025e-01, -3.6670e-01,\n",
       "           -9.0576e-02, -9.2239e-03,  4.5825e-01,  3.8223e-03, -1.4417e-01,\n",
       "            1.8579e-01, -2.1887e-01,  7.1777e-02, -9.2621e-03,  4.5874e-01,\n",
       "            5.9668e-01, -1.8994e-01,  1.2842e-01, -4.2065e-01,  2.7856e-01,\n",
       "            4.2328e-02,  1.0883e-01,  8.2764e-01, -4.5215e-01,  1.6756e-03,\n",
       "            7.0557e-02, -4.1650e-01, -4.1473e-02,  4.9927e-01,  4.0649e-01,\n",
       "           -1.4343e-01,  3.5913e-01, -3.1323e-01,  8.0444e-02, -2.7539e-01,\n",
       "           -1.7188e-01,  7.7515e-02,  1.5210e-01, -1.4734e-01, -2.8061e-02,\n",
       "           -1.1572e-01, -1.8152e-01,  1.4685e-01, -6.8896e-01,  3.3521e-01,\n",
       "           -1.6190e-02, -6.8652e-01, -1.0508e+00,  7.2266e-02,  1.0138e-01,\n",
       "           -7.1350e-02, -3.9600e-01, -2.7002e-01,  3.8135e-01, -2.6270e-01,\n",
       "            1.5149e-01,  2.2046e-01, -1.5552e-01,  2.0581e-01,  2.8003e-01,\n",
       "            2.7466e-01, -1.9202e-01,  1.9678e-01,  1.7859e-01, -1.9702e-01,\n",
       "           -2.8540e-01,  1.4624e-01,  3.5187e-02,  3.7720e-02,  3.3398e-01,\n",
       "           -2.4805e-01, -4.2725e-02, -6.9031e-02,  2.3727e-02,  2.6611e-02,\n",
       "           -9.0820e-02, -2.7808e-01,  4.0710e-02,  1.2830e-01, -8.7402e-02,\n",
       "           -3.7598e-02,  3.1677e-02, -2.6108e-02,  1.7322e-01,  4.7290e-01,\n",
       "            4.3555e-01, -8.2336e-02, -2.5513e-01,  3.6426e+00, -4.0649e-01,\n",
       "           -3.5303e-01, -1.2213e-01,  8.8074e-02,  8.8440e-02,  3.6743e-02,\n",
       "            3.5889e-02,  1.0419e-01, -2.0728e-01,  1.6638e-01,  1.9116e-01,\n",
       "           -2.9370e-01,  1.9043e-01, -2.1301e-01,  5.1074e-01, -1.4709e-01,\n",
       "           -7.2021e-02,  1.0760e-01,  2.7100e-01, -8.7891e-02, -1.0938e-01,\n",
       "            1.8469e-01,  4.4116e-01, -3.7817e-01, -2.6465e-01,  1.2231e-01,\n",
       "           -1.6687e-01, -3.4082e-01, -2.8885e-02,  1.9812e-01,  1.5625e-01,\n",
       "           -8.3694e-03,  4.1162e-01,  7.5012e-02,  3.8208e-01, -2.9468e-01,\n",
       "            2.2131e-01, -3.7750e-02,  4.2407e-01,  4.0723e-01, -1.8420e-01,\n",
       "           -2.4048e-02,  3.8037e-01,  2.0557e-01, -6.2744e-01, -3.6768e-01,\n",
       "           -1.7493e-01,  4.8071e-01,  2.6782e-01, -2.1594e-01, -2.4585e-01,\n",
       "           -9.8572e-02,  7.8418e-01,  8.6731e-02,  7.1729e-01,  3.6084e-01,\n",
       "            6.5771e-01,  3.7769e-01, -2.1863e-01, -2.0544e-01,  4.3335e-02,\n",
       "           -3.7817e-01, -6.7480e-01,  9.0234e-01,  3.7451e-01, -1.1127e-01,\n",
       "            2.4246e-02, -1.8091e-01, -1.7548e-02,  2.3743e-02, -3.1616e-02,\n",
       "            2.3291e-01,  3.0713e-01, -1.7822e-01, -7.1960e-02, -3.1342e-02,\n",
       "           -4.5532e-02,  6.3525e-01,  1.1859e-01,  1.1086e-02, -1.8298e-01,\n",
       "           -3.7933e-02,  2.9517e-01, -8.5693e-02,  2.2697e-03, -2.3718e-01,\n",
       "            2.3743e-01, -2.5977e-01, -2.6318e-01, -1.9894e-03, -5.2441e-01,\n",
       "           -1.5955e-01, -2.1033e-01, -1.7188e-01,  2.8345e-01, -3.3716e-01,\n",
       "            4.6753e-02,  2.6929e-01,  8.7524e-02,  3.6201e-03, -1.6187e-01,\n",
       "           -2.9663e-01,  2.9907e-01,  2.7802e-02,  1.0034e-01, -8.7219e-02,\n",
       "           -7.8064e-02,  1.2213e-01,  4.3396e-02,  1.2561e-01, -5.7812e-01,\n",
       "           -4.0063e-01,  5.2948e-02, -1.0284e-01, -2.2742e-01, -7.2144e-02,\n",
       "           -1.1725e-01, -2.0459e-01, -2.5049e-01, -8.0750e-02, -1.0675e-01,\n",
       "            9.4666e-02,  3.1104e-01, -3.3765e-01,  5.9235e-02, -8.6243e-02,\n",
       "           -3.7262e-02,  6.9336e-02, -1.3086e-01, -4.5557e-01,  1.4575e-01,\n",
       "           -2.7124e-01, -3.9948e-02, -1.6394e-01,  4.6973e-01, -2.8247e-01,\n",
       "           -1.5710e-01, -9.6375e-02,  3.2642e-01,  8.7402e-02, -1.5771e-01,\n",
       "           -1.5149e-01,  1.4679e-02, -1.9348e-02,  1.7676e-01, -5.9717e-01,\n",
       "            1.7468e-01,  7.4707e-02,  1.5015e-01,  1.7603e-01, -3.8770e-01,\n",
       "            1.8958e-01, -1.4917e-01, -4.7339e-01,  2.0093e-01, -3.3081e-01,\n",
       "            9.3262e-02,  5.2295e-01,  3.5596e-01,  3.7939e-01,  2.6636e-01,\n",
       "            6.0791e-01, -2.1143e-01,  1.0248e-01, -4.1656e-02, -2.7008e-02,\n",
       "            2.2729e-01,  3.4088e-02,  2.0215e-01,  2.0248e-02, -1.5442e-01,\n",
       "           -6.1719e-01,  1.7505e-01, -6.1378e-03,  1.7676e-01, -2.5562e-01,\n",
       "            1.5295e-01,  7.7148e-01,  3.6367e+00,  5.9229e-01,  1.9678e-01,\n",
       "            3.6255e-01,  3.4180e-01, -8.2458e-02,  1.6492e-01, -5.9296e-02,\n",
       "            1.7993e-01,  5.9357e-02, -8.6670e-02,  1.4820e-03, -3.1934e-01,\n",
       "            1.4087e-01, -9.5703e-02,  4.3457e-02,  8.0444e-02, -1.0967e+00,\n",
       "            4.8853e-01, -1.2366e-01,  7.7209e-02, -7.1484e-01,  1.6553e-01,\n",
       "           -6.3354e-02,  5.1331e-02,  3.3936e-02, -2.5366e-01,  3.2886e-01,\n",
       "           -1.0931e-01, -2.1008e-01,  2.5513e-01, -3.7305e-01,  2.8870e-02,\n",
       "           -6.3232e-02,  1.8799e-01,  3.2275e-01,  4.0747e-01, -1.0822e-01,\n",
       "            6.8994e-01, -9.1248e-02,  1.2769e-01, -1.5930e-01, -1.9421e-01,\n",
       "           -4.2847e-02,  5.2393e-01,  2.1326e-01,  1.2018e-01,  1.2732e-01,\n",
       "            2.6581e-02, -5.5371e-01,  1.6699e-01, -2.3059e-01,  2.3193e-01,\n",
       "           -1.4929e-01, -3.2715e-01,  3.2666e-01,  1.7670e-02,  3.1714e-01,\n",
       "           -1.8127e-01, -1.5393e-01,  3.8110e-01,  1.0931e-01, -1.7737e-01,\n",
       "           -1.5784e-01, -1.3257e-01, -3.8319e-03, -4.5215e-01,  5.4901e-02,\n",
       "           -2.7710e-01,  5.3070e-02,  1.4050e-01, -7.1240e-01, -5.8228e-02,\n",
       "           -1.9165e-01,  9.4452e-03, -4.6783e-02, -5.3320e-01,  2.1802e-01,\n",
       "           -3.7061e-01, -3.8483e-02, -1.2402e-01, -7.8491e-02,  1.7566e-01,\n",
       "            2.4384e-02,  5.5115e-02,  1.2311e-01,  2.1286e-02,  3.5986e-01,\n",
       "            2.2876e-01, -2.1680e-01,  2.3212e-03, -3.3984e-01, -6.9641e-02,\n",
       "           -1.4636e-01,  8.5815e-02,  5.4749e-02,  3.8892e-01, -1.7737e-01,\n",
       "            5.2441e-01, -5.8784e-03,  2.7954e-01,  2.4902e-01, -1.9849e-01,\n",
       "            2.8625e-02, -3.9124e-02, -1.5356e-01,  2.1143e-01, -3.3936e-01,\n",
       "            2.5122e-01, -1.8921e-01, -2.2266e-01, -2.0691e-01,  7.3730e-02,\n",
       "           -3.6499e-01, -1.5759e-01,  1.2384e-01,  1.2152e-01, -7.0251e-02,\n",
       "            2.7148e-01, -1.5002e-01,  1.6040e-01,  2.7832e-01, -3.5583e-02,\n",
       "           -1.1700e-01, -3.5278e-01,  3.5571e-01, -6.8787e-02,  3.1592e-01,\n",
       "           -4.4141e-01, -2.3145e-01, -6.8945e-01, -2.5562e-01, -2.8351e-02,\n",
       "            1.0461e-01,  1.2079e-01, -4.9414e-01, -2.1631e-01,  1.1127e-01,\n",
       "           -1.0229e-01, -4.0454e-01,  1.9852e-02,  1.2436e-02, -1.7566e-01,\n",
       "           -2.3535e-01,  3.4033e-01,  7.6050e-02,  1.4197e-01,  1.2226e-03,\n",
       "            8.9966e-02,  2.5903e-01,  4.4861e-02,  1.6223e-01,  3.6157e-01,\n",
       "           -2.0605e-01,  1.7603e-01, -4.2261e-01, -1.0193e-01,  1.7712e-01,\n",
       "            4.3311e-01,  2.6367e-01,  2.5098e-01,  1.3257e-01,  4.8126e-02,\n",
       "            3.8477e-01,  5.3418e-01,  1.9226e-01, -1.5771e-01,  2.4658e-01,\n",
       "           -9.2224e-02, -5.8594e-01,  1.2952e-01, -2.2717e-01, -1.1102e-01,\n",
       "           -2.1460e-01, -4.1161e-03, -1.5454e-01,  2.3975e-01,  1.3817e-02,\n",
       "            1.5613e-01,  3.4619e-01, -2.1835e-02,  7.1191e-01,  4.8804e-01,\n",
       "            6.9458e-02,  2.0044e-01,  2.9495e-02, -4.1162e-01,  2.6782e-01,\n",
       "           -4.2694e-02,  4.3304e-02, -2.2327e-01,  4.4043e-01,  6.3135e-01,\n",
       "            3.1647e-02, -2.0435e-01,  4.5532e-01, -2.9492e-01,  2.4744e-01,\n",
       "            1.5161e-01, -2.4792e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.4506e-06, 3.5327e-01, 2.4280e-01, 3.1177e-01, 9.2163e-02, 9.8228e-05]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  325.031738   47.402969  443.685608  424.679810    0.942099      0   \n",
       "  1  197.476608   94.078888  348.534546  425.042358    0.902220      0   \n",
       "  2  444.187073  171.293457  599.101013  423.452332    0.866583     37   \n",
       "  3  154.750305   24.389084  282.684631  422.549133    0.823680      0   \n",
       "  4  112.550560  205.505859  286.708618  311.567383    0.744836     37   \n",
       "  5  429.552979  225.576447  598.864014  422.906799    0.339119      0   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2  surfboard  \n",
       "  3     person  \n",
       "  4  surfboard  \n",
       "  5     person  ,\n",
       "  'caption': ['The surfboard of the young man with black hair.'],\n",
       "  'bbox_target': [113.41, 204.89, 235.86, 104.09]},\n",
       " 854: {'image_emb': tensor([[-0.0326,  0.3586, -0.1841,  ...,  1.0625, -0.1132,  0.1226],\n",
       "          [ 0.0674,  0.3848, -0.1980,  ...,  1.3281,  0.2191, -0.0490],\n",
       "          [-0.0061,  0.0409, -0.2825,  ...,  1.4727,  0.0478, -0.1191],\n",
       "          [ 0.0817,  0.1873, -0.4114,  ...,  1.3740,  0.1393,  0.0371],\n",
       "          [-0.1835,  0.1959, -0.3596,  ...,  1.2354, -0.0461,  0.2493],\n",
       "          [-0.0874,  0.2922, -0.1846,  ...,  0.4629,  0.3289,  0.4563]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0482,  0.0617, -0.4468,  ..., -0.1848, -0.1078,  0.1583],\n",
       "          [-0.0417, -0.1846, -0.2969,  ...,  0.1926,  0.1023, -0.1786]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.2432e-01, 4.1723e-04, 1.3603e-02, 5.8234e-05, 7.0953e-04, 6.0974e-02],\n",
       "          [2.5988e-04, 5.2185e-03, 3.8743e-06, 2.1994e-05, 1.2517e-05, 9.9463e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0   97.222389    1.135719  207.222885  253.767349    0.918811      0    person\n",
       "  1    0.000000  102.290108   96.920395  436.274628    0.902853      0    person\n",
       "  2   67.985443  155.768417  151.490356  271.606140    0.882486     28  suitcase\n",
       "  3  172.900772    0.000000  212.634140   59.986980    0.782035     26   handbag\n",
       "  4    0.592250  160.400421   64.246658  243.975739    0.700696     26   handbag\n",
       "  5  244.228943    0.130773  281.221771   32.903988    0.560493      0    person\n",
       "  6  182.881760   91.206429  218.701721  155.400391    0.518733     26   handbag\n",
       "  7  364.161499    0.016215  374.711700   12.764117    0.266472      0    person,\n",
       "  'caption': ['Back of a female wearing black shorts.',\n",
       "   'A lady wearing heels waiting for a train.'],\n",
       "  'bbox_target': [97.32, 0.11, 110.73, 262.86]},\n",
       " 855: {'image_emb': tensor([[-0.2805,  0.7134,  0.0252,  ...,  0.8252,  0.1281, -0.1228],\n",
       "          [-0.2969,  0.2281, -0.0364,  ...,  0.9199,  0.2416, -0.1225],\n",
       "          [ 0.2725,  0.4883, -0.1343,  ...,  0.7412,  0.0797,  0.2969],\n",
       "          ...,\n",
       "          [ 0.1520, -0.1489, -0.3757,  ...,  1.0137, -0.0662, -0.2561],\n",
       "          [ 0.5679,  0.3633, -0.1008,  ...,  1.2031,  0.1566, -0.0833],\n",
       "          [ 0.2036,  0.3562, -0.1019,  ...,  0.6406,  0.1140,  0.5166]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2030, -0.0452,  0.1407,  ...,  0.5234, -0.2600,  0.1024],\n",
       "          [ 0.2150,  0.0386,  0.2162,  ...,  0.6074, -0.3611,  0.0710]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.9605e-08, 2.3246e-06, 1.8921e-01, 1.9550e-04, 6.8115e-01, 5.9605e-08,\n",
       "           1.1921e-07, 5.9605e-08, 2.3842e-07, 3.3588e-03, 1.2598e-01],\n",
       "          [5.9605e-08, 1.5497e-06, 2.1045e-01, 1.3185e-04, 6.4795e-01, 5.9605e-08,\n",
       "           5.9605e-08, 5.9605e-08, 1.7881e-07, 9.6893e-03, 1.3171e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     8.260796  134.795227  147.863312  198.554077    0.932208      2   \n",
       "  1   306.002289   94.631149  414.972137  169.290649    0.908788      7   \n",
       "  2    93.437370  146.565918  253.110596  342.331970    0.903750      3   \n",
       "  3   143.901367   75.999298  302.707794  178.472626    0.903285      7   \n",
       "  4   281.840210  160.823486  385.799194  289.166870    0.882092      3   \n",
       "  5   555.103943  142.886017  578.075012  207.159882    0.880633      0   \n",
       "  6   581.362000  141.976562  609.693298  208.024536    0.874204      0   \n",
       "  7   542.558655  144.693481  558.797180  203.914185    0.836300      0   \n",
       "  8   505.382660  146.294159  516.234314  181.399323    0.727314      0   \n",
       "  9   318.211578  134.850494  448.752167  229.716339    0.718240      7   \n",
       "  10  525.970459  146.232391  539.680176  181.496368    0.699291      0   \n",
       "  11  612.456970   82.274307  621.868103   97.842972    0.645752      9   \n",
       "  12  317.357239  134.863281  446.290833  229.557617    0.313295      3   \n",
       "  \n",
       "               name  \n",
       "  0             car  \n",
       "  1           truck  \n",
       "  2      motorcycle  \n",
       "  3           truck  \n",
       "  4      motorcycle  \n",
       "  5          person  \n",
       "  6          person  \n",
       "  7          person  \n",
       "  8          person  \n",
       "  9           truck  \n",
       "  10         person  \n",
       "  11  traffic light  \n",
       "  12     motorcycle  ,\n",
       "  'caption': ['A police motorcycle behind another motorcycle.',\n",
       "   'A police motorcycle parked behind another motorcycle.'],\n",
       "  'bbox_target': [91.83, 150.87, 162.12, 192.1]},\n",
       " 856: {'image_emb': tensor([[ 0.1078,  0.1406, -0.0681,  ...,  1.2520,  0.1260,  0.0692],\n",
       "          [ 0.1027,  0.4778, -0.2239,  ...,  1.5176,  0.0118, -0.1707],\n",
       "          [-0.1226,  0.6216,  0.1625,  ...,  1.1650, -0.1398, -0.1715],\n",
       "          [ 0.7954,  0.4265,  0.1076,  ...,  0.5298, -0.1884,  0.0540]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-3.4790e-01, -1.6199e-01,  1.0339e-01, -3.0566e-01, -9.4543e-02,\n",
       "           -9.0515e-02,  7.8308e-02, -5.8398e-01, -8.0200e-02,  1.8774e-01,\n",
       "            2.4719e-01, -1.3824e-02,  1.8665e-01,  2.7881e-01, -2.9755e-02,\n",
       "           -4.7943e-02, -1.3367e-01, -2.5513e-01, -7.2327e-02,  3.0859e-01,\n",
       "            3.0762e-01, -2.9053e-01, -2.4353e-01,  1.8021e-02,  2.7930e-01,\n",
       "            5.8014e-02,  2.6245e-01,  1.2732e-01, -4.0063e-01, -1.0889e-01,\n",
       "           -1.4026e-01, -2.3407e-02,  1.9104e-01, -3.1647e-02, -6.3916e-01,\n",
       "           -2.0288e-01,  5.1819e-02, -3.7500e-01,  4.4629e-01,  2.5171e-01,\n",
       "            9.7534e-02,  3.2690e-01, -3.4668e-01, -2.0544e-01,  1.3428e-01,\n",
       "            2.2144e-03,  9.2712e-02,  2.7051e-01,  4.8279e-02,  5.0195e-01,\n",
       "           -4.8584e-02, -1.9116e-01,  1.1914e-01, -4.8730e-01, -3.4766e-01,\n",
       "           -3.6133e-01,  4.6509e-02,  4.7150e-02, -1.2878e-01,  2.6611e-01,\n",
       "           -2.4194e-01, -1.5778e-02, -3.2593e-01, -3.5938e-01, -4.8169e-01,\n",
       "            4.8431e-02,  8.0505e-02,  4.4702e-01,  5.5725e-02, -5.3174e-01,\n",
       "           -5.5273e-01,  1.7847e-01,  4.1602e-01,  1.9341e-03,  2.3132e-02,\n",
       "            3.1152e-01, -7.1716e-02,  5.7812e-01, -7.1594e-02,  3.2861e-01,\n",
       "           -3.0811e-01,  8.8440e-02,  1.1578e-01,  5.8197e-02, -5.4834e-01,\n",
       "            4.0100e-02,  9.5825e-02,  2.0801e-01, -4.5386e-01, -2.0943e-03,\n",
       "           -7.1411e-03, -6.3623e-01, -1.1543e+00,  5.3418e-01, -1.7920e-01,\n",
       "            3.4595e-01, -1.1542e-01, -8.1253e-03,  1.4702e-02,  4.4739e-02,\n",
       "            7.5989e-02,  1.7114e-01, -1.9397e-01, -2.2144e-01,  2.4939e-01,\n",
       "            2.1289e-01,  1.0040e-02,  2.4731e-01,  5.4108e-02,  2.1899e-01,\n",
       "           -2.4377e-01,  3.1543e-01,  1.7896e-01,  3.7170e-02, -7.2021e-02,\n",
       "            2.5177e-02, -4.5752e-01, -1.0663e-01, -3.4131e-01,  5.6104e-01,\n",
       "            9.2224e-02, -7.0508e-01, -4.8755e-01,  2.4573e-01,  7.6355e-02,\n",
       "            1.7334e-01,  4.5508e-01, -4.7192e-01, -2.1545e-01,  1.5045e-02,\n",
       "           -1.6333e-01, -2.8345e-01, -6.9458e-02,  3.6934e+00, -2.3834e-02,\n",
       "            2.2266e-01,  2.1716e-01, -2.7686e-01, -3.1470e-01, -4.4995e-01,\n",
       "           -6.0938e-01,  6.5918e-02, -2.7930e-01,  3.4375e-01, -3.8843e-01,\n",
       "           -2.4365e-01,  1.4929e-01, -2.8613e-01,  4.3896e-01,  2.1411e-01,\n",
       "            2.1713e-02, -1.2805e-01,  4.6899e-01,  8.1177e-02, -1.5503e-01,\n",
       "            1.7297e-01, -2.2980e-02, -4.2969e-01,  1.2744e-01, -2.6459e-02,\n",
       "            4.0283e-01,  3.0322e-01, -1.9751e-01,  4.8279e-02,  2.4365e-01,\n",
       "           -1.0468e-01,  2.8760e-01, -3.4473e-01,  1.7004e-01, -1.9934e-01,\n",
       "           -1.6882e-01,  1.3660e-01,  6.1914e-01,  4.6069e-01, -3.1836e-01,\n",
       "            7.9736e-01,  3.9764e-02, -7.9269e-03, -2.0782e-02, -1.9507e-01,\n",
       "           -2.2754e-01,  6.4880e-02, -5.2612e-02, -1.7664e-01, -3.8013e-01,\n",
       "           -4.5630e-01,  1.6333e-01, -1.3965e-01, -1.1230e-01,  2.7710e-02,\n",
       "            3.0469e-01,  1.8542e-01,  3.5461e-02, -2.3865e-01,  2.1667e-01,\n",
       "           -4.0009e-02, -2.7002e-01,  3.7170e-02, -3.0859e-01, -1.4600e-01,\n",
       "            2.6172e-01, -6.9214e-02,  1.1707e-01,  6.8115e-02, -4.9658e-01,\n",
       "            1.0474e-01,  3.6133e-01, -1.9421e-01, -7.6233e-02, -5.5469e-01,\n",
       "           -5.1117e-02,  5.2490e-01, -2.9370e-01,  4.3152e-02, -4.3732e-02,\n",
       "           -1.6876e-02,  3.1830e-02, -3.4473e-01,  5.1953e-01,  6.3416e-02,\n",
       "            4.1846e-01, -1.9238e-01, -3.6816e-01, -1.1652e-01, -1.9482e-01,\n",
       "            1.2030e-01,  1.3477e-01,  7.6172e-02, -2.6855e-01,  4.7302e-02,\n",
       "           -1.7981e-01,  3.5352e-01,  4.8584e-01,  1.2781e-01, -9.8755e-02,\n",
       "           -3.5229e-01,  2.3120e-01, -9.8450e-02, -1.4172e-01,  5.1392e-02,\n",
       "            1.7249e-01,  2.6025e-01,  1.5173e-01,  3.5400e-01, -1.8286e-01,\n",
       "            2.7417e-01, -1.4368e-01,  2.1057e-01,  1.8091e-01, -1.0277e-02,\n",
       "           -1.6333e-01, -4.3921e-01,  3.4155e-01, -3.3716e-01, -4.9585e-01,\n",
       "            7.9895e-02,  2.1143e-01,  3.4424e-02,  1.8481e-01, -1.3281e-01,\n",
       "           -1.5784e-01,  2.0068e-01, -6.2408e-02,  8.1177e-02, -1.1664e-01,\n",
       "            1.0022e-01, -1.2683e-01,  1.7786e-01, -2.3108e-01, -5.8868e-02,\n",
       "           -1.6992e-01,  4.4458e-01, -3.0182e-02, -1.1108e-01,  1.5405e-01,\n",
       "           -2.9272e-01, -3.5126e-02,  5.6982e-01,  2.3120e-01, -1.6998e-02,\n",
       "           -1.2292e-01,  2.9602e-02,  3.1097e-02,  2.6855e-01,  2.3608e-01,\n",
       "            1.6162e-01, -2.0105e-01,  1.3281e-01,  2.1851e-01,  3.1403e-02,\n",
       "            9.8114e-03, -8.4839e-02,  1.2769e-01, -3.1281e-02, -9.7504e-03,\n",
       "            3.4937e-01, -1.7346e-01,  4.0894e-01,  9.9609e-02, -2.9419e-01,\n",
       "           -2.1204e-01, -4.0967e-01, -3.5339e-02,  9.8816e-02, -8.3374e-02,\n",
       "           -8.2886e-02, -1.5173e-01,  2.0924e-03, -1.5173e-01, -6.7017e-02,\n",
       "           -2.2485e-01,  6.8701e-01,  3.6875e+00, -1.3635e-01, -6.5613e-02,\n",
       "            3.1586e-02,  1.9043e-01,  1.8384e-01,  2.9224e-01,  2.7026e-01,\n",
       "           -8.9905e-02,  1.9348e-02,  3.3081e-01,  1.7624e-02, -2.1252e-01,\n",
       "            1.6541e-02,  6.8848e-01, -1.0345e-01,  1.9043e-01, -1.7646e+00,\n",
       "            4.4092e-01, -1.8030e-01,  7.1472e-02, -1.5320e-01,  6.6162e-02,\n",
       "            1.9604e-01, -3.6987e-01, -3.2031e-01,  9.6359e-03,  3.8916e-01,\n",
       "           -5.1514e-01, -2.7539e-01,  1.2341e-01, -3.6816e-01,  6.7139e-01,\n",
       "           -3.1055e-01,  2.0154e-01,  7.9163e-02, -1.0150e-01,  1.7212e-01,\n",
       "           -1.0394e-01, -3.5181e-01,  2.7051e-01,  2.4500e-01, -2.4585e-01,\n",
       "           -2.3181e-01, -1.1151e-01, -8.9783e-02, -1.6345e-01,  1.4172e-01,\n",
       "           -3.4302e-01,  1.6577e-01, -1.3306e-01,  3.2422e-01,  1.2006e-01,\n",
       "           -2.0459e-01,  3.0029e-01, -1.5112e-01, -7.1167e-02, -4.2053e-02,\n",
       "           -1.9080e-01, -1.7908e-01, -4.9023e-01, -1.4355e-01, -1.4915e-03,\n",
       "            7.8247e-02, -1.3965e-01, -8.9722e-02, -5.4893e-03, -3.3716e-01,\n",
       "            7.2754e-02,  7.5493e-03,  1.9226e-01,  2.9395e-01, -4.9438e-01,\n",
       "            1.6675e-01,  1.4575e-01, -5.9631e-02, -4.6753e-02, -1.9849e-01,\n",
       "           -4.9634e-01,  2.0471e-01, -5.4199e-01, -9.0820e-02, -1.0339e-01,\n",
       "           -3.5303e-01,  3.4326e-01, -1.4685e-01, -8.2153e-02,  4.9268e-01,\n",
       "            6.6016e-01, -2.1960e-01,  4.4434e-01, -2.9770e-02,  1.6406e-01,\n",
       "           -4.5728e-01, -1.0651e-02, -1.2256e-01, -2.8052e-01,  2.6831e-01,\n",
       "            1.9739e-01,  1.2708e-01,  6.1401e-02,  1.8555e-01,  1.1035e-01,\n",
       "           -5.2637e-01,  2.3889e-01, -3.0279e-05,  4.5117e-01, -2.3010e-01,\n",
       "            3.9032e-02, -2.1350e-01,  2.0227e-01, -3.7769e-01, -1.5710e-01,\n",
       "            1.4966e-01, -2.7393e-01,  8.3130e-02,  1.5698e-01,  3.0908e-01,\n",
       "           -3.1592e-01, -4.7852e-02, -6.2622e-02,  2.2791e-01, -1.1658e-01,\n",
       "           -1.8213e-01, -1.5149e-01, -8.1055e-02, -1.2329e-01, -1.3354e-01,\n",
       "           -1.1780e-01, -1.1365e-01, -8.2397e-02,  6.0272e-03,  3.9581e-02,\n",
       "           -4.3750e-04, -1.0895e-01, -1.9617e-01, -1.4111e-01, -3.8574e-02,\n",
       "           -1.1237e-01,  2.0276e-01,  3.2788e-01, -1.5088e-01,  2.9712e-01,\n",
       "            1.3306e-02, -1.1456e-01,  2.2791e-01,  2.8711e-01, -5.6299e-01,\n",
       "           -1.8750e-01,  1.6638e-01,  1.9263e-01,  9.4666e-02,  2.8198e-01,\n",
       "            7.5073e-02, -2.4231e-01,  7.6965e-02, -2.1411e-01, -2.8214e-02,\n",
       "            5.5859e-01, -3.3984e-01, -3.2471e-01,  4.2572e-02,  1.1981e-01,\n",
       "           -3.7415e-02,  3.5571e-01, -1.9714e-02, -5.6787e-01,  5.4590e-01,\n",
       "            3.7060e-03, -5.1221e-01,  1.4661e-01,  3.7671e-01, -5.5634e-02,\n",
       "           -3.7720e-01, -1.1975e-01, -1.8616e-01, -4.4136e-03, -2.6172e-01,\n",
       "           -1.0187e-01, -5.3125e-01,  4.3384e-01,  9.0723e-01,  3.8770e-01,\n",
       "           -4.8462e-01,  3.9014e-01, -4.3286e-01,  1.8469e-01,  3.2251e-01,\n",
       "            3.0731e-02, -2.7783e-01, -1.2744e-01, -1.5640e-03,  4.3237e-01,\n",
       "           -3.3691e-01, -2.9144e-02, -1.3293e-01,  1.7981e-01,  1.3696e-01,\n",
       "           -5.4810e-02, -2.7148e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.7204e-04, 8.3691e-01, 1.5967e-01, 3.1147e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  259.802826  213.030060  301.249359  252.179596    0.810248     62   \n",
       "  1  547.217285  234.317047  639.454956  332.416046    0.730035     57   \n",
       "  2  154.974487  226.900146  298.043671  342.239197    0.704297     56   \n",
       "  3    0.261482  231.300629  200.530823  393.807892    0.592587     56   \n",
       "  4  478.605591  181.908600  537.765747  282.558441    0.521404     58   \n",
       "  5    0.404144  231.717529  200.585449  393.855286    0.470183     57   \n",
       "  \n",
       "             name  \n",
       "  0            tv  \n",
       "  1         couch  \n",
       "  2         chair  \n",
       "  3         chair  \n",
       "  4  potted plant  \n",
       "  5         couch  ,\n",
       "  'caption': ['A tan chair sitting to the left of another tan chair.'],\n",
       "  'bbox_target': [0.0, 231.8, 199.64, 163.04]},\n",
       " 857: {'image_emb': tensor([[ 0.4595,  0.1394, -0.0624,  ...,  0.9121, -0.1215, -0.1832],\n",
       "          [ 0.2379,  0.2024, -0.3843,  ...,  0.7070, -0.0737, -0.1498],\n",
       "          [ 0.2593,  0.2639, -0.2517,  ...,  0.5298, -0.0127, -0.2864],\n",
       "          [-0.0551,  0.3364, -0.2588,  ...,  1.3125,  0.1231,  0.0747],\n",
       "          [ 0.5312, -0.0023, -0.0396,  ...,  0.3242, -0.1697, -0.0122],\n",
       "          [ 0.5991,  0.0503,  0.0319,  ...,  0.3081, -0.1486, -0.0665]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0119, -0.0009, -0.0192,  ...,  0.1992,  0.1814, -0.4485],\n",
       "          [ 0.1301, -0.1415, -0.1779,  ..., -0.2947,  0.2336, -0.4946]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.6050e-01, 1.2573e-02, 1.6565e-01, 2.9802e-07, 2.1606e-01, 3.4521e-01],\n",
       "          [1.3501e-01, 4.3121e-02, 1.3928e-01, 2.7478e-05, 3.7842e-01, 3.0420e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  152.050079   73.538345  300.088593  245.600525    0.921737     53   pizza\n",
       "  1  136.370773  225.798950  496.330566  424.465088    0.915269     53   pizza\n",
       "  2  325.550415   36.940186  455.853271  226.095825    0.901408     53   pizza\n",
       "  3  521.849426    0.823471  639.226379  382.351868    0.818584      0  person\n",
       "  4    0.000000    0.000000  634.994690  424.044312    0.786998     59     bed,\n",
       "  'caption': ['A piece of pizza forming the left eye of a pizza smiley face.',\n",
       "   'The piece of pizza on the left.'],\n",
       "  'bbox_target': [152.16, 74.44, 148.57, 171.05]},\n",
       " 858: {'image_emb': tensor([[-0.1226,  0.4451, -0.0801,  ...,  1.2119,  0.3135,  0.2157],\n",
       "          [-0.4080, -0.1425, -0.0659,  ...,  0.9106, -0.1829, -0.6392],\n",
       "          [-0.1780,  0.4500,  0.0149,  ...,  1.0352, -0.0782,  0.4500],\n",
       "          ...,\n",
       "          [-0.1128,  0.4419,  0.0674,  ...,  0.8672, -0.1289, -0.2231],\n",
       "          [-0.0558,  0.3308, -0.2693,  ...,  0.9287,  0.0746,  0.1993],\n",
       "          [-0.5557,  0.0480, -0.0595,  ...,  0.6772,  0.1956, -0.3210]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0468, -0.3540, -0.0681,  ...,  0.2341, -0.1130,  0.1206],\n",
       "          [ 0.1698,  0.1007, -0.1815,  ...,  0.2598, -0.0210, -0.4021]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.2002e-01, 8.8120e-03, 1.7807e-02, 6.5498e-03, 4.4507e-01, 1.7726e-04,\n",
       "           9.0027e-04, 1.9467e-04, 4.9233e-05, 3.2592e-04],\n",
       "          [7.0996e-01, 6.9214e-02, 5.6488e-02, 1.4969e-02, 1.2732e-01, 1.2755e-04,\n",
       "           5.0926e-03, 8.9417e-03, 1.2684e-03, 6.3400e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   326.072662  109.123718  412.017426  216.558838    0.928233     41     cup\n",
       "  1   546.088196  197.192719  613.329041  297.902435    0.914099     39  bottle\n",
       "  2   121.874580   25.891037  195.515900  249.500793    0.904506     39  bottle\n",
       "  3   391.012421  252.488617  422.905975  334.533112    0.882415     51  carrot\n",
       "  4   169.782898   54.066620  225.461761  221.473785    0.876408     39  bottle\n",
       "  5   434.855621  215.672699  502.577179  297.335052    0.865674     51  carrot\n",
       "  6     0.197762  211.271881   70.726662  273.029327    0.820637     49  orange\n",
       "  7     0.011612  160.628769  128.464294  313.176758    0.744402     45    bowl\n",
       "  8    17.703789  152.093552   79.719353  200.788025    0.720914     49  orange\n",
       "  9     0.222635  184.914093   43.925102  233.212860    0.646968     49  orange\n",
       "  10   24.597309   47.287125  131.949997  187.509644    0.574814     39  bottle\n",
       "  11  197.312592   25.854271  251.050110  166.743103    0.352669     39  bottle,\n",
       "  'caption': ['Nescafe container',\n",
       "   'A jar that says Nescafe that is to the left of other bottles and jars and above a bowl of oranges.'],\n",
       "  'bbox_target': [26.78, 49.03, 112.28, 146.27]},\n",
       " 859: {'image_emb': tensor([[ 0.3667,  0.6118, -0.1566,  ...,  0.8428,  0.0156, -0.3127],\n",
       "          [ 0.0520,  0.1591,  0.2500,  ...,  0.9360,  0.4133, -0.1906],\n",
       "          [-0.3833,  0.2849,  0.0651,  ...,  0.8228, -0.1184, -0.5962],\n",
       "          [ 0.1985, -0.0208, -0.0750,  ...,  0.6123, -0.0658, -0.2150],\n",
       "          [ 0.1338,  0.2925, -0.1923,  ...,  1.2168, -0.0300, -0.0344],\n",
       "          [ 0.1515,  0.2539, -0.0621,  ...,  0.5356,  0.1664, -0.2881]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1570, -0.2472,  0.0041,  ..., -0.0425,  0.3235, -0.0817],\n",
       "          [-0.1720,  0.0865,  0.0069,  ...,  0.1587,  0.2825, -0.3008]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7664e-01, 3.7251e-03, 2.5848e-02, 1.6785e-03, 3.3045e-04, 7.9199e-01],\n",
       "          [9.1211e-01, 3.5763e-04, 4.0114e-05, 1.3471e-05, 6.5565e-07, 8.7524e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  193.240173   89.483978  382.018127  405.789825    0.948364      0   \n",
       "  1  149.386414   51.713348  250.928284  154.800262    0.936365     11   \n",
       "  2  344.615845   95.242920  514.966675  272.278137    0.877384      7   \n",
       "  3  150.137909  177.203430  166.276611  195.580811    0.797078     11   \n",
       "  4  408.632416   51.887604  438.678802  102.949677    0.700209      0   \n",
       "  5    4.475731  175.791946  155.272766  296.409729    0.681704      7   \n",
       "  6  213.932907  189.891632  269.390533  241.529388    0.572691      2   \n",
       "  7  213.726959  189.746063  269.823242  241.795563    0.411233      7   \n",
       "  8  393.827789  146.979950  408.112762  171.016418    0.406807      0   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1  stop sign  \n",
       "  2      truck  \n",
       "  3  stop sign  \n",
       "  4     person  \n",
       "  5      truck  \n",
       "  6        car  \n",
       "  7      truck  \n",
       "  8     person  ,\n",
       "  'caption': ['The closest construction worker holding a stop sign who has his back turned to the camera',\n",
       "   'A man holding a stop stands in the middle of a road directing traffic.'],\n",
       "  'bbox_target': [196.31, 88.45, 188.77, 319.28]},\n",
       " 860: {'image_emb': tensor([[ 0.0570,  0.2357, -0.3774,  ...,  0.6567,  0.1005, -0.1960],\n",
       "          [-0.1094,  0.0862, -0.3337,  ...,  0.8213, -0.1484, -0.1692],\n",
       "          [-0.3049, -0.0467, -0.3386,  ...,  0.5337, -0.0846, -0.3608],\n",
       "          [-0.6084,  0.0601, -0.1035,  ...,  0.6279,  0.2374, -0.1726],\n",
       "          [ 0.2251, -0.0060, -0.1927,  ...,  0.6001,  0.2247, -0.4126],\n",
       "          [-0.1829, -0.1554, -0.1569,  ...,  0.3843,  0.1886, -0.0765]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-8.0505e-02,  9.3842e-03, -2.2864e-01,  1.5149e-01,  1.6248e-01,\n",
       "            2.8625e-02,  5.1605e-02, -1.2852e+00, -3.0853e-02,  5.0354e-02,\n",
       "           -1.1749e-01, -1.9135e-02,  3.2715e-02,  5.5084e-03, -1.1060e-01,\n",
       "            3.6157e-01,  1.8323e-01,  2.7783e-01,  2.0911e-01,  2.7319e-01,\n",
       "            4.9268e-01,  7.3584e-01,  2.1622e-02,  6.6467e-02,  3.5010e-01,\n",
       "           -1.2549e-01, -2.9028e-01,  5.6274e-02, -4.2505e-01,  2.7271e-01,\n",
       "           -7.1716e-02,  5.7709e-02,  2.2449e-01,  5.5273e-01,  1.2445e-01,\n",
       "           -2.0496e-01, -3.0591e-01, -5.9021e-02,  4.7394e-02,  3.1128e-01,\n",
       "           -2.0703e-01, -9.6207e-03,  1.1523e-01, -4.7754e-01,  3.4839e-01,\n",
       "            1.4539e-01, -5.5908e-01,  3.8239e-02, -4.3365e-02, -5.0781e-01,\n",
       "            6.8665e-02,  8.9417e-02,  1.7996e-03, -5.0977e-01, -9.8816e-02,\n",
       "            4.5685e-02,  2.4341e-01, -5.7526e-02, -2.7328e-02,  1.8579e-01,\n",
       "            3.9209e-01,  4.4739e-02, -1.5002e-01,  8.7830e-02, -7.2449e-02,\n",
       "            1.8188e-01, -1.3196e-01,  6.2109e-01,  1.8311e-01, -1.1194e-01,\n",
       "            2.5366e-01,  3.6182e-01, -1.5564e-01,  2.3438e-01,  9.5886e-02,\n",
       "            4.7266e-01,  1.5820e-01,  8.6670e-03, -1.4270e-01,  1.1102e-01,\n",
       "           -3.7427e-01,  3.5980e-02, -2.2632e-01,  3.3594e-01,  1.0626e-01,\n",
       "           -2.5513e-01, -5.3864e-02, -1.7017e-01, -5.4138e-02,  4.2953e-03,\n",
       "            1.8445e-01, -1.7676e-01, -1.4629e+00, -7.4219e-02, -1.5637e-01,\n",
       "           -1.3940e-01, -3.0518e-02, -2.1558e-01,  1.8945e-01, -1.8494e-01,\n",
       "           -9.2224e-02,  2.7197e-01,  2.4792e-01, -3.7183e-01,  1.3329e-02,\n",
       "            1.5027e-01, -8.3984e-02,  3.3179e-01,  1.2000e-01, -2.0349e-01,\n",
       "            2.2009e-01,  6.9629e-01, -1.0724e-01, -2.9392e-03,  6.0791e-02,\n",
       "            2.6489e-01, -8.8623e-02,  9.1782e-03, -1.8970e-01,  5.9668e-01,\n",
       "           -4.6582e-01, -1.3477e-01,  1.8005e-01, -2.9590e-01,  2.9980e-01,\n",
       "            3.1982e-01,  2.0422e-01,  8.1421e-02, -2.1753e-01,  1.8225e-01,\n",
       "            1.6943e-01,  1.3306e-01, -2.8351e-02,  4.6836e+00, -4.2261e-01,\n",
       "           -1.9128e-01, -2.2705e-02, -2.6929e-01,  2.5391e-01, -7.7576e-02,\n",
       "           -5.5566e-01,  6.1615e-02, -1.9263e-01, -1.3245e-01, -1.0040e-01,\n",
       "           -4.3042e-01,  2.4512e-01,  1.5976e-02, -2.4170e-01, -4.6436e-01,\n",
       "            2.9724e-02,  9.1736e-02,  4.7607e-01, -1.9446e-01,  3.2251e-01,\n",
       "            2.5293e-01,  1.5190e-02, -3.1934e-01,  1.4319e-01, -3.2715e-01,\n",
       "            1.8021e-02,  2.6172e-01,  1.5808e-01,  1.6931e-01, -1.7017e-01,\n",
       "           -3.0103e-01,  8.3557e-02, -5.6519e-02, -2.7930e-01, -4.8645e-02,\n",
       "            3.9722e-01, -4.3457e-01,  1.3501e-01,  2.0447e-01, -1.7102e-01,\n",
       "            1.0742e-01,  1.7065e-01, -3.1079e-01, -1.0651e-01, -1.1572e-01,\n",
       "            1.2219e-01,  9.4116e-02,  4.6533e-01, -2.2247e-02, -1.0992e-01,\n",
       "           -1.0876e-01,  1.5979e-01, -1.0748e-01, -2.9053e-01,  3.0176e-01,\n",
       "           -6.2164e-02, -2.4036e-01,  1.4978e-01, -2.8906e-01, -8.0322e-02,\n",
       "           -7.2815e-02, -2.0142e-01,  3.2471e-01,  1.3220e-01, -1.2048e-01,\n",
       "            1.4880e-01,  7.9590e-02,  4.1211e-01,  1.0266e-01, -3.6816e-01,\n",
       "            3.7158e-01,  2.3022e-01,  2.6520e-02,  2.8223e-01, -7.8186e-02,\n",
       "           -2.0813e-02,  3.5400e-01,  1.7590e-01,  9.6313e-02, -2.1204e-01,\n",
       "           -1.8555e-01,  1.2421e-01, -8.0688e-02, -1.2524e-01, -6.1005e-02,\n",
       "            6.6467e-02, -5.8398e-01, -4.7119e-02, -1.7847e-01, -3.3472e-01,\n",
       "            9.2102e-02, -8.7219e-02,  2.4414e-01, -2.8320e-01,  2.1069e-01,\n",
       "            2.1454e-02, -1.1841e-01,  1.8570e-02, -3.4863e-01, -8.0688e-02,\n",
       "           -2.7393e-01,  3.9825e-02, -1.9727e-01, -1.1147e-02, -3.9331e-01,\n",
       "            1.3977e-01,  1.0577e-01,  1.7273e-02,  4.3243e-02, -8.7952e-02,\n",
       "            2.3572e-01,  1.0272e-01, -1.7859e-01, -9.6924e-02,  4.9902e-01,\n",
       "            6.5735e-02, -2.2644e-01, -1.1029e-01,  1.1346e-01, -3.8843e-01,\n",
       "           -1.8054e-01,  4.8779e-01,  2.4207e-01,  2.1875e-01, -4.9500e-02,\n",
       "           -1.8542e-01, -4.0649e-01,  5.5695e-02, -2.0801e-01,  2.5903e-01,\n",
       "           -2.2046e-01,  5.1575e-02, -3.2739e-01,  3.2153e-01,  2.6855e-01,\n",
       "           -6.6162e-02,  3.5010e-01,  1.2683e-01, -4.0942e-01,  3.8513e-02,\n",
       "            1.9775e-01, -2.4060e-01,  4.1504e-01, -9.6436e-02,  2.1997e-01,\n",
       "           -1.8726e-01,  2.6562e-01, -4.1504e-01,  7.7515e-02,  1.8555e-01,\n",
       "            1.5356e-01,  1.8787e-01, -4.7461e-01, -1.3684e-01, -1.0638e-01,\n",
       "            2.8296e-01, -1.9080e-01,  3.4851e-02, -8.6365e-02, -2.1973e-01,\n",
       "            1.1835e-01,  1.0876e-01,  1.2988e-01,  5.3320e-01, -1.3538e-01,\n",
       "           -5.5298e-02,  1.4746e-01,  5.7080e-01,  1.0303e-01,  2.2974e-01,\n",
       "           -5.9521e-01,  1.1432e-01,  5.9265e-02, -5.3894e-02, -1.5869e-01,\n",
       "           -3.7646e-01, -5.5450e-02,  4.6680e+00,  2.8906e-01,  2.8979e-01,\n",
       "            4.2944e-01,  1.2646e-01,  3.0609e-02,  3.2446e-01,  4.8926e-01,\n",
       "           -5.8411e-02, -3.4497e-01,  9.6375e-02,  1.0779e-01, -2.4643e-02,\n",
       "            1.6626e-01, -3.6523e-01,  1.8750e-01, -6.3721e-02, -2.6758e+00,\n",
       "           -3.9258e-01, -4.8859e-02,  2.5952e-01,  1.5625e-01, -1.5955e-01,\n",
       "            1.4893e-01, -3.8354e-01,  4.4312e-01, -1.1237e-01,  3.7915e-01,\n",
       "           -2.5635e-01, -2.4097e-01,  9.2224e-02,  4.0344e-02,  4.4464e-02,\n",
       "           -1.3647e-01, -1.3354e-01,  3.7109e-01,  8.6609e-02, -1.3092e-02,\n",
       "            4.6240e-01,  7.9834e-02,  1.6370e-01,  4.9194e-01,  1.9653e-01,\n",
       "           -8.5373e-03, -4.9225e-02, -3.4741e-01, -9.9182e-02, -2.8076e-01,\n",
       "            1.3831e-01, -1.6223e-01,  3.6792e-01,  1.6895e-01,  6.1572e-01,\n",
       "           -1.5198e-01, -1.9897e-01,  2.4353e-01, -3.3911e-01,  2.4475e-01,\n",
       "           -4.2334e-01, -5.0586e-01, -8.6914e-02,  4.3726e-01,  2.1741e-01,\n",
       "            1.3245e-01,  5.6580e-02,  1.3257e-01,  9.1324e-03,  2.3499e-01,\n",
       "           -6.1340e-02, -1.1420e-01, -1.9312e-01, -4.4897e-01, -9.3872e-02,\n",
       "           -1.5698e-01,  7.5989e-02, -2.5757e-01, -2.2961e-01,  9.6680e-02,\n",
       "           -6.7078e-02,  1.1713e-01, -3.1128e-01,  2.5000e-01,  3.5431e-02,\n",
       "            9.3536e-03, -2.9724e-02,  3.2642e-01, -3.2666e-01,  4.8633e-01,\n",
       "           -2.2415e-02,  9.2590e-02, -2.1118e-01, -2.2046e-01, -2.7893e-02,\n",
       "           -1.5427e-02,  8.7463e-02, -1.7871e-01, -1.8021e-02,  1.0162e-01,\n",
       "           -3.3350e-01,  6.7017e-02,  7.4768e-03, -4.9365e-01,  1.2744e-01,\n",
       "           -3.2178e-01,  2.3572e-01,  2.1027e-02,  1.4587e-01, -7.3975e-02,\n",
       "            3.5693e-01, -2.4857e-02, -1.8127e-01, -2.2766e-01,  1.3477e-01,\n",
       "           -3.3493e-03, -1.9775e-01,  1.5454e-01,  4.5239e-01, -1.2781e-01,\n",
       "           -7.9407e-02, -3.3154e-01, -2.9443e-01, -1.1041e-01, -1.3123e-01,\n",
       "           -1.9031e-01, -2.2449e-01, -4.9133e-02, -9.4238e-02, -1.9006e-01,\n",
       "            4.3481e-01, -2.1069e-01, -1.7310e-01, -5.5145e-02,  1.3281e-01,\n",
       "           -1.6553e-01, -6.8164e-01,  9.6680e-02,  9.6802e-02,  4.2603e-01,\n",
       "           -9.6924e-01, -9.8450e-02,  4.9500e-02,  1.7773e-01,  3.7018e-02,\n",
       "           -1.1890e-01, -3.5132e-01, -1.5662e-01, -3.8135e-01, -2.0337e-01,\n",
       "           -1.3818e-01, -4.6094e-01,  3.3276e-01,  2.6343e-01,  2.6685e-01,\n",
       "           -1.0260e-01, -9.3933e-02, -1.2140e-01, -2.3450e-01,  2.0520e-01,\n",
       "            1.0278e-01,  5.3497e-02,  2.6343e-01, -1.0431e-01, -1.2238e-01,\n",
       "           -2.0776e-01,  3.8501e-01, -8.6914e-02,  6.9824e-01, -1.1749e-01,\n",
       "           -1.4819e-01, -7.9297e-01, -7.3364e-02, -2.6123e-01, -5.2832e-01,\n",
       "            1.1542e-01,  7.2571e-02, -7.4463e-02, -8.0139e-02, -1.4734e-01,\n",
       "            1.9885e-01,  2.0239e-01, -6.7627e-02,  6.0889e-01,  4.5166e-01,\n",
       "            7.6866e-03,  3.2251e-01, -1.6248e-01,  2.3535e-01, -1.7725e-01,\n",
       "            2.6416e-01, -4.6997e-01, -2.6636e-01,  2.8564e-01,  3.8232e-01,\n",
       "           -2.6685e-01, -3.5767e-01,  6.5979e-02, -7.8064e-02, -1.1212e-01,\n",
       "            2.3108e-01, -6.4819e-02]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.5386e-01, 1.3208e-01, 4.4250e-02, 3.4790e-01, 7.5698e-06, 2.1896e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0    0.000000  105.795319  126.804764  348.225006    0.955400      0   person\n",
       "  1  447.634033  103.841278  577.449829  424.242065    0.949773      0   person\n",
       "  2  126.276909  155.038269  239.941284  424.622925    0.942769      0   person\n",
       "  3  316.037109  164.006378  433.486877  355.302032    0.932304      0   person\n",
       "  4  300.944366  256.250153  336.973358  276.457306    0.843150     29  frisbee,\n",
       "  'caption': ['The man in black and shorts.'],\n",
       "  'bbox_target': [0.0, 107.31, 125.11, 242.38]},\n",
       " 861: {'image_emb': tensor([[ 0.1627, -0.0762, -0.0355,  ...,  0.9634,  0.2717,  0.1351],\n",
       "          [-0.3179,  0.1938, -0.0621,  ...,  1.1826,  0.0161, -0.3315],\n",
       "          [-0.1445,  0.8564,  0.0942,  ...,  1.2012,  0.1152,  0.1996],\n",
       "          [-0.1503,  0.0619, -0.0882,  ...,  1.0166, -0.0043, -0.1969],\n",
       "          [-0.4998,  0.0297, -0.1488,  ...,  0.8462,  0.1128, -0.0970],\n",
       "          [ 0.0657,  0.1117, -0.1525,  ...,  0.9565,  0.2001, -0.2786]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1376, -0.1114, -0.1713,  ..., -0.1072,  0.2427, -0.2507],\n",
       "          [ 0.0560,  0.1788, -0.1156,  ...,  0.2058, -0.0624, -0.5903]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.2186e-02, 5.9009e-06, 1.7881e-07, 2.7603e-02, 7.0877e-03, 9.4336e-01],\n",
       "          [1.9372e-05, 3.5167e-06, 5.9605e-08, 5.1697e-02, 9.4580e-01, 2.6970e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   77.167259   45.383087  321.855743  393.653351    0.950217      0   \n",
       "  1  463.667633  304.158661  639.769775  381.118195    0.887681     56   \n",
       "  2  145.830185  406.440979  209.384903  479.103699    0.885670     41   \n",
       "  3  137.903671  337.140869  422.055420  435.508484    0.822620     53   \n",
       "  4  301.445312    0.107162  578.205383  361.247498    0.799922      0   \n",
       "  5  308.203857    0.063171  578.245361  362.554016    0.630834     72   \n",
       "  6   93.199066  342.237000  639.198364  477.411255    0.590367     60   \n",
       "  7  530.384460    9.589340  632.869934  322.322937    0.415660     72   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1         chair  \n",
       "  2           cup  \n",
       "  3         pizza  \n",
       "  4        person  \n",
       "  5  refrigerator  \n",
       "  6  dining table  \n",
       "  7  refrigerator  ,\n",
       "  'caption': ['A man dressed in black sprinkling cheese on a pizza with a young girl.',\n",
       "   'A father wearing a black baseball cap sprinkling cheese on a pizza while holding a white plate.'],\n",
       "  'bbox_target': [296.22, 2.7, 277.83, 363.25]},\n",
       " 862: {'image_emb': tensor([[ 0.1820,  0.1731,  0.1038,  ...,  1.0557,  0.0713,  0.2191],\n",
       "          [-0.3091,  0.4722,  0.1476,  ...,  0.9204,  0.0337,  0.0032],\n",
       "          [-0.2188,  0.6890, -0.0161,  ...,  1.1084,  0.0701,  0.1126],\n",
       "          ...,\n",
       "          [ 0.0657,  0.2844, -0.2644,  ...,  0.5815, -0.1693, -0.0330],\n",
       "          [-0.0594,  0.3118, -0.0852,  ...,  1.4609,  0.0260, -0.1917],\n",
       "          [-0.1082,  0.5205, -0.3059,  ...,  1.0732, -0.0192,  0.4392]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1227, -0.1213,  0.0455,  ...,  0.1600, -0.1770, -0.3352],\n",
       "          [ 0.1025,  0.0736, -0.2393,  ...,  0.0381,  0.0952,  0.1458]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.2031e-01, 8.7261e-05, 1.3101e-04, 3.3844e-02, 1.9287e-02, 1.2384e-01,\n",
       "           2.5291e-03, 1.7214e-04],\n",
       "          [1.6357e-02, 9.8228e-04, 5.6534e-03, 3.6084e-01, 9.5581e-02, 4.4897e-01,\n",
       "           6.6772e-02, 4.7607e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     0.000000    0.322052   93.172363  160.769073    0.940306      0   \n",
       "  1   243.500610   27.725616  478.856995  266.771881    0.932968     16   \n",
       "  2     0.000000   99.491608  600.167114  421.465332    0.918950      3   \n",
       "  3   379.752625    0.000000  494.155579  157.017914    0.899773      0   \n",
       "  4   460.311096    0.562271  630.910950  200.601624    0.811320      0   \n",
       "  5   194.730835    0.000000  298.363556  157.042267    0.793329      0   \n",
       "  6   117.342758   41.507523  212.993668  155.415710    0.710265     24   \n",
       "  7   469.298126  174.289703  530.502991  239.537384    0.424401     26   \n",
       "  8   233.139008   40.028076  250.609985   60.325836    0.347611     67   \n",
       "  9    21.836309   33.793274   42.439133   46.800201    0.320142     67   \n",
       "  10  607.845947   12.619705  639.799072  422.661743    0.319160      0   \n",
       "  \n",
       "            name  \n",
       "  0       person  \n",
       "  1          dog  \n",
       "  2   motorcycle  \n",
       "  3       person  \n",
       "  4       person  \n",
       "  5       person  \n",
       "  6     backpack  \n",
       "  7      handbag  \n",
       "  8   cell phone  \n",
       "  9   cell phone  \n",
       "  10      person  ,\n",
       "  'caption': ['a lady in a white shirt videoing', 'whtie shirt women'],\n",
       "  'bbox_target': [179.38, 0.0, 117.79, 137.6]},\n",
       " 863: {'image_emb': tensor([[-0.0144,  0.2078, -0.6606,  ...,  0.8140,  0.2866,  0.0770],\n",
       "          [-0.0397, -0.1294, -0.4224,  ...,  0.5234,  0.0876, -0.1263],\n",
       "          [-0.0176, -0.1470, -0.3779,  ...,  0.4526,  0.2212, -0.0205],\n",
       "          [ 0.1354,  0.0346, -0.4355,  ...,  0.5015,  0.1686,  0.0223]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1420, -0.0286,  0.0140,  ...,  0.1589,  0.3381,  0.1587],\n",
       "          [ 0.0828, -0.0780, -0.2983,  ...,  0.6821,  0.1910,  0.2465]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0015, 0.5498, 0.4348, 0.0140],\n",
       "          [0.0237, 0.2181, 0.6616, 0.0968]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  413.384613    1.412949  638.587769  420.764038    0.927768      6  train\n",
       "  1  221.760391  206.214569  407.441101  284.293304    0.885590      6  train\n",
       "  2    0.779961  200.535767  224.176758  281.723999    0.830976      6  train,\n",
       "  'caption': ['red train in the middle of the tracks.',\n",
       "   'train cars on the middle track'],\n",
       "  'bbox_target': [222.52, 207.26, 186.78, 78.79]},\n",
       " 864: {'image_emb': tensor([[-0.3423, -0.2949, -0.0731,  ...,  0.5200, -0.2001,  0.3105],\n",
       "          [-0.3689, -0.4099,  0.0731,  ...,  0.6509, -0.1113,  0.3411],\n",
       "          [-0.4556, -0.3213,  0.0510,  ...,  0.5088,  0.0092,  0.4038]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0295,  0.0900,  0.0472,  ...,  0.0804,  0.2332, -0.2202],\n",
       "          [-0.1628,  0.0998, -0.0504,  ..., -0.0902, -0.0989,  0.0466]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0409, 0.1749, 0.7842],\n",
       "          [0.9238, 0.0113, 0.0649]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax       ymax  confidence  class      name\n",
       "  0  254.955566  236.447876  494.048035  494.61145    0.947621     20  elephant\n",
       "  1    0.549332    8.412399  442.809204  505.20575    0.943660     20  elephant,\n",
       "  'caption': ['A baby elephant standing next to one of his parents.',\n",
       "   'A young elephant pointing its trunk upwards.'],\n",
       "  'bbox_target': [252.13, 234.75, 241.83, 262.45]},\n",
       " 865: {'image_emb': tensor([[-0.1002,  0.3516, -0.0996,  ...,  0.8433, -0.0145, -0.4353],\n",
       "          [-0.0264,  0.1339, -0.0518,  ...,  0.6807,  0.1738, -0.5791],\n",
       "          [-0.2416,  0.3149,  0.2764,  ...,  0.7627, -0.3550, -0.2500]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0186, -0.0112,  0.0294,  ...,  0.6816,  0.0328, -0.3816],\n",
       "          [ 0.1121, -0.0488,  0.2262,  ...,  0.8042, -0.0010, -0.6270]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5801, 0.0992, 0.3206],\n",
       "          [0.8643, 0.0970, 0.0386]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class  \\\n",
       "  0  360.419739  64.301147  466.853821  262.524902    0.893726     12   \n",
       "  1  290.605743  54.809692  373.096405  235.451508    0.864824     12   \n",
       "  2  264.156464  74.288651  299.183929  196.089600    0.536540     12   \n",
       "  3  359.455933   1.130417  640.000000  305.949036    0.448160      2   \n",
       "  \n",
       "              name  \n",
       "  0  parking meter  \n",
       "  1  parking meter  \n",
       "  2  parking meter  \n",
       "  3            car  ,\n",
       "  'caption': ['The furthestmost left of a pair of snow covered parking meters.',\n",
       "   'Snow covered parking meter during a blizzard'],\n",
       "  'bbox_target': [289.13, 58.51, 81.38, 178.08]},\n",
       " 866: {'image_emb': tensor([[-0.0151,  0.1743, -0.2050,  ...,  1.2686,  0.0943, -0.1898],\n",
       "          [-0.1135,  0.2341, -0.0835,  ...,  1.1094,  0.1788, -0.0820],\n",
       "          [-0.3289,  0.2394,  0.1744,  ...,  0.7339,  0.0782, -0.4468],\n",
       "          ...,\n",
       "          [-0.2673,  0.2334, -0.0995,  ...,  0.6680, -0.0778, -0.0569],\n",
       "          [-0.0331,  0.2612,  0.2905,  ...,  0.6118,  0.0640, -0.3037],\n",
       "          [-0.1764,  0.2844,  0.0681,  ...,  0.8188,  0.2164, -0.1063]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1227,  0.5273, -0.2428,  ..., -0.0081,  0.1230, -0.7056],\n",
       "          [ 0.3621,  0.4448, -0.2402,  ...,  0.4182,  0.0958, -0.1400]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.6441e-03, 1.0262e-02, 7.2479e-05, 1.3256e-03, 3.2473e-04, 1.0788e-05,\n",
       "           5.5265e-04, 1.3306e-01, 7.6562e-01, 7.5936e-05, 3.3112e-02, 4.6722e-02],\n",
       "          [2.2324e-02, 3.3703e-03, 1.6868e-05, 5.9128e-03, 6.2622e-02, 8.4639e-06,\n",
       "           3.0029e-02, 1.6492e-01, 5.9375e-01, 7.5951e-03, 4.9530e-02, 5.9723e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   454.580902  275.001587  549.314819  405.691284    0.922637     56   \n",
       "  1    77.958138  253.446777  160.058838  346.750977    0.895480     56   \n",
       "  2    23.323547  251.019165   84.077057  341.938232    0.887193     56   \n",
       "  3   205.088394  242.141876  268.335266  326.130951    0.858184     56   \n",
       "  4   350.455994  302.106720  460.040649  477.454590    0.852042     56   \n",
       "  5   158.364792  241.252777  196.934311  313.607941    0.844406     56   \n",
       "  6    48.039986  366.284943  198.830933  476.664795    0.841318     56   \n",
       "  7    27.496452  115.133667  251.147491  263.414307    0.789230     25   \n",
       "  8   148.658859  353.130890  452.021667  468.662292    0.749331     60   \n",
       "  9   117.268600  258.144867  218.386520  339.686371    0.733378     60   \n",
       "  10    7.622864    0.733887  638.363403  389.237183    0.707869     25   \n",
       "  11  530.107849  277.329987  585.389221  390.339996    0.526658     60   \n",
       "  12  381.115753  111.722290  639.390869  160.441132    0.521643     25   \n",
       "  13  343.690491  221.254272  366.401489  252.735107    0.477885     56   \n",
       "  14  223.002106  445.871155  307.083954  479.374329    0.470419     56   \n",
       "  15  529.562927  265.948578  585.202332  389.635223    0.452952     56   \n",
       "  16  308.261993  225.096191  328.829742  253.145874    0.445494     56   \n",
       "  17    1.861084    0.425629  640.000000  133.445007    0.433197     25   \n",
       "  18  381.614044  113.698990  639.150391  280.060547    0.364776     25   \n",
       "  19  366.326294  220.374023  382.825134  250.115356    0.281058     56   \n",
       "  20  130.043213  238.563629  206.060364  325.783783    0.263566     56   \n",
       "  21  330.675842  225.197235  351.283142  252.742523    0.254198     56   \n",
       "  \n",
       "              name  \n",
       "  0          chair  \n",
       "  1          chair  \n",
       "  2          chair  \n",
       "  3          chair  \n",
       "  4          chair  \n",
       "  5          chair  \n",
       "  6          chair  \n",
       "  7       umbrella  \n",
       "  8   dining table  \n",
       "  9   dining table  \n",
       "  10      umbrella  \n",
       "  11  dining table  \n",
       "  12      umbrella  \n",
       "  13         chair  \n",
       "  14         chair  \n",
       "  15         chair  \n",
       "  16         chair  \n",
       "  17      umbrella  \n",
       "  18      umbrella  \n",
       "  19         chair  \n",
       "  20         chair  \n",
       "  21         chair  ,\n",
       "  'caption': ['A white umbrella on a pole in front of other umbrellas.',\n",
       "   'Umbrella over a patio table.'],\n",
       "  'bbox_target': [0.0, 0.0, 639.64, 135.58]},\n",
       " 867: {'image_emb': tensor([[-0.2286,  0.4258,  0.1235,  ...,  1.0791, -0.3164,  0.1813],\n",
       "          [-0.1672,  0.1693, -0.2053,  ...,  0.7158,  0.1282, -0.0513],\n",
       "          [-0.4414,  0.1210, -0.1600,  ...,  0.6768, -0.1615,  0.1956],\n",
       "          ...,\n",
       "          [-0.2949,  0.1273,  0.2026,  ...,  1.2705, -0.2964,  0.2347],\n",
       "          [ 0.1517,  0.0690, -0.3740,  ...,  0.8872, -0.0782,  0.0344],\n",
       "          [-0.3682,  0.0911,  0.3491,  ...,  0.7715, -0.2847,  0.2178]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3372,  0.0104, -0.4412,  ...,  0.2378, -0.0867,  0.0959],\n",
       "          [ 0.0146,  0.0842, -0.1682,  ..., -0.0021, -0.4707, -0.3579]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.2432e-01, 1.9073e-06, 2.1994e-04, 8.6129e-05, 5.1928e-04, 1.0312e-05,\n",
       "           7.1289e-02, 1.2732e-04, 3.2825e-03],\n",
       "          [3.4271e-02, 2.7418e-04, 3.3203e-02, 8.0109e-03, 6.2370e-03, 2.5597e-03,\n",
       "           8.8330e-01, 6.8512e-03, 2.5070e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   116.016060  100.693970  279.222046  212.869965    0.942108     57   \n",
       "  1   312.826752  154.156067  342.978912  183.362473    0.874440     45   \n",
       "  2    59.110714   90.662621  106.529152  161.287964    0.859143     56   \n",
       "  3   161.130219   89.414024  181.001358  103.839554    0.766586     56   \n",
       "  4     3.018132   90.143402   48.698532  156.269165    0.757398     56   \n",
       "  5   125.048119   91.091934  158.034454  104.737999    0.740298     56   \n",
       "  6     0.311981  137.630341  100.807945  227.704147    0.719074     57   \n",
       "  7   251.025681  166.137802  294.899414  182.300034    0.703919     73   \n",
       "  8   246.903442  174.145142  299.527435  189.130035    0.676159     73   \n",
       "  9   317.944611  119.567574  342.804535  155.775482    0.646411     56   \n",
       "  10  221.160339  156.025589  342.089539  227.578110    0.433213     60   \n",
       "  11  247.990952   92.607491  264.232300  109.193977    0.430156     58   \n",
       "  12   52.037910   90.566093   79.531197  150.009583    0.374752     56   \n",
       "  13    0.009018  109.279449   17.710182  135.680710    0.336405     58   \n",
       "  14  221.164856  155.522171  341.618774  226.101929    0.317912     73   \n",
       "  15  103.715485   82.118828  118.092178  107.958939    0.258750     75   \n",
       "  \n",
       "              name  \n",
       "  0          couch  \n",
       "  1           bowl  \n",
       "  2          chair  \n",
       "  3          chair  \n",
       "  4          chair  \n",
       "  5          chair  \n",
       "  6          couch  \n",
       "  7           book  \n",
       "  8           book  \n",
       "  9          chair  \n",
       "  10  dining table  \n",
       "  11  potted plant  \n",
       "  12         chair  \n",
       "  13  potted plant  \n",
       "  14          book  \n",
       "  15          vase  ,\n",
       "  'caption': ['A bright yellow couch.', 'A white couch.'],\n",
       "  'bbox_target': [0.5, 139.5, 101.0, 90.5]},\n",
       " 868: {'image_emb': tensor([[-0.3130,  0.3604, -0.0809,  ...,  1.3613, -0.0141, -0.1570],\n",
       "          [-0.1273,  0.2803, -0.2449,  ...,  0.8945, -0.0594, -0.0984],\n",
       "          [-0.0136,  0.2659, -0.2507,  ...,  0.7637,  0.3042, -0.0016]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.5728,  0.0175, -0.1024,  ..., -0.1526, -0.2463, -0.3523],\n",
       "          [-0.3630,  0.1549, -0.0756,  ...,  0.0773,  0.3430, -0.3318]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.4180e-01, 5.0049e-03, 1.5332e-01],\n",
       "          [5.9174e-02, 5.4502e-04, 9.4043e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  433.056366  289.857788  601.231201  476.560730    0.893859     56   \n",
       "  1  265.666260  409.891785  548.745300  477.281799    0.742033     56   \n",
       "  2  436.566132  238.468384  467.482452  302.570251    0.580993     75   \n",
       "  3  541.979004  263.803772  597.182129  296.572388    0.525246     56   \n",
       "  4    0.000000    0.000000  606.433472  471.975647    0.435473     58   \n",
       "  5  406.429901  284.844849  542.927612  418.458496    0.275046     60   \n",
       "  \n",
       "             name  \n",
       "  0         chair  \n",
       "  1         chair  \n",
       "  2          vase  \n",
       "  3         chair  \n",
       "  4  potted plant  \n",
       "  5  dining table  ,\n",
       "  'caption': ['top of white chair closest to flowers',\n",
       "   'The top of a white chair by the yellow roses in focus.'],\n",
       "  'bbox_target': [276.13, 412.04, 294.48, 62.57]},\n",
       " 869: {'image_emb': tensor([[-0.0912,  0.3845, -0.1979,  ...,  0.3748,  0.0371,  0.2319],\n",
       "          [-0.1698,  0.3428, -0.1624,  ...,  0.3828,  0.1063,  0.1805]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1401,  0.0112,  0.0061,  ...,  0.0259,  0.0302,  0.1569],\n",
       "          [ 0.1353,  0.2001, -0.0702,  ..., -0.0980, -0.1809, -0.0406]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3345, 0.6655],\n",
       "          [0.2974, 0.7026]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0    4.858246    1.299789  640.000000  472.655090    0.883695      7  truck\n",
       "  1  255.464294  312.316132  269.653198  325.773651    0.309803      7  truck,\n",
       "  'caption': ['the heavy truck passing the road',\n",
       "   'The truck outside the window'],\n",
       "  'bbox_target': [1.25, 2.49, 637.09, 475.02]},\n",
       " 870: {'image_emb': tensor([[-1.5967e-01, -2.0862e-01,  1.3489e-01, -2.4390e-01, -3.0556e-03,\n",
       "           -8.2092e-02, -2.8345e-01,  2.7734e-01, -2.9419e-01, -4.2920e-01,\n",
       "            3.4790e-01, -9.6069e-02,  2.4780e-01,  3.3325e-02, -5.0781e-01,\n",
       "           -9.1736e-02,  1.9971e-01, -1.0602e-01, -7.7576e-02,  3.4277e-01,\n",
       "           -5.1807e-01,  3.5083e-01,  2.1790e-01, -1.9775e-01, -6.3110e-02,\n",
       "            1.6101e-01, -4.3701e-02,  5.0568e-02, -8.2458e-02, -1.0614e-01,\n",
       "           -3.2593e-01,  5.9033e-01, -8.6853e-02,  1.7322e-01,  1.1255e-01,\n",
       "           -7.9346e-02,  5.0812e-02, -2.0166e-01,  5.5573e-02,  2.6367e-01,\n",
       "           -3.2837e-01, -3.8354e-01, -2.1289e-01, -1.5991e-01,  1.6464e-02,\n",
       "            1.0345e-01,  4.1235e-01,  2.2620e-01, -1.6272e-01, -2.0703e-01,\n",
       "           -1.5430e-01,  2.5635e-01, -2.7930e-01,  5.3516e-01, -2.9633e-02,\n",
       "            2.5049e-01,  8.0566e-02,  2.0218e-02, -4.9341e-01,  3.4447e-03,\n",
       "            6.8018e-01, -4.2175e-02, -2.3730e-01,  4.1626e-01,  1.3904e-01,\n",
       "           -3.5522e-01,  8.8074e-02,  1.0020e+00, -1.7590e-01, -1.0712e-02,\n",
       "            8.3984e-02, -2.5928e-01, -3.7079e-02,  3.9746e-01,  5.5756e-02,\n",
       "            3.1372e-01, -1.3562e-01, -5.5713e-01, -2.4463e-01, -1.4307e-01,\n",
       "           -1.8359e-01, -3.7354e-02, -4.7424e-02,  3.8916e-01, -9.0149e-02,\n",
       "           -7.4890e-02,  9.0381e-01, -4.0015e-01, -3.2532e-02, -1.3745e-01,\n",
       "            1.1530e-01,  1.6797e-01, -6.4688e+00, -1.0858e-01, -4.1162e-01,\n",
       "            3.4570e-01,  1.8591e-01, -3.4473e-01, -2.8809e-01,  1.1650e+00,\n",
       "           -1.2482e-02, -3.7939e-01, -4.1309e-01,  6.6650e-02,  6.2012e-01,\n",
       "           -8.4915e-03,  5.6543e-01, -4.1443e-02, -2.8198e-01,  1.2061e-01,\n",
       "            1.4233e-01, -6.0059e-01,  2.1765e-01, -1.0046e-01, -2.7905e-01,\n",
       "           -1.1707e-01, -1.6040e-01,  3.4399e-01, -1.0156e-01,  3.7744e-01,\n",
       "            1.3049e-01, -2.2205e-01, -1.2152e-01, -5.9113e-02, -1.2030e-01,\n",
       "           -3.4741e-01, -2.0776e-01,  2.7686e-01, -6.8726e-02, -9.0576e-02,\n",
       "            3.5156e-02, -9.1492e-02, -1.7126e-01,  8.9844e-01, -6.2305e-01,\n",
       "           -9.6313e-02, -1.5564e-01, -5.1025e-01, -2.3914e-01,  1.5747e-01,\n",
       "            2.2266e-01, -3.0688e-01, -1.4331e-01, -3.2806e-02, -4.7681e-01,\n",
       "            1.1957e-01,  4.4922e-01, -4.2725e-01,  1.0529e-02,  1.0156e-01,\n",
       "           -3.9966e-01,  4.1040e-01, -6.1816e-01, -1.0376e-01, -2.1033e-01,\n",
       "            1.2549e-01, -4.5227e-02,  9.2102e-02, -1.3062e-01, -8.3350e-01,\n",
       "            5.1953e-01,  3.9819e-01,  1.8042e-01,  1.7773e-01,  2.1875e-01,\n",
       "           -4.7729e-01, -3.4863e-01,  5.3076e-01,  2.0733e-03,  4.9854e-01,\n",
       "           -1.9543e-01,  7.6514e-01,  3.3252e-01,  2.1652e-02, -2.8442e-01,\n",
       "            7.4585e-02, -1.1279e-01, -5.2490e-02,  1.4368e-01,  5.4346e-01,\n",
       "           -3.3887e-01,  2.5558e-02, -9.9182e-02, -3.7500e-01, -2.4139e-02,\n",
       "           -5.1453e-02,  6.0254e-01, -3.2153e-01,  1.9324e-01, -2.2473e-01,\n",
       "            5.3369e-01, -2.2018e-02,  3.0106e-02,  2.1484e-01, -2.1912e-01,\n",
       "           -5.8270e-04, -3.8965e-01, -5.8057e-01, -1.8711e+00, -1.2854e-01,\n",
       "            7.3975e-02,  5.6641e-01,  1.6687e-01,  4.2578e-01, -3.2642e-01,\n",
       "            1.7493e-01,  4.8730e-01, -2.2803e-01, -5.5566e-01,  2.9712e-01,\n",
       "           -2.8369e-01,  4.8145e-01, -2.1252e-01,  6.5039e-01,  8.9111e-02,\n",
       "           -1.0455e-01, -5.3894e-02, -7.0703e-01,  9.8047e-01, -3.6426e-01,\n",
       "            6.1816e-01,  4.7900e-01,  1.3367e-01,  3.4912e-01, -7.8186e-02,\n",
       "            4.3671e-02, -1.8295e-02, -1.3440e-01, -8.8623e-02, -1.5710e-01,\n",
       "           -9.1919e-02,  7.4158e-02, -2.9663e-01,  2.9883e-01,  5.4504e-02,\n",
       "           -3.9648e-01, -3.0322e-01,  1.0901e-01,  3.7964e-02, -1.8774e-01,\n",
       "            2.0898e-01, -3.8013e-01,  1.0986e-01,  2.2424e-01, -1.4111e-01,\n",
       "            1.4443e+00,  3.0737e-01, -2.8320e-01, -8.8013e-02,  3.5474e-01,\n",
       "            2.9663e-01,  1.4514e-01, -1.0541e-01, -2.1277e-01,  1.8005e-01,\n",
       "           -1.4783e-01, -1.4319e-01, -4.2725e-01, -1.6299e+00,  4.1089e-01,\n",
       "           -2.3865e-02,  9.0637e-02, -3.1396e-01,  1.3586e-01, -2.3352e-01,\n",
       "           -2.6782e-01, -4.8413e-01,  3.1470e-01, -5.7373e-01, -9.1431e-02,\n",
       "           -2.2437e-01,  1.3159e-01,  3.8403e-01, -1.6571e-02,  7.4890e-02,\n",
       "           -3.3594e-01, -6.7253e-03,  5.4321e-02, -3.0249e-01,  1.7200e-01,\n",
       "            2.1106e-01,  1.3159e-01, -6.3184e-01,  1.1676e-01, -4.1309e-01,\n",
       "            3.8867e-01, -4.7461e-01,  1.4458e-02,  4.7266e-01, -3.0444e-01,\n",
       "           -6.0974e-02, -4.1290e-02,  4.5459e-01,  3.1714e-01,  2.8125e-01,\n",
       "            4.0796e-01, -1.8555e-01, -3.3130e-01,  1.0522e-01,  1.9043e-01,\n",
       "            3.4253e-01,  2.8931e-01, -3.0298e-01, -1.2549e-01,  3.3386e-02,\n",
       "            4.2572e-02,  2.1570e-01,  1.4795e-01, -2.5684e-01,  4.6295e-02,\n",
       "           -4.4214e-01, -1.3077e-02,  8.9795e-01, -8.9111e-03, -1.3403e-01,\n",
       "            3.8354e-01,  1.6479e-01,  5.3564e-01,  1.1066e-01,  1.9104e-01,\n",
       "            5.3314e-02,  1.5996e+00, -2.1167e-01,  1.0046e-01, -3.0176e-01,\n",
       "           -1.9006e-01, -2.3853e-01,  4.8901e-01,  1.7737e-01, -8.4717e-02,\n",
       "            1.0742e-01, -5.4492e-01, -2.9834e-01, -1.8567e-01,  1.7883e-01,\n",
       "            2.5024e-01, -3.1836e-01,  2.3804e-02,  5.4443e-01, -7.7332e-02,\n",
       "            9.9365e-02,  6.7505e-02,  4.6204e-02, -1.2073e-01,  4.6558e-01,\n",
       "           -2.4933e-02, -2.1423e-02,  3.0396e-01,  2.8369e-01,  1.6647e-02,\n",
       "           -5.1392e-02, -2.5375e-02,  2.8906e-01, -3.1055e-01, -1.1932e-01,\n",
       "            4.0796e-01, -6.7090e-01, -4.0137e-01, -1.1749e-02,  8.0664e-01,\n",
       "            2.0172e-02, -3.7689e-02, -3.3905e-02, -4.4403e-02, -5.8984e-01,\n",
       "            1.3452e-01,  1.8091e-01, -1.5454e-01,  2.9199e-01, -4.7699e-02,\n",
       "            3.3838e-01,  3.7549e-01, -1.4417e-01, -7.0508e-01,  1.5793e-02,\n",
       "            1.7554e-01,  8.4717e-01,  6.3477e-01, -2.5708e-01,  1.3458e-02,\n",
       "            2.7832e-01, -6.3867e-01, -5.3613e-01,  5.6299e-01, -3.6774e-02,\n",
       "           -3.3234e-02,  2.7759e-01, -4.1089e-01,  1.0309e-01, -1.3770e+00,\n",
       "           -1.7212e-01, -5.6006e-01, -3.7140e-02, -1.1810e-01,  4.7241e-01,\n",
       "            6.9702e-02,  2.2070e-01, -2.1960e-01, -4.1357e-01, -6.8066e-01,\n",
       "           -1.7798e-01, -4.6310e-03,  6.0693e-01,  2.9639e-01, -5.7892e-02,\n",
       "            1.5717e-02, -7.4585e-02, -5.2881e-01,  5.6641e-01, -5.5859e-01,\n",
       "            5.4297e-01,  1.1572e-01, -4.5312e-01,  1.7480e-01,  3.9697e-01,\n",
       "           -6.2500e-01,  9.4604e-02,  2.5977e-01, -5.0232e-02, -5.7324e-01,\n",
       "           -2.6001e-01,  3.7549e-01,  2.1301e-01,  2.3230e-01,  5.8014e-02,\n",
       "            4.9011e-02, -1.8640e-01, -1.6858e-01,  1.4873e+00,  1.5283e-01,\n",
       "           -4.0619e-02,  4.6509e-01, -7.7246e-01, -2.1167e-01, -4.8633e-01,\n",
       "           -7.2266e-01,  3.9948e-02,  4.6997e-01,  4.5166e-01, -4.6313e-01,\n",
       "            1.6113e-01, -5.5615e-01, -1.4221e-01,  3.2227e-02,  6.3330e-01,\n",
       "           -4.1431e-01, -2.4316e-01,  1.1230e-01,  1.8127e-01, -3.5498e-01,\n",
       "           -1.9910e-01, -2.0227e-01, -4.0161e-01,  4.4629e-01,  3.1158e-02,\n",
       "           -1.1987e-01,  2.5659e-01,  5.4248e-01,  2.2925e-01,  4.8291e-01,\n",
       "           -2.2473e-01, -4.2017e-01, -4.5044e-02, -7.2693e-02,  3.8062e-01,\n",
       "            2.7222e-01, -3.7598e-01, -1.5271e-01,  2.2522e-01, -2.4072e-01,\n",
       "            6.7200e-02, -5.8984e-01, -4.9756e-01, -9.5825e-02, -4.8315e-01,\n",
       "           -3.5962e-01, -9.6741e-02, -2.6855e-01, -5.2979e-01, -2.1436e-01,\n",
       "            3.8525e-01, -3.5571e-01,  3.9062e-01,  3.1714e-01,  1.7407e-01,\n",
       "           -2.5562e-01,  5.6610e-02, -2.4036e-01, -1.0480e-01, -2.3544e-02,\n",
       "            1.0522e-01, -1.4917e-01,  1.4917e-01, -2.0032e-01,  2.9810e-01,\n",
       "            2.7686e-01, -1.6077e-01,  3.2422e-01,  1.5771e-01,  3.7964e-02,\n",
       "           -4.1199e-03,  1.8323e-01, -1.0706e-01,  4.9988e-02,  4.6948e-01,\n",
       "           -4.8950e-01, -1.8115e-01, -2.8760e-01,  2.3193e-01,  4.6313e-01,\n",
       "            1.3794e-01, -2.9736e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1726, -0.2959, -0.0943,  ..., -0.2205, -0.2693,  0.1548],\n",
       "          [ 0.0938, -0.1931, -0.3132,  ..., -0.1880, -0.2043, -0.0875]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.],\n",
       "          [1.]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class      name\n",
       "  0  321.566711  48.976608  632.369507  329.803467    0.674303     15       cat\n",
       "  1    1.130127  38.266235  304.582764  348.619507    0.600825     15       cat\n",
       "  2    2.441360   0.000000  308.223083  349.418091    0.565527     56     chair\n",
       "  3  317.586182   8.774384  635.919434  339.871735    0.554879     56     chair\n",
       "  4  486.712097  23.239212  639.207092   58.052994    0.340517     66  keyboard\n",
       "  5  170.960693  35.684616  315.795715   70.140182    0.295709     66  keyboard\n",
       "  6  116.942375  71.486145  304.458313  246.342834    0.261198     15       cat,\n",
       "  'caption': [\"the catyou can see it's eyes\", 'A cat with its eyes showing.'],\n",
       "  'bbox_target': [318.91, 50.73, 262.44, 258.64]},\n",
       " 871: {'image_emb': tensor([[ 0.1816, -0.0589, -0.0967,  ..., -0.1443,  0.0081, -0.2642],\n",
       "          [-0.3208,  0.4133,  0.3164,  ...,  0.8491,  0.2102, -0.3333],\n",
       "          [ 0.1076, -0.1550,  0.0514,  ...,  0.2561, -0.0673,  0.0203]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3962,  0.2839, -0.0209,  ...,  0.0136,  0.1897,  0.0506],\n",
       "          [-0.1660,  0.0254, -0.1233,  ..., -0.3850,  0.0649, -0.0866]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.1887e-04, 4.6806e-03, 9.9512e-01],\n",
       "          [5.1689e-04, 4.4670e-03, 9.9512e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    1.441813   24.535227  326.268677  498.823212    0.844735      0  person\n",
       "  1   13.419586  142.631607  189.512894  306.212128    0.834802     27     tie\n",
       "  2  146.648331   37.792278  399.522400  497.916687    0.475980      0  person,\n",
       "  'caption': ['A woman wearing a purple shirt who has a man tying a blue tie around her neck.',\n",
       "   'A woman getting a tie tied on.'],\n",
       "  'bbox_target': [1.13, 18.58, 324.32, 472.97]},\n",
       " 872: {'image_emb': tensor([[-0.5337,  0.4385, -0.1497,  ...,  0.5854,  0.0572,  0.2111],\n",
       "          [-0.0173,  0.2026, -0.3850,  ...,  1.0654,  0.0388, -0.0023],\n",
       "          [-0.4260,  0.5151, -0.0446,  ...,  1.0205, -0.0559,  0.2216],\n",
       "          ...,\n",
       "          [ 0.1121, -0.0534, -0.5005,  ...,  0.5322, -0.0455, -0.2549],\n",
       "          [ 0.0497, -0.2482, -0.2947,  ...,  1.0713, -0.2539,  0.2529],\n",
       "          [-0.2859,  0.4092, -0.1906,  ...,  0.8901,  0.2874,  0.3633]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 1.0144e-01,  2.8687e-02,  3.9624e-01,  ..., -7.6721e-02,\n",
       "            3.6621e-04, -2.9688e-01],\n",
       "          [ 4.3774e-01,  3.2471e-01,  1.6992e-01,  ...,  1.2915e-01,\n",
       "            1.5283e-01, -3.2153e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.9922e-01, 1.1921e-07, 3.8266e-05, 1.7285e-06, 2.9802e-06, 0.0000e+00,\n",
       "           3.0078e-01],\n",
       "          [8.4766e-01, 6.6137e-04, 1.0674e-02, 2.0370e-03, 5.7459e-04, 2.3365e-05,\n",
       "           1.3843e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0   28.660187  169.376495  490.695648  326.611115    0.927477      4  airplane\n",
       "  1  564.961365  376.372131  600.896912  451.799255    0.908540      0    person\n",
       "  2  512.599304  186.588806  639.327332  318.158813    0.851652      4  airplane\n",
       "  3  445.083405  313.943634  462.431854  342.151398    0.802600      0    person\n",
       "  4  486.205261  318.548218  501.349304  345.487488    0.735405      0    person\n",
       "  5  495.036041  306.749054  571.163086  346.213470    0.719576      7     truck\n",
       "  6  564.167847  319.608948  580.645264  349.774902    0.670987      0    person,\n",
       "  'caption': ['A white plane that says Kingfisher in red.',\n",
       "   'Kingfisher Airplane is landing'],\n",
       "  'bbox_target': [20.45, 167.46, 474.62, 144.22]},\n",
       " 873: {'image_emb': tensor([[-0.1632,  0.2832, -0.0917,  ...,  1.2197, -0.0532, -0.0967],\n",
       "          [-0.3701,  0.1433,  0.0144,  ...,  1.0332,  0.1157,  0.2808],\n",
       "          [ 0.2852, -0.0240, -0.1417,  ...,  0.6426,  0.5503, -0.4268]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.4067,  0.1153, -0.1547,  ...,  0.4111, -0.3042, -0.0388],\n",
       "          [-0.0945,  0.2086, -0.2355,  ..., -0.1538,  0.0335, -0.1191]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0013, 0.9492, 0.0495],\n",
       "          [0.0373, 0.8760, 0.0867]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  488.539673   20.225578  614.284912  108.088409    0.918557     41   \n",
       "  1   21.650272  169.001724   91.543015  467.852783    0.907561     42   \n",
       "  2  467.869873  264.481598  590.568115  359.443756    0.683862     50   \n",
       "  3  274.425964   80.821915  336.475830  144.433182    0.576184     50   \n",
       "  4   17.686096   25.577988  625.306396  463.449158    0.571892     60   \n",
       "  5  392.677063  153.327103  585.789246  306.362915    0.545871     50   \n",
       "  6  340.289551  308.418823  497.808289  452.065369    0.544626     50   \n",
       "  7  273.808472   54.923752  595.503296  451.545166    0.415653     50   \n",
       "  8  369.151825  227.182709  466.360809  326.360443    0.382091     50   \n",
       "  9  358.772522  112.870880  459.753357  194.079376    0.316895     50   \n",
       "  \n",
       "             name  \n",
       "  0           cup  \n",
       "  1          fork  \n",
       "  2      broccoli  \n",
       "  3      broccoli  \n",
       "  4  dining table  \n",
       "  5      broccoli  \n",
       "  6      broccoli  \n",
       "  7      broccoli  \n",
       "  8      broccoli  \n",
       "  9      broccoli  ,\n",
       "  'caption': ['A brown table where the meal on a plate, silverware and place mat are on.',\n",
       "   'table'],\n",
       "  'bbox_target': [19.78, 21.15, 450.54, 75.83]},\n",
       " 874: {'image_emb': tensor([[-0.4019,  0.2830,  0.2347,  ...,  1.2861,  0.2286,  0.0534],\n",
       "          [ 0.2151,  0.1204, -0.1316,  ...,  0.2032, -0.2971,  0.1851],\n",
       "          [-0.2734,  0.5801, -0.0343,  ...,  1.1074, -0.0849,  0.2825],\n",
       "          [-0.0666,  0.3259, -0.1364,  ...,  1.1055,  0.2111, -0.3577],\n",
       "          [-0.2905,  0.6489, -0.2847,  ...,  0.8335, -0.4399,  0.0287]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0754,  0.1775, -0.3901,  ..., -0.2067,  0.1052,  0.0361],\n",
       "          [ 0.0805, -0.0878, -0.3479,  ..., -0.1896, -0.0161,  0.2233]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.9316, 0.0256, 0.0104, 0.0025, 0.0300],\n",
       "          [0.1652, 0.0234, 0.0346, 0.0481, 0.7285]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0    0.275459    0.523163  127.810577  164.798248    0.928585      0    person\n",
       "  1   97.222107   34.483414  452.469971  424.174988    0.916702     63    laptop\n",
       "  2  200.775665  394.551300  318.343719  604.411987    0.900931     76  scissors\n",
       "  3  251.541595  496.376221  429.219635  616.758789    0.864985      0    person\n",
       "  4  360.496857  516.953125  477.628540  640.000000    0.286400     73      book,\n",
       "  'caption': ['two fingers on top of a book',\n",
       "   'Two fingers touching the magazine'],\n",
       "  'bbox_target': [0.0, 1.27, 129.01, 161.53]},\n",
       " 875: {'image_emb': tensor([[-0.2878,  0.2673,  0.0145,  ...,  0.9878, -0.2783, -0.2366],\n",
       "          [ 0.0438, -0.0575,  0.0409,  ...,  1.4414, -0.0300,  0.2302],\n",
       "          [-0.0297,  0.5825,  0.1656,  ...,  0.8911, -0.2512, -0.0060],\n",
       "          [-0.4148,  0.3511, -0.1597,  ...,  1.5342,  0.0202,  0.0377],\n",
       "          [-0.3301,  0.2876,  0.0173,  ...,  0.8931, -0.3381, -0.3020]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1654, -0.0251, -0.2576,  ..., -0.2649,  0.1720, -0.0826],\n",
       "          [ 0.0265,  0.3567, -0.4392,  ...,  0.0957,  0.1035, -0.0140]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.5703e-02, 4.6387e-01, 2.8589e-01, 9.2773e-02, 6.1798e-02],\n",
       "          [4.8935e-05, 9.9707e-01, 1.7357e-04, 2.5902e-03, 2.1696e-05]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   90.795227   26.927017  537.879089  468.883850    0.950854     16     dog\n",
       "  1    0.634789    0.000000  131.396667  331.722412    0.912123      0  person\n",
       "  2  451.945007    0.608826  633.002625  343.908936    0.870582      0  person\n",
       "  3  183.446793  340.086700  301.092987  398.593353    0.845253     45    bowl,\n",
       "  'caption': ['man is on his the ground with a silver watch on',\n",
       "   'An arm with a wrist watch.'],\n",
       "  'bbox_target': [1.55, 2.46, 132.67, 330.07]},\n",
       " 876: {'image_emb': tensor([[-0.0535,  0.0964, -0.3726,  ...,  0.9829,  0.3022,  0.3318],\n",
       "          [-0.0468,  0.1409, -0.2186,  ...,  0.5098,  0.1019, -0.0224],\n",
       "          [-0.1744,  0.2092, -0.2900,  ...,  0.8716,  0.1986,  0.0034],\n",
       "          ...,\n",
       "          [ 0.1587, -0.1184, -0.1700,  ...,  1.0273,  0.0397, -0.2466],\n",
       "          [ 0.0318,  0.0847, -0.3328,  ...,  1.1777,  0.0716, -0.3201],\n",
       "          [ 0.0958,  0.1364, -0.2484,  ...,  0.7388,  0.6421,  0.4409]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0773,  0.2396, -0.6104,  ...,  0.2839,  0.1528, -0.1746],\n",
       "          [-0.1499,  0.1342, -0.4517,  ...,  0.0439,  0.0844, -0.0159]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9951e-01, 0.0000e+00, 0.0000e+00, 1.0312e-05, 5.9605e-08, 0.0000e+00,\n",
       "           5.9605e-08, 5.1117e-04],\n",
       "          [9.9854e-01, 1.1921e-07, 5.4240e-06, 1.0878e-04, 1.0133e-06, 1.7881e-06,\n",
       "           6.1035e-05, 1.2064e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   339.596558  150.709946  534.760681  469.519287    0.934022      0  person\n",
       "  1    72.211914  185.611145  219.499176  475.483276    0.928226      0  person\n",
       "  2   535.308411  234.815643  639.651550  417.379547    0.915019     56   chair\n",
       "  3   218.474915  179.592926  392.561035  478.798096    0.875888      0  person\n",
       "  4   478.656616  427.294708  507.710388  475.080811    0.864605     41     cup\n",
       "  5   368.193665  165.612183  402.205566  213.892456    0.813547     56   chair\n",
       "  6   390.966919  236.113434  427.827209  325.660492    0.740171     27     tie\n",
       "  7   510.912781  289.099487  537.005188  301.516174    0.692277     65  remote\n",
       "  8   492.601227  244.709106  562.097717  271.423828    0.683372     63  laptop\n",
       "  9    65.904465  224.987671  529.328430  474.188965    0.648211     57   couch\n",
       "  10  546.388672  395.767059  639.028442  467.247314    0.543111     73    book\n",
       "  11  218.733154  179.759857  393.030762  476.715332    0.485835     57   couch\n",
       "  12  411.583282  235.436798  429.146271  289.386444    0.465490     27     tie\n",
       "  13  300.619507  362.422211  341.384949  396.126068    0.462742     65  remote\n",
       "  14  300.404816  361.265900  314.487885  380.265961    0.460980     65  remote\n",
       "  15  495.096069  296.211792  535.698242  362.190735    0.423644     39  bottle,\n",
       "  'caption': ['Young man in a white dress shirt and grey pants holding a beer while playing Wii.',\n",
       "   'a man in gray pant holding a controller'],\n",
       "  'bbox_target': [338.38, 149.73, 180.54, 313.51]},\n",
       " 877: {'image_emb': tensor([[-0.1639,  0.2561, -0.1350,  ...,  1.3691, -0.1398,  0.3035],\n",
       "          [ 0.0807,  0.6118, -0.2462,  ...,  1.1113, -0.3049,  0.0743],\n",
       "          [-0.2391,  0.1426, -0.0826,  ...,  1.0674,  0.0268, -0.0745],\n",
       "          [-0.3364,  0.3542, -0.2441,  ...,  1.1553, -0.3015,  0.1770],\n",
       "          [-0.3889,  0.0243,  0.1195,  ...,  0.9019, -0.0818,  0.2520]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0080, -0.0168,  0.1630, -0.1343, -0.1401,  0.1785, -0.1617,  0.0188,\n",
       "           -0.1307, -0.0116,  0.3171, -0.0569, -0.0106, -0.0056,  0.0286,  0.4116,\n",
       "            0.4060,  0.3315, -0.1372, -0.0346,  0.2524,  0.0674,  0.3799,  0.4492,\n",
       "           -0.0408,  0.2166,  0.1132, -0.1013, -0.3105, -0.2510, -0.2294, -0.0470,\n",
       "            0.1080,  0.0788, -1.2002, -0.0303, -0.0117, -0.4412,  0.1066,  0.0572,\n",
       "            0.1890, -0.0745, -0.1428, -0.0522,  0.0500,  0.3589,  0.6807,  0.2241,\n",
       "           -0.2839,  0.2854,  0.1096, -0.5356,  0.2925, -0.6108, -0.3362,  0.2046,\n",
       "            0.1963, -0.0560, -0.2289,  0.4041, -0.0085,  0.1731, -0.1913, -0.3091,\n",
       "           -0.1681, -0.2406,  0.2198,  0.0021, -0.1407, -0.3438, -0.3196, -0.2109,\n",
       "            0.2491,  0.0443,  0.0157, -0.0201,  0.1487,  0.2292, -0.1165,  0.0211,\n",
       "           -0.5884,  0.1000, -0.0150,  0.2615, -0.3142,  0.2218,  0.4692,  0.0619,\n",
       "           -0.2754,  0.1218,  0.1198, -0.1698, -0.8047, -0.0157, -0.5312,  0.2991,\n",
       "           -0.1415, -0.0580,  0.6099, -0.1123,  0.0582, -0.1020,  0.0306, -0.1499,\n",
       "            0.2993,  0.0809,  0.2271,  0.2644,  0.2125,  0.0504, -0.2756,  0.1888,\n",
       "            0.2227, -0.1155, -0.5620,  0.2383,  0.0654,  0.3899, -0.2018,  0.4429,\n",
       "            0.0394, -0.1675, -0.0925,  0.0594,  0.4341, -0.3074,  0.5952, -0.3137,\n",
       "           -0.2869,  0.2737,  0.0458, -0.3447,  0.0636,  2.0703, -0.3418,  0.0712,\n",
       "            0.0706, -0.0931, -0.4976, -0.4861, -0.5151,  0.0982, -0.3105,  0.5396,\n",
       "           -0.2925, -0.2595,  0.2776, -0.7251,  0.3098,  0.1902, -0.0262, -0.3567,\n",
       "            0.3440,  0.1782, -0.2184,  0.2114,  0.0805, -0.1152,  0.3962,  0.0341,\n",
       "            0.2369, -0.4521, -0.4824,  0.0499,  0.0032, -0.3611,  0.4651, -0.0872,\n",
       "            0.3057,  0.0049, -0.2140,  0.0790,  0.2554,  0.2457, -0.3953, -0.2452,\n",
       "            0.1942, -0.2164,  0.3738, -0.3650, -0.2773,  0.4290, -0.0057, -0.3081,\n",
       "           -0.2986, -0.2008,  0.1384, -0.2001, -0.1542, -0.0660,  0.4043, -0.1937,\n",
       "           -0.1173,  0.0414,  0.4233, -0.1000, -0.2483,  0.2205, -0.4021, -0.2111,\n",
       "           -0.0078, -0.0676, -0.3066, -0.2917, -0.1882,  0.0754,  0.1841, -0.0756,\n",
       "            0.0646, -0.3279,  0.2043,  0.3181, -0.4087,  0.2483,  0.2102, -0.0081,\n",
       "            0.0748, -0.3110,  0.3616,  0.1229,  0.4873,  0.5449, -0.1664, -0.5688,\n",
       "           -0.2549,  0.0416, -0.2947, -0.2642, -0.0556, -0.1941, -0.1198, -0.2053,\n",
       "            0.3818, -0.0471,  0.0676, -0.2549,  0.2175,  0.1794, -0.4248,  0.1865,\n",
       "           -0.1702,  0.3035,  0.0350,  0.3147,  0.0939, -0.1230, -0.2300, -0.0787,\n",
       "           -0.1108,  0.0911, -0.1353, -0.2703,  0.5410,  0.2573,  0.0311, -0.2993,\n",
       "            0.0809, -0.1349, -0.3118, -0.1598, -0.2644,  0.2788, -0.1895,  0.0964,\n",
       "            0.0844,  0.2739, -0.1807,  0.0271, -0.2737, -0.0846,  0.0139,  0.3811,\n",
       "            0.3696,  0.1588, -0.1720, -0.4536,  0.0160,  0.1921,  0.0975,  0.3110,\n",
       "           -0.0778, -0.1897,  0.4565,  0.4810, -0.0226,  0.0914, -0.4055,  0.3254,\n",
       "            0.2634, -0.1479, -0.0933, -0.1301,  0.2357,  0.2581, -0.1061,  0.3496,\n",
       "           -0.5200,  0.1780, -0.3115, -0.1035, -0.0971, -0.4929,  0.2866, -0.0063,\n",
       "            0.1560, -0.0697,  0.1425, -0.0232,  0.2568, -0.0505,  0.3501,  0.7754,\n",
       "            2.0684, -0.0956, -0.0512,  0.3896,  0.3323, -0.3455,  0.4465, -0.0851,\n",
       "            0.0558,  0.0593,  0.1643, -0.1545, -0.5732, -0.0274,  0.1868,  0.0861,\n",
       "            0.0707, -0.8716,  0.1600, -0.3525, -0.0964, -0.6377, -0.3284,  0.0460,\n",
       "           -0.1531, -0.0256,  0.4038,  0.2314, -0.1276, -0.3718,  0.5723, -0.0551,\n",
       "            0.2910, -0.4250,  0.1220,  0.2903,  0.1245,  0.0704,  0.2052, -0.4282,\n",
       "           -0.0495,  0.5688, -0.1782, -0.4683, -0.4949,  0.0409,  0.2556, -0.0771,\n",
       "           -0.4541, -0.3223,  0.0798,  0.9443, -0.3284, -0.2473,  0.1420, -0.0746,\n",
       "           -0.2732, -0.1004, -0.2473, -0.4199, -0.1730, -0.1309, -0.1663,  0.2042,\n",
       "           -0.1699, -0.2671, -0.0235,  0.0118,  0.3733,  0.0771,  0.2369,  0.3403,\n",
       "           -0.0998, -0.1951, -0.1641,  0.2500, -0.3157,  0.1478, -0.4026, -0.0499,\n",
       "           -0.1685,  0.1838, -0.4573, -0.1714,  0.2776,  0.1172, -0.5415, -0.4207,\n",
       "           -0.0747, -0.1985,  0.7339,  0.0267, -0.2947, -0.0328,  0.5576,  0.0095,\n",
       "            0.0552, -0.0419,  0.3616,  0.1134, -0.2061,  0.3503,  0.1368, -0.1404,\n",
       "            0.1044, -0.4819,  0.1260, -0.0504, -0.0248,  0.2649, -0.1167,  0.0618,\n",
       "           -0.2018,  0.1509, -0.1691,  0.3706,  0.0545,  0.0298, -0.3608, -0.2010,\n",
       "           -0.5376,  0.2379, -0.0213,  0.0377,  0.0510, -0.0800, -0.3972, -0.0359,\n",
       "            0.1515, -0.1191,  0.2837,  0.0030, -0.0041,  0.0090, -0.5044, -0.1693,\n",
       "           -0.2642,  0.1670,  0.2905, -0.0589,  0.3010, -0.0400,  0.5322, -0.3225,\n",
       "            0.4194,  0.5312,  0.2566,  0.1528, -0.0267, -0.0050,  0.3389,  0.3945,\n",
       "            0.1907,  0.0154, -0.0041, -0.2469, -0.1713,  0.1268,  0.1718, -0.3484,\n",
       "           -0.0812, -0.1049, -0.0224, -0.0523,  0.3237, -0.0560, -0.0854,  0.4773,\n",
       "            0.1103, -0.0599, -0.0355,  0.2333, -0.1041, -0.1058, -0.0241,  0.2588,\n",
       "            0.0825, -0.0724, -0.3284, -0.3354, -0.0640,  0.8042,  0.6089, -0.3003,\n",
       "            0.4045, -0.1670, -0.3230, -0.0729, -0.3511, -0.1748, -0.0969,  0.6069,\n",
       "            0.7764, -0.0428, -0.2654,  0.2168,  0.2419,  0.2551, -0.3926,  0.3027]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0651e-01, 5.3024e-03, 7.0667e-04, 2.3041e-02, 8.6426e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0   12.077721  143.821365  102.491531  328.477783    0.925276     56  chair\n",
       "  1  325.120697  185.286041  638.952393  353.637299    0.905968     57  couch\n",
       "  2    0.077232  185.490601   67.808540  384.742554    0.888182     56  chair\n",
       "  3  418.382080  316.593109  639.312988  475.555908    0.867188     56  chair,\n",
       "  'caption': ['A red chair facing forward in a hotel room.'],\n",
       "  'bbox_target': [11.93, 143.78, 90.39, 186.34]},\n",
       " 878: {'image_emb': tensor([[ 0.0925,  0.0435,  0.0397,  ...,  0.6841, -0.1918,  0.1671],\n",
       "          [ 0.1537,  0.1378,  0.1705,  ...,  0.4897, -0.3501,  0.2607],\n",
       "          [-0.1155,  0.3672, -0.3269,  ...,  0.9390, -0.0411,  0.1499],\n",
       "          [ 0.2646,  0.1639,  0.1600,  ...,  0.4165, -0.5034,  0.3452]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1414,  0.1887, -0.3508,  ...,  0.2081, -0.0375,  0.0872],\n",
       "          [ 0.2747, -0.0782,  0.0847,  ...,  0.2971, -0.7334,  0.4146]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0058, 0.0201, 0.9683, 0.0058],\n",
       "          [0.0018, 0.0020, 0.9854, 0.0108]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  105.780365  186.644119  333.186157  333.137177    0.924748      0   person\n",
       "  1  207.585281   78.419754  408.373505  307.926300    0.909998      0   person\n",
       "  2  143.006973    0.067431  407.906494   57.640385    0.848917      2      car\n",
       "  3  394.767090    0.085551  446.835999   28.832119    0.681548      2      car\n",
       "  4  474.895996    0.589848  499.665009   71.629265    0.646389      0   person\n",
       "  5  139.057953  324.502289  180.726440  338.805481    0.570187     29  frisbee,\n",
       "  'caption': ['SILVER SEDAN IN BACKGROUND',\n",
       "   'A blue color car parked near a tree'],\n",
       "  'bbox_target': [144.1, 0.84, 261.24, 56.46]},\n",
       " 879: {'image_emb': tensor([[-0.6592,  0.2145,  0.0316,  ...,  0.6279, -0.0217, -0.0737],\n",
       "          [-0.3953,  0.2096, -0.1774,  ...,  0.7583,  0.0645,  0.1586],\n",
       "          [-0.8350,  0.2896,  0.1609,  ...,  0.6797,  0.0877, -0.2306],\n",
       "          [-0.1434,  0.4990, -0.1774,  ...,  0.9897, -0.0977, -0.1918],\n",
       "          [-0.5571,  0.4753, -0.3059,  ...,  1.1182, -0.1025,  0.2607],\n",
       "          [-0.6802,  0.2639,  0.1985,  ...,  0.5415, -0.0352, -0.2529]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-2.1106e-01,  7.4158e-02, -8.0032e-03,  3.4985e-01,  3.4448e-01,\n",
       "            2.6758e-01, -3.2690e-01, -9.3896e-01, -5.6592e-01, -1.8750e-01,\n",
       "           -3.3374e-01, -3.8330e-01,  7.0251e-02, -4.1064e-01,  2.3389e-01,\n",
       "           -6.2225e-02,  1.9141e-01, -1.4258e-01, -9.3567e-02,  2.7100e-01,\n",
       "            1.3977e-01,  4.9951e-01,  3.2031e-01,  2.4854e-01,  2.5537e-01,\n",
       "            4.6558e-01,  1.2402e-01, -7.3547e-02,  4.1504e-02,  7.8552e-02,\n",
       "           -3.6499e-01, -5.3253e-02,  2.6031e-02,  3.8599e-01, -1.5552e-01,\n",
       "           -1.2854e-01, -2.4036e-01,  9.6619e-02,  6.3896e-03,  1.1377e-01,\n",
       "            3.1299e-01, -6.0333e-02, -2.4255e-01, -1.8652e-01,  4.9011e-02,\n",
       "            5.4932e-02,  3.0322e-01,  1.7993e-01,  7.5928e-02, -9.9060e-02,\n",
       "           -2.7734e-01, -1.3733e-01, -1.5637e-01, -1.9385e-01,  4.3488e-02,\n",
       "           -1.8682e-03, -5.7922e-02, -2.7026e-01, -4.7314e-01,  2.7637e-01,\n",
       "           -1.6504e-01, -3.0786e-01, -4.6338e-01, -3.8452e-01, -1.8958e-01,\n",
       "            4.7028e-02, -5.7465e-02,  3.4961e-01,  1.7444e-01,  7.4005e-03,\n",
       "           -4.4287e-01, -1.0669e-01, -2.5098e-01,  1.4600e-01,  2.0093e-01,\n",
       "            1.4905e-01,  1.4453e-01, -2.2278e-02, -1.8250e-01,  1.4832e-01,\n",
       "            1.3147e-01,  4.6883e-03, -1.5396e-02, -2.9297e-01,  1.6650e-01,\n",
       "           -3.8916e-01,  2.4304e-01,  2.7714e-03, -1.1255e-01, -2.4841e-01,\n",
       "            3.3594e-01,  1.1951e-01, -1.2939e+00,  1.8005e-01, -4.9902e-01,\n",
       "            1.1147e-02,  1.6553e-01,  4.8975e-01,  9.9609e-02,  3.1372e-01,\n",
       "            1.2573e-01, -4.1870e-02, -7.4463e-02,  1.1487e-01, -1.9592e-01,\n",
       "            1.3135e-01, -2.3328e-01, -2.6953e-01,  1.3196e-01,  5.2429e-02,\n",
       "           -1.0870e-01,  3.2324e-01,  1.7529e-01,  3.7646e-01,  5.4736e-01,\n",
       "           -1.6333e-01,  2.5488e-01,  2.6428e-02, -3.9111e-01, -1.4160e-02,\n",
       "            3.7384e-02, -9.3164e-01, -7.8964e-03,  1.8958e-01,  1.9031e-01,\n",
       "            7.4707e-02,  2.5864e-02,  7.4341e-02,  1.7590e-01,  4.8535e-01,\n",
       "            1.3293e-01,  8.4839e-02,  5.9717e-01,  4.9570e+00,  8.4167e-02,\n",
       "            6.7444e-02, -6.0400e-01, -3.2684e-02, -1.0059e-01, -5.4297e-01,\n",
       "           -6.0181e-02,  6.0196e-03, -6.0986e-01, -2.2083e-01,  4.9713e-02,\n",
       "           -4.1107e-02, -2.6123e-01, -3.2666e-01, -5.7617e-02, -4.4751e-01,\n",
       "           -1.0730e-01,  2.2119e-01,  8.3374e-02,  3.7109e-01, -3.9478e-01,\n",
       "            7.4707e-02,  3.4521e-01, -5.4834e-01, -3.3667e-01,  1.2305e-01,\n",
       "            7.5073e-02, -1.8018e-01,  1.2500e-01,  5.7953e-02, -5.6244e-02,\n",
       "            5.4352e-02,  8.1970e-02, -2.2705e-02,  5.9473e-01,  5.3101e-03,\n",
       "            3.5010e-01, -2.2339e-01, -2.6782e-01,  1.3260e-02,  1.1658e-01,\n",
       "            5.6396e-01, -2.7634e-02,  6.2927e-02,  3.7964e-01, -3.2129e-01,\n",
       "           -2.4634e-01,  4.2969e-02, -1.0870e-01, -1.7761e-01, -1.8018e-01,\n",
       "            9.4238e-02, -5.0244e-01,  1.2396e-01,  2.5488e-01, -4.7211e-02,\n",
       "            1.6223e-01,  5.2002e-01, -2.2232e-02, -3.6963e-01, -1.1127e-01,\n",
       "           -2.5162e-02, -9.5276e-02,  1.8567e-01, -1.8750e-01, -1.9287e-01,\n",
       "           -2.0410e-01,  1.7410e-02,  2.0862e-01,  2.4353e-01,  5.9509e-02,\n",
       "            5.1697e-02, -1.4087e-01,  2.3572e-01, -1.8091e-01, -9.7900e-02,\n",
       "            1.3843e-01, -1.2018e-01, -2.3792e-01,  2.9810e-01,  2.2437e-01,\n",
       "            3.7939e-01, -1.6199e-01, -1.7444e-01, -2.2534e-01,  2.2925e-01,\n",
       "            3.3765e-01, -3.0319e-02,  1.4697e-01, -3.5840e-01,  4.9438e-02,\n",
       "            1.9446e-01,  7.8491e-02,  4.5502e-02,  1.9727e-01,  1.7578e-01,\n",
       "           -3.9746e-01, -4.6082e-02,  2.0667e-01, -6.7810e-02,  1.8311e-01,\n",
       "           -3.1830e-02,  1.0480e-01, -3.1433e-03, -1.2451e-01,  3.2227e-02,\n",
       "            1.8445e-01,  2.0129e-01,  2.7237e-02, -3.0933e-01,  7.3608e-02,\n",
       "            2.3682e-02,  9.4299e-03, -1.4600e-01,  6.1218e-02,  1.8799e-01,\n",
       "           -5.5267e-02,  1.7798e-01,  2.3547e-01, -2.0911e-01,  2.4796e-02,\n",
       "            9.6313e-02, -7.6965e-02,  1.3684e-01,  3.2227e-02,  4.0039e-01,\n",
       "            2.3926e-01,  4.2285e-01,  1.3831e-01,  1.3428e-01, -2.0459e-01,\n",
       "           -1.3867e-01,  2.0642e-01, -4.0283e-01, -8.2886e-02,  6.4746e-01,\n",
       "           -1.8970e-01, -5.4346e-01, -1.6772e-01,  6.3232e-02, -5.1575e-02,\n",
       "            4.7241e-01,  1.8884e-01,  3.4332e-02,  1.9397e-01,  5.5634e-02,\n",
       "            1.1029e-01,  1.4824e-02,  2.5806e-01,  1.4343e-01, -2.3804e-01,\n",
       "           -1.9080e-01,  1.0468e-01,  1.8387e-02,  4.9902e-01, -1.9385e-01,\n",
       "           -5.7220e-02,  4.5380e-02,  2.1289e-01, -2.7734e-01, -6.7871e-02,\n",
       "            3.1342e-02,  9.2529e-02,  3.6963e-01, -5.6610e-02, -1.2291e-02,\n",
       "           -1.9531e-01, -2.6636e-01, -2.4731e-01,  1.5186e-01,  1.9763e-01,\n",
       "            1.3916e-01,  1.7500e-03, -1.0567e-02, -5.8105e-02,  2.3315e-01,\n",
       "           -2.5073e-01,  2.9907e-01,  4.9570e+00,  4.0479e-01,  3.1616e-02,\n",
       "            2.3364e-01,  1.4183e-02, -3.5248e-02,  3.5913e-01,  1.9028e-02,\n",
       "            1.2878e-01,  1.4160e-01, -1.7908e-01, -9.6985e-02, -1.7114e-01,\n",
       "            1.3354e-01, -3.5669e-01,  2.2058e-01, -6.1951e-02, -2.1211e+00,\n",
       "            1.4087e-01,  2.3938e-01,  2.5244e-01,  2.8369e-01,  4.1846e-01,\n",
       "           -2.8931e-01, -2.0740e-01, -1.9739e-01, -6.2622e-02,  3.9093e-02,\n",
       "           -2.4353e-01, -3.8940e-01,  6.4575e-02,  1.1816e-01,  1.0394e-01,\n",
       "            1.9849e-01,  3.2397e-01,  4.2236e-01,  1.5283e-01, -1.5160e-02,\n",
       "           -1.4246e-01, -3.5522e-01,  5.3436e-02, -3.5217e-02,  6.8848e-02,\n",
       "           -3.4717e-01, -5.2539e-01,  1.0077e-01, -2.0142e-01,  3.5620e-01,\n",
       "           -3.1226e-01, -1.3904e-01,  1.0297e-01,  1.0706e-01,  3.6084e-01,\n",
       "            2.5317e-01,  8.2764e-02,  2.2229e-01,  2.4841e-01,  1.9800e-01,\n",
       "            8.9050e-02, -6.7627e-02,  2.1387e-01,  1.2976e-01, -9.3140e-02,\n",
       "           -1.8799e-01, -1.9714e-01, -1.6125e-01, -2.9956e-01,  1.9397e-01,\n",
       "           -5.6543e-01,  9.3994e-02,  3.0991e-02, -4.0375e-02, -8.4534e-02,\n",
       "            3.2202e-01,  4.7729e-01,  2.1948e-01, -4.6844e-02, -4.9976e-01,\n",
       "           -4.0430e-01,  2.4646e-01, -3.2251e-01, -2.2302e-01, -3.2082e-03,\n",
       "            2.5977e-01,  7.9163e-02, -1.2274e-01, -1.3684e-01,  5.6396e-01,\n",
       "            8.5303e-01, -2.9150e-01, -1.2445e-01,  1.5747e-01, -2.8955e-01,\n",
       "           -4.9902e-01, -2.7100e-01,  8.7158e-02, -2.2693e-01,  2.9810e-01,\n",
       "           -1.0675e-01, -8.8577e-03, -1.2061e-01,  2.2534e-01,  3.5645e-01,\n",
       "           -2.0096e-02, -4.3994e-01,  4.9438e-01, -1.6333e-01, -3.4180e-01,\n",
       "            3.1860e-02,  1.1487e-01,  7.1484e-01, -2.2095e-01, -1.7834e-01,\n",
       "            4.3237e-01, -6.0974e-02, -1.0086e-02, -2.3544e-02,  1.0602e-01,\n",
       "           -8.1055e-02,  1.8494e-01,  1.3130e-02, -5.2399e-02,  2.1143e-01,\n",
       "           -2.8174e-01, -6.4697e-02,  9.7656e-03,  4.6387e-02,  4.5312e-01,\n",
       "           -2.8125e-01,  3.2959e-01,  1.6785e-01,  1.8262e-01, -1.6772e-01,\n",
       "            8.2214e-02, -1.2054e-01,  2.5830e-01,  1.0248e-01, -1.1566e-02,\n",
       "            6.2500e-02,  1.5430e-01,  1.5112e-01, -5.6543e-01,  2.0593e-01,\n",
       "           -9.7871e-05,  2.8955e-01, -1.3130e-02,  2.0264e-01, -8.6609e-02,\n",
       "           -8.4106e-02,  4.6936e-02, -3.0420e-01,  2.9565e-01,  1.3953e-01,\n",
       "           -3.1177e-01,  2.0093e-01, -4.0192e-02, -2.6562e-01,  1.1682e-01,\n",
       "           -3.3173e-02, -9.7580e-03, -1.4600e-01,  1.6998e-02, -1.4374e-02,\n",
       "            3.0594e-02,  1.7078e-01,  1.9434e-01,  5.4688e-01,  6.7041e-01,\n",
       "            2.5220e-01, -2.6270e-01,  2.8564e-01,  4.2023e-02,  3.7427e-01,\n",
       "            1.1078e-01,  9.5154e-02,  2.4734e-02, -2.4658e-01, -1.9519e-01,\n",
       "            3.9258e-01, -1.0992e-01, -1.6357e-01,  9.0039e-01, -1.0022e-01,\n",
       "            2.0435e-01, -1.5930e-01,  8.4778e-02, -2.3239e-02,  5.0018e-02,\n",
       "            3.1323e-01,  3.5278e-01,  2.6562e-01, -1.8768e-02,  3.3740e-01,\n",
       "            1.5833e-01, -4.2310e-01,  4.0259e-01, -5.1086e-02,  2.4582e-02,\n",
       "           -3.1113e-02,  3.4515e-02]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0150, 0.1493, 0.2703, 0.0705, 0.3872, 0.1075]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0    0.000000    3.629684  378.900177  258.367615    0.954789     45      bowl\n",
       "  1  403.625122  100.856277  640.000000  334.961609    0.949446     45      bowl\n",
       "  2   17.319916  149.768799  475.561188  424.439880    0.948790     45      bowl\n",
       "  3  306.918823    0.000000  551.681519  139.538284    0.927331     45      bowl\n",
       "  4  494.873413    0.179169  570.620728  104.637543    0.740477     42      fork\n",
       "  5  142.794617  364.783203  236.729736  407.439636    0.495906     51    carrot\n",
       "  6  108.267776  390.493286  185.218002  425.908447    0.391813     51    carrot\n",
       "  7    0.000000  194.050354   55.210014  422.232483    0.317869     45      bowl\n",
       "  8   38.485695  283.150482   80.521889  363.141693    0.268169     50  broccoli,\n",
       "  'caption': ['The compartment that is holding corn.'],\n",
       "  'bbox_target': [307.73, 0.81, 247.88, 139.17]},\n",
       " 880: {'image_emb': tensor([[-0.1541,  0.7876, -0.0269,  ...,  0.8955, -0.2764, -0.1316],\n",
       "          [-0.1885,  0.7148,  0.1098,  ...,  0.7988,  0.0183,  0.1755],\n",
       "          [ 0.0337, -0.2335, -0.2468,  ...,  0.8784, -0.0059,  0.0566],\n",
       "          [-0.3665,  0.2091, -0.2664,  ...,  0.9214,  0.0251, -0.2732],\n",
       "          [-0.2479,  0.7559,  0.1593,  ...,  0.8984, -0.2878,  0.0925]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0399,  0.4580, -0.4131,  ..., -0.3970,  0.0794, -0.4199],\n",
       "          [ 0.0310,  0.0784, -0.2856,  ..., -0.0562,  0.4651,  0.0990]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.8008e-01, 1.0901e-01, 1.3709e-06, 2.2888e-05, 3.1055e-01],\n",
       "          [9.6130e-02, 8.5645e-01, 2.4071e-03, 2.0142e-02, 2.4689e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  206.642456   93.117561  481.977264  328.652039    0.948478      0  person\n",
       "  1   13.792539   12.477249  291.893860  327.636261    0.941413      0  person\n",
       "  2  130.453873  127.855995  155.779404  312.129486    0.846133     27     tie\n",
       "  3  318.515381  214.129471  352.085388  330.174042    0.826799     27     tie\n",
       "  4  480.347198  299.010254  499.931488  329.717957    0.674972      0  person,\n",
       "  'caption': [\"A man in a suit stands with his elbow resting on his friend's shoulder.\",\n",
       "   'The man wearing the suit with the wrist watch'],\n",
       "  'bbox_target': [14.89, 11.91, 277.66, 319.82]},\n",
       " 881: {'image_emb': tensor([[-1.6342e-02,  2.1729e-01, -3.7109e-02,  ...,  3.5645e-01,\n",
       "            1.3878e-02, -1.2488e-01],\n",
       "          [-3.7085e-01,  1.8884e-01,  5.0079e-02,  ...,  7.7686e-01,\n",
       "           -1.8356e-02, -2.6709e-01],\n",
       "          [-3.0444e-01,  4.0918e-01, -6.4331e-02,  ...,  6.0352e-01,\n",
       "           -1.4008e-02, -1.3647e-01],\n",
       "          [-1.3580e-03,  2.7539e-01, -2.6131e-04,  ...,  3.5498e-01,\n",
       "           -7.5134e-02, -1.4246e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0278,  0.5337, -0.0141,  ...,  0.3367, -0.0047, -0.5049],\n",
       "          [-0.2107,  0.5371,  0.2499,  ..., -0.1414, -0.1559, -0.4592]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1024, 0.0414, 0.6680, 0.1884],\n",
       "          [0.0906, 0.0825, 0.8076, 0.0190]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  418.062439   49.755531  551.445557  186.821716    0.926503     49   \n",
       "  1  281.124817   94.663681  421.643280  231.262085    0.921378     49   \n",
       "  2  211.100693  199.861023  388.744476  371.646545    0.917207     49   \n",
       "  3   11.655865    7.333533  608.698914  431.940155    0.549821     60   \n",
       "  \n",
       "             name  \n",
       "  0        orange  \n",
       "  1        orange  \n",
       "  2        orange  \n",
       "  3  dining table  ,\n",
       "  'caption': ['An orange in the middle of two other oranges on a grey countertop.',\n",
       "   'A small orange with larger oranges above and below it.'],\n",
       "  'bbox_target': [280.83, 95.8, 140.55, 136.98]},\n",
       " 882: {'image_emb': tensor([[ 0.3816,  0.2876,  0.2544,  ...,  0.3877,  0.2524, -0.0089],\n",
       "          [-0.0114,  0.3010, -0.0702,  ...,  1.0332, -0.0502, -0.2820],\n",
       "          [ 0.1820, -0.3010, -0.3687,  ...,  0.9150,  0.0038, -0.0784],\n",
       "          ...,\n",
       "          [-0.1312, -0.1490, -0.2349,  ...,  1.1279,  0.2539,  0.0832],\n",
       "          [-0.0949,  0.0753, -0.3259,  ...,  0.8369, -0.1859, -0.1355],\n",
       "          [ 0.3098, -0.2321,  0.0780,  ...,  0.8730, -0.2345,  0.3518]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2783,  0.3057, -0.4119,  ..., -0.0168, -0.2141,  0.3286],\n",
       "          [-0.0439, -0.0644, -0.6006,  ...,  0.3013, -0.0931,  0.4138]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.5327e-01, 5.1193e-03, 6.2988e-01, 6.7825e-03, 3.8862e-04, 1.0080e-03,\n",
       "           3.1888e-05, 6.2466e-05, 3.1548e-03, 2.5487e-04],\n",
       "          [6.6748e-01, 1.8250e-01, 1.2939e-01, 6.4802e-04, 7.2289e-04, 8.7214e-04,\n",
       "           8.8549e-04, 2.9385e-05, 8.4000e-03, 8.9493e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   107.179688  285.995087  248.377930  485.227631    0.920509      0   \n",
       "  1   222.630035  409.184448  441.010345  639.428345    0.917058     56   \n",
       "  2   242.436676  284.318329  350.029144  415.105682    0.872536      0   \n",
       "  3   258.414398  279.376953  493.918335  616.505859    0.863884      0   \n",
       "  4     0.202515  441.961365   76.133682  579.653137    0.850883     56   \n",
       "  5   463.572754  323.295898  528.279541  457.341248    0.825460      1   \n",
       "  6   133.618698  404.382202  196.133560  489.995117    0.787593     56   \n",
       "  7   215.754639  302.610016  263.335693  379.670563    0.776096      0   \n",
       "  8    85.215240  383.797638  120.864624  441.544342    0.749808     56   \n",
       "  9   328.730377  316.670197  361.596283  366.708954    0.612440      1   \n",
       "  10  461.412170  256.947876  527.824036  351.742920    0.482484      2   \n",
       "  11    0.000000  416.416687   36.096161  449.868530    0.407737     60   \n",
       "  12  462.098267  257.657501  527.409790  346.974823    0.320292      7   \n",
       "  13  272.880798  609.043030  527.577148  639.895203    0.285315     60   \n",
       "  14  371.280701  319.608734  385.194763  362.411041    0.255902      1   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1          chair  \n",
       "  2         person  \n",
       "  3         person  \n",
       "  4          chair  \n",
       "  5        bicycle  \n",
       "  6          chair  \n",
       "  7         person  \n",
       "  8          chair  \n",
       "  9        bicycle  \n",
       "  10           car  \n",
       "  11  dining table  \n",
       "  12         truck  \n",
       "  13  dining table  \n",
       "  14       bicycle  ,\n",
       "  'caption': ['a man in a blue shirt and green pants',\n",
       "   'man with green pants sitting at table'],\n",
       "  'bbox_target': [106.34, 286.44, 136.96, 200.77]},\n",
       " 883: {'image_emb': tensor([[-0.4487,  0.2720,  0.0938,  ...,  0.7920,  0.2186,  0.0330],\n",
       "          [-0.2025,  0.4736,  0.2281,  ...,  0.2952,  0.2585, -0.1103],\n",
       "          [-0.2893,  0.3376, -0.0860,  ...,  1.2764,  0.0131, -0.0593],\n",
       "          [-0.2678,  0.0650, -0.0017,  ...,  1.2881,  0.0091, -0.3945],\n",
       "          [-0.4199,  0.2773,  0.2805,  ...,  0.2240,  0.1907,  0.2047]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1600, -0.0926, -0.0466,  ..., -0.3452, -0.3398,  0.0147],\n",
       "          [ 0.0975, -0.1681,  0.0209,  ...,  0.6079, -0.3718, -0.4448]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.9937e-02, 8.0176e-01, 7.2266e-02, 5.3711e-02, 1.2177e-02],\n",
       "          [7.2289e-04, 9.5605e-01, 4.1351e-02, 1.0853e-03, 8.1873e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  140.527954  123.076599  295.257874  327.149170    0.916895      0    person\n",
       "  1  379.802765  144.171448  505.077789  311.098328    0.878703      0    person\n",
       "  2  276.721008  196.912720  394.579163  321.100586    0.771529     56     chair\n",
       "  3  163.897461  209.720490  262.318604  326.259430    0.752291     56     chair\n",
       "  4  406.620270  258.962585  582.359375  327.519470    0.628208     28  suitcase\n",
       "  5    0.283226  160.501862   49.829216  330.824127    0.570118     56     chair\n",
       "  6  145.378265  135.690674  288.832642  328.365295    0.335355     56     chair,\n",
       "  'caption': ['A woman in a red shirt sitting on the floor.',\n",
       "   'woman with bindi and sari'],\n",
       "  'bbox_target': [381.23, 144.83, 126.93, 168.65]},\n",
       " 884: {'image_emb': tensor([[ 0.0246,  0.3276,  0.5552,  ...,  0.4165, -0.0651, -0.0611],\n",
       "          [-0.4985,  0.2000,  0.0831,  ...,  0.2123, -0.6318, -0.2959],\n",
       "          [-0.3130,  0.3938, -0.0295,  ...,  0.5171, -0.2151, -0.2064],\n",
       "          [-0.5913,  0.1869, -0.0233,  ...,  1.0811, -0.3999,  0.0160],\n",
       "          [-0.0383,  0.1799,  0.5928,  ...,  0.3572, -0.0969, -0.2300]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0221, -0.0901, -0.1100,  ..., -0.3030,  0.4456,  0.0012],\n",
       "          [-0.1136,  0.2935, -0.3435,  ...,  0.7437, -0.4248, -0.2803]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.4255, 0.1360, 0.2878, 0.0558, 0.0949],\n",
       "          [0.0980, 0.3064, 0.4600, 0.1238, 0.0117]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    3.286484   30.851227  378.163818  419.424805    0.956572      0   \n",
       "  1  322.551544  128.029465  528.653137  420.175110    0.943095      0   \n",
       "  2  448.054260   32.144135  639.959778  419.439209    0.930588      0   \n",
       "  3  534.115601  218.365540  586.283203  413.836548    0.795731      0   \n",
       "  4    0.256635  346.296173   31.853138  422.987549    0.518452     34   \n",
       "  5  448.717194   48.156921  484.190460  124.428070    0.404437     35   \n",
       "  \n",
       "               name  \n",
       "  0          person  \n",
       "  1          person  \n",
       "  2          person  \n",
       "  3          person  \n",
       "  4    baseball bat  \n",
       "  5  baseball glove  ,\n",
       "  'caption': ['The kid with the orange shirt on and has the number 8 on the back of his shirt.',\n",
       "   'boy in baseball uniform'],\n",
       "  'bbox_target': [3.81, 25.73, 370.64, 393.51]},\n",
       " 885: {'image_emb': tensor([[-0.3176, -0.1309,  0.0147,  ...,  1.0049,  0.2512, -0.3867],\n",
       "          [-0.0176,  0.2515,  0.0745,  ...,  0.6719, -0.4729, -0.1006],\n",
       "          [-0.1847,  0.1500, -0.3630,  ...,  1.1953, -0.0771, -0.2156],\n",
       "          [-0.1543, -0.0062, -0.0582,  ...,  1.2295,  0.3203, -0.1842],\n",
       "          [ 0.1044,  0.1625,  0.0412,  ...,  0.8208,  0.0341,  0.1680]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2371,  0.1406,  0.2156,  ..., -0.0160, -0.4858, -0.3999],\n",
       "          [-0.1415, -0.4705,  0.4175,  ...,  0.3047, -0.2440,  0.0909]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.1952e-02, 9.3408e-01, 3.3879e-04, 9.4452e-03, 2.4124e-02],\n",
       "          [1.4938e-02, 8.2861e-01, 4.1306e-05, 7.9529e-02, 7.7087e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  129.668732  122.995239  275.609833  392.375854    0.824836     48   \n",
       "  1    0.162949  135.353622  140.794815  408.069702    0.808891     48   \n",
       "  2  127.596733    0.000000  175.806442  101.865936    0.740970     43   \n",
       "  3  371.543854  118.412064  514.109741  350.598389    0.736770     48   \n",
       "  4  424.880432  105.799835  564.867920  339.196686    0.648649     48   \n",
       "  5  263.117126  104.722412  430.716919  362.697144    0.553223     48   \n",
       "  6  472.858582  109.154266  639.924133  332.643524    0.509751     48   \n",
       "  7  505.093140  108.947220  638.476318  328.195496    0.414024     55   \n",
       "  8    0.115631    0.000000  638.884277  205.460571    0.341830     60   \n",
       "  \n",
       "             name  \n",
       "  0      sandwich  \n",
       "  1      sandwich  \n",
       "  2         knife  \n",
       "  3      sandwich  \n",
       "  4      sandwich  \n",
       "  5      sandwich  \n",
       "  6      sandwich  \n",
       "  7          cake  \n",
       "  8  dining table  ,\n",
       "  'caption': ['A small sandwich with an egg type filling is to the right on a white plate',\n",
       "   'Egg salad finger sandwich in white bread.'],\n",
       "  'bbox_target': [463.61, 108.41, 176.39, 219.11]},\n",
       " 886: {'image_emb': tensor([[-0.3782,  0.7241, -0.1681,  ...,  1.0371, -0.0194, -0.3601],\n",
       "          [-0.5034,  0.5796, -0.1169,  ...,  0.7295, -0.0806, -0.4446],\n",
       "          [-0.1337,  0.3728, -0.0274,  ...,  0.2620, -0.2389, -0.1738]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-7.3975e-02,  1.1646e-01, -9.4482e-02,  1.8433e-01, -1.4941e-01,\n",
       "           -1.4526e-01, -2.0227e-01, -1.1377e+00,  1.1938e-01,  2.0056e-01,\n",
       "           -4.6570e-02, -1.5251e-02, -2.0789e-01, -1.8494e-02,  2.3132e-01,\n",
       "           -1.1261e-01, -3.0786e-01,  3.1647e-02, -1.1188e-01,  2.5659e-01,\n",
       "            2.4963e-01,  2.9541e-02, -3.5547e-01,  3.2349e-01,  1.9012e-02,\n",
       "            8.2275e-02,  7.1167e-02,  1.9189e-01,  1.3908e-02,  2.3022e-03,\n",
       "           -3.0200e-01, -1.0986e-02,  8.6731e-02, -2.3950e-01, -2.3840e-01,\n",
       "            8.1482e-02,  3.0078e-01, -6.4148e-02,  1.6455e-01,  3.9703e-02,\n",
       "           -4.8315e-01, -2.9541e-01, -1.5942e-01, -3.1860e-02,  1.4819e-01,\n",
       "           -4.9927e-02,  2.1011e-02,  5.4230e-02, -1.8005e-01,  9.5337e-02,\n",
       "            5.8716e-02, -3.4399e-01, -6.0883e-02, -3.4302e-02, -6.2598e-01,\n",
       "            1.0742e-01, -2.8442e-01,  1.4014e-01,  4.8975e-01,  3.3295e-02,\n",
       "           -1.5515e-01, -1.0236e-01,  2.7759e-01,  1.9946e-01,  1.5173e-01,\n",
       "           -5.0146e-01, -2.9199e-01, -1.1987e-01,  2.0142e-01, -9.0149e-02,\n",
       "           -4.8486e-01,  2.5925e-02,  2.6514e-01, -3.6194e-02, -2.7588e-02,\n",
       "           -2.0251e-01,  2.3743e-02,  4.3030e-02,  1.6931e-01, -1.0979e-02,\n",
       "            1.9519e-01, -2.1875e-01,  1.4307e-01,  7.6172e-02, -9.2957e-02,\n",
       "           -2.2583e-01,  1.3660e-01,  2.1948e-01, -7.8735e-02, -7.6538e-02,\n",
       "            1.2250e-01,  4.2065e-01, -1.6826e+00,  7.8369e-01, -2.3999e-01,\n",
       "            2.9443e-01,  1.3501e-01, -2.9810e-01, -8.7769e-02, -2.2192e-01,\n",
       "           -1.0449e-01,  7.4524e-02,  1.9470e-01, -2.3499e-01, -2.1631e-01,\n",
       "            2.1423e-01, -2.4048e-01,  1.2866e-01,  3.6865e-02,  2.8656e-02,\n",
       "           -2.4011e-01,  3.7280e-01, -2.1655e-01, -2.1484e-01, -1.9543e-01,\n",
       "            1.6235e-01,  1.6815e-02, -5.8380e-02, -4.1602e-01, -1.8201e-01,\n",
       "           -3.5309e-02, -2.2156e-01,  2.8320e-01, -1.5442e-01, -1.7969e-01,\n",
       "           -1.2396e-01, -2.3181e-01,  4.5837e-02,  2.0691e-02,  2.8247e-01,\n",
       "            2.4796e-02,  1.0748e-01, -1.3464e-01,  5.9297e+00,  2.3804e-03,\n",
       "           -3.0591e-01, -2.1082e-01, -2.6440e-01, -3.4180e-01, -4.9744e-03,\n",
       "            1.1401e-01,  1.1627e-01, -4.6948e-01, -2.2656e-01, -4.1431e-01,\n",
       "           -1.8921e-02, -8.1055e-02, -4.1211e-01, -5.0354e-02,  1.0864e-01,\n",
       "            3.1952e-02,  6.4507e-03,  4.7363e-01,  1.1469e-01,  2.5635e-02,\n",
       "           -1.1603e-01,  4.5776e-01, -5.4736e-01,  1.2444e-02,  1.8262e-01,\n",
       "            2.4200e-02, -1.0277e-02,  6.6895e-02,  1.5820e-01, -6.9885e-02,\n",
       "            4.6921e-03,  3.7988e-01, -6.4148e-02,  6.9275e-02, -1.3647e-01,\n",
       "           -2.2083e-01, -2.2766e-02,  2.2107e-01,  2.6050e-01, -5.3369e-01,\n",
       "           -1.4587e-01, -5.2588e-01, -3.0078e-01,  1.3672e-01,  1.2988e-01,\n",
       "           -3.7354e-02, -9.2957e-02,  1.6785e-01,  9.9121e-02, -2.8732e-02,\n",
       "           -2.8979e-01,  2.5711e-03, -3.5254e-01,  2.9907e-01, -1.8359e-01,\n",
       "            1.7493e-01,  2.4805e-01,  4.8560e-01, -5.6000e-02,  2.3010e-01,\n",
       "            2.5024e-01,  6.1035e-03, -1.0248e-01,  1.0120e-01, -2.9810e-01,\n",
       "            4.0698e-01, -6.2408e-02,  1.7505e-01, -4.0009e-02,  3.3496e-01,\n",
       "            7.0801e-03, -1.2378e-01, -4.9731e-01, -5.6229e-03, -2.9834e-01,\n",
       "           -1.6699e-01,  1.8848e-01,  7.2876e-02, -2.3364e-01,  1.0696e-02,\n",
       "            1.4514e-01, -3.6041e-02, -2.5830e-01,  9.4177e-02, -2.4939e-01,\n",
       "           -9.7961e-02,  1.2032e-02, -1.5247e-01,  1.7212e-01, -2.3743e-01,\n",
       "            3.0121e-02,  1.2561e-01,  1.1450e-01, -1.9727e-01,  1.8811e-01,\n",
       "           -1.5613e-01,  1.6394e-01,  5.6793e-02, -1.8994e-01,  6.8298e-02,\n",
       "            1.0516e-01,  3.2690e-01, -7.9895e-02, -2.2144e-01, -1.3574e-01,\n",
       "            9.1614e-02,  2.9468e-01,  1.8799e-02, -1.7126e-01, -3.0176e-01,\n",
       "            3.0762e-01, -5.9784e-02,  1.3159e-01,  7.4829e-02,  4.2381e-03,\n",
       "           -1.4331e-01,  6.3293e-02,  1.4465e-01, -1.8262e-01, -3.6694e-01,\n",
       "           -4.0820e-01,  7.9163e-02, -6.2469e-02,  3.3374e-01,  1.1450e-01,\n",
       "            3.2520e-01,  2.9343e-02,  4.8853e-01,  1.3635e-01,  2.4438e-01,\n",
       "           -6.3293e-02, -5.4016e-02,  3.4241e-02, -1.1969e-01,  3.1592e-01,\n",
       "           -2.8760e-01,  6.0760e-02, -3.9001e-02, -4.3433e-01, -4.3286e-01,\n",
       "            6.6223e-02, -1.7468e-01,  3.0322e-01,  9.6191e-02,  1.3000e-01,\n",
       "            1.7346e-01,  2.6196e-01,  5.6201e-01,  2.5171e-01, -2.4316e-01,\n",
       "           -6.6261e-03,  3.3594e-01,  1.6943e-01,  3.5156e-01, -3.2440e-02,\n",
       "           -2.5192e-02,  3.3276e-01, -5.9845e-02, -9.2407e-02,  6.3354e-02,\n",
       "            8.8806e-02, -5.3711e-01, -1.5576e-01, -1.8457e-01,  8.6487e-02,\n",
       "           -4.6387e-01,  2.2009e-01, -1.2903e-01,  5.7251e-02,  3.0319e-02,\n",
       "           -1.8530e-01,  2.2766e-01, -2.6294e-01,  2.4158e-01,  5.1193e-03,\n",
       "           -3.6133e-01,  3.3569e-01,  5.9219e+00, -1.8225e-01,  2.5830e-01,\n",
       "           -2.6581e-02,  3.9917e-02,  6.7969e-01,  2.8638e-01,  1.1387e+00,\n",
       "           -2.6904e-01,  2.7756e-02, -5.7129e-02, -2.6416e-01, -8.0566e-02,\n",
       "            8.1543e-02,  1.8140e-01, -2.9688e-01, -1.5881e-01, -2.9824e+00,\n",
       "            1.5918e-01, -2.1594e-01,  3.6646e-01, -1.3562e-01, -1.3684e-01,\n",
       "           -4.4434e-01, -1.7761e-02, -3.4912e-02,  5.3223e-02,  4.8755e-01,\n",
       "           -4.6558e-01,  1.1176e-01,  4.1046e-02, -1.6187e-01,  1.1401e-01,\n",
       "           -5.7343e-02, -5.2399e-02,  1.9299e-01, -3.3508e-02,  1.5515e-01,\n",
       "            3.6328e-01,  9.5947e-02,  4.3677e-01,  4.2676e-01, -2.0032e-01,\n",
       "            8.2581e-02, -2.5659e-01,  2.5253e-02,  7.6843e-02,  9.7473e-02,\n",
       "           -3.3496e-01, -5.5756e-02,  2.6416e-01,  7.1045e-02, -2.8223e-01,\n",
       "            1.7151e-01, -2.9272e-01,  4.2969e-01,  1.0144e-01, -3.3691e-02,\n",
       "           -1.8872e-01,  4.0436e-03, -1.4038e-01, -9.0637e-02, -6.5430e-02,\n",
       "            3.2104e-02, -5.4359e-03,  5.1941e-02, -8.9905e-02,  8.6609e-02,\n",
       "           -4.1565e-02,  1.7981e-01,  1.1554e-01, -8.6609e-02,  3.0231e-03,\n",
       "            4.4141e-01, -1.7163e-01,  4.1290e-02,  1.7731e-02, -2.7930e-01,\n",
       "           -7.6758e-01,  1.4185e-01,  1.4783e-01,  1.5491e-01, -2.4939e-01,\n",
       "           -6.8787e-02, -1.5002e-01,  4.7852e-02, -3.7915e-01,  3.8770e-01,\n",
       "            5.2930e-01,  5.6152e-02, -2.8564e-01, -8.6731e-02, -7.6866e-03,\n",
       "            9.9365e-02,  2.1164e-02, -2.1167e-01, -2.7441e-01,  3.0045e-02,\n",
       "            2.9419e-01, -3.6523e-01, -1.8274e-01, -1.0797e-01, -1.4832e-01,\n",
       "           -1.9629e-01, -3.0811e-01, -1.3452e-01,  3.0518e-01, -1.3702e-02,\n",
       "            2.3792e-01, -2.2546e-01, -1.5869e-01,  8.0338e-03, -7.5317e-02,\n",
       "           -9.8572e-03, -1.9287e-01, -1.7981e-01,  5.7159e-02,  4.3774e-01,\n",
       "           -3.5205e-01,  3.8428e-01, -4.1382e-02, -4.6570e-02,  3.1885e-01,\n",
       "           -5.4565e-02,  6.9092e-02, -4.4647e-02,  7.2449e-02, -5.8098e-03,\n",
       "           -2.5903e-01, -1.7725e-01,  1.2337e-02,  1.8372e-01, -2.5684e-01,\n",
       "            1.4478e-01, -3.4302e-01,  5.9692e-02,  1.7908e-01, -3.0298e-01,\n",
       "            3.8159e-01, -9.5947e-02, -2.5299e-02, -3.9014e-01,  3.1143e-02,\n",
       "            1.9055e-01,  1.0773e-01, -5.2521e-02,  1.1884e-01, -6.8457e-01,\n",
       "            1.8591e-01,  5.3650e-02, -7.4463e-02, -1.4124e-01, -4.3182e-02,\n",
       "            1.4551e-01, -1.6101e-01, -2.0654e-01, -1.4087e-01, -2.0789e-01,\n",
       "            5.7715e-01, -1.6516e-01,  3.1274e-01, -3.6401e-01, -1.6650e-01,\n",
       "            2.1899e-01, -5.3589e-02,  9.1675e-02,  7.8735e-02,  8.3398e-01,\n",
       "            1.2756e-01, -7.8076e-01,  3.0908e-01, -4.5227e-02, -1.6614e-01,\n",
       "           -2.4487e-01,  3.3016e-03, -2.4731e-01,  1.6919e-01, -4.7577e-02,\n",
       "            6.0364e-02, -2.3865e-01, -4.2511e-02,  4.6753e-01,  4.0479e-01,\n",
       "           -2.7808e-01, -2.5463e-03,  1.3318e-01,  2.0935e-01,  2.5342e-01,\n",
       "            5.7275e-01, -4.0479e-01,  3.7292e-02,  2.2449e-03, -2.8271e-01,\n",
       "           -3.2898e-02, -4.9255e-02, -7.8552e-02, -9.7839e-02, -4.3042e-01,\n",
       "            1.3855e-01, -3.8452e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1052, 0.2407, 0.6543]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  145.083267  143.653931  314.519470  305.044373    0.913962     49  orange\n",
       "  1  289.382538   72.903091  500.741180  397.724731    0.839935     49  orange\n",
       "  2  286.664978   71.870697  440.317932  231.427399    0.253987     49  orange,\n",
       "  'caption': ['the orange on the far left'],\n",
       "  'bbox_target': [143.0, 146.53, 169.19, 156.1]},\n",
       " 887: {'image_emb': tensor([[ 0.0718,  0.3901, -0.0515,  ...,  0.5767,  0.0797,  0.3137],\n",
       "          [ 0.0855,  0.0317, -0.4775,  ...,  1.2100,  0.0701,  0.3049],\n",
       "          [ 0.0634,  0.1544, -0.2001,  ...,  0.8574, -0.0983,  0.0745],\n",
       "          [-0.0257,  0.1826, -0.1626,  ...,  0.5630,  0.1111, -0.0159],\n",
       "          [-0.0558, -0.2163, -0.1781,  ...,  0.7212, -0.1114, -0.0128],\n",
       "          [ 0.1987,  0.4341, -0.3206,  ...,  0.5566, -0.0440,  0.1851]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0692,  0.0557, -0.4509,  ...,  0.0701, -0.3533, -0.0178],\n",
       "          [-0.0712, -0.1788, -0.0508,  ..., -0.4294,  0.0324, -0.1181]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.5736e-04, 9.9316e-01, 1.1694e-04, 2.6369e-04, 7.3910e-04, 5.4626e-03],\n",
       "          [1.3266e-03, 9.2480e-01, 2.9430e-03, 1.4709e-02, 4.0009e-02, 1.6418e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  225.330582   29.204636  482.366577  420.095642    0.931266      0  person\n",
       "  1    0.000000  241.303955  127.077003  425.317078    0.877459      0  person\n",
       "  2  219.693161  318.500671  244.540207  341.887451    0.813680     41     cup\n",
       "  3  246.627213  317.882629  273.066895  354.530273    0.797716     41     cup\n",
       "  4  199.125885  289.092041  220.206146  358.188538    0.749664     39  bottle\n",
       "  5  460.464294  220.787384  578.220276  422.848267    0.666934     56   chair\n",
       "  6  335.756683   40.803391  361.781158   59.176010    0.664377     65  remote\n",
       "  7  243.825058  310.267059  266.905945  332.046661    0.374565     41     cup\n",
       "  8   95.626846  356.877319  195.852554  425.095154    0.306374     57   couch\n",
       "  9  243.889343  310.928894  267.507416  320.653931    0.279996     41     cup,\n",
       "  'caption': ['man on a couch.', 'the man watching the scene'],\n",
       "  'bbox_target': [0.0, 242.77, 112.27, 174.63]},\n",
       " 888: {'image_emb': tensor([[ 0.0318,  0.4302, -0.1846,  ...,  1.2979,  0.1129, -0.1176],\n",
       "          [-0.2195,  0.5112, -0.0976,  ...,  1.2314,  0.2040,  0.2290]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0803,  0.0829, -0.0555,  ...,  0.4207, -0.2886, -0.4207],\n",
       "          [-0.1909,  0.0721,  0.4365,  ..., -0.0838, -0.4482, -0.3450]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5078, 0.4922],\n",
       "          [0.5620, 0.4377]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0   35.810127  104.496399  280.547028  233.449860    0.737031     50  broccoli\n",
       "  1   80.885040    0.002983  248.662750   47.741573    0.657837     41       cup\n",
       "  2    2.633789   16.300323  479.403687  638.574707    0.656032     45      bowl\n",
       "  3   99.225159  316.521210  227.199707  405.884064    0.322416     51    carrot\n",
       "  4  193.090027  157.113129  277.734802  230.544830    0.281979     51    carrot,\n",
       "  'caption': ['A shish kabob of assorted vegetables with sesame seeds on top',\n",
       "   'A white plate with food on it.'],\n",
       "  'bbox_target': [1.44, 29.12, 478.56, 602.61]},\n",
       " 889: {'image_emb': tensor([[-0.0592,  0.3799, -0.2449,  ...,  0.6992,  0.1548,  0.1814],\n",
       "          [-0.1523,  0.0735, -0.5181,  ...,  0.7085,  0.0388, -0.1667],\n",
       "          [-0.1115,  0.2837, -0.3811,  ...,  1.0938,  0.1213, -0.1531],\n",
       "          [-0.3691,  0.2715,  0.2133,  ...,  0.5962,  0.2939,  0.0122]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-4.0430e-01, -4.3060e-02, -1.7639e-01, -1.9861e-01,  8.9172e-02,\n",
       "            2.0264e-01, -3.3228e-01, -4.2480e-01, -2.4268e-01, -1.5796e-01,\n",
       "            1.7847e-01, -1.3501e-01,  4.2456e-01,  1.1035e-01,  5.1575e-02,\n",
       "           -3.8940e-01, -2.5806e-01, -9.2773e-02, -1.3428e-01,  1.5735e-01,\n",
       "           -5.0385e-02,  2.6172e-01, -8.7585e-02,  7.1899e-02, -2.3438e-01,\n",
       "            2.0605e-01, -5.7312e-02, -5.5573e-02,  3.9478e-01, -2.8174e-01,\n",
       "            3.4937e-01,  6.3110e-02,  2.1753e-01, -2.7246e-01, -6.2305e-01,\n",
       "           -6.6504e-01,  5.4688e-01,  2.8271e-01, -1.5808e-01, -2.0386e-01,\n",
       "           -2.6514e-01,  2.0923e-01,  7.6660e-02,  2.4963e-01,  2.7832e-01,\n",
       "           -1.3733e-01,  3.4473e-01,  9.6741e-02, -5.9418e-02,  1.2573e-01,\n",
       "            1.3628e-03, -2.3071e-01,  1.7786e-01,  2.5073e-01, -4.8657e-01,\n",
       "           -2.0288e-01, -7.8735e-03,  9.2773e-02, -2.4585e-01, -1.4612e-01,\n",
       "            3.1567e-01,  3.4119e-02, -3.6774e-02,  1.3452e-01,  1.4526e-02,\n",
       "           -3.4033e-01,  1.6678e-02,  3.8892e-01, -8.7585e-03,  1.0663e-01,\n",
       "           -1.4966e-01, -4.5068e-01, -1.6602e-01,  2.2659e-02,  3.8696e-01,\n",
       "           -2.2888e-02,  3.7549e-01, -2.4506e-02,  5.8563e-02, -3.3179e-01,\n",
       "           -2.1411e-01, -2.0227e-01,  3.6499e-02,  4.0698e-01, -1.0077e-01,\n",
       "            3.2864e-03, -2.1033e-01,  3.1226e-01, -2.9199e-01,  4.3359e-01,\n",
       "            1.1060e-01,  1.0284e-01, -1.0957e+00,  9.2383e-01, -1.2384e-01,\n",
       "           -1.6089e-01, -1.4502e-01,  5.4108e-02,  1.3867e-01, -1.4673e-01,\n",
       "            1.8677e-01,  3.8672e-01,  1.8616e-01, -2.1130e-01, -1.5466e-01,\n",
       "            1.1833e-02, -6.5283e-01,  4.0356e-01, -2.1240e-01, -1.3647e-03,\n",
       "           -2.0801e-01, -2.7441e-01,  2.2253e-01,  5.4047e-02,  2.8491e-01,\n",
       "           -3.1494e-01, -2.2720e-02, -2.3682e-01, -7.3624e-04,  1.8872e-01,\n",
       "            1.7676e-01, -8.2947e-02,  2.1820e-03,  2.6953e-01, -1.2219e-01,\n",
       "            7.6904e-02, -2.6587e-01, -2.0447e-01,  1.9336e-01,  8.6279e-01,\n",
       "           -5.9700e-03, -3.2715e-01, -2.9443e-01,  4.1680e+00, -5.8984e-01,\n",
       "           -6.7261e-02, -9.4727e-02, -2.3608e-01, -4.8828e-02, -1.0651e-01,\n",
       "            1.6809e-01,  1.7181e-02, -6.8262e-01,  3.7573e-01, -9.9304e-02,\n",
       "           -3.3325e-01, -2.7417e-01, -6.9641e-02,  5.6610e-02,  1.6541e-02,\n",
       "           -1.4844e-01, -1.9580e-01, -1.0913e-01,  1.2189e-01, -4.9408e-02,\n",
       "           -8.9294e-02,  5.7251e-02, -3.2153e-01,  5.1544e-02,  4.6234e-02,\n",
       "           -3.9819e-01, -2.1204e-01,  6.9199e-03,  1.4343e-02, -5.0873e-02,\n",
       "            3.7500e-01,  2.7612e-01, -1.2805e-01,  3.8403e-01, -3.7988e-01,\n",
       "            1.8370e-04, -4.0460e-04,  3.3594e-01,  1.4404e-01, -5.7764e-01,\n",
       "           -1.2201e-01,  2.0459e-01,  3.0981e-01, -4.7681e-01, -3.3716e-01,\n",
       "           -2.4609e-01,  3.5596e-01,  2.2876e-01, -4.7754e-01, -1.7969e-01,\n",
       "           -4.4775e-01,  5.8789e-01, -5.1544e-02,  4.1992e-01,  1.6138e-01,\n",
       "            2.7856e-01,  5.2295e-01,  2.2205e-01, -4.6204e-02, -2.1045e-01,\n",
       "           -1.4722e-01, -5.5328e-02,  5.7373e-01, -5.7007e-02, -1.9519e-01,\n",
       "            4.2578e-01, -1.6711e-01,  4.4769e-02,  1.4221e-02,  3.9355e-01,\n",
       "            2.3315e-01,  2.6886e-02, -1.3806e-01, -1.3452e-01,  1.4624e-01,\n",
       "           -3.3844e-02,  1.4197e-01,  2.2803e-01,  2.3389e-01, -2.3523e-01,\n",
       "           -2.4658e-01,  2.1790e-01, -1.4941e-01, -5.2887e-02, -8.9172e-02,\n",
       "            5.0342e-01,  1.3818e-01, -1.9934e-01,  2.2766e-01, -2.7634e-02,\n",
       "            3.5034e-02,  7.5134e-02,  5.3131e-02,  1.5083e-02, -3.2007e-01,\n",
       "           -4.2432e-01,  1.7261e-01, -8.7341e-02, -4.6753e-02, -2.4872e-02,\n",
       "           -9.8816e-02,  9.1919e-02, -1.2598e-01, -1.8018e-01, -4.6509e-02,\n",
       "           -9.9792e-02,  3.1934e-01,  1.7078e-01, -1.3928e-01, -1.8604e-01,\n",
       "            1.6162e-01, -4.1321e-02,  7.3340e-01, -3.7988e-01,  1.3191e-02,\n",
       "           -4.3060e-02, -2.4133e-01, -3.8257e-01, -5.5428e-03, -1.6809e-01,\n",
       "            7.8674e-02, -2.1289e-01,  1.8433e-02,  1.1127e-01,  4.0497e-02,\n",
       "            6.1310e-02,  2.5610e-01, -3.1586e-02, -7.9102e-02, -1.5186e-01,\n",
       "           -5.5371e-01, -1.5210e-01, -3.1885e-01,  1.1072e-01,  8.2947e-02,\n",
       "           -2.3120e-01, -1.0638e-01,  6.5381e-01,  1.7847e-01, -9.2529e-02,\n",
       "           -2.3608e-01,  2.0496e-01,  5.4102e-01,  3.6646e-01, -7.4524e-02,\n",
       "           -1.3416e-01, -4.4067e-02,  2.4854e-01,  2.2827e-01, -2.3206e-01,\n",
       "            1.5613e-01,  1.4807e-01, -1.5723e-01,  1.7456e-01, -4.1553e-01,\n",
       "            3.4888e-01,  1.4526e-01,  3.9404e-01,  1.8298e-01, -9.7595e-02,\n",
       "            2.7515e-01, -5.8594e-01,  5.2643e-02,  3.5449e-01,  1.2695e-01,\n",
       "            5.3833e-02,  3.0664e-01,  7.9773e-02,  7.9041e-02,  2.3392e-02,\n",
       "           -6.6797e-01,  1.7029e-01,  1.5454e-01,  2.4750e-02, -3.2349e-01,\n",
       "           -1.7432e-01,  1.0077e-01,  4.1602e+00,  2.7676e-03,  2.0950e-02,\n",
       "            5.8154e-01,  2.7148e-01, -1.2708e-01, -1.7407e-01,  4.4800e-01,\n",
       "            1.8970e-01,  1.1926e-01, -2.0825e-01, -2.4780e-01, -8.5083e-02,\n",
       "           -9.7534e-02, -3.9453e-01,  4.9408e-02, -1.4807e-01, -1.4873e+00,\n",
       "            9.6436e-03,  5.7648e-02,  1.3867e-01, -5.3955e-01,  3.5425e-01,\n",
       "           -9.6497e-02,  1.5637e-01,  5.9021e-02, -1.0083e-01,  3.0859e-01,\n",
       "           -1.0950e-01, -2.0935e-01,  3.9429e-01, -3.4375e-01,  4.0771e-01,\n",
       "            1.2573e-01,  2.7319e-01,  2.8589e-01,  1.9360e-01, -8.8013e-02,\n",
       "            6.3867e-01, -2.6221e-01,  1.0541e-01,  1.2097e-01, -6.4600e-01,\n",
       "            2.3706e-01,  4.7388e-01,  1.7065e-01,  1.0696e-02,  2.6587e-01,\n",
       "            1.4490e-01, -1.1841e-01,  3.0469e-01, -3.1323e-01, -3.6108e-01,\n",
       "           -1.7395e-03, -2.4719e-01,  5.3662e-01, -1.0101e-01, -1.3672e-01,\n",
       "            1.1253e-02,  8.6548e-02,  5.0488e-01,  1.7078e-01, -7.5806e-02,\n",
       "           -4.6899e-01,  8.5083e-02, -2.2021e-01,  1.3135e-01, -3.4521e-01,\n",
       "           -5.7495e-02,  4.5227e-02,  2.9639e-01, -7.5537e-01, -4.6655e-01,\n",
       "           -2.2766e-01, -4.0430e-01, -4.6411e-01, -4.7925e-01,  1.2115e-01,\n",
       "           -6.9287e-01,  3.1128e-01, -7.1716e-02,  2.2485e-01, -7.8796e-02,\n",
       "            3.7231e-01,  3.9893e-01,  3.0701e-02,  2.5610e-01,  3.5742e-01,\n",
       "            4.4775e-01,  8.3313e-02, -3.9398e-02, -6.2347e-02,  5.6915e-02,\n",
       "            2.3346e-02, -1.0785e-01,  1.1731e-01,  2.7539e-01,  2.3621e-01,\n",
       "            4.0625e-01, -2.5586e-01,  1.1853e-01,  3.4363e-02, -9.3079e-02,\n",
       "           -1.2077e-02, -2.1838e-01, -4.2163e-01,  3.0029e-01, -1.1635e-02,\n",
       "            4.4434e-02, -5.5969e-02, -9.1064e-02, -1.3989e-01, -1.5976e-02,\n",
       "           -2.3535e-01, -1.2891e-01, -2.1240e-01,  3.2666e-01, -1.0384e-02,\n",
       "           -1.2927e-01, -4.4373e-02,  2.7295e-01, -1.7163e-01, -2.6147e-01,\n",
       "           -2.3926e-01, -3.1934e-01,  4.0625e-01, -1.5613e-01,  1.5356e-01,\n",
       "           -3.5352e-01, -3.4497e-01, -5.3320e-01,  3.3417e-02, -8.8745e-02,\n",
       "           -7.8613e-02, -3.0957e-01, -4.4580e-01,  1.7578e-01,  5.6836e-01,\n",
       "           -3.2715e-01, -2.2961e-01,  9.6741e-02,  2.4292e-01,  1.1884e-01,\n",
       "           -7.8064e-02,  2.6147e-01, -9.1064e-02,  8.8074e-02, -2.6147e-01,\n",
       "            1.9849e-01,  2.6416e-01,  5.0659e-02, -3.7903e-02, -2.4445e-02,\n",
       "            1.8164e-01,  5.7800e-02, -1.9458e-01, -1.6785e-01,  2.2314e-01,\n",
       "            5.0293e-01,  4.1333e-01,  2.2339e-01, -2.5098e-01,  2.6245e-01,\n",
       "            3.4497e-01,  1.6101e-01, -2.3389e-01,  3.4542e-03, -1.4429e-01,\n",
       "           -1.1816e-01, -8.0615e-01,  5.0195e-01, -6.5332e-01,  3.7378e-01,\n",
       "            4.9255e-02, -2.6001e-01, -2.3120e-01,  2.0837e-01,  5.5206e-02,\n",
       "            4.4141e-01,  3.1812e-01, -1.5881e-01,  5.4053e-01,  3.1006e-01,\n",
       "            1.4214e-02,  1.1188e-01,  1.0059e-01, -2.3499e-01,  4.0222e-02,\n",
       "           -1.1163e-01,  4.6167e-01, -1.4282e-01,  3.2080e-01,  4.4507e-01,\n",
       "            2.1533e-01,  2.9388e-02, -2.2437e-01, -3.4253e-01, -2.0166e-01,\n",
       "            3.8477e-01, -3.7329e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5386, 0.3762, 0.0110, 0.0740]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  113.234154   40.376953  248.212280  352.081482    0.922829      0    person\n",
       "  1   24.777584   73.220840  146.092560  346.933411    0.915727      0    person\n",
       "  2  158.564117   82.972412  215.424713  155.384338    0.762133     24  backpack\n",
       "  3    0.639282  293.815460  149.798172  359.062347    0.674925     30      skis\n",
       "  4  177.370056  273.937042  303.774902  359.680573    0.593215     30      skis,\n",
       "  'caption': ['skating board of the boy in the right side of the image'],\n",
       "  'bbox_target': [114.2, 277.72, 188.77, 82.28]},\n",
       " 890: {'image_emb': tensor([[-0.4233,  0.5391,  0.0575,  ...,  0.8599,  0.2563, -0.2722],\n",
       "          [-0.3462,  0.5850, -0.3318,  ...,  0.8652, -0.2494, -0.2079],\n",
       "          [-0.1654,  0.3337, -0.3499,  ...,  1.0889,  0.0403,  0.0974],\n",
       "          ...,\n",
       "          [ 0.0091,  0.0301, -0.3481,  ...,  0.9517, -0.0516,  0.2102],\n",
       "          [-0.2930,  0.2301, -0.4395,  ...,  1.2754, -0.3784,  0.0668],\n",
       "          [-0.1771, -0.0648, -0.1047,  ...,  0.9624,  0.2478, -0.0164]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3083,  0.0504, -0.1932,  ..., -0.3486,  0.4673, -0.4858],\n",
       "          [-0.2053,  0.4587,  0.0008,  ..., -0.0844,  0.2281, -0.1451]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7748e-03, 5.6696e-04, 5.3048e-06, 9.1943e-01, 5.2750e-05, 5.4121e-04,\n",
       "           7.7820e-02],\n",
       "          [5.2295e-01, 3.6550e-04, 1.1325e-06, 4.7607e-01, 1.7345e-05, 3.2425e-05,\n",
       "           8.6308e-04]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   211.330032   46.233116  438.771973  197.842255    0.920819     62   \n",
       "  1   229.986176  319.643707  460.263458  400.746246    0.915858     66   \n",
       "  2   509.397217  355.664825  549.580688  401.695343    0.901539     64   \n",
       "  3     0.710098   51.484222  218.833054  261.263123    0.897977     62   \n",
       "  4   581.460205  233.492798  625.333740  330.091919    0.834445     39   \n",
       "  5     0.179073  290.999023   44.393234  358.726196    0.778613     66   \n",
       "  6    68.244682    0.461807   89.692665   36.875610    0.546661     73   \n",
       "  7   531.463806    0.073273  594.554260   67.006958    0.520195      0   \n",
       "  8    55.215210    0.289581   76.051636   37.433868    0.476521     73   \n",
       "  9    84.442139    0.375053  102.763931   35.418388    0.447052     73   \n",
       "  10  222.235809  249.537598  253.532043  308.354980    0.437076     67   \n",
       "  11   13.442411    0.772301   53.397377   40.631996    0.374196     73   \n",
       "  12    0.000000    0.782181   20.912735   51.678909    0.312901     73   \n",
       "  \n",
       "            name  \n",
       "  0           tv  \n",
       "  1     keyboard  \n",
       "  2        mouse  \n",
       "  3           tv  \n",
       "  4       bottle  \n",
       "  5     keyboard  \n",
       "  6         book  \n",
       "  7       person  \n",
       "  8         book  \n",
       "  9         book  \n",
       "  10  cell phone  \n",
       "  11        book  \n",
       "  12        book  ,\n",
       "  'caption': ['The monitor on the left with bottles pictured on it.',\n",
       "   'A computer monitor that is showing a Flickr webpage image with alchoholic bottles.'],\n",
       "  'bbox_target': [3.96, 58.64, 216.74, 197.94]},\n",
       " 891: {'image_emb': tensor([[-0.3994, -0.1801,  0.1760,  ...,  0.6416, -0.4172,  0.0238],\n",
       "          [-0.4028,  0.3376,  0.1075,  ...,  0.9385, -0.2054, -0.0025],\n",
       "          [-0.2786,  0.2991, -0.1307,  ...,  1.0752, -0.4231, -0.0500],\n",
       "          [-0.4202, -0.0911, -0.0043,  ...,  0.8516, -0.3491,  0.0950]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-4.0100e-02, -4.3237e-01, -1.4453e-01,  1.7871e-01,  1.5479e-01,\n",
       "            1.0620e-01, -7.1167e-02, -1.3711e+00, -2.9800e-02,  6.3184e-01,\n",
       "           -2.0374e-01, -7.4707e-02,  9.5886e-02, -3.0685e-02,  7.1960e-02,\n",
       "           -1.5015e-01,  2.5439e-01, -2.6001e-01,  2.1629e-03,  4.0381e-01,\n",
       "            2.0056e-01,  3.2275e-01, -7.8064e-02,  1.7505e-01, -2.2852e-01,\n",
       "           -4.1565e-02, -8.6914e-02, -1.3196e-01, -6.5979e-02,  1.1115e-01,\n",
       "           -1.7517e-01,  3.2715e-02, -1.7749e-01, -2.9495e-02,  1.0315e-01,\n",
       "            2.3853e-01,  1.2415e-01, -8.7158e-02, -3.5547e-01,  9.9915e-02,\n",
       "           -3.4814e-01, -4.0283e-02, -4.7424e-02,  3.4961e-01,  9.7412e-02,\n",
       "           -1.7297e-01, -2.5366e-01,  1.4624e-01, -1.1432e-01, -2.9590e-01,\n",
       "            1.6479e-01, -5.5078e-01,  7.0190e-02, -9.5703e-02, -2.5391e-01,\n",
       "            2.4561e-01, -4.4385e-01,  2.7515e-01,  1.5527e-01,  1.5747e-01,\n",
       "            1.5723e-01,  1.2976e-01,  5.3613e-01,  3.7109e-01, -2.0105e-01,\n",
       "           -6.9946e-02, -4.3994e-01, -1.5112e-01,  4.8767e-02, -4.0112e-01,\n",
       "           -2.3047e-01,  2.2415e-02, -4.4647e-02, -1.7053e-01,  7.3608e-02,\n",
       "            6.7261e-02, -2.8760e-01, -5.4626e-02,  7.0679e-02,  7.2708e-03,\n",
       "           -1.7761e-01,  3.5571e-01, -1.2830e-01,  2.6855e-01,  1.0150e-01,\n",
       "           -6.1035e-02, -6.9336e-02,  1.7908e-01,  1.0780e-02,  2.1509e-01,\n",
       "           -2.3840e-01, -6.2317e-02, -1.6514e+00,  7.7197e-01, -2.4939e-01,\n",
       "           -2.6807e-01,  7.6111e-02,  1.2317e-01, -5.0125e-03, -2.6587e-01,\n",
       "            1.1395e-01, -5.6366e-02,  1.7517e-01, -3.5059e-01, -4.1235e-01,\n",
       "            9.7717e-02, -1.8542e-01,  3.0127e-01,  9.7839e-02,  4.1382e-01,\n",
       "           -3.5840e-01,  1.2122e-01, -1.7297e-01, -2.3499e-01,  3.4595e-01,\n",
       "           -1.5778e-02, -1.1212e-01, -4.8364e-01, -1.8176e-01,  7.3364e-02,\n",
       "            1.2225e-01, -3.6523e-01, -1.3953e-01, -5.9319e-03,  6.7322e-02,\n",
       "           -1.4551e-01, -5.7640e-03,  7.7148e-02,  1.1102e-01,  5.7129e-01,\n",
       "           -4.2236e-02, -4.3604e-01, -6.7078e-02,  5.5781e+00, -1.5637e-01,\n",
       "           -2.0471e-01, -1.5405e-01, -5.0146e-01, -3.3130e-01, -9.2697e-03,\n",
       "            2.3132e-01, -1.9739e-01, -3.1616e-01, -2.5952e-01, -6.5918e-01,\n",
       "           -2.1130e-01, -9.7839e-02, -3.1860e-01, -3.7500e-01, -3.6475e-01,\n",
       "            2.2375e-01,  1.9934e-01, -1.0486e-01, -1.0614e-01,  1.0565e-01,\n",
       "           -8.8684e-02,  3.4033e-01, -7.2998e-02,  3.5034e-01, -1.4600e-01,\n",
       "            2.0386e-02, -1.9702e-01,  1.7651e-01,  1.1154e-02,  1.1517e-01,\n",
       "            3.3008e-01,  1.7993e-01, -4.6484e-01, -1.7944e-02, -8.0627e-02,\n",
       "            1.8591e-01, -2.8735e-01,  1.8225e-01, -5.0720e-02, -1.3257e-01,\n",
       "           -2.7637e-01, -1.2305e-01, -1.9092e-01, -4.8920e-02,  1.5479e-01,\n",
       "           -9.0027e-02,  1.1548e-01,  4.8401e-02,  2.2546e-01, -6.8176e-02,\n",
       "           -2.6440e-01,  9.3323e-02, -8.5327e-02, -1.4084e-02,  2.1167e-01,\n",
       "           -3.3545e-01,  2.5070e-02,  2.4866e-01, -2.6636e-01,  1.1267e-01,\n",
       "           -8.0933e-02,  3.2666e-01,  3.7085e-01, -2.8271e-01, -1.8579e-01,\n",
       "            1.3965e-01,  1.5430e-01,  1.3586e-01,  8.1909e-02,  2.9816e-02,\n",
       "            3.6450e-01, -7.6355e-02, -3.3057e-01, -1.2988e-01, -2.9932e-01,\n",
       "           -2.6953e-01, -3.8391e-02,  2.1594e-01,  1.6525e-02, -3.6206e-01,\n",
       "            1.6626e-01,  1.4893e-01, -5.1758e-01,  4.0918e-01, -2.8442e-01,\n",
       "            1.1755e-01,  9.8450e-02, -3.6255e-02, -1.4539e-01,  1.7242e-02,\n",
       "           -5.0110e-02, -1.9348e-01,  6.6040e-02, -2.0557e-01,  2.4573e-01,\n",
       "           -1.7807e-02,  3.6890e-01,  1.2659e-01,  3.3984e-01,  2.7115e-02,\n",
       "            4.4312e-01,  3.2812e-01, -4.2554e-01, -1.5466e-01, -1.8402e-02,\n",
       "            2.4231e-01,  3.8721e-01,  3.8330e-01, -4.2297e-02, -1.3672e-01,\n",
       "            6.2790e-03, -1.3184e-01, -1.4587e-01,  3.7659e-02,  6.8298e-02,\n",
       "           -5.2032e-02, -1.0931e-01,  9.4910e-02,  3.5864e-01, -3.1885e-01,\n",
       "           -3.0151e-01, -3.4375e-01,  1.7700e-01,  1.4771e-01,  2.3413e-01,\n",
       "           -4.8523e-02, -7.8247e-02,  2.5708e-01,  1.0223e-01,  1.6724e-01,\n",
       "           -9.0698e-02,  2.7930e-01, -3.1519e-01, -1.8604e-01,  4.4586e-02,\n",
       "            9.1980e-02,  3.8086e-01,  2.5635e-01,  5.4871e-02, -2.1997e-01,\n",
       "            1.9971e-01,  4.7852e-02,  2.7417e-01, -1.5466e-01, -1.9482e-01,\n",
       "            2.6636e-01,  2.0288e-01,  4.3311e-01,  2.2522e-02, -4.2847e-02,\n",
       "            2.5464e-01,  5.0000e-01,  2.1228e-01, -7.3547e-03,  2.0309e-02,\n",
       "           -3.5986e-01,  2.8662e-01, -1.3464e-01, -4.1382e-02, -1.6760e-01,\n",
       "            2.2021e-01, -4.6899e-01, -1.5198e-01,  2.4414e-01,  6.5430e-02,\n",
       "           -5.4413e-02,  8.9600e-02,  4.1870e-02, -8.0627e-02, -2.4939e-01,\n",
       "           -2.2058e-01,  2.1863e-01, -1.5918e-01,  8.2275e-02, -7.4524e-02,\n",
       "            2.2498e-01,  4.6729e-01,  5.5664e+00,  1.7786e-01,  3.2056e-01,\n",
       "            9.6741e-02, -5.5046e-03,  2.5439e-01,  6.4209e-02,  5.3076e-01,\n",
       "            1.6516e-01,  1.2097e-01, -1.1682e-01,  3.4027e-02,  2.5171e-01,\n",
       "            3.4790e-01,  9.4727e-02, -4.5312e-01, -1.6797e-01, -2.9453e+00,\n",
       "           -3.6804e-02, -3.9978e-02,  1.9458e-01, -1.2177e-02, -1.7981e-01,\n",
       "           -1.3257e-01, -7.6782e-02, -1.9424e-02, -8.9844e-02,  1.2970e-02,\n",
       "           -1.2177e-01,  3.8745e-01, -1.9385e-01, -1.7578e-01,  1.6174e-01,\n",
       "            1.7908e-01, -6.0242e-02, -2.3560e-02, -2.1667e-01, -1.1176e-01,\n",
       "            3.5059e-01,  1.2573e-01,  1.2337e-02,  2.5977e-01,  1.9751e-01,\n",
       "            1.3330e-01, -2.6001e-01,  9.6893e-03, -3.8672e-01,  8.8196e-02,\n",
       "            9.3323e-02,  5.7831e-02,  7.6050e-02, -1.3928e-01, -1.0938e-01,\n",
       "            1.1853e-01, -3.0566e-01,  2.0166e-01, -1.6449e-02, -2.9834e-01,\n",
       "            2.9443e-01, -1.9897e-02,  2.8809e-01,  3.8794e-01, -1.7990e-02,\n",
       "           -8.2474e-03, -6.9214e-02,  3.7632e-03, -2.2607e-01,  2.2693e-01,\n",
       "           -2.2217e-01, -6.8787e-02,  2.0056e-01,  5.7831e-02, -1.9409e-01,\n",
       "            1.8872e-01,  4.7437e-01,  8.7204e-03,  2.3743e-02, -1.9189e-01,\n",
       "           -9.1943e-01, -4.7485e-02, -1.7212e-01, -3.4607e-02,  2.5806e-01,\n",
       "            3.0176e-01, -6.6895e-02,  2.2046e-01, -2.8198e-01,  6.6211e-01,\n",
       "            8.8623e-01, -9.4421e-02,  2.4731e-01, -4.5093e-01,  9.1248e-02,\n",
       "           -1.5549e-02,  2.4796e-02, -4.0454e-01,  2.4536e-01,  6.1760e-03,\n",
       "            8.1738e-01, -2.1057e-01, -1.7664e-01,  4.2953e-03, -1.2134e-01,\n",
       "           -4.1089e-01, -1.7004e-01, -4.0039e-02,  1.6858e-01, -2.3022e-01,\n",
       "            2.1436e-01, -1.5625e-01,  1.6138e-01, -9.8938e-02,  9.8633e-02,\n",
       "            1.5820e-01, -5.6732e-02,  1.0626e-01,  3.1787e-01,  2.2485e-01,\n",
       "           -4.2114e-01,  1.3953e-01, -1.5771e-01, -1.4319e-01,  1.9495e-01,\n",
       "            6.3110e-02, -2.0276e-01, -1.9690e-01, -1.6187e-01, -1.1133e-01,\n",
       "           -3.4302e-01,  9.6436e-02, -1.1511e-01,  2.4048e-02,  2.1741e-01,\n",
       "           -7.3853e-02, -6.7322e-02, -1.0168e-01,  6.3281e-01,  2.6270e-01,\n",
       "            1.2097e-01,  1.4844e-01, -4.1479e-01, -1.7041e-01,  3.4760e-02,\n",
       "           -3.7598e-01, -4.1602e-01, -1.0083e-01,  3.4912e-01, -3.2324e-01,\n",
       "           -5.3894e-02,  2.1716e-01,  3.0884e-01, -2.0728e-01, -1.3159e-01,\n",
       "            5.5008e-03, -6.5771e-01,  4.9652e-02, -2.1875e-01, -1.1517e-01,\n",
       "           -4.4678e-02, -2.4158e-01,  4.5459e-01, -3.6304e-01,  7.4707e-02,\n",
       "            1.0034e-01,  2.3560e-01,  2.0361e-01,  2.6953e-01,  2.0959e-01,\n",
       "           -2.1301e-01, -6.5576e-01,  3.3130e-01, -4.7363e-02, -2.5146e-01,\n",
       "            5.3101e-02,  1.5961e-02, -2.3096e-01, -3.7567e-02, -5.9021e-02,\n",
       "            2.1960e-01, -6.0394e-02, -8.2703e-02,  5.1758e-01,  2.7686e-01,\n",
       "           -2.7026e-01, -4.3259e-03,  3.9990e-01, -1.0150e-01,  1.6479e-01,\n",
       "            3.9185e-01,  9.6497e-02,  2.2217e-01, -7.6843e-02, -4.3066e-01,\n",
       "            8.3679e-02,  1.5857e-01, -8.4290e-02,  2.3468e-02, -1.0907e-01,\n",
       "           -1.3171e-01, -1.4124e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3303, 0.0143, 0.5620, 0.0932]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   37.907059   49.451843  441.080627  476.218567    0.917993     77   \n",
       "  1  364.751038   42.265839  615.437988  475.099365    0.894923     77   \n",
       "  2  257.341614    0.508171  429.418335  200.816742    0.884859     77   \n",
       "  3    0.214874    2.122383  215.866272  212.184998    0.668544     77   \n",
       "  4  572.431458  113.856812  639.554626  460.105469    0.466743     77   \n",
       "  5    0.000000  204.556122   86.169785  476.098267    0.399936     77   \n",
       "  6    0.470146    0.664131  217.129883   93.442810    0.252107     77   \n",
       "  \n",
       "           name  \n",
       "  0  teddy bear  \n",
       "  1  teddy bear  \n",
       "  2  teddy bear  \n",
       "  3  teddy bear  \n",
       "  4  teddy bear  \n",
       "  5  teddy bear  \n",
       "  6  teddy bear  ,\n",
       "  'caption': ['bear far left top corner'],\n",
       "  'bbox_target': [0.05, 0.02, 214.6, 206.64]},\n",
       " 892: {'image_emb': tensor([[-0.6396,  0.3779,  0.0278,  ...,  1.0869,  0.3206, -0.1978],\n",
       "          [-0.1298,  0.1428, -0.5059,  ...,  0.9106,  0.2566,  0.4121],\n",
       "          [ 0.2311,  0.0751, -0.6436,  ...,  0.7983,  0.0721,  0.3215]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3372,  0.0842, -0.5332,  ..., -0.0465, -0.0226, -0.1089],\n",
       "          [ 0.0850,  0.1949, -0.5322,  ..., -0.0359, -0.1642, -0.2098]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0563, 0.6753, 0.2686],\n",
       "          [0.2610, 0.6768, 0.0620]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0   92.570755  310.274292  442.683716  475.607178    0.912257      0    person\n",
       "  1    0.000000    0.869110  240.857880  378.075775    0.788755      0    person\n",
       "  2  105.713089   55.940552  566.230469  300.996277    0.528244     48  sandwich\n",
       "  3  106.185547   52.359650  563.405884  304.520111    0.487138     52   hot dog\n",
       "  4  145.383545    0.382355  249.153198   80.079865    0.404084      0    person,\n",
       "  'caption': ['A hand holding what appears to be a jelly donut.',\n",
       "   'The hand holding the pastry.'],\n",
       "  'bbox_target': [91.69, 307.42, 357.03, 167.19]},\n",
       " 893: {'image_emb': tensor([[ 0.1331,  0.4434, -0.2742,  ...,  1.4014,  0.2612, -0.0363],\n",
       "          [ 0.2590, -0.0947, -0.0918,  ...,  1.2109, -0.2292,  0.3618],\n",
       "          [ 0.1328,  0.1520, -0.4121,  ...,  1.4258, -0.2252,  0.0880],\n",
       "          [-0.0277,  0.0228, -0.1471,  ...,  0.3762, -0.4214, -0.1794]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0378,  0.1085, -0.0491,  ...,  0.1450, -0.0955, -0.7017],\n",
       "          [-0.0386, -0.1880, -0.1147,  ...,  0.2240, -0.0770, -0.6880]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6543, 0.0266, 0.2727, 0.0467],\n",
       "          [0.9346, 0.0021, 0.0495, 0.0140]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  156.041504    0.000000  374.885193   91.971733    0.881583     77   \n",
       "  1  134.246948  364.956024  340.204773  521.557495    0.798835     77   \n",
       "  2  366.300964    0.267639  479.869995  138.538406    0.716636     77   \n",
       "  3  433.578979  110.523949  471.989136  162.565948    0.571918     77   \n",
       "  4  429.023376  534.418579  479.886414  639.899292    0.478456     77   \n",
       "  5    0.682526  597.971985   75.487213  639.676575    0.450714     77   \n",
       "  6  440.493591  189.476608  479.881409  276.229065    0.272861     77   \n",
       "  \n",
       "           name  \n",
       "  0  teddy bear  \n",
       "  1  teddy bear  \n",
       "  2  teddy bear  \n",
       "  3  teddy bear  \n",
       "  4  teddy bear  \n",
       "  5  teddy bear  \n",
       "  6  teddy bear  ,\n",
       "  'caption': ['A white stuffed animal on top of a shelve.',\n",
       "   'white teddy bear on top of the cabinet'],\n",
       "  'bbox_target': [149.57, 1.0, 224.36, 94.92]},\n",
       " 894: {'image_emb': tensor([[ 0.3081,  0.0540, -0.1013,  ...,  0.7026,  0.1792, -0.2678],\n",
       "          [ 0.0644,  0.2002, -0.2520,  ...,  1.2607,  0.0990,  0.0137],\n",
       "          [-0.0560, -0.1824, -0.3335,  ...,  0.7466, -0.1141, -0.0598],\n",
       "          [ 0.2900, -0.0573,  0.3108,  ...,  0.6909, -0.1984,  0.2544]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3489,  0.0030, -0.3677,  ..., -0.2047, -0.0158,  0.1373],\n",
       "          [-0.0151,  0.2218, -0.1406,  ..., -0.2693,  0.0255, -0.1680]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.4319e-05, 9.9951e-01, 3.0696e-05, 3.6263e-04],\n",
       "          [4.2272e-04, 9.9658e-01, 9.2316e-04, 2.0161e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  211.545715   69.843048  379.928833  633.631104    0.935493      0   \n",
       "  1   57.758698   33.490570  244.214203  638.619019    0.931958      0   \n",
       "  2  180.247894  133.893616  204.362885  244.337463    0.747725     27   \n",
       "  3  299.495453  345.464844  311.015045  354.988464    0.493365     74   \n",
       "  4  352.092163  276.274261  425.296875  383.233185    0.423334     58   \n",
       "  5   57.488113  123.302155   78.589462  208.016083    0.378929      0   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2           tie  \n",
       "  3         clock  \n",
       "  4  potted plant  \n",
       "  5        person  ,\n",
       "  'caption': ['A happy man wearing a gray suit.', 'A tall man in a suit'],\n",
       "  'bbox_target': [57.79, 33.76, 187.81, 599.55]},\n",
       " 895: {'image_emb': tensor([[ 0.0052,  0.9390,  0.0387,  ...,  1.0234,  0.0099, -0.2024],\n",
       "          [-0.2539,  0.4236, -0.1069,  ...,  1.2021,  0.0096, -0.0400],\n",
       "          [ 0.0264,  0.6021, -0.0770,  ...,  1.1250,  0.3286,  0.1646],\n",
       "          [-0.1395,  0.2983, -0.2388,  ...,  0.9653, -0.0018,  0.0276],\n",
       "          [ 0.2391,  0.7241, -0.0141,  ...,  1.3193,  0.2058, -0.1691]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-4.0253e-02,  2.6703e-02, -4.5752e-01, -1.7126e-01, -4.7217e-01,\n",
       "           -6.8176e-02, -3.4717e-01, -6.0889e-01, -3.6831e-03,  2.7725e-02,\n",
       "            1.1652e-01, -7.7881e-02,  1.9312e-01, -4.5898e-01,  2.1021e-01,\n",
       "            8.7341e-02, -5.0659e-02, -4.1040e-01, -4.7577e-02,  4.0283e-02,\n",
       "            3.3960e-01,  2.2583e-01,  1.3855e-01, -1.7883e-01, -2.7417e-01,\n",
       "           -1.7273e-02, -7.1045e-02,  2.6733e-02,  2.8271e-01,  4.9023e-01,\n",
       "           -2.6538e-01, -3.9551e-02, -6.6846e-01,  2.1216e-01, -2.4194e-01,\n",
       "           -3.0933e-01,  1.4801e-02,  2.0630e-01,  1.9177e-01, -2.9102e-01,\n",
       "           -2.4011e-01,  1.6406e-01,  2.3224e-02, -4.1577e-01, -2.4304e-01,\n",
       "            3.0493e-01,  7.5635e-01,  1.6174e-01,  1.3390e-02,  1.5247e-01,\n",
       "           -9.9304e-02,  1.8872e-01,  3.0249e-01,  2.9004e-01,  2.6459e-02,\n",
       "            1.2421e-01, -1.5099e-02, -7.3425e-02, -1.4282e-01,  4.1772e-01,\n",
       "           -5.2094e-02, -9.1309e-02,  8.6792e-02,  1.4099e-01,  1.2280e-01,\n",
       "           -6.2073e-02,  1.5625e-01,  5.7526e-02, -1.9241e-02,  2.6709e-01,\n",
       "           -3.5973e-03, -3.1421e-01,  3.0396e-01, -2.2998e-01,  8.0811e-02,\n",
       "           -1.2122e-01,  2.5024e-01,  2.2302e-01,  2.9004e-01,  3.0103e-01,\n",
       "           -2.7515e-01, -3.9282e-01,  3.6206e-01,  5.0842e-02, -3.7842e-01,\n",
       "           -4.5166e-02,  2.6196e-01, -1.4868e-01, -3.0713e-01,  2.3572e-01,\n",
       "            4.1064e-01,  1.3818e-01, -1.1592e+00,  3.3984e-01, -6.6895e-01,\n",
       "            2.8906e-01,  1.6504e-01, -5.7373e-01,  1.0779e-01, -4.2725e-02,\n",
       "           -3.1689e-01,  6.8896e-01, -6.4697e-02, -1.9861e-01, -1.7432e-01,\n",
       "            4.3945e-01, -1.9849e-01, -5.8789e-01, -2.8711e-01, -2.2644e-01,\n",
       "           -4.7821e-02, -5.1086e-02, -6.2286e-02,  2.5317e-01, -4.3884e-02,\n",
       "           -1.7242e-02,  3.7305e-01,  1.0297e-01, -5.3369e-01,  9.2102e-02,\n",
       "            4.2206e-02, -4.5068e-01, -1.0712e-01, -9.1248e-03,  3.3740e-01,\n",
       "           -1.0321e-01,  2.8589e-01, -3.4277e-01,  3.6084e-01,  1.9263e-01,\n",
       "           -1.1957e-01, -5.2588e-01, -3.3154e-01,  4.9648e+00, -7.1838e-02,\n",
       "           -8.6487e-02, -1.0828e-01, -1.4734e-01, -2.2876e-01, -2.0386e-01,\n",
       "           -3.5919e-02,  6.1816e-01,  2.1741e-01,  4.9292e-01, -2.4280e-01,\n",
       "           -7.6233e-02,  1.8311e-01, -4.2505e-01, -1.1597e-01, -1.0862e-03,\n",
       "           -3.3984e-01, -4.8755e-01,  2.1899e-01, -2.9480e-02, -3.2837e-02,\n",
       "           -1.1517e-01,  1.1829e-01, -5.1855e-01, -1.7725e-01,  2.9126e-01,\n",
       "            1.7151e-01,  4.3518e-02, -5.4736e-01, -1.1978e-02,  1.6711e-01,\n",
       "            4.1199e-03,  5.6299e-01, -2.3145e-01,  3.1592e-01, -5.8075e-02,\n",
       "           -3.1201e-01, -3.9014e-01, -1.8738e-01,  3.3936e-02, -1.9568e-01,\n",
       "            1.1426e-01,  2.3972e-02,  2.5830e-01,  4.9469e-02, -2.0886e-01,\n",
       "           -5.2881e-01, -1.1395e-01, -1.7358e-01, -3.9111e-01, -1.2939e-01,\n",
       "           -4.7485e-01,  1.2756e-01, -3.0615e-01,  2.6294e-01, -2.7734e-01,\n",
       "            3.7384e-02,  5.5762e-01, -1.1816e-01, -4.3213e-01,  1.0284e-01,\n",
       "           -2.4841e-02, -7.4036e-02, -1.5625e-02, -3.1177e-01, -1.1969e-01,\n",
       "            3.0127e-01,  9.6252e-02,  6.4636e-02, -3.1982e-01,  6.7444e-03,\n",
       "            3.6279e-01, -1.6003e-01, -2.5415e-01, -3.7476e-01, -1.8982e-01,\n",
       "           -9.2468e-03,  4.3726e-01,  6.8909e-02,  1.2067e-01, -3.1201e-01,\n",
       "            5.9784e-02,  2.5146e-01, -4.7485e-01,  2.7686e-01, -1.4441e-01,\n",
       "            1.7065e-01, -3.8184e-01, -6.2103e-02,  5.4413e-02, -2.5464e-01,\n",
       "           -1.9562e-02,  1.9849e-01, -1.4978e-01,  1.4270e-01, -2.8516e-01,\n",
       "           -3.4302e-01,  7.9163e-02, -4.5868e-02, -1.3466e-02,  3.0762e-01,\n",
       "           -9.5032e-02, -2.8149e-01, -2.8931e-01,  5.1788e-02,  3.4595e-01,\n",
       "           -5.2100e-01, -1.7029e-01,  6.3477e-02,  2.5220e-01, -3.8013e-01,\n",
       "            5.5811e-01, -2.0337e-01,  4.6411e-01,  1.4661e-01,  1.1597e-01,\n",
       "           -4.2920e-01, -2.1985e-01, -1.9360e-01, -4.2896e-01, -1.7078e-01,\n",
       "            5.0171e-02, -2.7124e-01,  2.2095e-01, -1.0391e-02, -1.2891e-01,\n",
       "           -1.2164e-01,  2.1045e-01, -1.8689e-01,  2.0618e-01,  3.0322e-01,\n",
       "           -1.0101e-01,  2.6904e-01,  1.7151e-01,  2.7809e-03, -1.3721e-01,\n",
       "           -1.6650e-01, -6.5979e-02, -5.7312e-02, -2.3438e-01,  1.2032e-02,\n",
       "            1.3525e-01,  2.3468e-02, -5.5847e-02,  3.1787e-01,  2.1631e-01,\n",
       "            3.8965e-01, -8.4961e-02,  3.5962e-01,  2.4500e-01, -1.6516e-01,\n",
       "           -2.4463e-01,  1.4938e-02,  1.3110e-01,  4.1846e-01,  1.1505e-01,\n",
       "           -2.0581e-01,  1.8311e-01, -2.5955e-02, -3.8055e-02, -1.0059e-01,\n",
       "           -2.1863e-01, -7.1411e-03, -4.2847e-01,  4.4385e-01,  3.7292e-02,\n",
       "           -4.2603e-01,  1.6162e-01,  6.4392e-03, -6.7627e-02,  2.1606e-01,\n",
       "           -2.9712e-01, -5.3027e-01, -2.9312e-02, -1.9556e-01, -5.8563e-02,\n",
       "            2.4033e-03,  6.2549e-01,  4.9688e+00,  3.2227e-02,  4.1284e-01,\n",
       "            1.1816e-01,  3.7646e-01, -4.8584e-02,  3.4033e-01,  4.9194e-01,\n",
       "            1.4062e-01,  2.1866e-02, -1.6541e-01,  1.7319e-02, -5.0635e-01,\n",
       "           -1.5198e-01,  2.0959e-01,  5.9375e-01,  1.0974e-01, -1.8047e+00,\n",
       "            7.4219e-02,  7.3303e-02,  3.1525e-02, -2.4719e-01,  2.7390e-03,\n",
       "           -1.2012e-01,  1.1487e-01, -1.3354e-01,  7.5439e-02, -4.4434e-02,\n",
       "            7.8064e-02,  8.0139e-02,  3.9032e-02, -3.9185e-01,  3.6987e-01,\n",
       "            1.0300e-02,  8.9539e-02, -1.1285e-01,  6.0822e-02, -1.7676e-01,\n",
       "            2.0764e-01, -6.3818e-01, -1.5723e-01, -4.5624e-02, -5.0439e-01,\n",
       "           -4.3530e-01, -3.4961e-01,  2.7295e-01,  1.9238e-01,  5.1941e-02,\n",
       "           -1.3635e-01, -2.9565e-01,  1.5175e-02, -2.1973e-02, -1.7236e-01,\n",
       "            6.8176e-02,  4.4800e-01,  1.3843e-01, -3.0918e-03,  1.2683e-01,\n",
       "            5.9766e-01,  5.8655e-02, -4.4098e-02, -1.5295e-01,  1.0565e-01,\n",
       "           -1.4380e-01, -1.7603e-01, -6.9542e-03, -1.5479e-01, -1.1310e-01,\n",
       "           -2.7771e-03,  8.6609e-02, -9.3018e-02, -2.4939e-01, -5.0439e-01,\n",
       "            1.3013e-01, -8.4229e-03, -4.4861e-03, -4.4507e-01, -4.4983e-02,\n",
       "           -1.7688e-01,  3.6572e-01, -9.7961e-02, -1.3330e-01, -2.3669e-01,\n",
       "            1.2030e-01, -9.4238e-02,  9.5215e-02, -4.7510e-01,  2.1118e-01,\n",
       "            3.7842e-01,  2.1896e-02,  1.7224e-01,  3.8672e-01,  1.2158e-01,\n",
       "           -1.6870e-01, -5.4248e-01, -5.7587e-02,  5.3925e-02,  2.5562e-01,\n",
       "            1.8616e-01, -4.9048e-01, -5.5615e-01,  3.2251e-01,  1.6260e-01,\n",
       "           -4.6045e-01,  1.3159e-01, -3.1763e-01,  4.4141e-01,  3.3228e-01,\n",
       "           -2.1863e-01, -1.8835e-01,  1.4075e-01, -4.0918e-01, -4.1455e-01,\n",
       "           -1.0358e-01, -4.6875e-02,  2.6099e-01, -8.0444e-02,  5.3223e-01,\n",
       "           -1.7395e-01, -2.4811e-02, -5.2881e-01, -3.7994e-02,  1.1005e-01,\n",
       "           -1.4734e-01, -3.0859e-01,  1.2512e-01,  6.8176e-02, -3.2373e-01,\n",
       "           -2.7612e-01, -3.5327e-01, -3.5010e-01, -4.4434e-01, -2.8687e-01,\n",
       "           -2.2221e-03,  1.8982e-02,  6.0394e-02, -2.3468e-02, -1.0571e-01,\n",
       "           -3.0591e-01,  1.7480e-01, -2.0715e-01, -1.3037e-01, -4.5947e-01,\n",
       "           -2.5711e-02,  2.5586e-01, -1.1578e-01, -2.3621e-02,  1.3054e-02,\n",
       "            1.2286e-01,  2.5537e-01, -4.8584e-02, -7.6866e-04,  2.0337e-01,\n",
       "            5.5469e-01, -2.4878e-01,  2.3755e-01, -2.3804e-02, -8.5220e-03,\n",
       "            3.0518e-02,  1.8726e-01,  3.2104e-01,  3.2764e-01,  3.7727e-03,\n",
       "            2.1448e-01, -9.2651e-02,  8.0688e-02, -1.0907e-01,  5.7520e-01,\n",
       "           -6.4453e-02, -6.9385e-01,  1.7044e-02,  7.9712e-02, -5.7678e-02,\n",
       "           -1.0046e-01,  6.6956e-02, -1.8164e-01,  1.0040e-02, -1.3599e-01,\n",
       "           -2.2375e-01, -3.8037e-01,  7.3669e-02,  1.0596e+00,  4.1064e-01,\n",
       "           -1.0645e-01,  2.6587e-01,  1.1707e-01, -4.8145e-01, -4.9591e-02,\n",
       "            8.0505e-02, -2.0752e-02,  8.4610e-03, -2.2412e-01,  4.7119e-01,\n",
       "           -1.3123e-01,  6.8604e-02,  2.3145e-01,  2.6318e-01,  1.2817e-01,\n",
       "           -2.3785e-03, -9.2957e-02]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.0210e-01, 1.5991e-01, 1.6602e-02, 7.2181e-05, 4.2139e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  277.022491   70.764961  453.257172  244.034439    0.950549     63   \n",
       "  1  148.594315  250.799530  313.518860  318.860992    0.924999     66   \n",
       "  2    0.936007   41.140411  207.708008  278.406250    0.920400     63   \n",
       "  3  347.222656  245.551880  398.696716  279.606201    0.859402     64   \n",
       "  4  185.193008  196.003510  251.700928  235.116318    0.484368     67   \n",
       "  5  426.157898  160.376602  457.783356  186.320740    0.448694     67   \n",
       "  6  288.612427  169.923874  434.037170  210.466293    0.348589     66   \n",
       "  7    0.256214  261.023254   70.550690  371.651611    0.305309      0   \n",
       "  \n",
       "           name  \n",
       "  0      laptop  \n",
       "  1    keyboard  \n",
       "  2      laptop  \n",
       "  3       mouse  \n",
       "  4  cell phone  \n",
       "  5  cell phone  \n",
       "  6    keyboard  \n",
       "  7      person  ,\n",
       "  'caption': ['The keyboard on the left laptop.'],\n",
       "  'bbox_target': [6.84, 179.2, 167.96, 62.5]},\n",
       " 896: {'image_emb': tensor([[-0.0680,  0.7490, -0.0745,  ...,  1.1523,  0.2417, -0.3562],\n",
       "          [-0.1074,  0.2910, -0.2607,  ...,  0.0367,  0.5127, -0.6978]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2036,  0.0131,  0.1604,  ..., -0.3481,  0.0203,  0.0845],\n",
       "          [-0.2036,  0.0131,  0.1604,  ..., -0.3481,  0.0203,  0.0845]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6226, 0.3774],\n",
       "          [0.6226, 0.3774]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  149.283630    1.159865  261.733276  272.373474    0.805967      9   \n",
       "  1   14.006257    2.300829  145.714386  276.290955    0.681096      9   \n",
       "  2  449.094574  256.057251  479.059692  307.353546    0.599852      9   \n",
       "  \n",
       "              name  \n",
       "  0  traffic light  \n",
       "  1  traffic light  \n",
       "  2  traffic light  ,\n",
       "  'caption': ['A red light', 'a red light'],\n",
       "  'bbox_target': [153.4, 0.75, 111.5, 270.89]},\n",
       " 897: {'image_emb': tensor([[-0.2273, -0.1375,  0.1249,  ...,  1.0068, -0.1903, -0.3359],\n",
       "          [-0.0459,  0.1853, -0.3701,  ...,  1.1152,  0.0563, -0.2563],\n",
       "          [-0.2957, -0.0290,  0.0822,  ...,  1.1562, -0.1141, -0.2003],\n",
       "          [ 0.0422,  0.2815, -0.1710,  ...,  0.4712, -0.0990,  0.0731],\n",
       "          [ 0.0422,  0.2585,  0.0833,  ...,  0.3899,  0.0510,  0.0851]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0227, -0.2207, -0.0665,  ...,  0.1932, -0.2656,  0.2505],\n",
       "          [-0.1226,  0.1304, -0.1460,  ..., -0.2019, -0.0679,  0.1658]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0389e-04, 3.0655e-02, 9.6875e-01, 4.0436e-04, 3.8743e-06],\n",
       "          [4.6468e-04, 1.4458e-02, 9.8242e-01, 2.3594e-03, 5.4312e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  116.494904  133.768738  377.333466  283.215485    0.928190     15   \n",
       "  1    0.697693   37.338982  122.133240  192.270599    0.890711     62   \n",
       "  2  321.639282  116.093590  478.757202  367.374664    0.862901     56   \n",
       "  3  110.477112  269.400116  347.687012  576.369141    0.841963     28   \n",
       "  4  417.397552  355.869385  437.649048  375.834595    0.564081     49   \n",
       "  5  315.434906  157.022552  477.553955  196.813171    0.297747     60   \n",
       "  6  321.695251  114.159348  400.793152  164.734329    0.282992     56   \n",
       "  \n",
       "             name  \n",
       "  0           cat  \n",
       "  1            tv  \n",
       "  2         chair  \n",
       "  3      suitcase  \n",
       "  4        orange  \n",
       "  5  dining table  \n",
       "  6         chair  ,\n",
       "  'caption': ['A dark wooden chair tucked under a matching table.',\n",
       "   'The chair pushed into the table'],\n",
       "  'bbox_target': [322.04, 116.6, 156.84, 253.35]},\n",
       " 898: {'image_emb': tensor([[-0.2003,  0.7417, -0.2720,  ...,  1.3281, -0.2512, -0.2861],\n",
       "          [-0.0669,  0.6558,  0.1750,  ...,  0.8979, -0.1102, -0.0202],\n",
       "          [-0.0740,  0.8906,  0.0968,  ...,  1.3213, -0.1399, -0.1333],\n",
       "          [ 0.0237,  0.7544,  0.1768,  ...,  0.9829, -0.1797,  0.0158]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2642,  0.1458, -0.3018,  ..., -0.1642, -0.1970,  0.1500],\n",
       "          [ 0.0595,  0.1379, -0.2125,  ...,  0.0354, -0.2263,  0.2610]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0211, 0.4236, 0.1252, 0.4302],\n",
       "          [0.7285, 0.0589, 0.0971, 0.1153]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  159.901642  205.714478  233.334595  314.136780    0.923013     67   \n",
       "  1  198.372971    0.475616  637.001038  473.530762    0.878109      0   \n",
       "  2    1.771683  112.352386  323.144592  473.074951    0.783163      0   \n",
       "  \n",
       "           name  \n",
       "  0  cell phone  \n",
       "  1      person  \n",
       "  2      person  ,\n",
       "  'caption': ['A person in a blue shirt.',\n",
       "   \"A person's shirt that is light blue\"],\n",
       "  'bbox_target': [0.0, 109.35, 327.17, 370.65]},\n",
       " 899: {'image_emb': tensor([[-0.0426,  0.1978,  0.0776,  ...,  0.6812,  0.2644,  0.2101],\n",
       "          [-0.1868,  0.5371, -0.1598,  ...,  1.2656, -0.1106,  0.1194],\n",
       "          [ 0.5005,  0.1248, -0.5854,  ...,  0.2382,  0.3118,  0.4707],\n",
       "          [ 0.4402,  0.1125, -0.4443,  ...,  0.7905,  0.3396,  0.4421],\n",
       "          [ 0.0723,  0.0521, -0.2059,  ...,  0.3752,  0.0472,  0.2966]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2086,  0.5054, -0.2246,  ..., -0.4756, -0.1714, -0.4709],\n",
       "          [ 0.2795,  0.3865, -0.3403,  ..., -0.0139, -0.0635, -0.2175]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.6441e-04, 3.5362e-03, 1.4830e-04, 6.4790e-05, 9.9609e-01],\n",
       "          [1.4424e-04, 5.2738e-04, 5.1651e-03, 1.0109e-02, 9.8389e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0    0.000000  139.375000  204.332153  524.317993    0.942320     25  umbrella\n",
       "  1   45.634613  430.956024  359.842316  549.746460    0.909418      0    person\n",
       "  2  230.095184  232.083969  330.971222  448.629822    0.898259      0    person\n",
       "  3  320.143524  246.245331  379.536346  461.510345    0.833747      0    person,\n",
       "  'caption': ['a person laying under an umbrella',\n",
       "   'Someones legs coming out from under a beach umbrella.'],\n",
       "  'bbox_target': [53.66, 426.27, 307.26, 122.89]},\n",
       " 900: {'image_emb': tensor([[-0.0694,  0.5781, -0.2788,  ...,  1.2070,  0.1326,  0.2052],\n",
       "          [ 0.0977,  0.5537, -0.5742,  ...,  1.0723, -0.1552,  0.0091],\n",
       "          [-0.0956,  0.3545, -0.6353,  ...,  1.0342,  0.3013,  0.0301],\n",
       "          ...,\n",
       "          [-0.1340,  0.4619, -0.2793,  ...,  1.4043, -0.5078,  0.2418],\n",
       "          [ 0.2683,  0.6255, -0.5898,  ...,  1.3096, -0.0577,  0.2124],\n",
       "          [ 0.0536,  0.6646, -0.5156,  ...,  0.4370,  0.2539,  0.3645]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0950, -0.1158, -0.3250,  ..., -0.0316, -0.0056, -0.2661],\n",
       "          [ 0.1519,  0.3137, -0.3066,  ..., -0.1241,  0.0729,  0.0577]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1150, 0.0097, 0.0230, 0.1866, 0.2673, 0.0329, 0.1699, 0.1956],\n",
       "          [0.0671, 0.0066, 0.6777, 0.0931, 0.0280, 0.0602, 0.0032, 0.0640]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   207.340485   64.269028  396.125885  413.173035    0.951233      0   \n",
       "  1   396.564209  102.753922  546.441406  424.738037    0.913844      0   \n",
       "  2    95.476776   62.270538  209.638702  422.343140    0.902048      0   \n",
       "  3     5.598877   66.481812  636.661804  251.198853    0.813574      6   \n",
       "  4     0.000000  320.889832  125.088013  425.524109    0.801153     28   \n",
       "  5   266.888367  126.408325  337.109497  183.496155    0.740663     24   \n",
       "  6   508.854187  158.903381  592.399231  305.373840    0.728539     24   \n",
       "  7    62.767616  140.553177   82.047371  179.675812    0.642868      0   \n",
       "  8    62.294426  170.539856  103.951103  280.915527    0.523499     24   \n",
       "  9   410.938416  237.578094  481.684204  305.331573    0.516202     26   \n",
       "  10  413.360870  149.737915  480.659210  307.577881    0.460916     26   \n",
       "  11  204.318298   58.841492  397.876404  403.755432    0.424035      6   \n",
       "  12  551.788818  171.942261  572.127808  194.129456    0.419769      0   \n",
       "  13   62.159348  135.452866  152.435669  279.969299    0.398296     24   \n",
       "  14  513.957886  156.778564  593.702637  307.510742    0.273501     26   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2     person  \n",
       "  3      train  \n",
       "  4   suitcase  \n",
       "  5   backpack  \n",
       "  6   backpack  \n",
       "  7     person  \n",
       "  8   backpack  \n",
       "  9    handbag  \n",
       "  10   handbag  \n",
       "  11     train  \n",
       "  12    person  \n",
       "  13  backpack  \n",
       "  14   handbag  ,\n",
       "  'caption': ['Man wearing cap is pulling the trolly bag.',\n",
       "   'The man wearing the black shirt.'],\n",
       "  'bbox_target': [93.5, 64.1, 111.82, 359.56]},\n",
       " 901: {'image_emb': tensor([[-0.1440,  0.0564, -0.0679,  ...,  0.6069,  0.2776, -0.0904],\n",
       "          [-0.0665,  0.3125,  0.3088,  ...,  1.0234,  0.4858,  0.0923],\n",
       "          [ 0.1418,  0.2152, -0.0931,  ...,  0.8564,  0.0073, -0.1974],\n",
       "          ...,\n",
       "          [ 0.1243, -0.0651, -0.2678,  ...,  0.8569,  0.3376, -0.2671],\n",
       "          [ 0.3894,  0.2947, -0.0748,  ...,  0.6240, -0.1311,  0.1150],\n",
       "          [-0.0252,  0.3699, -0.0954,  ...,  0.4607,  0.2042, -0.0406]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0927, -0.2100, -0.3604,  ..., -0.2307,  0.0695,  0.1180],\n",
       "          [-0.1342, -0.2559, -0.4529,  ..., -0.0193,  0.0180,  0.0963]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.0332e-03, 1.8940e-03, 3.1719e-03, 4.6420e-04, 4.4464e-02, 4.8935e-05,\n",
       "           4.7607e-03, 9.3604e-01],\n",
       "          [1.9653e-02, 1.9760e-03, 6.8970e-03, 1.4238e-03, 2.1576e-02, 2.8038e-04,\n",
       "           1.4238e-03, 9.4678e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   331.120972  138.245300  400.150635  257.006744    0.884990     56   \n",
       "  1   109.449860  191.107895  238.527817  322.408569    0.880505     56   \n",
       "  2   178.849625  140.049271  241.132568  185.818573    0.836756     63   \n",
       "  3   281.599457  127.405655  349.319733  168.280884    0.786547     63   \n",
       "  4   235.378021   82.788048  329.134979  256.964111    0.765694      0   \n",
       "  5   451.507507    0.848877  492.750397  117.167488    0.760191     39   \n",
       "  6   124.323288  104.543350  187.955322  202.341705    0.745492      0   \n",
       "  7   435.628113   11.747562  458.587372  113.925705    0.642372     39   \n",
       "  8   412.755981  145.774719  438.533600  244.623444    0.487119     39   \n",
       "  9   274.620575  157.933044  356.251282  271.359741    0.484756     60   \n",
       "  10  155.405182  175.087875  250.353058  222.575623    0.362587     60   \n",
       "  \n",
       "              name  \n",
       "  0          chair  \n",
       "  1          chair  \n",
       "  2         laptop  \n",
       "  3         laptop  \n",
       "  4         person  \n",
       "  5         bottle  \n",
       "  6         person  \n",
       "  7         bottle  \n",
       "  8         bottle  \n",
       "  9   dining table  \n",
       "  10  dining table  ,\n",
       "  'caption': ['The empty chair nearest to the woman in the white t-shirt.',\n",
       "   'the empty chair beside the girl wearing white t-shirt'],\n",
       "  'bbox_target': [116.29, 196.09, 119.67, 119.66]},\n",
       " 902: {'image_emb': tensor([[ 0.2935,  0.4822,  0.1317,  ...,  0.8115,  0.1163, -0.3760],\n",
       "          [ 0.2092,  0.5894, -0.1589,  ...,  1.1006,  0.0454, -0.2308],\n",
       "          [ 0.0389,  0.3911, -0.2791,  ...,  0.6611, -0.1510,  0.1149],\n",
       "          [ 0.2620,  0.4226, -0.2241,  ...,  0.9341,  0.3308,  0.0522],\n",
       "          [ 0.4443,  0.2861, -0.0735,  ...,  0.5078, -0.0188, -0.2749]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.4314, -0.0879, -0.4441,  ...,  0.4377, -0.0234, -0.3669],\n",
       "          [ 0.2502,  0.1460, -0.2700,  ...,  0.5884,  0.0320, -0.0575]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.1182e-03, 5.8685e-02, 1.5793e-02, 5.4884e-04, 9.1797e-01],\n",
       "          [4.8676e-02, 9.1846e-01, 6.6280e-04, 7.0930e-05, 3.1921e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class name\n",
       "  0    9.235016  100.275681  425.412109  369.204834    0.956208     16  dog\n",
       "  1  399.670746   40.570236  532.801208  164.059845    0.912726     16  dog\n",
       "  2  102.131622   41.691498  200.729309  124.714905    0.866760     16  dog\n",
       "  3  209.642227   28.053772  280.983551   94.526642    0.836565     16  dog,\n",
       "  'caption': [\"A black and brown dog with it's tongue out near 4 other dogs.\",\n",
       "   'German shepard in water.'],\n",
       "  'bbox_target': [397.73, 44.3, 137.6, 117.81]},\n",
       " 903: {'image_emb': tensor([[-0.3643,  0.0359, -0.4773,  ...,  0.8955,  0.1093,  0.4451],\n",
       "          [-0.3469,  0.1168, -0.5938,  ...,  0.8286, -0.0503,  0.4272],\n",
       "          [-0.2262,  0.3203, -0.3867,  ...,  1.1357,  0.1316,  0.2585]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0054,  0.0037, -0.3184,  ...,  0.3459, -0.1470,  0.0554],\n",
       "          [ 0.2391, -0.3157, -0.4773,  ...,  0.0054,  0.1022, -0.0062]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.7988, 0.1755, 0.0257],\n",
       "          [0.5781, 0.1486, 0.2732]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  321.348419    0.000000  637.202026  215.232544    0.911443      0  person\n",
       "  1    0.571884    0.000000  317.685852  214.351837    0.904280      0  person\n",
       "  2  495.262543  119.886566  589.022339  215.031708    0.539750     15     cat\n",
       "  3  174.934708  120.489441  268.110901  215.125122    0.514221     15     cat\n",
       "  4   43.750168  119.103027  299.520264  216.099731    0.251030     15     cat,\n",
       "  'caption': [\"A black cat in a man's lap.\", 'The black cat on the left.'],\n",
       "  'bbox_target': [171.5, 120.0, 93.0, 96.5]},\n",
       " 904: {'image_emb': tensor([[-2.0056e-01,  8.3008e-02, -4.9683e-02,  ...,  9.1992e-01,\n",
       "            3.7079e-02, -2.7808e-01],\n",
       "          [-6.0608e-02,  3.2227e-02, -2.9541e-01,  ...,  1.2744e+00,\n",
       "            9.7900e-02,  1.3708e-01],\n",
       "          [-2.8418e-01, -3.7155e-03, -1.9568e-01,  ...,  8.8623e-01,\n",
       "            1.1978e-03, -3.9746e-01],\n",
       "          [-2.0337e-01,  1.3351e-02,  7.0618e-02,  ...,  7.1289e-01,\n",
       "            3.7201e-02, -2.4805e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0640,  0.0962, -0.3586,  ..., -0.1332,  0.0498, -0.3018],\n",
       "          [-0.0418, -0.2148,  0.0187,  ...,  0.1943,  0.2310, -0.0037]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6255, 0.0639, 0.2230, 0.0873],\n",
       "          [0.2209, 0.0007, 0.6006, 0.1776]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  170.158630  172.740479  429.885864  401.330322    0.938411     17   horse\n",
       "  1  148.984039  163.086395  196.219452  237.083588    0.787091      0  person\n",
       "  2  319.988770  188.694427  521.913635  382.944550    0.750730     17   horse\n",
       "  3  137.282700  174.237793  158.520645  204.294128    0.325172      0  person,\n",
       "  'caption': ['The horse directly in front of the man wearing the hat.',\n",
       "   'horse closest to camera pulling wagon'],\n",
       "  'bbox_target': [177.46, 177.59, 253.0, 221.15]},\n",
       " 905: {'image_emb': tensor([[ 0.5059, -0.1108,  0.1460,  ...,  0.3059,  0.1150,  0.0344],\n",
       "          [-0.1627, -0.1531, -0.2222,  ...,  0.5752,  0.0363,  0.1978],\n",
       "          [ 0.1510,  0.0695, -0.3613,  ...,  0.5962,  0.0353,  0.0822],\n",
       "          ...,\n",
       "          [ 0.2708, -0.3435, -0.5513,  ...,  1.0088, -0.1191, -0.1416],\n",
       "          [ 0.1852, -0.4026, -0.5376,  ...,  1.1797, -0.0142, -0.2325],\n",
       "          [-0.1099,  0.0439, -0.1141,  ...,  0.5952, -0.2078,  0.2683]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0488,  0.1217, -0.3621,  ...,  0.0504, -0.5059, -0.3831],\n",
       "          [-0.0400,  0.1675, -0.2063,  ...,  0.3853, -0.3726, -0.7451]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.3779e-01, 1.7517e-01, 1.5497e-06, 5.7554e-04, 6.5184e-04, 4.1437e-04,\n",
       "           8.5388e-02],\n",
       "          [7.1777e-01, 9.2316e-04, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           2.8125e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    22.052185   83.545303  284.481812  378.236755    0.955460      0   \n",
       "  1   380.521118   37.492279  639.205078  355.948212    0.953180      0   \n",
       "  2   465.920715  357.140106  552.346191  396.050690    0.844373     53   \n",
       "  3     0.970673  341.957489  637.356079  424.772766    0.801507     60   \n",
       "  4   372.670624  134.703583  385.927582  145.354401    0.751780      2   \n",
       "  5   348.262207  134.354034  363.093506  145.685516    0.732002      2   \n",
       "  6   290.003876  131.276062  305.810577  144.420837    0.692532      2   \n",
       "  7   312.242096  133.380310  328.389923  144.833527    0.575493      2   \n",
       "  8   392.281006  132.864059  412.876587  145.729935    0.570413      2   \n",
       "  9   224.285049  128.851273  240.466385  140.194229    0.537857      2   \n",
       "  10  333.093597  132.717834  347.957245  145.350464    0.487789      2   \n",
       "  11  441.774597  140.287216  460.953369  153.943939    0.485312      2   \n",
       "  12  404.885437  133.963928  419.579163  145.717377    0.386756      2   \n",
       "  13   35.731884  157.411835   98.915894  201.350861    0.382693     53   \n",
       "  14   35.713570  157.427460   99.004341  201.640900    0.265761     48   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2          pizza  \n",
       "  3   dining table  \n",
       "  4            car  \n",
       "  5            car  \n",
       "  6            car  \n",
       "  7            car  \n",
       "  8            car  \n",
       "  9            car  \n",
       "  10           car  \n",
       "  11           car  \n",
       "  12           car  \n",
       "  13         pizza  \n",
       "  14      sandwich  ,\n",
       "  'caption': ['A boy in a blue ball cap',\n",
       "   'A child with a baseball hat eating a slice of pizza next to a child in a hoodie.'],\n",
       "  'bbox_target': [23.08, 82.78, 258.73, 287.58]},\n",
       " 906: {'image_emb': tensor([[ 0.0863,  0.4246,  0.0428,  ...,  0.3960,  0.6890, -0.0263],\n",
       "          [-0.4258,  0.4924,  0.1611,  ...,  0.2739,  0.3293, -0.0341],\n",
       "          [-0.3101,  0.5938, -0.0293,  ...,  0.4626,  0.5015, -0.1149],\n",
       "          ...,\n",
       "          [-0.1533,  0.7598, -0.2214,  ...,  0.5713,  0.3684, -0.2727],\n",
       "          [-0.7451,  0.2705,  0.0329,  ...,  0.3589,  0.5015,  0.1042],\n",
       "          [-0.2053,  0.1237,  0.1132,  ..., -0.1807, -0.2242,  0.2141]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1068, -0.0672, -0.1740,  ...,  0.1283,  0.0455, -0.0738],\n",
       "          [ 0.0985, -0.1215, -0.1217,  ..., -0.0789, -0.0947, -0.1372]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.7902e-03, 2.2263e-02, 1.9455e-03, 1.9043e-02, 1.2610e-01, 1.9958e-02,\n",
       "           5.2872e-03, 1.2369e-03, 7.9736e-01],\n",
       "          [6.6185e-03, 1.1986e-02, 2.5520e-03, 1.2558e-02, 6.5796e-02, 1.5388e-02,\n",
       "           3.9520e-03, 7.3099e-04, 8.8037e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  438.316803   23.681213  565.456543  309.785583    0.913412      0  person\n",
       "  1  390.793274   88.850937  580.654297  451.265137    0.912842     17   horse\n",
       "  2  320.755493   53.363449  419.173767  303.609985    0.876252      0  person\n",
       "  3  229.682327  111.322693  415.706543  472.256592    0.866682     17   horse\n",
       "  4  160.218582  120.808167  304.046692  450.818481    0.865116     17   horse\n",
       "  5   44.215958  114.421936  214.844223  446.708679    0.832452     17   horse\n",
       "  6  101.235985   73.019409  202.346375  327.888000    0.826081      0  person\n",
       "  7  227.496506   60.684128  313.635803  194.057983    0.748500      0  person,\n",
       "  'caption': ['second horse to the left', 'Second horse from the left'],\n",
       "  'bbox_target': [159.51, 126.48, 128.4, 315.71]},\n",
       " 907: {'image_emb': tensor([[ 0.1689,  0.3372, -0.2046,  ...,  0.9272,  0.2058,  0.1770],\n",
       "          [ 0.0685,  0.6558, -0.2676,  ...,  1.0332,  0.2183,  0.2489],\n",
       "          [-0.0851,  0.0079, -0.2394,  ...,  1.0713,  0.2910,  0.1477],\n",
       "          [ 0.1121,  0.2098, -0.1985,  ...,  0.9980, -0.0790,  0.2610]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0623,  0.2725, -0.1974,  ..., -0.3103, -0.3440, -0.1899],\n",
       "          [ 0.0529, -0.0522, -0.0216,  ...,  0.3079, -0.2527, -0.1229]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.6348, 0.0505, 0.2448, 0.0701],\n",
       "          [0.7031, 0.0908, 0.0139, 0.1923]], dtype=torch.float16),\n",
       "  'df_boxes':        xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  0.529022  127.366028  424.709961  446.138977    0.921750     28  suitcase\n",
       "  1  0.359360    0.000000  425.277344  169.778259    0.913005     28  suitcase\n",
       "  2  0.961472  380.358276  424.008118  638.281921    0.862751     28  suitcase,\n",
       "  'caption': ['The trunk in the middle.',\n",
       "   'An older style suitcase that is inbetween two other ones.'],\n",
       "  'bbox_target': [0.0, 121.8, 425.23, 328.65]},\n",
       " 908: {'image_emb': tensor([[-0.2179,  0.3586,  0.0467,  ...,  0.6748,  0.0640, -0.0591],\n",
       "          [-0.3459,  0.1201, -0.2104,  ...,  1.1250,  0.1873,  0.1804],\n",
       "          [ 0.1709,  0.2006, -0.0835,  ...,  1.0596,  0.3672, -0.3674],\n",
       "          [ 0.5317,  0.0797, -0.1683,  ...,  0.1443,  0.2249, -0.0659]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2294, -0.0442, -0.1587,  ..., -0.1027,  0.3662, -0.0472],\n",
       "          [ 0.0261, -0.2957, -0.1575,  ..., -0.0455,  0.2422, -0.3992]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.6562e-01, 4.4739e-02, 2.1805e-02, 6.6797e-01],\n",
       "          [4.5166e-02, 4.0092e-03, 2.1923e-04, 9.5068e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  327.044983  187.597137  433.411804  417.209259    0.934416      0  person\n",
       "  1  188.264572  293.908600  269.542114  509.285919    0.929567      0  person\n",
       "  2  270.617798    7.788555  491.825256  105.865829    0.862897     33    kite,\n",
       "  'caption': ['One boy wearing red shirt and pyzama is seeing the flag with his friend',\n",
       "   'A little boy in red playing with a kite.'],\n",
       "  'bbox_target': [321.23, 192.94, 104.15, 230.98]},\n",
       " 909: {'image_emb': tensor([[-3.4180e-02,  6.0596e-01, -1.3354e-01,  ...,  1.0342e+00,\n",
       "            2.9932e-01, -3.4424e-01],\n",
       "          [ 1.8311e-01,  7.5049e-01, -3.7793e-01,  ...,  9.4141e-01,\n",
       "            1.8774e-01, -4.8218e-01],\n",
       "          [-4.3518e-02,  5.8545e-01, -3.6743e-01,  ...,  9.2529e-01,\n",
       "            1.6589e-01, -3.6060e-01],\n",
       "          [ 1.5106e-02,  5.7812e-01, -3.2959e-01,  ...,  7.5781e-01,\n",
       "            5.6982e-01,  1.3855e-01],\n",
       "          [-1.1520e-02,  5.4590e-01, -4.9988e-02,  ...,  5.8398e-01,\n",
       "            1.2842e-01, -2.4414e-04]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0280, -0.0635, -0.3044,  ..., -0.2305, -0.0820,  0.1392],\n",
       "          [ 0.0569,  0.1097, -0.1740,  ...,  0.1267, -0.1587, -0.1901]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0216, 0.1429, 0.7847, 0.0264, 0.0244],\n",
       "          [0.0009, 0.1376, 0.6875, 0.1608, 0.0130]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  482.875824  230.930389  639.764771  407.660095    0.930789     39   \n",
       "  1  395.658173  185.126221  547.513062  414.879395    0.901826     77   \n",
       "  2  357.651611  263.660767  450.371094  402.441528    0.823080     77   \n",
       "  3  305.583618  147.300476  407.590393  370.835754    0.716558     77   \n",
       "  \n",
       "           name  \n",
       "  0      bottle  \n",
       "  1  teddy bear  \n",
       "  2  teddy bear  \n",
       "  3  teddy bear  ,\n",
       "  'caption': ['The rock bear in the back.', 'tallest stone bear'],\n",
       "  'bbox_target': [311.07, 148.55, 104.77, 231.58]},\n",
       " 910: {'image_emb': tensor([[-0.1356,  0.5571, -0.4028,  ...,  0.9360,  0.0449,  0.1517],\n",
       "          [-0.6025,  0.0523, -0.2041,  ...,  0.8604,  0.0790,  0.0733],\n",
       "          [-0.6147,  0.3416, -0.0209,  ...,  1.1826, -0.5601, -0.1445],\n",
       "          [-0.3616,  0.1449,  0.1183,  ...,  0.7520, -0.2296,  0.0048],\n",
       "          [-0.3691,  0.3640, -0.3486,  ...,  0.8911, -0.2881, -0.1329]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0341,  0.3770, -0.2898,  ..., -0.1683,  0.1172,  0.3367],\n",
       "          [ 0.2407,  0.2947, -0.2213,  ..., -0.0020,  0.0271, -0.3042]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.4219e-01, 6.6846e-01, 2.0742e-04, 1.5497e-05, 8.9050e-02],\n",
       "          [9.8535e-01, 1.2604e-02, 3.6883e-04, 3.8862e-05, 1.4362e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.000000   12.801056  122.955978  318.471710    0.920469      0   \n",
       "  1  273.476288   10.795807  631.024902  419.216064    0.904556      0   \n",
       "  2   63.273415  265.447083  285.175201  412.229858    0.895472     55   \n",
       "  3  102.116547  122.038651  228.826782  262.588257    0.859988     55   \n",
       "  4  356.719543    0.150612  500.043701  135.213470    0.611310      0   \n",
       "  5  196.175385  175.394897  289.838623  284.955261    0.529334     77   \n",
       "  6    1.329529  302.324982  481.190460  423.443176    0.393233     60   \n",
       "  7   56.164749   60.582413   89.412704  105.131882    0.388165      0   \n",
       "  8    0.967339   61.199860   89.188934  125.706757    0.289047      0   \n",
       "  9    0.560017  367.857574   42.658688  423.229492    0.250320     45   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2          cake  \n",
       "  3          cake  \n",
       "  4        person  \n",
       "  5    teddy bear  \n",
       "  6  dining table  \n",
       "  7        person  \n",
       "  8        person  \n",
       "  9          bowl  ,\n",
       "  'caption': ['Young woman wearing jeans and a striped shortsleeve shirt.',\n",
       "   'The woman in the red, white and black shirt'],\n",
       "  'bbox_target': [1.91, 18.19, 127.31, 302.47]},\n",
       " 911: {'image_emb': tensor([[ 0.0108,  0.1293, -0.1660,  ...,  0.9482, -0.2104,  0.4736],\n",
       "          [-0.3142,  0.2742, -0.0262,  ...,  0.9033, -0.0581,  0.0662],\n",
       "          [ 0.1050,  0.0919, -0.2502,  ...,  0.6660, -0.0619,  0.0992],\n",
       "          ...,\n",
       "          [-0.3005,  0.3950, -0.2991,  ...,  0.9766,  0.0020,  0.2312],\n",
       "          [-0.2832,  0.4885, -0.1145,  ...,  1.1797, -0.1497,  0.0441],\n",
       "          [-0.1925,  0.1174, -0.2227,  ...,  0.8091,  0.0180, -0.2244]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2465, -0.0913, -0.1956,  ...,  0.2966,  0.2874,  0.2181],\n",
       "          [-0.0561,  0.0028, -0.3064,  ..., -0.2500, -0.1438,  0.0180]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.4643e-03, 9.9414e-01, 1.4007e-05, 9.3555e-04, 7.5150e-04, 1.1349e-04,\n",
       "           9.2983e-06, 2.1875e-04, 1.7571e-04, 5.3310e-04, 1.6630e-05, 1.5271e-04,\n",
       "           3.3879e-04],\n",
       "          [6.8665e-03, 9.8779e-01, 6.5267e-05, 1.0052e-03, 8.7309e-04, 1.2004e-04,\n",
       "           3.2783e-05, 1.7633e-03, 1.1277e-04, 4.7469e-04, 1.5175e-04, 2.8348e-04,\n",
       "           3.8743e-04]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   302.307220  338.587158  365.285858  439.551392    0.927491     41   \n",
       "  1   121.493744   18.403839  324.335480  322.534149    0.915525      0   \n",
       "  2     0.524307  236.765427  476.820496  638.253845    0.898479     60   \n",
       "  3    30.153793  299.635986   70.939926  364.269104    0.886691     41   \n",
       "  4   241.624969  300.273193  300.815155  407.364502    0.865597     40   \n",
       "  5   378.217316  418.575500  459.608398  485.199707    0.838767     44   \n",
       "  6    86.444672  513.052856  203.826996  603.198853    0.829788     44   \n",
       "  7   355.792145  425.604156  477.749878  514.812622    0.820254     43   \n",
       "  8    60.410004  495.566895  205.351135  589.340820    0.813022     43   \n",
       "  9     0.000000  279.052429   41.980209  315.007080    0.769123     47   \n",
       "  10    0.921753  386.165070   64.072678  436.170258    0.764334     42   \n",
       "  11  258.420197  541.622864  382.418732  627.717102    0.732515     42   \n",
       "  12  252.269684   56.432434  478.582886  372.380859    0.691582     57   \n",
       "  13  157.311432  286.674866  239.213837  323.666565    0.687657     43   \n",
       "  14    0.192169  306.171753   20.553818  366.152527    0.624716     41   \n",
       "  15   27.377808  262.732361   72.413055  300.702881    0.436645     47   \n",
       "  16  354.468140  427.974182  477.781250  511.924438    0.424551     44   \n",
       "  17  118.090805  279.317932  206.655212  303.213867    0.385155     42   \n",
       "  18  260.487488  544.949402  381.811157  626.238464    0.343838     43   \n",
       "  \n",
       "              name  \n",
       "  0            cup  \n",
       "  1         person  \n",
       "  2   dining table  \n",
       "  3            cup  \n",
       "  4     wine glass  \n",
       "  5          spoon  \n",
       "  6          spoon  \n",
       "  7          knife  \n",
       "  8          knife  \n",
       "  9          apple  \n",
       "  10          fork  \n",
       "  11          fork  \n",
       "  12         couch  \n",
       "  13         knife  \n",
       "  14           cup  \n",
       "  15         apple  \n",
       "  16         spoon  \n",
       "  17          fork  \n",
       "  18         knife  ,\n",
       "  'caption': ['Girl in a pink robe.', 'the girl dressed in red'],\n",
       "  'bbox_target': [122.71, 18.86, 204.17, 313.47]},\n",
       " 912: {'image_emb': tensor([[-0.0193,  0.1129, -0.0178,  ...,  1.0898, -0.1118,  0.0323],\n",
       "          [-0.2067, -0.0203, -0.1730,  ...,  0.9854,  0.1582, -0.1963],\n",
       "          [-0.2285,  0.2218,  0.0030,  ...,  1.2334,  0.3000, -0.0715],\n",
       "          ...,\n",
       "          [-0.2233,  0.1602, -0.2695,  ...,  1.0908,  0.1443, -0.1104],\n",
       "          [-0.1465,  0.0358, -0.2964,  ...,  1.2305,  0.1328, -0.0236],\n",
       "          [ 0.3057,  0.1578, -0.1104,  ...,  0.5776, -0.1400,  0.0787]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1030, -0.0796,  0.3733,  ..., -0.0089, -0.2239,  0.1255],\n",
       "          [-0.1115, -0.2441,  0.1758,  ...,  0.2996, -0.2620,  0.0371]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.0817e-04, 6.6528e-02, 6.7651e-05, 9.3262e-01, 3.5107e-05, 3.2783e-04,\n",
       "           3.6955e-06],\n",
       "          [2.1422e-04, 1.4026e-01, 7.2122e-06, 8.5938e-01, 2.1219e-05, 1.7488e-04,\n",
       "           2.1458e-06]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   88.178001   66.179001  298.719360  283.397949    0.909331     15     cat\n",
       "  1  408.976318    2.213961  499.575958  192.146774    0.896121     56   chair\n",
       "  2  422.885651  178.113083  499.124207  234.530350    0.870773     64   mouse\n",
       "  3  369.373596    0.613058  445.201294  142.936279    0.849340     56   chair\n",
       "  4  192.216125  124.249672  421.993927  263.607941    0.832286     15     cat\n",
       "  5    0.157054   66.887543   48.216682  187.504929    0.723960     73    book\n",
       "  6   75.630043    0.829422  478.904297  367.544891    0.519170     63  laptop\n",
       "  7    0.516513  214.618896   92.592491  372.331299    0.377611     73    book,\n",
       "  'caption': ['A wooden color chair', 'brown wooden chair.'],\n",
       "  'bbox_target': [411.23, 0.0, 88.4, 195.86]},\n",
       " 913: {'image_emb': tensor([[-0.0196, -0.0779,  0.1858,  ...,  0.3328,  0.1527,  0.2375],\n",
       "          [ 0.2603,  0.3569, -0.3044,  ...,  1.2910,  0.0543,  0.0529],\n",
       "          [ 0.1732,  0.1964, -0.1599,  ...,  1.2461,  0.1992, -0.1067],\n",
       "          ...,\n",
       "          [-0.1982,  0.2125, -0.2379,  ...,  1.0908, -0.4102, -0.1191],\n",
       "          [-0.3425,  0.1534, -0.2969,  ...,  1.2754, -0.0340, -0.1278],\n",
       "          [ 0.2568,  0.0536, -0.3928,  ...,  0.7261, -0.1076, -0.0302]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0174,  0.0367, -0.3530,  ..., -0.1523,  0.1147, -0.0368],\n",
       "          [ 0.4092,  0.0656, -0.6890,  ..., -0.0624, -0.0160,  0.1373]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.1100e-03, 8.2715e-01, 1.0681e-01, 1.9014e-05, 2.0180e-03, 1.6470e-03,\n",
       "           3.5167e-06, 6.2883e-05, 2.3246e-06, 1.6632e-02, 6.6566e-04, 4.7445e-05,\n",
       "           2.0828e-03, 9.4833e-03, 1.5569e-04, 3.4750e-05, 2.4979e-02, 8.5950e-05],\n",
       "          [1.5557e-05, 4.8351e-04, 3.9642e-02, 1.2085e-02, 3.9864e-03, 5.1451e-04,\n",
       "           2.8833e-01, 5.0873e-02, 1.4809e-02, 1.7691e-03, 6.7787e-03, 5.7324e-01,\n",
       "           5.9223e-04, 1.2696e-05, 9.7656e-04, 1.6613e-03, 3.1528e-03, 1.0233e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   111.854919   11.449631  324.652344  331.943237    0.932740      0   \n",
       "  1   244.805847    0.573074  382.886658  285.954468    0.926316      0   \n",
       "  2     0.023735    0.719872  148.268799  196.528198    0.922994      0   \n",
       "  3     1.974655  241.438797  409.361145  637.194031    0.898811     60   \n",
       "  4   321.633728  235.462112  480.000000  639.157593    0.897302      0   \n",
       "  5    42.410225  377.039612  106.027374  471.485657    0.877006     41   \n",
       "  6   184.228027  436.675446  250.058777  504.037872    0.859921     54   \n",
       "  7   242.068695  419.254578  313.136993  485.335632    0.857601     54   \n",
       "  8   210.300629  342.329773  280.778839  397.545959    0.843484     54   \n",
       "  9   100.105103  289.209778  145.789520  366.610352    0.827478     41   \n",
       "  10  202.685974  394.142151  272.878052  440.502625    0.811990     54   \n",
       "  11  153.439972  408.972321  206.997955  468.411652    0.801392     54   \n",
       "  12    0.192818  310.169342   68.547943  350.393219    0.798676     44   \n",
       "  13   32.378204  236.270020   92.236389  332.360840    0.791584     41   \n",
       "  14  265.028656  372.978546  322.432465  419.550934    0.770172     54   \n",
       "  15    0.000000  340.123108   55.106125  401.817139    0.753426     45   \n",
       "  16    0.007240  178.489746   53.869446  255.474060    0.700468     56   \n",
       "  17  287.040009  430.212219  346.057404  488.160461    0.682959     54   \n",
       "  18  164.240204  354.579132  218.003754  415.623199    0.671133     54   \n",
       "  19  296.421021  386.121155  342.376099  436.274719    0.640892     54   \n",
       "  20    0.011215  266.891571   34.374130  297.679169    0.378488     44   \n",
       "  21  145.182724    0.000000  175.622757   52.807884    0.287829     39   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3   dining table  \n",
       "  4         person  \n",
       "  5            cup  \n",
       "  6          donut  \n",
       "  7          donut  \n",
       "  8          donut  \n",
       "  9            cup  \n",
       "  10         donut  \n",
       "  11         donut  \n",
       "  12         spoon  \n",
       "  13           cup  \n",
       "  14         donut  \n",
       "  15          bowl  \n",
       "  16         chair  \n",
       "  17         donut  \n",
       "  18         donut  \n",
       "  19         donut  \n",
       "  20         spoon  \n",
       "  21        bottle  ,\n",
       "  'caption': ['A person with neon yellow wristband and wearing black shorts.',\n",
       "   \"A person's hand with lots of wristbands reaching for a donut.\"],\n",
       "  'bbox_target': [323.6, 238.74, 156.4, 401.26]},\n",
       " 914: {'image_emb': tensor([[-0.3975,  0.4395,  0.1766,  ...,  1.1758,  0.0992,  0.2072],\n",
       "          [-0.0848,  0.4045,  0.0027,  ...,  0.6748,  0.0629, -0.0378],\n",
       "          [-0.3010,  0.5420, -0.0363,  ...,  0.8242, -0.0559,  0.1037]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-8.6731e-02, -6.3416e-02, -3.6987e-01, -2.2681e-01, -3.8281e-01,\n",
       "           -1.7273e-01, -3.9917e-01, -1.2588e+00, -5.8899e-02,  1.9690e-01,\n",
       "            2.7695e-02,  7.9468e-02, -4.3799e-01, -2.6904e-01, -1.5247e-01,\n",
       "           -2.9922e-02,  2.6294e-01, -2.1716e-01, -2.5049e-01,  2.0325e-01,\n",
       "            4.8187e-02,  2.7490e-01,  2.0081e-01,  9.1614e-02, -7.4646e-02,\n",
       "            1.6541e-01,  7.9224e-02,  2.0068e-01, -1.8921e-01,  2.6782e-01,\n",
       "           -2.8214e-02, -2.0874e-02, -3.0225e-01,  1.2854e-01, -2.2534e-01,\n",
       "           -7.3364e-02,  2.1838e-01, -1.6797e-01,  2.9028e-01,  9.4543e-02,\n",
       "           -1.8585e-02,  1.5869e-01,  1.0254e-01,  1.9821e-02, -7.0312e-02,\n",
       "           -1.2000e-01,  5.3223e-02,  1.6162e-01, -2.2241e-01,  2.6138e-02,\n",
       "            1.6943e-01, -4.4727e-01,  7.8278e-03, -2.7197e-01, -5.5878e-02,\n",
       "           -4.2694e-02, -4.3610e-02, -4.7607e-02,  3.1030e-01,  8.3679e-02,\n",
       "           -1.5039e-01, -5.1636e-02,  7.5623e-02, -9.6985e-02,  2.6343e-01,\n",
       "           -2.3633e-01, -1.6931e-01,  3.1714e-01, -1.9568e-01, -7.5798e-03,\n",
       "            2.7197e-01, -1.4870e-02,  1.2988e-01,  2.0386e-01,  2.4561e-01,\n",
       "           -4.2725e-02, -2.5830e-01, -9.6588e-03,  3.6652e-02, -1.8469e-01,\n",
       "           -8.9905e-02,  2.1570e-01,  3.0322e-01,  2.7637e-01, -7.2144e-02,\n",
       "           -3.0933e-01,  6.3801e-04, -6.1981e-02,  5.0684e-01, -1.2268e-01,\n",
       "            4.9225e-02,  3.2104e-01, -1.7637e+00,  4.8071e-01, -2.8687e-01,\n",
       "           -3.2379e-02,  2.2717e-01, -1.2744e-01, -1.7932e-01, -6.8726e-02,\n",
       "            1.8494e-01,  2.5635e-01,  4.8975e-01, -6.1182e-01, -1.0608e-01,\n",
       "           -1.4209e-01, -5.1727e-03,  3.6499e-01,  1.5549e-02,  1.1780e-01,\n",
       "           -2.4246e-02,  2.1313e-01, -3.4888e-01,  4.7119e-02, -5.9875e-02,\n",
       "           -1.4978e-01,  8.3191e-02, -4.3579e-02, -3.7085e-01,  2.2400e-01,\n",
       "            2.9102e-01, -4.2505e-01,  4.5532e-02, -9.7412e-02,  1.1804e-01,\n",
       "           -2.3376e-01, -2.6230e-02,  9.9792e-02,  3.0566e-01,  2.3584e-01,\n",
       "            1.5125e-01,  2.9343e-02, -1.3535e-02,  6.2734e+00,  8.1940e-03,\n",
       "           -4.1779e-02, -4.2725e-01, -2.3743e-01, -7.7759e-02,  2.6260e-02,\n",
       "           -3.9551e-02,  2.3157e-01, -9.2590e-02,  9.1125e-02, -9.4055e-02,\n",
       "           -8.0017e-02, -4.3610e-02, -4.4312e-01, -2.4109e-01, -1.4124e-01,\n",
       "           -1.9336e-01,  5.8319e-02,  2.8125e-01,  9.4849e-02,  6.9153e-02,\n",
       "           -2.3340e-01, -3.5736e-02, -5.8789e-01,  2.3108e-01, -1.2262e-01,\n",
       "           -1.7786e-01, -2.7148e-01, -1.0284e-01,  4.8999e-01, -1.3452e-01,\n",
       "            1.4917e-01,  1.1481e-01,  1.3062e-01,  1.5491e-01, -7.3608e-02,\n",
       "           -1.8823e-01,  9.6680e-02,  2.7295e-01, -2.1912e-01, -3.5840e-01,\n",
       "            4.8291e-01, -4.1284e-01,  1.3477e-01,  2.1820e-02,  2.5558e-02,\n",
       "            8.5571e-02,  5.6427e-02, -2.6465e-01, -4.1626e-02, -4.8431e-02,\n",
       "           -1.6028e-01, -1.4246e-01, -1.2769e-01,  2.1985e-01,  9.9121e-02,\n",
       "            2.0190e-01,  3.5645e-01,  1.5698e-01, -2.8101e-01, -3.4180e-02,\n",
       "            3.5187e-02, -2.7051e-01,  2.6978e-01, -8.6853e-02,  1.3660e-01,\n",
       "            1.6434e-02, -5.6366e-02,  1.8433e-01, -3.3667e-01,  9.3994e-02,\n",
       "            2.6953e-01,  2.1801e-03, -1.5906e-01, -7.0618e-02,  2.4307e-04,\n",
       "           -1.3220e-01,  3.7329e-01,  1.8860e-01, -5.0140e-02, -2.2754e-01,\n",
       "            1.7383e-01, -8.4900e-02, -6.9092e-01,  1.0681e-01,  7.6477e-02,\n",
       "           -5.3925e-02, -7.1472e-02,  2.6685e-01,  3.6804e-02, -3.9581e-02,\n",
       "           -7.0923e-02,  1.3420e-02,  2.8076e-01,  1.3074e-01, -2.8809e-01,\n",
       "           -1.9312e-01,  5.5908e-01,  1.2250e-01, -1.5564e-01,  3.5667e-03,\n",
       "            2.2241e-01, -9.5459e-02, -8.3008e-02,  7.5951e-03, -3.6304e-01,\n",
       "            1.4502e-01,  1.2201e-01, -3.5156e-02, -2.8229e-02, -3.7817e-01,\n",
       "           -2.1558e-01,  2.1191e-01,  3.5425e-01, -8.5815e-02,  3.1323e-01,\n",
       "           -1.5503e-01,  1.3293e-01,  2.1509e-01, -1.9910e-01, -1.8066e-01,\n",
       "            2.3865e-02, -1.3232e-01, -6.2988e-02,  1.2622e-01,  5.0391e-01,\n",
       "           -1.9897e-01, -9.4482e-02,  1.8713e-01, -1.0124e-02,  1.9568e-01,\n",
       "           -3.4766e-01, -8.4045e-02, -2.8702e-02,  2.1164e-02,  1.2280e-01,\n",
       "            5.3467e-02,  8.9172e-02, -1.7090e-01, -1.7334e-01,  1.6083e-02,\n",
       "            1.6296e-01, -5.7526e-02,  1.0535e-01,  1.2103e-01,  4.5776e-02,\n",
       "            5.1221e-01, -2.0691e-01,  2.3608e-01,  5.5908e-01, -2.7515e-01,\n",
       "            1.1841e-01,  6.7711e-04,  3.4106e-01,  1.4246e-01, -2.8906e-01,\n",
       "            3.2910e-01,  2.9419e-01,  2.0004e-02,  5.5725e-02,  4.9194e-02,\n",
       "            4.5532e-02, -1.8579e-01, -8.2031e-02, -2.7734e-01, -2.3889e-01,\n",
       "           -4.9634e-01, -1.2665e-02, -1.2048e-01, -3.8330e-01,  1.5640e-02,\n",
       "            2.0691e-01, -1.0620e-02, -1.8628e-01,  2.4216e-02, -3.2666e-01,\n",
       "           -3.3374e-01,  9.1370e-02,  6.2539e+00, -3.3936e-02,  6.2103e-02,\n",
       "            3.7048e-02,  3.9771e-01,  3.8662e-03,  3.6938e-01,  7.5586e-01,\n",
       "            8.6182e-02,  2.6465e-01,  4.5349e-02, -3.1201e-01, -1.5735e-01,\n",
       "           -2.0767e-02, -6.0211e-02,  7.6180e-03, -1.7761e-01, -3.1465e+00,\n",
       "            4.4617e-02, -8.5815e-02, -7.4081e-03,  2.6970e-03, -2.6929e-01,\n",
       "            2.9877e-02,  2.2522e-01, -3.0908e-01,  1.7334e-01,  1.1627e-01,\n",
       "           -4.2993e-01,  3.2715e-01,  1.1835e-01, -9.5154e-02, -1.1884e-01,\n",
       "            8.9966e-02, -3.9429e-02,  2.0386e-01,  9.3262e-02, -2.4185e-02,\n",
       "            5.1855e-01,  1.8265e-02,  1.0583e-01,  1.3037e-01, -1.9629e-01,\n",
       "            6.1249e-02, -3.7524e-01, -2.0117e-01,  1.7981e-01,  2.8760e-01,\n",
       "           -7.5928e-02, -2.4268e-01,  2.9712e-01, -1.6577e-01, -1.7480e-01,\n",
       "           -6.6711e-02, -2.9785e-01,  1.8323e-01,  2.2266e-01, -1.7181e-02,\n",
       "            7.5134e-02,  3.5339e-02,  6.9092e-02, -1.1938e-01,  2.5391e-01,\n",
       "            3.5596e-01, -1.1835e-01,  2.9526e-02, -1.3464e-01,  6.6711e-02,\n",
       "           -1.5576e-01, -1.9812e-01,  7.4036e-02,  5.2063e-02, -2.1072e-02,\n",
       "            1.1255e-01,  1.0384e-02, -8.4656e-02, -2.2046e-01, -5.6488e-02,\n",
       "           -1.9788e-01,  2.5928e-01, -1.7322e-01, -5.2338e-02,  1.1566e-01,\n",
       "            2.0569e-01, -1.0052e-01, -7.0312e-02, -9.7290e-02,  5.4346e-01,\n",
       "            2.6318e-01,  1.9226e-01,  2.4354e-04, -4.9347e-02, -1.1334e-01,\n",
       "           -1.2286e-01, -1.9348e-01,  1.3452e-01,  4.8657e-01, -3.2410e-02,\n",
       "            6.8787e-02,  1.7297e-01, -3.5669e-01,  1.4246e-01,  8.9111e-03,\n",
       "            1.1163e-01, -1.3989e-01,  1.3037e-01,  2.6367e-01,  4.9927e-01,\n",
       "            6.3416e-02, -4.0503e-01,  2.5903e-01, -9.8999e-02,  9.1064e-02,\n",
       "           -2.0390e-03, -3.0127e-01, -1.6052e-01,  1.3196e-01,  5.3833e-02,\n",
       "           -2.0813e-01, -1.9470e-02,  4.3152e-02,  4.6265e-02, -2.0032e-01,\n",
       "            1.1847e-01, -1.4709e-01,  1.1853e-01, -3.5815e-01,  3.9856e-02,\n",
       "           -3.4912e-01, -1.8518e-01, -1.8945e-01,  1.4502e-01, -2.6172e-01,\n",
       "            4.3726e-01,  8.7708e-02, -6.4636e-02, -1.9238e-01,  1.5038e-02,\n",
       "            1.2756e-01, -1.2115e-01, -1.3135e-01,  2.1399e-01,  2.5024e-01,\n",
       "            1.5735e-01, -6.3972e-03, -1.9629e-01, -1.0730e-01, -3.2202e-01,\n",
       "           -3.5767e-02, -2.8793e-02, -1.9275e-01, -5.6580e-02,  3.2910e-01,\n",
       "            5.4004e-01, -1.1212e-01,  2.1863e-01, -9.3872e-02, -8.3313e-02,\n",
       "            8.0017e-02,  1.7810e-01,  4.6875e-01, -3.1982e-01,  8.1177e-02,\n",
       "            1.9226e-01,  2.1899e-01, -7.0862e-02,  5.7770e-02,  7.8271e-01,\n",
       "            5.2393e-01, -8.0078e-01,  4.3091e-01, -2.9834e-01, -2.0056e-01,\n",
       "           -8.4381e-03, -1.7114e-01,  8.8074e-02,  9.0698e-02, -2.0679e-01,\n",
       "            2.6562e-01, -2.7939e-02,  9.0088e-02,  5.1318e-01,  3.7549e-01,\n",
       "            1.3782e-01,  1.5112e-01,  1.5405e-01, -1.6931e-01,  3.3325e-01,\n",
       "           -5.7312e-02, -1.3330e-01,  2.5928e-01,  2.9761e-01,  2.3071e-01,\n",
       "            2.0129e-01, -5.2917e-02,  9.7229e-02,  1.1432e-01, -2.6538e-01,\n",
       "           -1.0748e-01, -3.5083e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1628, 0.0358, 0.8013]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  456.233337   49.230087  639.412048  409.751465    0.844929     47   apple\n",
       "  1    8.406189   85.777130  517.742004  475.807495    0.821701     47   apple\n",
       "  2    0.922180    1.974655  631.174377  438.110962    0.669324     46  banana\n",
       "  3  485.323486  426.284546  639.726440  478.772583    0.663904     47   apple\n",
       "  4  299.089539   83.613770  478.057129  184.209045    0.390580     49  orange\n",
       "  5    2.651642    0.000000  634.505249  469.828125    0.273788     45    bowl,\n",
       "  'caption': ['The fruit to the right.'],\n",
       "  'bbox_target': [453.02, 48.45, 186.98, 359.71]},\n",
       " 915: {'image_emb': tensor([[-0.2563,  0.0490, -0.3550,  ..., -0.1593,  0.0667,  0.0611],\n",
       "          [ 0.0706,  0.2029, -0.3464,  ...,  0.6514,  0.2084, -0.0792],\n",
       "          [ 0.0161,  0.2094, -0.0478,  ...,  0.5796, -0.9077, -0.1257],\n",
       "          ...,\n",
       "          [ 0.2217,  0.4253, -0.5000,  ...,  0.9966,  0.1157, -0.1071],\n",
       "          [ 0.2065,  0.0975, -0.2512,  ..., -0.0492,  0.2781, -0.0817],\n",
       "          [ 0.0895,  0.1293,  0.0836,  ..., -0.1139, -0.3926,  0.0112]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0753,  0.5098,  0.0353,  ..., -0.2727,  0.3062, -0.0578],\n",
       "          [ 0.0837, -0.3809, -0.2318,  ...,  0.1991,  0.2202, -0.0894]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.1324e-03, 1.5906e-01, 1.3342e-03, 2.6112e-03, 1.2531e-03, 8.3350e-01,\n",
       "           2.3246e-06],\n",
       "          [8.1730e-04, 3.9795e-01, 2.5868e-05, 3.7212e-03, 3.3545e-04, 5.9717e-01,\n",
       "           6.2048e-05]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  358.512634    9.797867  622.957397  354.711731    0.949076      0   \n",
       "  1    0.466400   72.148499  124.520546  356.854004    0.875035      0   \n",
       "  2   56.285965  123.271301  499.443176  356.848694    0.863296      3   \n",
       "  3    0.785286  131.944336  137.067627  218.849670    0.840279      2   \n",
       "  4  537.722961  188.284149  639.460144  357.130524    0.778151      3   \n",
       "  5  178.483261   29.869904  374.011414  269.357330    0.722882      0   \n",
       "  6  459.121399  119.527527  483.549377  132.975647    0.654679      2   \n",
       "  7  360.122009   20.893768  376.245667   64.651123    0.566416      0   \n",
       "  8  609.886414  109.663452  640.000000  231.326050    0.262406      2   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2  motorcycle  \n",
       "  3         car  \n",
       "  4  motorcycle  \n",
       "  5      person  \n",
       "  6         car  \n",
       "  7      person  \n",
       "  8         car  ,\n",
       "  'caption': ['A bald man with glasses wearing a yellow and red jacket.',\n",
       "   'Older man wearing glasses and holding up what looks like a check.'],\n",
       "  'bbox_target': [180.4, 33.1, 215.04, 236.78]},\n",
       " 916: {'image_emb': tensor([[ 0.2479,  0.2917, -0.2219,  ...,  1.1016,  0.0965, -0.4216],\n",
       "          [ 0.0041,  0.0672, -0.2186,  ...,  1.4668,  0.0726, -0.1306],\n",
       "          [-0.0490,  0.2375, -0.5269,  ...,  1.2295, -0.0247, -0.0188],\n",
       "          ...,\n",
       "          [ 0.3472,  0.0496, -0.3850,  ...,  1.1357, -0.0738,  0.0097],\n",
       "          [ 0.1368,  0.2220, -0.2698,  ...,  0.8335, -0.0358,  0.2791],\n",
       "          [-0.2244,  0.1349, -0.0142,  ...,  0.7788,  0.3623,  0.2457]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 2.4597e-01,  2.7496e-02, -4.8608e-01,  ...,  3.4332e-05,\n",
       "           -2.2339e-01,  8.8135e-02],\n",
       "          [ 1.6785e-01,  1.2354e-01, -4.3750e-01,  ..., -2.0496e-01,\n",
       "           -2.3938e-01, -4.4580e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.2662e-04, 9.2334e-01, 2.2064e-02, 3.9337e-02, 1.0878e-04, 7.2050e-04,\n",
       "           1.4023e-02],\n",
       "          [8.9216e-04, 1.0014e-04, 3.6194e-02, 1.9568e-01, 7.2122e-05, 4.9782e-03,\n",
       "           7.6221e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   463.562683  150.077148  540.663574  334.956665    0.914787      0   \n",
       "  1     0.000000  306.245483  307.133911  476.228760    0.911022     57   \n",
       "  2   254.065964  168.080734  367.157928  472.281738    0.901696      0   \n",
       "  3   316.138184  264.763245  583.117371  480.000000    0.882210      0   \n",
       "  4   146.037079  279.373840  205.829620  313.350891    0.808856     56   \n",
       "  5    98.867935  274.488708  129.781906  310.725830    0.782543     56   \n",
       "  6   352.425842  326.710144  629.221558  479.007446    0.566674     56   \n",
       "  7   270.707153  219.237366  351.516724  320.434814    0.434663     24   \n",
       "  8   292.846191  218.501648  349.999817  273.231873    0.416323     24   \n",
       "  9   190.446594  277.022736  256.291412  316.731781    0.400139     56   \n",
       "  10  247.239487  234.528534  284.421112  317.223175    0.377941     58   \n",
       "  11  350.624146  368.427826  373.637024  382.405670    0.265263     65   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1          couch  \n",
       "  2         person  \n",
       "  3         person  \n",
       "  4          chair  \n",
       "  5          chair  \n",
       "  6          chair  \n",
       "  7       backpack  \n",
       "  8       backpack  \n",
       "  9          chair  \n",
       "  10  potted plant  \n",
       "  11        remote  ,\n",
       "  'caption': ['A woman wearing a backpack resting her hand on the sofa',\n",
       "   'A woman with a white shirt and a backpack standing beside two men.'],\n",
       "  'bbox_target': [254.56, 171.51, 113.26, 299.86]},\n",
       " 917: {'image_emb': tensor([[-0.4043,  1.0088,  0.0423,  ...,  1.0166,  0.3662,  0.4241],\n",
       "          [ 0.3801,  0.3750,  0.2717,  ...,  1.2793, -0.2445,  0.2800],\n",
       "          [ 0.2644, -0.3081,  0.2178,  ...,  0.7373, -0.5073,  0.1694],\n",
       "          [-0.1394,  0.2196, -0.4631,  ...,  1.2402, -0.1812, -0.1255],\n",
       "          [ 0.3772,  0.1052, -0.0906,  ...,  0.8687, -0.0115,  0.1903],\n",
       "          [ 0.2070,  0.0656,  0.2712,  ...,  0.6724,  0.0480,  0.4502]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1289, -0.0691, -0.3503,  ..., -0.1893, -0.0200, -0.1072],\n",
       "          [-0.4290,  0.1312, -0.2297,  ...,  0.0923, -0.1404, -0.5034]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.0233e-02, 2.3285e-02, 1.9302e-02, 2.0435e-01, 5.5518e-01, 1.7749e-01],\n",
       "          [7.4685e-05, 1.3590e-05, 9.5367e-07, 3.2127e-05, 1.1920e-01, 8.8086e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  316.821320   14.946541  438.670746  244.990906    0.924846     41   \n",
       "  1  201.134109  105.976257  324.811584  173.233856    0.894230     48   \n",
       "  2    0.000000  254.667175  310.655365  430.644836    0.892033     48   \n",
       "  3    0.985886    0.870667  145.324844  125.851929    0.879957      0   \n",
       "  4    1.942902  118.767395  640.000000  474.867981    0.751105     60   \n",
       "  5  150.786667  217.104553  412.752014  259.090454    0.689268     43   \n",
       "  6  159.342224    0.000000  181.533844   23.525536    0.493370     41   \n",
       "  7  225.202240   30.574188  335.752167  129.718475    0.420784     24   \n",
       "  \n",
       "             name  \n",
       "  0           cup  \n",
       "  1      sandwich  \n",
       "  2      sandwich  \n",
       "  3        person  \n",
       "  4  dining table  \n",
       "  5         knife  \n",
       "  6           cup  \n",
       "  7      backpack  ,\n",
       "  'caption': ['The table the food is on',\n",
       "   'a table with food and drink on it with a doll sitting on it'],\n",
       "  'bbox_target': [2.15, 11.78, 637.13, 462.79]},\n",
       " 918: {'image_emb': tensor([[-0.3115,  0.4795, -0.0025,  ...,  1.0625, -0.0677,  0.0082],\n",
       "          [ 0.1863,  0.5171, -0.2155,  ...,  0.7729, -0.0998,  0.0240],\n",
       "          [-0.2014,  0.6924,  0.0279,  ...,  1.0371,  0.0795,  0.0484],\n",
       "          [ 0.1373,  0.5908, -0.2595,  ...,  1.1094, -0.0092, -0.0123],\n",
       "          [-0.0659,  0.4055, -0.1118,  ...,  1.2920,  0.1538, -0.0748],\n",
       "          [-0.0349,  0.3450, -0.1710,  ...,  0.7437, -0.0925, -0.0839]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3604,  0.4207,  0.3303,  ..., -0.2471,  0.0937, -0.4834],\n",
       "          [-0.5830,  0.1376, -0.0201,  ...,  0.0551, -0.2478, -0.2300]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.4990e-01, 6.1131e-04, 3.4790e-01, 1.5318e-05, 1.4000e-03, 1.2040e-04],\n",
       "          [6.5869e-01, 2.4002e-02, 2.7466e-01, 3.4761e-04, 9.8495e-03, 3.2288e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   394.300232  262.032898  597.622864  418.909119    0.935774     45   \n",
       "  1     2.736755   58.338043  636.583435  421.179077    0.892349     60   \n",
       "  2   182.101868  252.483276  366.828186  397.849731    0.890043     45   \n",
       "  3   341.905212  189.226868  439.373047  246.909912    0.776141     54   \n",
       "  4    29.292461    0.000000  109.196701   99.085678    0.775289     58   \n",
       "  5   159.877472  103.837265  309.001587  171.198792    0.616053     45   \n",
       "  6   355.363007    0.589325  503.523407   90.259033    0.614753     56   \n",
       "  7   266.968750  163.638336  383.027405  214.153717    0.578121     54   \n",
       "  8   187.923615  117.132812  233.787872  154.911621    0.573616     49   \n",
       "  9   248.259293  110.530014  298.410522  148.946487    0.526837     49   \n",
       "  10  243.821075    0.000000  368.783905   77.943481    0.504558     56   \n",
       "  11  218.182114  104.063126  245.357956  135.848557    0.373888     49   \n",
       "  12   54.731174  186.847015  140.636368  249.696442    0.336091     54   \n",
       "  13  228.114716  131.256714  251.221985  153.600159    0.308188     49   \n",
       "  14  244.432709  104.961334  261.273224  131.390228    0.252804     49   \n",
       "  \n",
       "              name  \n",
       "  0           bowl  \n",
       "  1   dining table  \n",
       "  2           bowl  \n",
       "  3          donut  \n",
       "  4   potted plant  \n",
       "  5           bowl  \n",
       "  6          chair  \n",
       "  7          donut  \n",
       "  8         orange  \n",
       "  9         orange  \n",
       "  10         chair  \n",
       "  11        orange  \n",
       "  12         donut  \n",
       "  13        orange  \n",
       "  14        orange  ,\n",
       "  'caption': ['A soup bowl on which a small piece of vegetable is floating',\n",
       "   'the bowl with chopsticks most right and low'],\n",
       "  'bbox_target': [395.26, 263.99, 201.0, 154.84]},\n",
       " 919: {'image_emb': tensor([[ 0.0706,  0.1591,  0.1646,  ...,  1.1484,  0.0789, -0.3032],\n",
       "          [ 0.1379,  0.2554, -0.3262,  ...,  1.0908,  0.0835, -0.3787],\n",
       "          [ 0.0176, -0.0365, -0.0596,  ...,  1.5020, -0.0668, -0.0249],\n",
       "          [-0.1311, -0.0558,  0.0209,  ...,  1.0918,  0.0701, -0.0974],\n",
       "          [-0.2522, -0.0449, -0.0669,  ...,  0.9272,  0.3022, -0.1382]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0139,  0.0649, -0.3159,  ...,  0.0299, -0.3525, -0.0684],\n",
       "          [-0.0762, -0.1831, -0.1259,  ..., -0.0728, -0.3989,  0.0815]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9512e-01, 5.7650e-04, 7.6389e-04, 3.2673e-03, 4.8566e-04],\n",
       "          [9.8877e-01, 1.9908e-05, 5.1079e-03, 6.2561e-03, 1.5974e-05]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   321.762177  121.523026  465.174591  343.321411    0.940828      0   \n",
       "  1   447.506256    8.006195  610.906494  520.588989    0.935045      0   \n",
       "  2     0.000000  132.141800  108.434937  321.687317    0.931829      0   \n",
       "  3    26.025024   49.583076  193.286728  197.532104    0.831281     58   \n",
       "  4   539.604553  223.689178  588.452576  274.574005    0.682753     26   \n",
       "  5   447.350250  194.435593  476.204742  270.581757    0.551826     26   \n",
       "  6   547.515259  119.437271  571.566650  135.469193    0.498324     67   \n",
       "  7    96.047897  191.243057  120.066177  232.429504    0.371800     24   \n",
       "  8    88.834618  169.680878  120.492775  232.468475    0.339490     26   \n",
       "  9   545.612183  129.095612  569.411011  146.974182    0.323724     67   \n",
       "  10   79.235565  168.653717  119.967163  232.286469    0.266816     24   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3   potted plant  \n",
       "  4        handbag  \n",
       "  5        handbag  \n",
       "  6     cell phone  \n",
       "  7       backpack  \n",
       "  8        handbag  \n",
       "  9     cell phone  \n",
       "  10      backpack  ,\n",
       "  'caption': ['A man with food and a blue shirt.',\n",
       "   'A man in a blue shirt sitting on the bench.'],\n",
       "  'bbox_target': [319.65, 124.23, 151.2, 218.66]},\n",
       " 920: {'image_emb': tensor([[ 1.3879e-01,  2.7783e-01,  1.0757e-02,  ...,  9.3066e-01,\n",
       "            4.2920e-01,  1.7517e-01],\n",
       "          [ 4.1809e-02,  1.6443e-01,  2.3911e-02,  ...,  1.0156e+00,\n",
       "            1.3013e-01, -4.9634e-01],\n",
       "          [ 3.3188e-04,  4.1138e-02, -2.5586e-01,  ...,  1.1484e+00,\n",
       "            7.9407e-02, -1.5640e-02],\n",
       "          [-2.2107e-01,  3.8843e-01, -3.7817e-01,  ...,  1.0156e+00,\n",
       "           -1.3806e-01, -2.4316e-01],\n",
       "          [-1.3879e-01,  2.9932e-01, -2.0520e-01,  ...,  4.6948e-01,\n",
       "            3.5498e-01,  2.1655e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0062, -0.2303, -0.3494,  ...,  0.6406, -0.0175, -0.5249],\n",
       "          [-0.0844,  0.1410, -0.0399,  ..., -0.3567, -0.1396,  0.0518]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.8945e-01, 7.8979e-02, 8.9407e-07, 1.8359e-01, 5.4785e-01],\n",
       "          [1.0175e-01, 1.1238e-02, 8.7036e-02, 7.4023e-01, 5.9814e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  153.531570   89.172531  374.973785  497.812256    0.944090      0   \n",
       "  1    0.000000  258.688080  129.500824  448.122131    0.896830     55   \n",
       "  2   98.014854  346.440674  160.296722  380.145813    0.745092     43   \n",
       "  3  207.824203  116.545044  262.397919  228.420166    0.729939     27   \n",
       "  4    0.626922  408.378510  229.721954  498.954498    0.565807     60   \n",
       "  5  207.792160  160.021988  237.016510  227.423279    0.377326     27   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1          cake  \n",
       "  2         knife  \n",
       "  3           tie  \n",
       "  4  dining table  \n",
       "  5           tie  ,\n",
       "  'caption': ['groom in white vest and red tie cutting wedding cake',\n",
       "   'a man was smilling'],\n",
       "  'bbox_target': [140.67, 41.28, 146.84, 300.14]},\n",
       " 921: {'image_emb': tensor([[-0.0882,  0.2583, -0.0626,  ...,  1.0205,  0.2029, -0.3127],\n",
       "          [-0.3025,  0.2888, -0.1509,  ...,  0.7832,  0.2373, -0.5537],\n",
       "          [-0.0368,  0.1655,  0.0297,  ...,  0.5649,  0.0043,  0.0730]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1410,  0.1749, -0.2666,  ..., -0.2148, -0.1542, -0.0903],\n",
       "          [ 0.0853,  0.2637, -0.2468,  ..., -0.1753,  0.0368, -0.1129]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0963, 0.3413, 0.5625],\n",
       "          [0.0254, 0.6553, 0.3193]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  206.537354  155.436523  640.000000  422.965637    0.909457     53   pizza\n",
       "  1  559.690308    0.000000  639.876343   74.135895    0.835591     53   pizza\n",
       "  2    1.157745    0.912582  360.821136  417.473755    0.575645      0  person\n",
       "  3  357.371582    0.408875  519.083679  142.661194    0.528961      0  person\n",
       "  4   46.337219   31.209320  583.122314  356.359955    0.498015     43   knife\n",
       "  5    1.338966  275.700226  153.674652  423.183777    0.474276      0  person,\n",
       "  'caption': ['The left hand of a person.', 'the hand holding the cheese'],\n",
       "  'bbox_target': [0.96, 3.54, 364.63, 253.33]},\n",
       " 922: {'image_emb': tensor([[-0.3772,  0.3984, -0.1296,  ...,  1.3457, -0.2013,  0.2336],\n",
       "          [ 0.1337,  0.4133, -0.3054,  ...,  0.9048,  0.3025,  0.0924],\n",
       "          [-0.2480,  0.0872, -0.2539,  ...,  1.0381,  0.0145, -0.1213],\n",
       "          ...,\n",
       "          [-0.4072,  0.4006, -0.2986,  ...,  0.9990,  0.0945,  0.3875],\n",
       "          [ 0.2310,  0.2285, -0.6060,  ...,  0.7539,  0.1465,  0.0873],\n",
       "          [-0.1538, -0.2123,  0.0183,  ...,  1.0459, -0.0015,  0.0468]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2395, -0.2494, -0.2306,  ..., -0.2313,  0.0145,  0.1187],\n",
       "          [-0.2852,  0.0013, -0.2920,  ..., -0.3320, -0.2280, -0.1169]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.4219e-01, 6.6956e-02, 1.3098e-01, 2.1393e-02, 4.9055e-05, 1.0383e-04,\n",
       "           3.8147e-02],\n",
       "          [4.4507e-01, 2.0056e-01, 1.5137e-01, 1.2549e-01, 1.0920e-04, 1.7452e-04,\n",
       "           7.7332e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   484.994019  236.739166  612.083862  421.058289    0.931426     56   \n",
       "  1     0.000000  261.190643  505.775757  420.654907    0.873589     60   \n",
       "  2   249.200165  210.432343  350.736298  267.223541    0.859389     56   \n",
       "  3     0.757126  195.756195  350.130554  323.234406    0.807781     57   \n",
       "  4    96.639114  108.062759  189.026596  180.841431    0.803140     62   \n",
       "  5   219.009262  253.048279  258.766785  335.655884    0.790805     75   \n",
       "  6   264.541016  118.575058  391.738708  219.340179    0.657017     56   \n",
       "  7   206.706787   47.645386  230.285706   85.786713    0.573964     58   \n",
       "  8   113.609451  120.887650  155.962570  165.367035    0.440682      0   \n",
       "  9   248.386688  146.089340  283.688141  192.316895    0.340136     58   \n",
       "  10  381.065552  369.867035  473.464294  423.880188    0.264326     56   \n",
       "  11   85.626411   30.545624  113.334358   67.891022    0.254525     58   \n",
       "  \n",
       "              name  \n",
       "  0          chair  \n",
       "  1   dining table  \n",
       "  2          chair  \n",
       "  3          couch  \n",
       "  4             tv  \n",
       "  5           vase  \n",
       "  6          chair  \n",
       "  7   potted plant  \n",
       "  8         person  \n",
       "  9   potted plant  \n",
       "  10         chair  \n",
       "  11  potted plant  ,\n",
       "  'caption': ['An empty wooden chair facing to the left.',\n",
       "   'The chair to the right of the table'],\n",
       "  'bbox_target': [482.58, 241.01, 127.04, 178.23]},\n",
       " 923: {'image_emb': tensor([[ 0.1666,  0.3101,  0.0978,  ...,  0.8608, -0.0039,  0.0677],\n",
       "          [-0.1654,  0.2054, -0.2366,  ...,  1.4434,  0.0188,  0.0781],\n",
       "          [ 0.2605,  0.3447, -0.1121,  ...,  0.8584,  0.1599, -0.2976],\n",
       "          [ 0.2296,  0.0525, -0.0905,  ..., -0.0416, -0.2164,  0.2756]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3169,  0.2449, -0.0300,  ..., -0.1771,  0.0281, -0.4351],\n",
       "          [ 0.1976,  0.4397, -0.4553,  ..., -0.0447, -0.1010, -0.3552]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.1592e-01, 2.1515e-03, 1.3340e-04, 1.8201e-01],\n",
       "          [1.4954e-02, 2.9802e-05, 5.9605e-07, 9.8486e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  213.826782   60.314026  350.152466  272.148773    0.946155     33    kite\n",
       "  1  162.589813  255.665527  201.011261  290.891357    0.907586     33    kite\n",
       "  2  274.656189  413.829651  303.296997  485.768005    0.902124      0  person\n",
       "  3   70.423294  433.872375   77.594070  442.614319    0.478835      0  person\n",
       "  4  233.382996  458.982513  246.996155  480.647858    0.296114     33    kite,\n",
       "  'caption': ['The biggest kite in the sky.',\n",
       "   'A large red and white parachute drifting near a smaller blue parachute above a man on a beach'],\n",
       "  'bbox_target': [212.36, 59.52, 140.38, 215.85]},\n",
       " 924: {'image_emb': tensor([[-0.0137, -0.0281, -0.3027,  ...,  1.0605,  0.0162, -0.4163],\n",
       "          [ 0.0507,  0.2886, -0.1528,  ...,  1.4180, -0.0140, -0.2590],\n",
       "          [-0.2991,  0.2915, -0.3425,  ...,  0.1641,  0.3347, -0.2991]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0211,  0.0663,  0.0687,  ...,  0.0191,  0.2795, -0.0522],\n",
       "          [-0.3806,  0.2305,  0.0405,  ..., -0.0731,  0.0455, -0.0713]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.7964, 0.0146, 0.1891],\n",
       "          [0.7207, 0.0011, 0.2778]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  195.114975  138.867203  371.965820  307.479431    0.908969      9   \n",
       "  1  408.506622   40.300095  535.962219  298.385681    0.908445      9   \n",
       "  \n",
       "              name  \n",
       "  0  traffic light  \n",
       "  1  traffic light  ,\n",
       "  'caption': ['A stoplight', 'A light that shows a man on a horse.'],\n",
       "  'bbox_target': [197.15, 141.85, 174.07, 167.34]},\n",
       " 925: {'image_emb': tensor([[ 0.1644,  0.2086,  0.1661,  ...,  0.9468,  0.2465,  0.3438],\n",
       "          [ 0.0263,  0.7661,  0.0514,  ...,  1.1641,  0.1531,  0.2003],\n",
       "          [-0.1606,  0.1409,  0.0051,  ...,  1.0723, -0.0617, -0.2830],\n",
       "          [ 0.1570,  0.6343, -0.2024,  ...,  0.5571,  0.2078,  0.0590]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0753,  0.1190, -0.0219,  ...,  0.3076,  0.0326, -0.5918],\n",
       "          [-0.1631,  0.4597,  0.0928,  ..., -0.0252, -0.0872, -0.4890]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.5339e-02, 2.1997e-01, 5.2834e-04, 7.4414e-01],\n",
       "          [3.0422e-03, 9.5557e-01, 5.2567e-03, 3.5919e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  239.081787   16.300392  456.492706  215.762405    0.811513     46  banana\n",
       "  1   63.585411   30.216705  248.705292  213.246155    0.751229     49  orange\n",
       "  2  142.630600    3.335318  278.909149   78.827301    0.735132     49  orange\n",
       "  3  134.916260  222.392136  387.302399  360.479889    0.623594     49  orange\n",
       "  4   62.982971  203.073715  156.050690  322.443420    0.611795     39  bottle\n",
       "  5    4.789519  299.680542  243.779587  499.315887    0.589574     49  orange\n",
       "  6   21.288227    3.215337  487.108337  242.755859    0.567363     45    bowl\n",
       "  7  391.286621  140.740784  499.700317  348.611511    0.299074     46  banana,\n",
       "  'caption': ['A wooden basket holding a unripe banana and two oranges',\n",
       "   'a bowl with an orange and a banana'],\n",
       "  'bbox_target': [17.98, 4.49, 477.53, 235.96]},\n",
       " 926: {'image_emb': tensor([[-3.1494e-01,  3.5645e-02, -3.2471e-01,  ...,  1.0703e+00,\n",
       "            4.2017e-01,  1.4856e-01],\n",
       "          [ 7.4402e-02,  1.6895e-01, -5.3613e-01,  ...,  9.9365e-01,\n",
       "           -6.6996e-04,  4.1382e-01],\n",
       "          [ 1.0913e-01,  1.2915e-01, -1.0724e-01,  ...,  9.5361e-01,\n",
       "           -2.4243e-01,  4.8218e-02],\n",
       "          ...,\n",
       "          [ 1.6284e-01,  3.8647e-01, -3.4399e-01,  ...,  7.8564e-01,\n",
       "            4.3604e-01, -2.5659e-01],\n",
       "          [ 4.0601e-01, -2.7515e-01, -1.2695e-01,  ...,  5.1855e-01,\n",
       "           -4.2114e-02, -3.9453e-01],\n",
       "          [-4.7412e-01,  4.6265e-02, -1.6150e-01,  ...,  9.9463e-01,\n",
       "            1.5820e-01,  2.8418e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2201,  0.3162, -0.1324,  ...,  0.1403,  0.2510, -0.0647],\n",
       "          [-0.1006,  0.1658, -0.0058,  ..., -0.0717,  0.2042, -0.0118]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.1514e-01, 4.6924e-01, 3.6373e-03, 1.8015e-03, 0.0000e+00, 5.9605e-08,\n",
       "           1.0201e-02],\n",
       "          [9.9561e-01, 4.2648e-03, 5.9605e-08, 1.1921e-07, 0.0000e+00, 1.1921e-07,\n",
       "           1.9026e-04]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0     1.326797   44.367661  364.101685  474.694031    0.959049      0  person\n",
       "  1   315.285583   61.764160  639.459045  474.593506    0.949112      0  person\n",
       "  2   178.104767  218.393799  244.746948  480.000000    0.912425     27     tie\n",
       "  3   452.218781  257.323334  509.953033  477.123779    0.867873     27     tie\n",
       "  4   304.717590  206.447296  394.702515  237.070465    0.845105      2     car\n",
       "  5   612.577332  225.304443  629.828430  256.459106    0.815656      0  person\n",
       "  6   577.959167  199.344910  594.884094  245.086975    0.674880      0  person\n",
       "  7   290.137085  200.349640  316.915955  212.312103    0.599799      2     car\n",
       "  8     0.107850  230.886475   12.398575  244.535461    0.546024      2     car\n",
       "  9   613.870850  205.849426  639.697632  219.442200    0.541675      2     car\n",
       "  10   12.411581  214.103149   47.338036  244.470947    0.505679      2     car\n",
       "  11  544.830933  203.701752  638.120972  235.644135    0.475572      2     car\n",
       "  12   11.922997  214.127808   47.204025  244.269165    0.459715      7   truck\n",
       "  13  606.884460  207.707245  640.000000  232.266449    0.283327      2     car,\n",
       "  'caption': ['A multi-colored, patterned tie that the man in the grey jacket is wearing.',\n",
       "   'A blue tie being worn by a man in sunglasses.'],\n",
       "  'bbox_target': [182.7, 222.16, 62.71, 252.98]},\n",
       " 927: {'image_emb': tensor([[-0.0563,  0.3718, -0.1055,  ...,  0.5083,  0.2211, -0.0562],\n",
       "          [ 0.1042,  0.3577, -0.1434,  ...,  0.8506,  0.3096, -0.1365],\n",
       "          [-0.0883,  0.2852, -0.2524,  ...,  0.8018,  0.3337, -0.1600],\n",
       "          [-0.0818, -0.1101, -0.2030,  ...,  0.7310, -0.0132,  0.1492]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0363, -0.1586, -0.2734,  ...,  0.0501, -0.0163, -0.7124],\n",
       "          [-0.2695, -0.0781, -0.2080,  ...,  0.4211,  0.1436, -0.4893]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0709, 0.8501, 0.0125, 0.0666],\n",
       "          [0.3340, 0.4858, 0.0224, 0.1577]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  233.276276  195.118958  638.029907  429.284363    0.936619     53  pizza\n",
       "  1  236.883804    0.450478  639.577271  193.713165    0.933461     53  pizza\n",
       "  2    0.370667   39.062515  230.754608  338.903442    0.926400     53  pizza,\n",
       "  'caption': ['pizza to the left of two other pizzas',\n",
       "   'The pizza on the left hand side of the photo. It is in a long, rectangular pan.'],\n",
       "  'bbox_target': [1.08, 44.3, 231.91, 293.39]},\n",
       " 928: {'image_emb': tensor([[-0.3625,  0.6660, -0.1277,  ...,  1.0371,  0.1797, -0.1411],\n",
       "          [-0.0439,  0.4998, -0.2654,  ...,  1.1992,  0.2302, -0.2769],\n",
       "          [-0.2712,  0.6304,  0.0046,  ...,  0.8101,  0.1017, -0.1102],\n",
       "          [-0.3828,  0.3137, -0.0292,  ...,  0.9565,  0.0579, -0.0244]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1412,  0.1697, -0.3013,  ...,  0.1294,  0.1292, -0.2422],\n",
       "          [-0.3374,  0.1259, -0.3027,  ..., -0.1536,  0.0518, -0.4106]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0981, 0.5557, 0.3167, 0.0294],\n",
       "          [0.1859, 0.4255, 0.2791, 0.1093]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class  name\n",
       "  0  346.194794  62.073040  466.042084  258.689514    0.943378     75  vase\n",
       "  1  222.856110  69.843506  322.307892  262.982117    0.936609     75  vase\n",
       "  2   33.484467  76.579903  195.773224  260.263672    0.934426     75  vase,\n",
       "  'caption': ['A vase with bumps on it in between two other vases.',\n",
       "   'The middle vase with the bumps.'],\n",
       "  'bbox_target': [225.87, 67.96, 98.37, 198.04]},\n",
       " 929: {'image_emb': tensor([[ 0.1603,  0.1378, -0.2507,  ...,  0.3459,  0.1053,  0.4165],\n",
       "          [-0.0386,  0.1661, -0.2108,  ...,  1.3125,  0.0024, -0.0943],\n",
       "          [-0.2683,  0.1825, -0.1429,  ...,  1.3252, -0.1880, -0.3394],\n",
       "          [ 0.1638,  0.1909, -0.3169,  ...,  0.2993,  0.1061,  0.0238]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0474, -0.3225, -0.3291,  ..., -0.1082,  0.2185, -0.1455],\n",
       "          [-0.0214, -0.1344, -0.1688,  ..., -0.3486,  0.2404, -0.4541]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3962, 0.0657, 0.0210, 0.5171],\n",
       "          [0.6851, 0.0637, 0.0180, 0.2332]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  136.629379    0.773987  490.526184  420.153625    0.854660     74  clock\n",
       "  1  539.061707    0.000000  638.755920  136.830322    0.800492     73   book\n",
       "  2  520.816284  148.728165  586.347290  425.327148    0.708509     73   book\n",
       "  3  587.164429  150.494873  640.000000  425.351990    0.644815     73   book,\n",
       "  'caption': ['The Opkins binder on the right of the clock.',\n",
       "   'A white sign with black writing next to a clock.'],\n",
       "  'bbox_target': [534.9, 0.0, 102.53, 136.75]},\n",
       " 930: {'image_emb': tensor([[-0.0308,  0.4607, -0.2944,  ...,  0.9370,  0.0654, -0.1672],\n",
       "          [ 0.2496,  0.1020,  0.1053,  ...,  0.9072, -0.1367,  0.0646],\n",
       "          [-0.1665,  0.5552, -0.1316,  ...,  1.1270, -0.0329, -0.1542],\n",
       "          [ 0.0086,  0.2517,  0.0857,  ...,  0.6089, -0.0751, -0.1737]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2465,  0.0496,  0.0729,  ..., -0.0896,  0.0732, -0.2510],\n",
       "          [ 0.0636,  0.4236, -0.2131,  ...,  0.1438,  0.1675, -0.0931]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.1182e-01, 1.5557e-05, 3.4326e-01, 4.5013e-02],\n",
       "          [2.1716e-01, 1.1325e-06, 7.8223e-01, 5.6410e-04]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  377.800110  213.091980  576.070862  335.344788    0.919748      1  bicycle\n",
       "  1    1.802246   72.309845  346.413910  422.775146    0.919623      7    truck\n",
       "  2  425.953186  138.180588  523.491821  293.749451    0.895488      0   person\n",
       "  3  572.693542  182.378235  639.912170  220.288025    0.558305      7    truck\n",
       "  4  593.353577  208.804749  640.000000  277.494080    0.540189      2      car\n",
       "  5  594.162048  208.638733  639.860046  277.447449    0.527717      7    truck,\n",
       "  'caption': ['a man on a bike',\n",
       "   'A man wearing a t-shirt and jeans is riding a bike.'],\n",
       "  'bbox_target': [427.76, 136.19, 94.37, 150.29]},\n",
       " 931: {'image_emb': tensor([[-0.0583,  0.1724, -0.1388,  ...,  0.1244, -0.1770,  0.3350],\n",
       "          [-0.2375,  0.0638, -0.0922,  ...,  0.9141, -0.2839,  0.1093],\n",
       "          [-0.1876,  0.0744, -0.2269,  ..., -0.0030, -0.2771,  0.1215],\n",
       "          [-0.3926,  0.0508, -0.2235,  ...,  1.3584, -0.2253, -0.1682],\n",
       "          [ 0.0782,  0.2239, -0.1780,  ...,  0.3813, -0.1713,  0.2961]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2576, -0.2905, -0.4888,  ..., -0.7251,  0.0538,  0.2742],\n",
       "          [-0.2539, -0.0356, -0.1783,  ...,  0.5352, -0.3840, -0.0453]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7310e-01, 5.2490e-01, 1.3696e-01, 2.1332e-02, 1.4355e-01],\n",
       "          [1.2024e-02, 9.1162e-01, 1.6367e-04, 7.5989e-02, 1.5867e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   125.078117   89.238373  300.080170  386.188507    0.936772      0   \n",
       "  1   441.437042   70.397064  567.800110  276.653229    0.914952      0   \n",
       "  2   193.394302    0.000000  319.400543  276.808289    0.907444      0   \n",
       "  3   140.630035   38.857819  209.803558   92.857971    0.808632      2   \n",
       "  4   242.842270  188.742889  303.824707  232.416595    0.678362     34   \n",
       "  5   590.433716    4.593712  639.857544  242.112000    0.669015      0   \n",
       "  6    80.681343   35.157074  179.421387   88.719940    0.650294      2   \n",
       "  7    13.475859  123.442108   26.841345  264.593414    0.527403     34   \n",
       "  8     0.000000  242.044739  100.279320  270.899719    0.353379     24   \n",
       "  9   556.730042  236.784637  612.378357  274.528107    0.318072     24   \n",
       "  10  273.250671   32.309952  341.024597  115.269638    0.295784      2   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3            car  \n",
       "  4   baseball bat  \n",
       "  5         person  \n",
       "  6            car  \n",
       "  7   baseball bat  \n",
       "  8       backpack  \n",
       "  9       backpack  \n",
       "  10           car  ,\n",
       "  'caption': ['The man in the background sitting down.',\n",
       "   'The man behind the fence sitting down watching the baseball game.'],\n",
       "  'bbox_target': [434.66, 70.85, 143.61, 208.71]},\n",
       " 932: {'image_emb': tensor([[ 0.2336,  0.7856,  0.2180,  ...,  0.6548,  0.0179,  0.3694],\n",
       "          [ 0.1687,  0.6729, -0.0479,  ...,  1.1162, -0.1589, -0.1318],\n",
       "          [-0.0931,  0.4343, -0.0070,  ...,  0.9492,  0.0846,  0.4182],\n",
       "          [-0.1438,  0.3274, -0.1595,  ...,  1.1621,  0.1271,  0.0720],\n",
       "          [ 0.1294,  0.3540,  0.1648,  ...,  0.6182,  0.0325,  0.1669]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0345,  0.2900, -0.0870,  ...,  0.2144, -0.1591, -0.0907],\n",
       "          [ 0.1306,  0.2032, -0.2769,  ...,  0.2493,  0.0511, -0.0898]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.7012e-01, 1.4069e-02, 3.1090e-04, 8.0185e-03, 1.0724e-01],\n",
       "          [5.2295e-01, 2.0483e-01, 1.4458e-03, 3.6354e-03, 2.6709e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  455.467285  131.797134  627.359375  358.134216    0.913253     63  laptop\n",
       "  1  299.236023  244.235535  624.863281  638.432190    0.911485     63  laptop\n",
       "  2  461.927338   11.788902  606.377197  174.858154    0.899643     63  laptop\n",
       "  3    1.003250    1.007942  314.694580  160.255768    0.710989     57   couch\n",
       "  4  508.062653  465.935242  634.000000  639.723816    0.453020      0  person\n",
       "  5  205.516678   22.564140  630.906555  229.812988    0.443184      0  person\n",
       "  6   63.157860  437.165558  208.075867  489.522491    0.431433     73    book\n",
       "  7  115.474487   67.781738  629.832642  359.491119    0.359661      0  person\n",
       "  8   22.081100  429.889954  280.528503  584.548706    0.336513     73    book,\n",
       "  'caption': ['A silver laptop sitting on the lap of someone wearing blue pants and white socks.',\n",
       "   'middle laptop on person lap'],\n",
       "  'bbox_target': [454.78, 132.76, 168.89, 226.78]},\n",
       " 933: {'image_emb': tensor([[-0.3779,  0.6182, -0.3027,  ...,  1.3096,  0.0038,  0.3311],\n",
       "          [-0.3511,  0.3206,  0.1544,  ...,  1.2725,  0.3186, -0.1060],\n",
       "          [ 0.0319,  0.4124, -0.0847,  ...,  0.9087,  0.0278,  0.1038]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1671, -0.0016, -0.1198,  ..., -0.0775, -0.0544, -0.3010],\n",
       "          [-0.0354, -0.0327, -0.3276,  ..., -0.0839, -0.0991, -0.3433]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1257, 0.5376, 0.3364],\n",
       "          [0.1353, 0.5786, 0.2864]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0    3.447487   52.448822  187.863068  317.735321    0.807838     51  carrot\n",
       "  1  283.261108   60.302109  465.420654  318.148743    0.802228     51  carrot\n",
       "  2  239.059052   34.334183  331.039948  378.377686    0.595021     51  carrot\n",
       "  3  152.198380   47.886581  248.677612  408.433228    0.586810     51  carrot\n",
       "  4    5.014618   39.100021  478.000000  483.127502    0.528457     45    bowl\n",
       "  5  434.804321  172.966003  467.532959  207.651672    0.255601     51  carrot,\n",
       "  'caption': ['The carrot on the far right.',\n",
       "   'The fried piece of food on top and to the right of others.'],\n",
       "  'bbox_target': [283.47, 62.53, 175.93, 222.73]},\n",
       " 934: {'image_emb': tensor([[ 0.3657,  0.2103, -0.0886,  ...,  0.7354, -0.0340,  0.0709],\n",
       "          [-0.2646,  0.6196,  0.1433,  ...,  1.1455,  0.3250, -0.1140],\n",
       "          [-0.3843,  0.4250, -0.1600,  ...,  1.1982,  0.1644,  0.0279],\n",
       "          [ 0.2258,  0.0272,  0.0964,  ...,  0.8574, -0.1392,  0.0534]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0652, -0.1445, -0.1555,  ...,  0.2128,  0.3120,  0.1256],\n",
       "          [ 0.0893,  0.0986, -0.2008,  ..., -0.0225,  0.2032, -0.0319]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1703, 0.0060, 0.6953, 0.1285],\n",
       "          [0.2400, 0.0282, 0.4556, 0.2764]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   56.197296   73.885910  640.000000  445.899536    0.869459     53   \n",
       "  1  187.552933    0.004120  293.575989   86.742477    0.823435     40   \n",
       "  2  293.646210    1.027939  639.716797   91.513809    0.819847      0   \n",
       "  3    0.212402  351.457977  166.590042  476.712708    0.691617     43   \n",
       "  4    0.000000    2.410889  639.481873  465.206909    0.298887     60   \n",
       "  \n",
       "             name  \n",
       "  0         pizza  \n",
       "  1    wine glass  \n",
       "  2        person  \n",
       "  3         knife  \n",
       "  4  dining table  ,\n",
       "  'caption': ['the black handle of a pizza knife sticking out from under a slice of pizza',\n",
       "   'the handle that is going to lift the pizza'],\n",
       "  'bbox_target': [0.0, 350.22, 166.89, 129.78]},\n",
       " 935: {'image_emb': tensor([[ 0.1348,  0.1746, -0.0267,  ...,  1.0615,  0.1160,  0.0058],\n",
       "          [-0.0545,  0.0480,  0.1220,  ...,  0.8301,  0.1526,  0.0011],\n",
       "          [-0.0692,  0.4670,  0.0975,  ...,  0.6948,  0.1433, -0.1912],\n",
       "          [-0.1229,  0.4565,  0.0893,  ...,  1.0312,  0.4116, -0.1406],\n",
       "          [-0.1172, -0.1667,  0.2517,  ...,  0.6753,  0.1477,  0.0921]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2500, -0.3269, -0.4443,  ..., -0.0075,  0.1212, -0.0785],\n",
       "          [ 0.3528,  0.2466, -0.3003,  ..., -0.2019, -0.2505,  0.1714]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1786, 0.3389, 0.1600, 0.2898, 0.0325],\n",
       "          [0.5005, 0.0038, 0.4924, 0.0020, 0.0014]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   52.162376  114.988388  198.105713  422.501099    0.940882      0  person\n",
       "  1  160.258331   71.779327  608.238770  423.866455    0.922328     17   horse\n",
       "  2  382.214478  160.587952  575.068665  424.242065    0.916911      0  person\n",
       "  3  403.843414   57.396759  516.755676  249.680573    0.880343      0  person\n",
       "  4  434.897552    0.368881  452.128510   59.217117    0.604570      0  person\n",
       "  5  570.331604  149.229492  640.000000  247.410889    0.538388      7   truck\n",
       "  6  570.102356  148.888992  639.654358  247.217804    0.529984      2     car,\n",
       "  'caption': ['woman in a blue shirt holding a little boy`s leg.',\n",
       "   'A woman wearing a blue t-shirt'],\n",
       "  'bbox_target': [382.86, 162.16, 189.03, 259.08]},\n",
       " 936: {'image_emb': tensor([[-0.2468,  0.8623, -0.2764,  ...,  1.0137, -0.1655, -0.1698],\n",
       "          [ 0.1192,  0.8994, -0.2834,  ...,  0.4438,  0.0360, -0.0793],\n",
       "          [-0.1582,  0.7041, -0.3872,  ...,  0.8965, -0.4595,  0.2169],\n",
       "          [ 0.1360,  0.8853, -0.3049,  ...,  0.5474,  0.0311, -0.0576]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0774,  0.2173, -0.6641,  ...,  0.4734, -0.2260,  0.1196],\n",
       "          [-0.0803,  0.5137, -0.2198,  ...,  0.3179, -0.3372, -0.0892]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0194, 0.6519, 0.0112, 0.3176],\n",
       "          [0.7407, 0.0678, 0.0912, 0.1002]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  412.641205  134.735626  528.902283  375.219818    0.934513     67   \n",
       "  1  133.245850   36.162598  539.488708  474.104980    0.917871      0   \n",
       "  2  101.626862  218.788666  216.194122  304.400726    0.895101     67   \n",
       "  \n",
       "           name  \n",
       "  0  cell phone  \n",
       "  1      person  \n",
       "  2  cell phone  ,\n",
       "  'caption': ['a black cellphone in a little girls hand',\n",
       "   'A black flip cell phone'],\n",
       "  'bbox_target': [409.11, 135.8, 119.76, 239.06]},\n",
       " 937: {'image_emb': tensor([[-0.0698,  0.4167, -0.1520,  ...,  1.3330, -0.2429,  0.3074],\n",
       "          [ 0.0487,  0.1221, -0.2142,  ...,  1.1299,  0.1385,  0.0231],\n",
       "          [-0.0224,  0.2861, -0.5005,  ...,  0.8096,  0.0365, -0.1425],\n",
       "          ...,\n",
       "          [ 0.6719,  0.5542, -0.4629,  ...,  1.2480, -0.1111,  0.6289],\n",
       "          [-0.0908,  0.4639, -0.1866,  ...,  1.1465,  0.1803,  0.3494],\n",
       "          [-0.0341,  0.3623, -0.0272,  ...,  0.6055,  0.2018,  0.3108]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0374, -0.0458, -0.1029,  ..., -0.0486, -0.1484,  0.1687],\n",
       "          [ 0.0321,  0.1631,  0.0266,  ..., -0.2013, -0.0499,  0.0093]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.3447e-05, 9.1612e-05, 1.7285e-06, 7.1526e-07, 5.8740e-01, 4.0991e-01,\n",
       "           2.1515e-03, 3.2473e-04],\n",
       "          [6.9737e-06, 4.4703e-06, 5.9605e-08, 0.0000e+00, 9.0820e-01, 8.9905e-02,\n",
       "           1.2827e-03, 4.5753e-04]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   400.577545  180.922531  486.989990  274.261749    0.899755     56   \n",
       "  1   230.017563  196.478409  306.726562  242.534393    0.892254     63   \n",
       "  2   301.689636  189.503998  332.533234  262.930786    0.859435     75   \n",
       "  3   332.819336  184.332855  355.981720  273.820068    0.844742     39   \n",
       "  4     0.246477  193.702606  167.363708  373.591003    0.758357     56   \n",
       "  5    41.009361  265.736053  178.976471  373.354065    0.730501     24   \n",
       "  6   174.262817  223.385544  478.146637  368.447876    0.718556     60   \n",
       "  7   351.513580  233.964615  400.341370  257.693695    0.674670     73   \n",
       "  8   143.919327  193.759125  226.064682  354.059326    0.588342     56   \n",
       "  9   312.281189  261.950684  500.000000  373.153351    0.573102     56   \n",
       "  10  414.994720  259.702789  500.000000  373.718353    0.474760     56   \n",
       "  11  388.068420  270.758453  467.471313  309.758728    0.402275     73   \n",
       "  12  285.084717  114.307693  352.728943  263.317383    0.341773     58   \n",
       "  13  361.451996  274.939484  391.097748  311.661011    0.332563     75   \n",
       "  14  284.344086  258.080902  300.285492  270.498566    0.294823     64   \n",
       "  15  364.157959  254.588654  403.798004  271.755798    0.278102     73   \n",
       "  \n",
       "              name  \n",
       "  0          chair  \n",
       "  1         laptop  \n",
       "  2           vase  \n",
       "  3         bottle  \n",
       "  4          chair  \n",
       "  5       backpack  \n",
       "  6   dining table  \n",
       "  7           book  \n",
       "  8          chair  \n",
       "  9          chair  \n",
       "  10         chair  \n",
       "  11          book  \n",
       "  12  potted plant  \n",
       "  13          vase  \n",
       "  14         mouse  \n",
       "  15          book  ,\n",
       "  'caption': ['Chair holding blue jacket.',\n",
       "   'A white chair with a blue and black jacket draped over the back.'],\n",
       "  'bbox_target': [140.83, 153.17, 62.76, 197.96]},\n",
       " 938: {'image_emb': tensor([[-0.0180,  0.2976, -0.1060,  ...,  0.7490, -0.0508,  0.1431],\n",
       "          [ 0.0027,  0.0664, -0.2411,  ...,  0.7554,  0.1168,  0.1925]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0972,  0.2781,  0.1560,  ...,  0.3435, -0.2881, -0.1015],\n",
       "          [ 0.1798, -0.1741,  0.1478,  ...,  0.2373, -0.4075,  0.0126]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.8481, 0.1520],\n",
       "          [0.6826, 0.3174]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0    2.387118   17.963505  459.461304  361.031097    0.769738     50  broccoli\n",
       "  1  311.248077   19.649971  388.058136   85.364044    0.365788     50  broccoli\n",
       "  2  256.994141  198.000687  347.910095  299.458282    0.345138     50  broccoli\n",
       "  3  151.817352  239.185760  245.309662  355.323792    0.311884     50  broccoli\n",
       "  4  322.414276  134.729187  412.067993  212.579056    0.303736     50  broccoli,\n",
       "  'caption': ['large pieces of cut up brocolli',\n",
       "   'Broccoli curry in the plate'],\n",
       "  'bbox_target': [10.98, 22.38, 439.19, 342.91]},\n",
       " 939: {'image_emb': tensor([[ 0.1576,  0.1182,  0.0276,  ...,  0.7188,  0.0807, -0.0965],\n",
       "          [ 0.2205,  0.4028, -0.0053,  ...,  0.1284, -0.2208,  0.0068],\n",
       "          [-0.2135,  0.3125, -0.2418,  ...,  0.6694,  0.0917, -0.6011],\n",
       "          [ 0.0937,  0.4724,  0.0936,  ...,  0.0875, -0.2920, -0.0060],\n",
       "          [ 0.2220,  0.4739, -0.1039,  ..., -0.1179, -0.2771,  0.1417],\n",
       "          [ 0.1589,  0.6157, -0.0743,  ..., -0.1400, -0.0930, -0.2026]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0760,  0.1525,  0.1838,  ...,  0.4287, -0.0513,  0.1957],\n",
       "          [ 0.0859,  0.0654,  0.0668,  ...,  0.4653, -0.1305,  0.0585]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.0079e-03, 6.2939e-01, 6.0730e-03, 8.6487e-02, 1.4722e-01, 1.2781e-01],\n",
       "          [1.1683e-05, 3.0640e-01, 1.1950e-03, 8.6426e-02, 3.6377e-01, 2.4231e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  597.940979  121.665039  639.641541  195.099487    0.861330      5   \n",
       "  1  122.303406  161.419250  276.765289  300.026550    0.860434      3   \n",
       "  2  232.807541  141.546509  379.851807  193.695618    0.859143      5   \n",
       "  3  265.749512  155.668213  452.598328  324.166138    0.826646      3   \n",
       "  4   66.838409  110.165497  609.084595  391.776947    0.787102      7   \n",
       "  5  195.622498  144.691895  209.789673  176.900116    0.686004      9   \n",
       "  6   71.154327  113.099823  606.628906  399.579193    0.658482      3   \n",
       "  7  407.653046   42.759682  474.045868  104.620834    0.403566     17   \n",
       "  \n",
       "              name  \n",
       "  0            bus  \n",
       "  1     motorcycle  \n",
       "  2            bus  \n",
       "  3     motorcycle  \n",
       "  4          truck  \n",
       "  5  traffic light  \n",
       "  6     motorcycle  \n",
       "  7          horse  ,\n",
       "  'caption': ['The motorcycle at the rear of the trailer.',\n",
       "   'A blue and green checkered bike behind a matching bike on a trailer'],\n",
       "  'bbox_target': [269.08, 156.25, 183.69, 165.83]},\n",
       " 940: {'image_emb': tensor([[-0.0049,  0.3162, -0.1261,  ...,  0.6147,  0.2769, -0.0835],\n",
       "          [-0.1088,  0.5337,  0.1433,  ...,  0.3301,  0.0483, -0.3066],\n",
       "          [ 0.0663,  0.3677, -0.4417,  ...,  0.2847,  0.1920, -0.4043],\n",
       "          ...,\n",
       "          [-0.1958,  0.4148, -0.1770,  ...,  0.6123, -0.0437, -0.3362],\n",
       "          [-0.0937, -0.1615, -0.2080,  ...,  0.7715,  0.0913,  0.1848],\n",
       "          [ 0.2388,  0.0948, -0.4395,  ...,  0.0374,  0.0930, -0.0540]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-2.5928e-01, -1.2036e-01, -2.8061e-02,  2.1155e-01,  1.0822e-01,\n",
       "            2.6764e-02,  1.8661e-02, -5.6445e-01,  1.7261e-01, -1.9690e-01,\n",
       "           -1.9653e-01, -1.6101e-01,  2.7881e-01, -1.4587e-01, -2.7759e-01,\n",
       "           -1.1401e-01,  1.0645e-01,  1.8530e-01,  7.9285e-02, -2.0313e-03,\n",
       "            7.7246e-01,  5.9326e-02, -1.9873e-01,  1.6943e-01,  4.3457e-01,\n",
       "            8.3923e-02, -1.8506e-01,  2.2046e-01,  2.9712e-01,  1.3077e-02,\n",
       "           -3.3173e-02,  8.2642e-02, -1.4758e-01, -1.4209e-01, -6.4011e-03,\n",
       "           -7.4036e-02,  2.4246e-02,  2.2485e-01, -2.2241e-01,  7.0496e-02,\n",
       "            1.0529e-01,  4.5923e-01, -3.1616e-01, -1.6577e-01,  3.4424e-01,\n",
       "            2.1680e-01,  6.5967e-01,  5.1904e-01,  2.5436e-02, -1.3843e-01,\n",
       "           -3.2886e-01, -3.6304e-01,  8.3679e-02,  6.5063e-02, -1.7258e-02,\n",
       "            1.0724e-01, -2.6807e-01, -4.3921e-01, -3.4106e-01,  4.1870e-01,\n",
       "            2.1375e-01,  3.6523e-01, -3.0615e-01, -2.1204e-01, -3.5791e-01,\n",
       "            7.6538e-02,  1.0022e-01,  4.7144e-01, -3.2104e-02,  1.8799e-01,\n",
       "            1.6174e-01,  9.5947e-02,  1.9092e-01,  1.8701e-01,  2.2852e-01,\n",
       "           -5.4352e-02, -4.2773e-01, -4.0588e-02, -1.2756e-02, -4.2847e-02,\n",
       "           -7.1869e-03,  1.0602e-01,  3.7079e-02, -1.2622e-01,  8.9264e-03,\n",
       "            4.9707e-01,  7.1924e-01, -9.6069e-02,  2.7908e-02,  2.5586e-01,\n",
       "            1.1896e-01,  2.9785e-01, -1.2471e+00,  5.6641e-01, -7.6050e-02,\n",
       "            5.2719e-03,  1.7346e-01, -3.8062e-01,  2.3645e-01,  3.5339e-02,\n",
       "           -4.3884e-02, -4.7760e-02,  3.7036e-01,  2.5830e-01, -3.6719e-01,\n",
       "            1.3843e-01,  1.4905e-01, -2.7191e-02,  3.1519e-01,  6.2866e-02,\n",
       "           -1.6541e-01, -9.3201e-02,  2.8271e-01, -2.1948e-01, -2.3413e-01,\n",
       "           -5.5762e-01,  2.9468e-01,  2.7441e-01,  2.7344e-01, -2.3779e-01,\n",
       "           -2.4060e-01, -5.2148e-01,  2.9102e-01, -3.9233e-01,  7.4585e-02,\n",
       "           -3.7866e-01, -1.4392e-01,  1.1188e-01, -3.0981e-01,  2.7319e-01,\n",
       "           -1.6235e-01, -2.7124e-01, -7.5623e-02,  4.9688e+00,  1.7944e-02,\n",
       "           -1.5906e-01, -4.1187e-01, -4.4604e-01, -1.3513e-01, -3.7866e-01,\n",
       "            2.4011e-01,  5.7910e-01, -1.0162e-01, -1.7554e-01, -4.4373e-02,\n",
       "           -9.9060e-02, -8.5831e-03, -2.5854e-01,  4.3579e-02,  1.5819e-04,\n",
       "           -4.1504e-01,  9.3506e-02,  1.9470e-01,  1.5778e-02,  1.0162e-01,\n",
       "           -5.8060e-03,  1.9800e-01, -4.0747e-01, -1.6528e-01,  1.4795e-01,\n",
       "           -9.4238e-02, -3.4570e-01,  6.5771e-01,  7.2754e-02, -9.8328e-02,\n",
       "           -9.0942e-02,  3.3447e-01, -3.2373e-01,  1.3855e-01,  1.1615e-01,\n",
       "           -1.4969e-02, -4.6387e-01,  1.1337e-02, -4.6191e-01, -3.9160e-01,\n",
       "           -5.5615e-01, -2.3730e-01,  7.7454e-02,  4.6167e-01, -1.1865e-01,\n",
       "           -2.9639e-01,  2.4219e-01,  1.5381e-01, -8.7341e-02,  2.1765e-01,\n",
       "            2.0752e-02,  2.6270e-01,  1.1249e-01,  1.7358e-01,  4.4342e-02,\n",
       "            8.9905e-02, -4.0503e-01,  3.3008e-01, -7.1777e-02,  5.2930e-01,\n",
       "           -1.1639e-01, -1.4417e-01, -1.1316e-01, -1.8713e-01,  3.6011e-01,\n",
       "           -1.0297e-01,  3.2257e-02,  1.3843e-01, -3.0591e-01, -2.6294e-01,\n",
       "            2.8275e-02,  2.6270e-01, -9.5154e-02, -5.7129e-02, -2.5977e-01,\n",
       "           -5.1460e-03, -1.3989e-01, -8.1421e-02, -1.8384e-01, -2.7246e-01,\n",
       "            8.2764e-02, -8.2626e-03, -1.7700e-01,  4.7510e-01,  2.3462e-01,\n",
       "            4.6631e-01,  1.9394e-02,  1.0358e-01,  6.4087e-02,  8.9294e-02,\n",
       "           -3.8281e-01,  2.9663e-01, -5.3192e-02,  1.7615e-01,  4.4312e-01,\n",
       "            3.8721e-01,  3.9478e-01, -1.2292e-01, -1.5002e-01, -1.5100e-01,\n",
       "           -1.5381e-02, -4.2938e-02, -2.6538e-01, -6.5247e-02,  2.2363e-01,\n",
       "           -2.0203e-01,  2.9370e-01, -5.9631e-02, -6.3416e-02, -1.2708e-01,\n",
       "           -4.7241e-01, -1.2964e-01,  2.1240e-01,  8.0688e-02, -7.8186e-02,\n",
       "           -7.1594e-02, -2.0288e-01,  5.4749e-02, -1.7419e-01, -3.3960e-01,\n",
       "           -2.3535e-01,  8.5938e-02,  3.2654e-02, -7.7759e-02,  2.6929e-01,\n",
       "           -5.8441e-02, -7.4768e-02, -1.4450e-02, -2.0264e-02,  2.1655e-01,\n",
       "           -9.4849e-02, -2.9395e-01, -3.0411e-02, -3.9282e-01,  1.0828e-01,\n",
       "           -3.5327e-01, -2.0642e-01, -3.6060e-01, -5.2094e-02, -1.0858e-01,\n",
       "           -2.1469e-02,  4.0894e-01,  5.3369e-01,  4.1626e-02,  2.3608e-01,\n",
       "            1.0498e-01,  9.0149e-02,  7.0068e-01,  2.5830e-01, -1.7605e-03,\n",
       "           -1.0089e-01,  1.5601e-01,  2.3621e-01,  2.3621e-01, -5.0879e-01,\n",
       "            5.2148e-01,  5.3076e-01,  8.8074e-02, -1.8030e-01, -8.4167e-02,\n",
       "            2.6025e-01, -4.7943e-02,  5.6348e-01,  2.2070e-01, -1.7932e-01,\n",
       "            4.4312e-02, -7.9285e-02, -2.9077e-01,  2.6245e-01,  3.9600e-01,\n",
       "           -8.8440e-02,  2.4094e-02, -5.1855e-01,  4.1772e-01,  4.7684e-03,\n",
       "           -1.3318e-01,  2.0325e-01,  4.9727e+00, -2.5940e-02, -7.9407e-02,\n",
       "            1.5088e-01,  5.7648e-02, -2.5464e-01,  3.5547e-01,  2.4585e-01,\n",
       "            4.5020e-01,  1.7603e-01,  2.5732e-01,  1.3660e-01, -6.0303e-02,\n",
       "            4.5581e-01, -3.2562e-02,  3.2373e-01,  3.4302e-02, -1.5537e+00,\n",
       "           -2.0630e-01, -2.9688e-01, -3.5156e-02,  1.2659e-01,  1.8457e-01,\n",
       "           -4.1479e-01,  3.4473e-01,  1.3501e-01, -1.3953e-01, -7.3059e-02,\n",
       "           -4.1479e-01, -3.2715e-01,  5.5322e-01, -1.0266e-01, -1.4600e-01,\n",
       "            1.1853e-01,  3.5864e-01, -2.0264e-01, -1.4246e-01, -2.9175e-01,\n",
       "            2.9688e-01, -5.2124e-02, -1.3611e-01, -1.2672e-02,  4.4922e-02,\n",
       "           -4.1016e-01,  1.9617e-01,  9.1675e-02,  1.6626e-01,  6.3599e-02,\n",
       "           -4.3066e-01, -1.1853e-01,  1.8951e-02,  6.4453e-01,  2.8027e-01,\n",
       "            9.1064e-02,  1.5442e-01, -2.3499e-02,  2.1204e-01, -3.0688e-01,\n",
       "           -1.1395e-01, -3.6230e-01,  2.6062e-02,  1.6785e-01,  6.7261e-02,\n",
       "           -4.7363e-01, -3.1226e-01, -1.7200e-01,  1.5289e-02, -4.3091e-01,\n",
       "            2.9150e-01, -2.4622e-01,  5.4248e-01,  8.7952e-02,  2.2546e-01,\n",
       "           -9.6191e-02, -1.6675e-01,  1.9336e-01, -2.7612e-01, -1.1938e-01,\n",
       "           -6.3232e-01,  7.6355e-02, -1.3412e-02, -8.7830e-02,  3.4790e-02,\n",
       "            2.0178e-01,  1.4771e-01,  1.7944e-01, -3.0127e-01,  2.0837e-01,\n",
       "            3.3008e-01,  2.3523e-01,  2.3633e-01, -9.2957e-02, -6.0791e-02,\n",
       "            7.1899e-02, -3.6621e-01, -6.7688e-02,  3.1323e-01,  2.9163e-03,\n",
       "           -3.6084e-01,  2.3560e-02, -6.6284e-02,  3.0859e-01,  2.2925e-01,\n",
       "           -1.3354e-01, -3.3472e-01, -5.4291e-02, -3.8544e-02, -4.1187e-01,\n",
       "            2.1277e-01, -2.9199e-01,  2.9419e-01, -4.5044e-01,  4.7095e-01,\n",
       "            1.5979e-01, -1.2457e-01,  7.5378e-02,  5.7556e-02,  2.8174e-01,\n",
       "           -5.3857e-01, -2.3877e-01, -3.1421e-01, -2.0462e-02, -8.0200e-02,\n",
       "           -4.2920e-01,  7.5989e-02,  1.5283e-01, -3.3173e-02,  2.2614e-02,\n",
       "           -3.5553e-02, -3.4375e-01,  1.3708e-01,  7.6721e-02,  1.8713e-01,\n",
       "           -1.4328e-02,  5.0659e-02, -5.3027e-01,  1.2091e-01,  2.2058e-01,\n",
       "            1.5601e-01,  1.3916e-01,  1.8799e-01, -2.4255e-01, -1.7981e-01,\n",
       "            3.4882e-02,  3.8422e-02, -1.4050e-01,  7.5562e-02, -3.2471e-01,\n",
       "           -1.3672e-01, -1.5271e-01,  5.3040e-02,  2.1594e-01, -1.2524e-01,\n",
       "           -3.0347e-01, -3.1311e-02,  1.9653e-01,  3.7769e-01, -2.8223e-01,\n",
       "            2.0105e-01,  1.4258e-01,  8.1177e-02,  5.6946e-02, -4.2053e-02,\n",
       "            4.8065e-02,  1.3623e-01,  2.0117e-01, -3.5767e-01,  7.6843e-02,\n",
       "           -3.6224e-02, -3.4863e-01, -3.2440e-02, -1.3130e-02, -2.1240e-01,\n",
       "           -5.4871e-02, -8.7357e-03, -2.9907e-01, -4.1748e-01, -4.4873e-01,\n",
       "            1.7090e-03, -1.3130e-02, -1.1847e-01,  1.1963e+00,  2.7881e-01,\n",
       "            2.5952e-01,  4.2847e-01,  1.8030e-01, -3.4180e-01,  3.0322e-01,\n",
       "            6.6992e-01, -6.0303e-02,  4.3335e-02,  3.5303e-01,  2.3865e-01,\n",
       "           -1.8579e-01,  4.1016e-01,  1.8140e-01,  3.1921e-02,  1.7114e-01,\n",
       "           -1.2993e-02, -4.0991e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.9848e-04, 8.7595e-04, 8.6084e-01, 3.1233e-04, 5.4512e-03, 1.3435e-04,\n",
       "           1.3208e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  231.548920    1.206650  637.816162  165.292175    0.940895      0   \n",
       "  1  129.219101  101.457077  533.640625  255.869568    0.913126     53   \n",
       "  2    2.088348  105.594635  637.725830  413.172241    0.824840     60   \n",
       "  3   45.765911    1.957932  167.097641  127.023788    0.823888     56   \n",
       "  4    5.427155  254.870789  637.955444  423.287048    0.782219     53   \n",
       "  5    0.275808    0.922577   45.234573  117.144836    0.742142     56   \n",
       "  6  161.630280    0.000000  259.542023  129.049591    0.651284     56   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1         pizza  \n",
       "  2  dining table  \n",
       "  3         chair  \n",
       "  4         pizza  \n",
       "  5         chair  \n",
       "  6         chair  ,\n",
       "  'caption': ['slice of pizza directly in the middle closest to the camera'],\n",
       "  'bbox_target': [285.65, 252.95, 145.24, 115.42]},\n",
       " 941: {'image_emb': tensor([[ 0.0660,  0.2764, -0.2441,  ...,  0.6797, -0.0850, -0.0254],\n",
       "          [ 0.2316,  0.1937, -0.1476,  ...,  0.7031, -0.1150,  0.1327],\n",
       "          [-0.2812,  0.2842, -0.1873,  ...,  1.4443, -0.2805,  0.1788],\n",
       "          [ 0.3772,  0.2424, -0.2424,  ...,  0.5464, -0.0889,  0.0707]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1603, -0.4634,  0.2209,  ..., -0.2206, -0.4280, -0.0333],\n",
       "          [ 0.1827, -0.2544,  0.0358,  ..., -0.4443, -0.2605, -0.2039]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5557, 0.2076, 0.0014, 0.2352],\n",
       "          [0.3608, 0.4089, 0.0007, 0.2294]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  178.949829  105.331894  413.859619  407.661316    0.891826     48   \n",
       "  1  323.556427  126.586380  575.224243  395.342468    0.816286     48   \n",
       "  2   33.492279    1.238602  476.840973   94.198135    0.768622     44   \n",
       "  3    3.368652    4.796265  636.563599  412.192383    0.535786     60   \n",
       "  \n",
       "             name  \n",
       "  0      sandwich  \n",
       "  1      sandwich  \n",
       "  2         spoon  \n",
       "  3  dining table  ,\n",
       "  'caption': ['Half a piece of toast with nothing spread on it.',\n",
       "   'a piece of bread with nothing on it'],\n",
       "  'bbox_target': [316.56, 124.17, 258.39, 274.23]},\n",
       " 942: {'image_emb': tensor([[-0.1558, -0.2188,  0.0656,  ...,  0.1175,  0.0152,  0.2161],\n",
       "          [-0.4849, -0.1124,  0.0500,  ...,  0.6846, -0.0208,  0.2820]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0558,  0.1033, -0.0120,  ...,  0.2996,  0.1989,  0.0840],\n",
       "          [ 0.1504,  0.2573, -0.1163,  ..., -0.0598,  0.1455, -0.0331]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.9888, 0.0113],\n",
       "          [0.9956, 0.0045]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class name\n",
       "  0  254.145172   94.374146  622.271606  402.271484    0.901073     41  cup\n",
       "  1   29.609070  102.950760  258.855713  368.107056    0.572190     41  cup\n",
       "  2   57.328392  162.726776  221.092010  292.342804    0.548460     15  cat\n",
       "  3  291.159943  167.980591  478.809174  315.080444    0.433677     16  dog,\n",
       "  'caption': ['A mug with a dog face stenciled on it.',\n",
       "   'A mug with a dog face on it.'],\n",
       "  'bbox_target': [258.51, 98.63, 366.49, 305.23]},\n",
       " 943: {'image_emb': tensor([[ 0.2676,  0.0566, -0.1328,  ...,  0.6494, -0.0441, -0.0966],\n",
       "          [-0.1608,  0.6040, -0.1732,  ...,  1.0684,  0.4543, -0.0326],\n",
       "          [-0.0199,  0.0822, -0.1329,  ...,  0.7534, -0.1089,  0.0188],\n",
       "          [ 0.1704,  0.0693,  0.1930,  ...,  0.5396, -0.2039,  0.2340],\n",
       "          [ 0.0493,  0.1190,  0.1547,  ...,  0.6782, -0.2362,  0.1692]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 3.4497e-01, -1.5479e-01,  4.0259e-01,  2.7783e-01, -1.2079e-01,\n",
       "            5.5237e-02, -1.3501e-01, -6.4893e-01,  2.2705e-02,  2.9663e-01,\n",
       "            1.5051e-01,  1.9324e-01,  2.1985e-01, -1.4038e-01,  1.0272e-01,\n",
       "            3.9703e-02,  1.4539e-01,  2.7197e-01, -1.9531e-01,  1.6577e-01,\n",
       "            8.7891e-02,  1.4246e-01, -2.3999e-01,  2.3901e-01,  4.7534e-01,\n",
       "           -1.1920e-01, -9.5581e-02,  4.3872e-01, -1.6449e-02,  4.2358e-01,\n",
       "            9.8206e-02,  2.5293e-01, -9.7412e-02,  1.6174e-01, -2.2620e-01,\n",
       "            6.2988e-02,  1.7834e-01,  8.3984e-02, -4.3793e-02,  6.8909e-02,\n",
       "           -1.5881e-01, -1.2354e-01,  1.6614e-01, -2.6099e-01,  1.7578e-01,\n",
       "            2.1265e-01,  6.5381e-01,  5.7275e-01,  2.8076e-01, -2.6367e-01,\n",
       "           -3.5547e-01, -3.9404e-01,  6.7017e-02,  1.3782e-01,  1.0645e-01,\n",
       "            1.8101e-03, -2.0096e-02, -2.9102e-01, -2.5171e-01,  2.4561e-01,\n",
       "            2.8638e-01,  3.8849e-02, -2.9053e-01,  1.2976e-01,  6.8665e-02,\n",
       "            2.7328e-02,  2.6392e-01,  3.8013e-01,  2.7649e-02,  5.1819e-02,\n",
       "            2.1729e-01, -1.7236e-01,  5.0488e-01,  4.5068e-01,  3.3936e-01,\n",
       "            2.6880e-01,  2.0056e-01, -2.1582e-01,  5.2826e-02,  1.7468e-01,\n",
       "           -1.0852e-01,  3.2446e-01,  1.4503e-02,  3.1128e-01, -1.1719e-02,\n",
       "            4.5068e-01,  3.6768e-01, -4.8035e-02, -2.0715e-01,  3.9551e-01,\n",
       "            2.1765e-01,  1.8018e-01, -1.2988e+00,  8.6572e-01, -9.6619e-02,\n",
       "           -1.7065e-01,  1.5625e-01,  2.3425e-01,  1.6064e-01,  9.8389e-02,\n",
       "            7.3181e-02,  2.7295e-01, -2.9175e-02, -1.0876e-01, -5.3857e-01,\n",
       "            3.6523e-01,  2.2156e-02,  3.5889e-02, -3.9368e-02,  1.4331e-01,\n",
       "           -1.1481e-01,  1.0425e-01,  2.7148e-01, -3.4619e-01, -1.8323e-01,\n",
       "           -4.0625e-01,  2.7661e-01,  4.5441e-02,  2.4451e-01,  8.2321e-03,\n",
       "           -2.4915e-01, -7.7686e-01,  1.3196e-01, -9.3201e-02,  2.2546e-01,\n",
       "            2.8076e-01, -1.0327e-01,  9.7595e-02,  7.6675e-04,  5.9424e-01,\n",
       "            1.1682e-01,  1.5778e-02,  9.6191e-02,  5.0977e+00, -1.2891e-01,\n",
       "           -9.7046e-02, -3.9453e-01, -6.1475e-01, -1.7322e-01, -3.4302e-01,\n",
       "            5.1465e-01,  2.3450e-01, -1.4734e-01, -7.8247e-02, -3.0151e-01,\n",
       "           -1.1011e-01,  1.8445e-01, -1.1200e-01, -2.2913e-01, -1.4319e-01,\n",
       "           -2.5708e-01,  4.0741e-02,  5.2637e-01,  1.5576e-01,  1.6064e-01,\n",
       "            1.4380e-01,  1.1145e-01, -1.7004e-01, -9.7900e-02,  2.3486e-01,\n",
       "           -2.5955e-02,  6.9031e-02,  3.6084e-01,  6.3965e-02, -4.7314e-01,\n",
       "           -3.0444e-01,  5.7373e-02, -3.1787e-01,  1.6724e-01,  8.9172e-02,\n",
       "            2.7563e-01, -4.3750e-01,  9.7229e-02, -3.2031e-01, -2.1802e-01,\n",
       "           -1.4465e-01, -9.6436e-02,  1.4575e-01,  2.9858e-01,  2.7979e-01,\n",
       "           -2.4670e-01,  2.5317e-01,  7.2937e-02, -2.1106e-01, -2.7710e-01,\n",
       "            4.1870e-01,  4.7379e-03,  1.9974e-02,  3.2764e-01, -1.6418e-01,\n",
       "            5.1025e-01, -3.8379e-01,  5.2399e-02, -1.5906e-01,  3.6890e-01,\n",
       "            2.9266e-02, -1.8115e-01,  6.8909e-02,  1.1168e-03, -2.9980e-01,\n",
       "           -1.9263e-01, -5.8838e-02,  2.0911e-01, -1.5881e-01,  1.6772e-01,\n",
       "            2.3242e-01,  1.6785e-01, -3.2788e-01,  2.3315e-01, -2.7979e-01,\n",
       "            1.2012e-01,  9.3567e-02, -4.4342e-02, -1.8628e-01,  2.7725e-02,\n",
       "            2.0984e-01, -2.0007e-01, -7.1289e-02,  9.3445e-02, -2.0447e-01,\n",
       "            1.9482e-01,  3.5858e-02,  8.3557e-02, -1.4091e-02,  2.2937e-01,\n",
       "           -2.8613e-01,  9.3567e-02,  7.1045e-02, -1.7480e-01,  1.6418e-01,\n",
       "           -1.0211e-01,  1.7065e-01,  1.5625e-01, -2.3926e-01, -3.0859e-01,\n",
       "            3.1525e-02, -8.9172e-02, -1.8036e-02, -1.1523e-01, -2.0264e-01,\n",
       "           -2.0020e-01,  1.4124e-01,  2.9688e-01,  4.0131e-02, -4.8187e-02,\n",
       "           -1.9080e-01,  8.2275e-02,  1.2375e-02, -5.8380e-02,  3.1708e-02,\n",
       "           -1.1818e-02, -5.5084e-02,  1.5564e-01, -2.9150e-01, -6.3232e-01,\n",
       "           -1.4246e-01, -1.7029e-01,  3.3722e-02, -3.5492e-02,  1.3275e-02,\n",
       "           -8.4900e-02, -8.1299e-02,  4.1008e-04,  1.2642e-02, -3.8306e-01,\n",
       "           -1.9543e-01, -1.0913e-01,  2.0117e-01, -3.6548e-01,  1.9580e-01,\n",
       "           -1.1395e-01, -2.0337e-01, -3.6572e-01,  1.0132e-01, -7.6818e-04,\n",
       "           -1.2683e-01,  2.2522e-02,  3.2373e-01,  2.0032e-01,  3.8422e-02,\n",
       "            3.0835e-01, -2.6514e-01,  2.7319e-01,  5.1172e-01, -1.6235e-01,\n",
       "            1.3635e-01,  9.4482e-02, -4.5502e-02,  4.9487e-01,  5.2216e-02,\n",
       "            3.3911e-01,  3.1250e-01, -4.1443e-02, -7.2441e-03,  1.4307e-01,\n",
       "           -1.4673e-01, -2.0752e-01,  3.3521e-01,  2.3327e-03,  1.9238e-01,\n",
       "           -2.9175e-01, -3.5938e-01, -3.5718e-01, -1.8753e-02,  2.2302e-01,\n",
       "            2.3499e-02, -5.9723e-02, -1.3818e-01,  4.7046e-01,  6.3538e-02,\n",
       "           -3.3130e-01, -1.0968e-01,  5.0977e+00,  1.9873e-01,  3.5645e-02,\n",
       "            3.0960e-02, -2.1875e-01,  3.7720e-02,  7.6111e-02,  1.7773e-01,\n",
       "            2.7832e-01,  3.5327e-01,  1.1823e-01,  1.7395e-01, -2.4402e-01,\n",
       "            8.8428e-01,  7.6721e-02,  3.8916e-01,  1.3330e-01, -2.0332e+00,\n",
       "           -2.5635e-01, -4.2749e-01,  1.0269e-02, -3.0737e-01, -1.2903e-01,\n",
       "           -1.3733e-01,  3.0078e-01,  9.1064e-02, -7.2754e-02, -1.3916e-01,\n",
       "           -3.2666e-01, -1.8164e-01,  5.7080e-01,  6.8359e-02, -1.5442e-01,\n",
       "            6.0120e-02,  2.8198e-01, -3.0249e-01, -1.7297e-01,  2.8589e-01,\n",
       "            3.3789e-01,  2.8906e-01,  1.8079e-01, -6.0120e-02, -8.4900e-02,\n",
       "           -3.0054e-01,  1.0773e-01, -1.8713e-01, -2.6489e-01,  4.0894e-01,\n",
       "           -2.4292e-01, -2.1533e-01, -1.1377e-01,  1.6443e-01,  5.9479e-02,\n",
       "            1.8823e-01,  2.5732e-01,  1.1334e-01,  6.5041e-04,  2.7485e-03,\n",
       "            1.4380e-01, -1.0217e-01, -2.1509e-01,  3.9990e-01,  1.0168e-01,\n",
       "           -2.2046e-01, -1.5430e-01, -1.3867e-01, -1.3757e-01, -2.1362e-01,\n",
       "            2.1399e-01, -2.1997e-01,  4.1479e-01, -2.9126e-01,  2.8467e-01,\n",
       "           -2.0801e-01, -1.4486e-03,  2.7374e-02, -4.0674e-01, -2.1704e-01,\n",
       "           -5.8057e-01,  2.7515e-01,  7.0312e-02, -2.6196e-01,  1.1432e-01,\n",
       "           -2.3163e-02, -1.4076e-02,  1.4490e-01, -3.0591e-01,  3.0347e-01,\n",
       "            3.2410e-02, -1.6736e-01,  2.2864e-01,  3.0960e-02, -3.5596e-01,\n",
       "            2.7496e-02,  3.1174e-02,  1.4282e-01,  2.5122e-01, -1.9373e-01,\n",
       "           -1.4514e-01, -2.8027e-01,  2.3865e-02,  4.4165e-01, -1.0596e-01,\n",
       "           -1.6687e-01, -4.2285e-01,  1.2817e-01,  4.8413e-01, -3.8239e-02,\n",
       "            3.7988e-01, -3.6670e-01,  4.5105e-02, -3.5425e-01,  2.1021e-01,\n",
       "           -1.5588e-01, -1.7969e-01,  4.1077e-02,  8.0322e-02,  2.0593e-01,\n",
       "           -6.5674e-01, -3.7012e-01, -2.0166e-01,  4.5264e-01, -4.8511e-01,\n",
       "           -2.5732e-01, -1.7212e-01,  5.1544e-02, -5.4382e-02,  1.0902e-02,\n",
       "           -2.6489e-02, -3.1689e-01, -2.4084e-01, -6.2305e-01,  2.0349e-01,\n",
       "            1.6956e-01,  2.9443e-01, -4.1699e-01,  2.7002e-01,  4.7974e-01,\n",
       "            1.5955e-01, -6.6185e-03,  3.9624e-01, -2.3926e-01,  9.5520e-02,\n",
       "            3.0566e-01,  3.4882e-02,  1.1780e-01,  3.9673e-01, -3.0420e-01,\n",
       "           -2.7051e-01,  1.3049e-01, -1.6052e-01,  2.4646e-01, -5.7648e-02,\n",
       "           -3.7793e-01,  2.2144e-01, -7.3303e-02, -4.2786e-02, -9.9854e-02,\n",
       "           -1.5405e-01,  1.7542e-01, -4.7485e-02,  2.9688e-01,  4.4342e-02,\n",
       "            8.2855e-03,  6.8176e-02, -1.0339e-01,  1.1597e-01,  7.5073e-02,\n",
       "           -2.9248e-01, -3.8794e-01,  6.1096e-02, -1.6260e-01, -1.7310e-01,\n",
       "            5.0690e-02,  2.7237e-03, -2.9678e-02, -1.4722e-01, -1.6553e-01,\n",
       "           -2.8125e-01, -1.1566e-01, -1.9394e-02,  1.0020e+00,  2.3901e-01,\n",
       "           -4.1687e-02,  5.8496e-01,  6.2402e-01, -3.8281e-01,  5.9277e-01,\n",
       "            3.1201e-01, -5.8990e-02, -7.5073e-02,  5.2734e-01,  9.6497e-02,\n",
       "           -2.0044e-01,  3.6914e-01, -1.6846e-01, -6.8176e-02,  6.0974e-02,\n",
       "           -4.4189e-01, -6.4453e-02]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.7026, 0.0022, 0.2111, 0.0595, 0.0244]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  324.870300    0.304199  542.691345  259.091248    0.888600     48   \n",
       "  1  203.927185    1.323029  406.361572  108.712799    0.823510     45   \n",
       "  2  211.805420  149.789307  553.842834  354.213257    0.762107     48   \n",
       "  3    0.000000    0.000000  640.000000  354.667236    0.703676     60   \n",
       "  \n",
       "             name  \n",
       "  0      sandwich  \n",
       "  1          bowl  \n",
       "  2      sandwich  \n",
       "  3  dining table  ,\n",
       "  'caption': ['bottom slice of sandwich'],\n",
       "  'bbox_target': [211.48, 151.43, 343.86, 208.57]},\n",
       " 944: {'image_emb': tensor([[ 0.1081,  0.5493, -0.2233,  ...,  1.0469, -0.0407, -0.0630],\n",
       "          [ 0.1061,  0.4849,  0.0833,  ...,  0.7510,  0.2256, -0.2484],\n",
       "          [-0.0914,  0.1644, -0.3079,  ...,  0.9590,  0.0992, -0.4543],\n",
       "          [ 0.3806, -0.0104, -0.1349,  ...,  1.0566, -0.0448, -0.2133]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1917,  0.1896, -0.1765,  ...,  0.2822,  0.1652, -0.0154],\n",
       "          [ 0.1047,  0.1294, -0.5459,  ..., -0.1964,  0.0729, -0.0254]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2327, 0.7056, 0.0458, 0.0161],\n",
       "          [0.0297, 0.0154, 0.0019, 0.9531]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   38.442593   67.579414  164.447647  369.774353    0.936616      0  person\n",
       "  1  191.806778   22.440004  387.068420  368.906067    0.929790      0  person\n",
       "  2  238.484818  205.373642  499.074036  336.474915    0.786079     57   couch\n",
       "  3   67.849503  136.112366   79.614464  147.454285    0.619686     65  remote\n",
       "  4  429.770264  301.718262  499.320038  372.698792    0.515331     57   couch\n",
       "  5  182.414352  164.374420  195.049088  194.591934    0.480487     39  bottle\n",
       "  6  187.058594   94.536758  241.612869  162.811783    0.454028     65  remote\n",
       "  7  103.207993  146.541855  113.541344  157.226212    0.384770     65  remote\n",
       "  8   89.966484  146.476562  113.661278  172.257324    0.280543     65  remote,\n",
       "  'caption': ['man in black tee-shirt',\n",
       "   'the person on the right with the wii remote'],\n",
       "  'bbox_target': [196.79, 22.8, 189.19, 347.98]},\n",
       " 945: {'image_emb': tensor([[-1.8933e-01,  3.5425e-01, -4.0802e-02,  ...,  9.4531e-01,\n",
       "            3.7750e-02, -2.1814e-01],\n",
       "          [ 7.6294e-06,  4.4482e-01,  1.5747e-01,  ...,  9.5703e-01,\n",
       "           -2.0569e-02, -2.2632e-01],\n",
       "          [-2.5439e-01,  4.5312e-01,  4.2236e-01,  ...,  1.0918e+00,\n",
       "            5.6213e-02, -4.1748e-01],\n",
       "          [-6.9946e-02,  3.1006e-01,  1.5295e-01,  ...,  8.3447e-01,\n",
       "           -1.5466e-01,  2.1439e-03],\n",
       "          [ 1.1987e-01,  4.1382e-01, -2.2125e-02,  ...,  8.4082e-01,\n",
       "           -2.2156e-01,  1.8909e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3682, -0.0420, -0.1678,  ..., -0.1744, -0.1959, -0.2966],\n",
       "          [ 0.0342, -0.0316,  0.0244,  ..., -0.5234,  0.0431, -0.1025]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.7629e-04, 3.4256e-03, 9.9561e-01, 2.0576e-04, 1.2094e-04],\n",
       "          [7.1411e-02, 1.4209e-01, 7.4463e-01, 1.4977e-02, 2.7115e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class    name\n",
       "  0  343.788910   2.303635  639.764771  421.689026    0.890353      0  person\n",
       "  1    0.000000   1.450760  375.970154  297.437195    0.874994      0  person\n",
       "  2  154.601089  70.327423  345.119934  261.423431    0.848495      0  person\n",
       "  3  101.325150  73.810135  476.317871  421.728760    0.807362      0  person\n",
       "  4   22.109558  21.088837  583.213440  421.619019    0.451951     59     bed,\n",
       "  'caption': ['a baby with a bow in her hair and her hand in her mouth.',\n",
       "   'baby'],\n",
       "  'bbox_target': [154.75, 64.21, 192.77, 196.5]},\n",
       " 946: {'image_emb': tensor([[ 0.2266, -0.2163,  0.1587,  ...,  1.1084,  0.0651, -0.1953],\n",
       "          [-0.3625,  0.1542,  0.3374,  ...,  1.0703, -0.0978,  0.0948],\n",
       "          [-0.3140, -0.0020, -0.0903,  ...,  0.8447,  0.1611, -0.1610],\n",
       "          ...,\n",
       "          [ 0.1781, -0.2632,  0.0428,  ...,  0.9565,  0.1656,  0.0030],\n",
       "          [-0.2198, -0.0951, -0.0588,  ...,  1.0811,  0.0260,  0.0953],\n",
       "          [-0.2859, -0.0609,  0.0525,  ...,  1.0498, -0.1360, -0.1094]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0576, -0.1208, -0.1552,  ...,  0.1273,  0.0735, -0.0949],\n",
       "          [-0.0724,  0.1475, -0.4570,  ...,  0.2158,  0.2162, -0.0493]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.5676e-05, 1.2465e-03, 1.8415e-03, 1.1108e-02, 3.4571e-06, 4.8876e-06,\n",
       "           9.8438e-01, 1.4124e-03],\n",
       "          [1.1921e-07, 1.9360e-04, 5.5122e-04, 1.6373e-02, 0.0000e+00, 0.0000e+00,\n",
       "           9.8145e-01, 1.3866e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   211.692047  423.072296  398.647430  636.027771    0.921648     18   \n",
       "  1   240.948425   67.927338  386.071594  213.696228    0.920742     17   \n",
       "  2   311.141876  235.911835  479.566406  566.430298    0.912091     17   \n",
       "  3   330.702454  130.651245  467.728027  412.565308    0.904825      0   \n",
       "  4     0.257156  385.968506  165.663651  574.217651    0.879668     18   \n",
       "  5   111.273849  386.177277  224.886353  621.299316    0.840374     18   \n",
       "  6     2.606743  207.856888  253.087891  430.302307    0.789893     17   \n",
       "  7   147.048370  322.498901  278.357635  388.205200    0.616943     18   \n",
       "  8   151.429901  610.325806  214.782928  639.965088    0.542296     16   \n",
       "  9    19.395844  249.814484  127.863556  387.187714    0.532806     24   \n",
       "  10  100.274033  417.271149  169.547012  573.761841    0.474891     18   \n",
       "  11  156.230377  358.333252  297.698517  402.214233    0.431983     18   \n",
       "  12  209.386841  396.784393  318.704956  436.857697    0.369163     18   \n",
       "  13  202.931763  421.059418  349.255737  456.262115    0.338980     18   \n",
       "  14  240.762695  376.777924  296.203674  406.986908    0.264105     18   \n",
       "  \n",
       "          name  \n",
       "  0      sheep  \n",
       "  1      horse  \n",
       "  2      horse  \n",
       "  3     person  \n",
       "  4      sheep  \n",
       "  5      sheep  \n",
       "  6      horse  \n",
       "  7      sheep  \n",
       "  8        dog  \n",
       "  9   backpack  \n",
       "  10     sheep  \n",
       "  11     sheep  \n",
       "  12     sheep  \n",
       "  13     sheep  \n",
       "  14     sheep  ,\n",
       "  'caption': ['dark mule carrying bags',\n",
       "   'A dark brown donkey carrying bags strapped down to its middle and being led with a rope by the man on the horse.'],\n",
       "  'bbox_target': [0.0, 206.45, 260.13, 199.57]},\n",
       " 947: {'image_emb': tensor([[-7.6050e-02,  2.6392e-01, -1.9434e-01,  ...,  7.0605e-01,\n",
       "            1.1349e-03, -2.3645e-01],\n",
       "          [-9.9548e-02,  2.7368e-01, -1.7273e-01,  ...,  7.1191e-01,\n",
       "           -5.9235e-02,  1.2134e-01],\n",
       "          [-2.0459e-01,  3.3276e-01, -2.8247e-01,  ...,  1.4043e+00,\n",
       "            6.9092e-02, -1.7834e-01],\n",
       "          [ 1.6895e-01,  6.7725e-01, -1.7249e-01,  ...,  1.1211e+00,\n",
       "            2.4109e-01, -1.3892e-01],\n",
       "          [ 4.1992e-01, -1.2524e-01, -3.1348e-01,  ...,  4.9756e-01,\n",
       "            1.1163e-01, -1.7847e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1484, -0.2402, -0.0163,  ...,  0.0795, -0.1880, -0.1490],\n",
       "          [ 0.2472, -0.3035,  0.0301,  ...,  0.0576, -0.1927,  0.0077]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.6635e-04, 9.5557e-01, 3.2196e-02, 1.0843e-03, 1.0780e-02],\n",
       "          [2.4939e-01, 4.5166e-01, 8.0994e-02, 1.8530e-01, 3.2715e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  120.086090  187.166504  549.501221  326.540649    0.913927      4  airplane\n",
       "  1    1.280556  291.088623  144.783905  422.593933    0.903561      5       bus\n",
       "  2  475.760742  294.635193  589.159424  373.159668    0.864306      5       bus\n",
       "  3  313.841675    0.922974  638.345215  125.427185    0.859932      4  airplane\n",
       "  4  498.724487  312.177917  638.708862  440.345703    0.296212      7     truck,\n",
       "  'caption': ['A bus with white design in an airport',\n",
       "   'A blue and white bus to the left of a plane.'],\n",
       "  'bbox_target': [0.0, 288.11, 145.95, 136.21]},\n",
       " 948: {'image_emb': tensor([[-0.3118, -0.0348, -0.2529,  ...,  0.2783, -0.0505,  0.0599],\n",
       "          [-0.2942, -0.0110, -0.1237,  ...,  0.1605,  0.0544,  0.0073],\n",
       "          [-0.3936,  0.0337, -0.1874,  ...,  0.1188, -0.0590,  0.0936]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1993, -0.5059,  0.2837,  ...,  0.2683, -0.0782,  0.3745],\n",
       "          [-0.4102, -0.5522,  0.1836,  ...,  0.4854, -0.1605,  0.4690]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0655, 0.7266, 0.2081],\n",
       "          [0.1219, 0.4253, 0.4529]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  106.827011  231.963409  401.175781  515.866089    0.930950     22  zebra\n",
       "  1   25.119064  269.959106  296.456360  491.746643    0.880013     22  zebra,\n",
       "  'caption': ['A zebra with its head down eating hay.',\n",
       "   'A zebra that is kneeling down eating grass.'],\n",
       "  'bbox_target': [23.36, 271.17, 273.29, 226.84]},\n",
       " 949: {'image_emb': tensor([[-0.0439,  0.2423, -0.1874,  ...,  0.6499,  0.1737, -0.0092],\n",
       "          [-0.0604,  0.1918, -0.0127,  ...,  0.8638,  0.1327,  0.2983],\n",
       "          [-0.2776,  0.1448, -0.0531,  ...,  0.7354,  0.3599, -0.2379],\n",
       "          ...,\n",
       "          [ 0.1534, -0.2347, -0.6055,  ...,  1.3369, -0.0834, -0.1987],\n",
       "          [-0.2023,  0.1432, -0.0838,  ...,  0.6704,  0.2820, -0.2472],\n",
       "          [-0.4941, -0.0865, -0.1969,  ...,  0.2343, -0.1188,  0.0998]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.3564,  0.0964, -0.4033,  ...,  0.0647, -0.2375,  0.2032],\n",
       "          [ 0.1521, -0.3464,  0.0208,  ...,  0.0048,  0.4758, -0.0635]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9219e-01, 3.2759e-04, 1.9968e-05, 1.2960e-03, 5.6305e-03, 7.1943e-05,\n",
       "           2.4343e-04, 1.3447e-04, 2.3603e-04, 5.1260e-06],\n",
       "          [9.0674e-01, 4.5395e-03, 4.4022e-03, 1.5121e-02, 4.6570e-02, 3.2711e-03,\n",
       "           4.2648e-03, 2.9774e-03, 1.1963e-02, 5.3704e-05]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0   49.870975  127.247650  162.618469  390.630035    0.945359      0   person\n",
       "  1  422.488708  133.634369  496.438599  338.392670    0.929712      0   person\n",
       "  2  155.935425  251.484070  186.497314  280.998291    0.908611     29  frisbee\n",
       "  3  149.532333  116.789719  231.361801  358.331116    0.896150      0   person\n",
       "  4  271.654510  116.778625  335.241669  346.696716    0.895692      0   person\n",
       "  5  326.789948  214.072388  351.121857  238.301025    0.858387     29  frisbee\n",
       "  6  222.581696  337.839630  251.890839  348.157562    0.826212     29  frisbee\n",
       "  7  195.339203  235.259735  228.878113  246.499115    0.812476     29  frisbee\n",
       "  8  305.651886  227.514191  330.279816  249.602386    0.774696     29  frisbee\n",
       "  9  469.158447  212.655884  490.709534  224.458191    0.584446     29  frisbee,\n",
       "  'caption': ['A woman with blue t-shirt',\n",
       "   'The woman in flip flops and sunglasses on her hair.'],\n",
       "  'bbox_target': [48.65, 128.85, 113.82, 264.47]},\n",
       " 950: {'image_emb': tensor([[ 0.3831,  0.4270, -0.0377,  ...,  1.0791,  0.0647,  0.0297],\n",
       "          [ 0.0390,  0.4307, -0.1252,  ...,  0.2742, -0.1031,  0.0787],\n",
       "          [-0.1495,  0.0911, -0.4695,  ...,  1.3184, -0.0094,  0.2413],\n",
       "          [ 0.1316,  0.3752, -0.2515,  ...,  0.4980, -0.0494,  0.1492]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1853,  0.0373, -0.2163,  ...,  0.2087, -0.1577,  0.1790],\n",
       "          [-0.2454, -0.2003, -0.0487,  ..., -0.1824, -0.1270, -0.0698]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3816, 0.3113, 0.3066, 0.0006],\n",
       "          [0.2622, 0.1404, 0.5908, 0.0066]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0   18.680183    7.048950  247.454834  640.000000    0.945979      0  person\n",
       "  1  223.700775  310.064758  389.720856  515.934753    0.843881      0  person\n",
       "  2    0.000000  396.604187  117.834564  616.625122    0.799362      0  person\n",
       "  3   46.391754    4.342327   80.271088   78.391174    0.679446     65  remote\n",
       "  4    0.127823  533.892639   65.309799  639.108459    0.638019     57   couch\n",
       "  5    0.574432  357.262848  257.465179  626.765015    0.622781     57   couch\n",
       "  6  224.313141  367.366882  343.836090  508.024353    0.458169     56   chair\n",
       "  7  251.044830  497.685272  272.565521  539.505859    0.405637     39  bottle\n",
       "  8  404.843506  441.019012  424.419312  537.154663    0.369672     56   chair\n",
       "  9  225.592255  369.973724  342.888458  507.214996    0.315928     57   couch,\n",
       "  'caption': ['a woman with spiky hair, a white shirt, and black pants, sitting on a green chair',\n",
       "   'The woman sitting on the light chair.'],\n",
       "  'bbox_target': [221.86, 312.25, 164.96, 201.1]},\n",
       " 951: {'image_emb': tensor([[ 0.0170, -0.0269, -0.0411,  ...,  0.2739, -0.0530, -0.3723],\n",
       "          [-0.3188, -0.2103,  0.1151,  ...,  0.4473,  0.1340, -0.2412],\n",
       "          [-0.1168, -0.4585,  0.1058,  ...,  0.4287, -0.1337, -0.2651]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2783, -0.2778, -0.2747,  ...,  0.0853, -0.2925,  0.0230],\n",
       "          [ 0.2881,  0.0533, -0.3135,  ..., -0.3745, -0.3550, -0.2607]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5293, 0.3933, 0.0775],\n",
       "          [0.5239, 0.2437, 0.2324]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  298.994873  166.436340  456.435669  530.612183    0.934424     23  giraffe\n",
       "  1   63.786606  283.620361  201.415710  514.028076    0.932873     23  giraffe,\n",
       "  'caption': ['A giraffe looking at the tree,',\n",
       "   'The giraffe standing tall next to the tree'],\n",
       "  'bbox_target': [299.64, 171.66, 156.86, 360.24]},\n",
       " 952: {'image_emb': tensor([[ 0.1169,  0.2184, -0.2069,  ...,  0.4019,  0.0464,  0.3425],\n",
       "          [ 0.0254,  0.2671, -0.2793,  ...,  0.4912, -0.0889, -0.2401],\n",
       "          [-0.0054,  0.4565, -0.3362,  ...,  1.0928,  0.3694,  0.2778],\n",
       "          ...,\n",
       "          [ 0.2086, -0.1359, -0.0374,  ...,  0.3999, -0.1644, -0.0537],\n",
       "          [-0.3311,  0.4705, -0.2886,  ...,  1.1025, -0.1373,  0.0999],\n",
       "          [ 0.0538,  0.0892, -0.2649,  ...,  0.4563,  0.1000,  0.4858]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0265, -0.0797, -0.4121,  ..., -0.0078, -0.2605, -0.3518],\n",
       "          [ 0.2996,  0.0815, -0.0815,  ...,  0.0052, -0.0547, -0.3862]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.1543e-01, 1.3173e-05, 8.4043e-06, 9.2387e-06, 1.1086e-05, 8.3447e-07,\n",
       "           0.0000e+00, 1.8481e-01],\n",
       "          [9.5264e-01, 1.1921e-07, 2.3842e-07, 5.9605e-08, 5.9605e-08, 0.0000e+00,\n",
       "           0.0000e+00, 4.7424e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   182.372589    0.000000  444.340668  322.309845    0.954233      0   \n",
       "  1   567.058655  270.369415  640.000000  424.597351    0.938283     41   \n",
       "  2    75.063461  281.287231  164.272781  424.344971    0.934414     41   \n",
       "  3   443.142670   65.714264  507.637604  136.120270    0.841726      0   \n",
       "  4   335.502716  101.390137  367.349579  125.898560    0.765335      0   \n",
       "  5   202.964676  136.547760  222.475510  159.253998    0.707609     42   \n",
       "  6   478.755829  334.641846  533.687561  418.930420    0.706993     43   \n",
       "  7     0.105377  303.695312  637.202515  422.718384    0.676898     60   \n",
       "  8   388.773773  133.950302  464.458710  164.090881    0.676770     56   \n",
       "  9   523.852356  142.578491  576.525574  160.003967    0.617219     56   \n",
       "  10  534.798889  163.252319  638.792908  322.602173    0.603330     56   \n",
       "  11  291.044495  306.748962  381.664612  351.965149    0.587309     48   \n",
       "  12  283.170013  197.368744  351.443878  275.358490    0.581499     48   \n",
       "  13  481.405579  134.666672  515.510925  161.946655    0.580969     56   \n",
       "  14  489.678711  330.205780  545.084839  384.012421    0.570303     42   \n",
       "  15   43.016151  388.997467   93.262108  425.443909    0.561865     41   \n",
       "  16  512.385254   16.276016  555.465454   55.624557    0.525441     62   \n",
       "  17  414.137634   16.365822  440.589417   42.456146    0.514396     74   \n",
       "  18  457.040985  336.865143  518.249817  389.460541    0.397495     42   \n",
       "  19  111.273407  163.422150  640.000000  325.702789    0.369696     13   \n",
       "  20  112.004486  162.048828  637.188721  324.671997    0.354761     57   \n",
       "  21  431.880585  113.657471  449.829926  134.794647    0.332010     56   \n",
       "  22  362.947937  125.163086  409.447144  135.953766    0.327321     56   \n",
       "  23  462.271576  133.686905  488.003448  162.111511    0.294646     56   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1            cup  \n",
       "  2            cup  \n",
       "  3         person  \n",
       "  4         person  \n",
       "  5           fork  \n",
       "  6          knife  \n",
       "  7   dining table  \n",
       "  8          chair  \n",
       "  9          chair  \n",
       "  10         chair  \n",
       "  11      sandwich  \n",
       "  12      sandwich  \n",
       "  13         chair  \n",
       "  14          fork  \n",
       "  15           cup  \n",
       "  16            tv  \n",
       "  17         clock  \n",
       "  18          fork  \n",
       "  19         bench  \n",
       "  20         couch  \n",
       "  21         chair  \n",
       "  22         chair  \n",
       "  23         chair  ,\n",
       "  'caption': ['A girl in a white shirt eating',\n",
       "   'A female with glasses in an white shirt holding a piece of a sandwich'],\n",
       "  'bbox_target': [183.27, 2.59, 260.04, 324.32]},\n",
       " 953: {'image_emb': tensor([[-0.1222,  0.1292, -0.0465,  ...,  0.4143,  0.0801, -0.3689],\n",
       "          [-0.0574,  0.5127, -0.2058,  ...,  1.3076,  0.4541,  0.0109],\n",
       "          [ 0.1168,  0.3958, -0.1377,  ...,  0.5913,  0.2896,  0.1821],\n",
       "          ...,\n",
       "          [ 0.0955,  0.1218, -0.2991,  ...,  0.9873,  0.2656,  0.0891],\n",
       "          [-0.4829,  0.4717,  0.0346,  ...,  1.0811,  0.0543, -0.2805],\n",
       "          [ 0.4993,  0.0731, -0.3074,  ...,  0.6685,  0.4900,  0.2267]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1240,  0.0916,  0.0589,  ...,  0.3462,  0.0457, -0.6299],\n",
       "          [-0.2334,  0.1288,  0.0801,  ...,  0.2593, -0.0455, -0.5171]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0156, 0.0158, 0.0941, 0.0912, 0.0392, 0.0041, 0.7397],\n",
       "          [0.0012, 0.0046, 0.0183, 0.2450, 0.1486, 0.4648, 0.1176]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   263.418457    0.429688  368.961304  206.041351    0.927040     39   \n",
       "  1    85.184723   55.447266  239.686859  238.645630    0.923397     41   \n",
       "  2    70.765366  236.128906  440.069458  462.887817    0.901199     53   \n",
       "  3   382.269531  142.929581  638.835938  292.293640    0.881012     53   \n",
       "  4   604.250977   35.615021  639.847412   80.930115    0.818708     53   \n",
       "  5   492.895050   61.548584  627.941772  173.769043    0.781401     45   \n",
       "  6     4.426819    0.000000  631.232971  470.261536    0.676488     60   \n",
       "  7   234.741348    0.348457  265.172974  107.079437    0.590332     39   \n",
       "  8    18.571144    0.238510  108.272858  101.808975    0.584422     39   \n",
       "  9   392.721222  220.578125  639.880493  473.454102    0.535094     60   \n",
       "  10  235.085983    0.658165  265.676971  107.917908    0.502216     41   \n",
       "  11  134.306213    0.455063  194.864288   91.532349    0.486421     39   \n",
       "  12  363.690887   42.745041  474.407501   54.982224    0.435003     43   \n",
       "  13  368.221863    1.743591  466.563110   30.955124    0.422296     42   \n",
       "  14  368.598114    1.358170  465.294037   30.216354    0.394410     44   \n",
       "  \n",
       "              name  \n",
       "  0         bottle  \n",
       "  1            cup  \n",
       "  2          pizza  \n",
       "  3          pizza  \n",
       "  4          pizza  \n",
       "  5           bowl  \n",
       "  6   dining table  \n",
       "  7         bottle  \n",
       "  8         bottle  \n",
       "  9   dining table  \n",
       "  10           cup  \n",
       "  11        bottle  \n",
       "  12         knife  \n",
       "  13          fork  \n",
       "  14         spoon  ,\n",
       "  'caption': ['white table uner pizza and drinks',\n",
       "   'Beige table with food on top of it.'],\n",
       "  'bbox_target': [400.0, 217.5, 240.0, 256.25]},\n",
       " 954: {'image_emb': tensor([[-0.1416,  0.1930, -0.1005,  ...,  0.5151,  0.2786, -0.3677],\n",
       "          [ 0.4260,  0.0673, -0.1940,  ..., -0.1159,  0.2981,  0.1514],\n",
       "          [ 0.5303, -0.1729, -0.0075,  ...,  0.2305,  0.2325,  0.0661],\n",
       "          ...,\n",
       "          [ 0.0485,  0.2236, -0.1187,  ...,  1.0449,  0.3962, -0.1498],\n",
       "          [-0.0706,  0.2971, -0.2554,  ...,  0.6743,  0.2568,  0.2534],\n",
       "          [ 0.3186,  0.0193,  0.1064,  ..., -0.2476,  0.1252,  0.3218]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1150,  0.0138, -0.3000,  ...,  0.1559, -0.2379,  0.1794],\n",
       "          [ 0.2742,  0.4067, -0.2015,  ...,  0.1672, -0.1816, -0.0638]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.9791e-02, 7.7515e-03, 3.3142e-02, 5.8197e-02, 2.2430e-02, 2.6642e-02,\n",
       "           7.3547e-02, 1.0429e-02, 6.3525e-01, 4.4617e-02, 6.8054e-02],\n",
       "          [1.0114e-01, 3.3691e-01, 3.8605e-03, 4.3518e-02, 4.3945e-01, 3.4084e-03,\n",
       "           1.4801e-02, 2.1000e-03, 7.3671e-04, 5.4138e-02, 1.9610e-05]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   494.947968  130.206406  610.981018  468.086792    0.915570      0   \n",
       "  1   131.671738  153.431183  324.955750  468.471680    0.913883      0   \n",
       "  2   291.158722  134.601486  493.497406  466.905762    0.905447      0   \n",
       "  3   478.493988    0.230179  572.158081  104.499161    0.877974      0   \n",
       "  4    59.137161    9.360931  183.498550  320.382507    0.871640      0   \n",
       "  5   253.811005    0.315796  369.409943  307.358643    0.775822      0   \n",
       "  6   423.813660  200.470062  545.275146  446.686035    0.766720      0   \n",
       "  7   371.495758    1.127899  590.365295  373.422333    0.746430      0   \n",
       "  8   550.869385   36.099327  639.791626  223.138214    0.740114      2   \n",
       "  9     0.000000   58.154373   75.814911  456.267761    0.717287      0   \n",
       "  10    0.000000  193.541138   75.360214  304.200134    0.655006     26   \n",
       "  11   72.091125  265.474670  188.140778  470.961975    0.626844     24   \n",
       "  12   41.221115   66.297684   96.654434  355.338745    0.444363      0   \n",
       "  13    0.000000  163.299835   74.565216  335.245209    0.418633      0   \n",
       "  14  506.197876  304.045898  554.767212  331.274048    0.323857     54   \n",
       "  15  146.328705   15.636696  297.434631  162.836334    0.317159      2   \n",
       "  16  504.838257  304.434113  554.386597  331.354889    0.254514     52   \n",
       "  17    3.998093    9.155464  297.241333  161.391724    0.251933      7   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2     person  \n",
       "  3     person  \n",
       "  4     person  \n",
       "  5     person  \n",
       "  6     person  \n",
       "  7     person  \n",
       "  8        car  \n",
       "  9     person  \n",
       "  10   handbag  \n",
       "  11  backpack  \n",
       "  12    person  \n",
       "  13    person  \n",
       "  14     donut  \n",
       "  15       car  \n",
       "  16   hot dog  \n",
       "  17     truck  ,\n",
       "  'caption': ['the black car behind the crowed',\n",
       "   'A black woman wearing a t-shirt and glasses.'],\n",
       "  'bbox_target': [2.13, 17.01, 299.78, 146.7]},\n",
       " 955: {'image_emb': tensor([[ 0.0783,  0.9004, -0.1526,  ...,  1.1172,  0.1119,  0.2949],\n",
       "          [-0.1914,  0.1981,  0.0344,  ...,  0.4270,  0.1026, -0.0516],\n",
       "          [-0.3315, -0.1874, -0.0475,  ...,  0.8955,  0.2537,  0.1409],\n",
       "          ...,\n",
       "          [-0.4050,  0.4961, -0.2214,  ...,  1.3945,  0.5430, -0.2485],\n",
       "          [-0.4741,  0.3047,  0.0748,  ...,  0.9546, -0.1298, -0.1097],\n",
       "          [-0.4639,  0.3618,  0.0867,  ...,  0.4167,  0.0557,  0.1278]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1136, -0.1819, -0.4878,  ...,  0.2974, -0.1514,  0.2671],\n",
       "          [-0.0244, -0.3938,  0.1277,  ...,  0.2180, -0.2025, -0.3447]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.6357e-02, 5.3101e-03, 2.1216e-01, 7.0763e-04, 1.8349e-03, 2.6392e-01,\n",
       "           3.9624e-01, 3.6144e-04, 4.7333e-02, 1.3351e-02, 3.8635e-02, 2.0909e-04,\n",
       "           3.5381e-03],\n",
       "          [2.2340e-04, 2.0630e-01, 3.2135e-02, 2.1960e-01, 4.6021e-02, 1.9791e-02,\n",
       "           3.6049e-03, 4.1675e-01, 4.0436e-04, 3.4389e-03, 1.0597e-02, 6.8808e-04,\n",
       "           4.0619e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    15.976013  392.055725  116.466125  488.987000    0.919159     41   \n",
       "  1     0.000000  208.628250  425.942322  637.153320    0.871300     60   \n",
       "  2   113.282730  116.490547  167.078491  176.975693    0.854299      0   \n",
       "  3   227.721436  520.917480  355.750305  639.512329    0.848599     53   \n",
       "  4   276.883911  281.449036  347.566406  345.982727    0.840728     53   \n",
       "  5   157.408997    8.073349  398.784363  345.714233    0.811187      0   \n",
       "  6    47.158936  325.862579  134.572296  394.576996    0.758846     56   \n",
       "  7   155.994232  375.986938  298.405548  558.992065    0.756304     53   \n",
       "  8   386.814941  425.095795  426.041382  498.426666    0.730297     43   \n",
       "  9    51.589462  375.819824  115.404068  430.222229    0.727537     56   \n",
       "  10    0.000000  573.028564   85.083405  640.000000    0.726366     41   \n",
       "  11  353.335999  492.289215  425.455933  567.598022    0.700656     42   \n",
       "  12    0.659897  296.465668   85.190414  380.395416    0.680516     60   \n",
       "  13   57.665634   11.260773  418.397461  389.872498    0.648023     56   \n",
       "  14    0.000000  367.812225   42.510941  473.453339    0.608493     56   \n",
       "  15  201.383514  327.341675  283.775787  370.950562    0.371009     42   \n",
       "  16   32.343414  205.594788   67.306854  246.453033    0.337845      0   \n",
       "  17  154.565002  454.061920  227.684875  558.025940    0.334007     53   \n",
       "  18  102.674240  196.792374  115.206070  210.305679    0.281246      0   \n",
       "  \n",
       "              name  \n",
       "  0            cup  \n",
       "  1   dining table  \n",
       "  2         person  \n",
       "  3          pizza  \n",
       "  4          pizza  \n",
       "  5         person  \n",
       "  6          chair  \n",
       "  7          pizza  \n",
       "  8          knife  \n",
       "  9          chair  \n",
       "  10           cup  \n",
       "  11          fork  \n",
       "  12  dining table  \n",
       "  13         chair  \n",
       "  14         chair  \n",
       "  15          fork  \n",
       "  16        person  \n",
       "  17         pizza  \n",
       "  18        person  ,\n",
       "  'caption': ['lady seated across table',\n",
       "   'woman sprinikling pepper on her pizza'],\n",
       "  'bbox_target': [153.14, 6.86, 245.6, 326.5]},\n",
       " 956: {'image_emb': tensor([[-0.2639,  0.7661, -0.1223,  ...,  1.0361,  0.0538, -0.2415],\n",
       "          [-0.6538,  0.2939,  0.0741,  ...,  1.1553,  0.4504, -0.1400],\n",
       "          [-0.2339,  0.5845, -0.2744,  ...,  0.9419, -0.0767,  0.2700],\n",
       "          [-0.4363,  0.8301, -0.1936,  ...,  0.7754,  0.1499, -0.1174]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2961,  0.0685,  0.0339,  ...,  0.2412, -0.0362, -0.2052],\n",
       "          [ 0.3291,  0.0274,  0.1912,  ...,  0.0552, -0.0555,  0.0228]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.1443e-02, 2.3520e-04, 9.5801e-01, 1.8895e-04],\n",
       "          [6.2500e-02, 2.7046e-03, 9.3311e-01, 1.8015e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  315.254822  197.090149  438.003784  360.873535    0.866301     50   \n",
       "  1  513.069641    0.524330  639.829895  177.394714    0.860105     41   \n",
       "  2  297.055939  358.309265  447.501556  477.532410    0.746386     50   \n",
       "  3    1.416138    7.609695  640.000000  480.000000    0.385807     45   \n",
       "  4    2.186554    2.785141  640.000000  473.847900    0.323029     60   \n",
       "  5  242.817719  430.385315  384.435944  479.509705    0.280180     51   \n",
       "  \n",
       "             name  \n",
       "  0      broccoli  \n",
       "  1           cup  \n",
       "  2      broccoli  \n",
       "  3          bowl  \n",
       "  4  dining table  \n",
       "  5        carrot  ,\n",
       "  'caption': ['The upside down piece of broccoli on top of another piece of broccoli.',\n",
       "   'A piece of upside down broccoli.'],\n",
       "  'bbox_target': [319.39, 200.52, 116.59, 158.76]},\n",
       " 957: {'image_emb': tensor([[ 0.3328,  0.4421, -0.0802,  ...,  0.5825,  0.0587,  0.0037],\n",
       "          [ 0.0225,  0.1941, -0.0617,  ...,  1.2910, -0.1990,  0.1589],\n",
       "          [ 0.0746,  0.4033, -0.3713,  ...,  1.4199, -0.3577,  0.3662],\n",
       "          ...,\n",
       "          [ 0.1003,  0.2448, -0.1453,  ...,  1.1123,  0.1836, -0.0544],\n",
       "          [-0.2285,  0.3708, -0.3306,  ...,  1.3506, -0.3665, -0.1080],\n",
       "          [-0.0624,  0.3203, -0.3794,  ...,  0.9961, -0.6616, -0.1094]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0858,  0.1091, -0.1298,  ...,  0.0994, -0.0827,  0.1561],\n",
       "          [ 0.1748,  0.1075, -0.1820,  ...,  0.0174, -0.0040,  0.0445]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.4402e-02, 3.5763e-05, 1.4067e-03, 8.2568e-01, 5.3902e-03, 5.9605e-08,\n",
       "           6.2683e-02, 3.0548e-02],\n",
       "          [1.7834e-03, 5.8794e-04, 7.4339e-04, 3.9111e-01, 3.5167e-06, 1.1623e-05,\n",
       "           6.0596e-01, 1.4484e-05]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0     0.565895  160.435669  189.817810  475.138062    0.931451      0   \n",
       "  1   326.518463  313.304626  403.302704  412.752319    0.899860      2   \n",
       "  2   534.392700  242.571838  639.580078  475.368713    0.898807      0   \n",
       "  3   372.531616  285.639221  495.879517  477.372131    0.896366      0   \n",
       "  4    70.198273  110.462219  335.978210  476.177490    0.854392      5   \n",
       "  5     2.241173  113.389420   80.598213  184.777252    0.792285      9   \n",
       "  6   390.819855  286.955109  558.016968  378.335480    0.771582      2   \n",
       "  7   490.035950  307.186371  559.792419  441.613159    0.682616      2   \n",
       "  8   324.993896  300.006836  351.462341  316.650208    0.575147      2   \n",
       "  9   491.232971  305.748352  640.000000  443.035034    0.530903      2   \n",
       "  10  392.529907  407.680603  471.073975  479.160583    0.464463     26   \n",
       "  11    0.000000    0.528275   69.068214   30.063095    0.406553      9   \n",
       "  \n",
       "               name  \n",
       "  0          person  \n",
       "  1             car  \n",
       "  2          person  \n",
       "  3          person  \n",
       "  4             bus  \n",
       "  5   traffic light  \n",
       "  6             car  \n",
       "  7             car  \n",
       "  8             car  \n",
       "  9             car  \n",
       "  10        handbag  \n",
       "  11  traffic light  ,\n",
       "  'caption': ['A police woman standing in front of a bus.',\n",
       "   'A woman with black hair and sunglasses.'],\n",
       "  'bbox_target': [375.37, 288.27, 120.81, 184.45]},\n",
       " 958: {'image_emb': tensor([[-0.1768,  0.0645,  0.1361,  ...,  0.6396,  0.0691,  0.3228],\n",
       "          [-0.2189,  0.1993, -0.2207,  ...,  0.9751, -0.2720, -0.0124],\n",
       "          [-0.2627,  0.1890, -0.0150,  ...,  1.5547, -0.0966,  0.3542],\n",
       "          ...,\n",
       "          [ 0.3606,  0.0646, -0.0504,  ...,  1.0859, -0.2939,  0.1670],\n",
       "          [ 0.2357, -0.0724, -0.4558,  ...,  0.6577,  0.0394,  0.1498],\n",
       "          [ 0.1805, -0.0859,  0.0074,  ...,  0.7168, -0.0318,  0.0864]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0399, -0.3503, -0.3547,  ..., -0.0586,  0.0122, -0.0422],\n",
       "          [ 0.0979, -0.1584, -0.4272,  ..., -0.0388, -0.2654,  0.0953]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.4140e-05, 4.4067e-01, 1.9670e-05, 3.6743e-02, 2.3258e-04, 9.7942e-04,\n",
       "           1.0848e-05, 4.8389e-01, 1.5497e-04, 7.2327e-03, 2.9984e-02, 2.4140e-05],\n",
       "          [2.3842e-07, 4.9683e-01, 2.4438e-06, 1.0315e-02, 5.9605e-07, 4.4365e-03,\n",
       "           8.5831e-06, 4.8145e-01, 6.0380e-05, 1.2386e-04, 6.6566e-03, 3.5763e-07]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    19.841721    1.629364  333.363892  387.186859    0.932473      0   \n",
       "  1   412.430420  194.046631  530.116516  340.535706    0.913836     56   \n",
       "  2   558.595825  344.945801  639.465576  424.553101    0.884179      0   \n",
       "  3   458.097534  104.700134  591.515747  262.828430    0.844539      0   \n",
       "  4   118.761391  323.774841  255.957184  416.020325    0.842722     48   \n",
       "  5    24.499031   67.494446   95.795708  210.645752    0.840947      0   \n",
       "  6     0.123650  100.655960   54.441658  197.995178    0.817157      0   \n",
       "  7   348.018768  179.564850  424.168610  314.036346    0.814454     56   \n",
       "  8   105.483818   68.843384  190.062805  142.770386    0.801324      0   \n",
       "  9     0.000000  297.215454  568.338623  422.959961    0.770571     60   \n",
       "  10   16.639095  178.348145   39.854881  240.105164    0.763662     39   \n",
       "  11  402.853882  146.207764  463.731812  252.514160    0.630399      0   \n",
       "  12  605.415161  149.517639  640.000000  324.272400    0.622868      0   \n",
       "  13  354.835602  128.593887  420.950775  192.303741    0.616052      0   \n",
       "  14  281.779449  114.231140  351.105316  181.446686    0.568796      0   \n",
       "  15   32.370220  188.483765   61.240978  227.987732    0.527758     41   \n",
       "  16  283.013519  114.950623  353.753754  273.650269    0.492706      0   \n",
       "  17  568.026855  368.898529  589.880127  399.799896    0.407519     67   \n",
       "  18  386.603577  176.838440  426.569397  244.041504    0.382282     56   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1          chair  \n",
       "  2         person  \n",
       "  3         person  \n",
       "  4       sandwich  \n",
       "  5         person  \n",
       "  6         person  \n",
       "  7          chair  \n",
       "  8         person  \n",
       "  9   dining table  \n",
       "  10        bottle  \n",
       "  11        person  \n",
       "  12        person  \n",
       "  13        person  \n",
       "  14        person  \n",
       "  15           cup  \n",
       "  16        person  \n",
       "  17    cell phone  \n",
       "  18         chair  ,\n",
       "  'caption': ['empty chair next to woman sitting next to the wall',\n",
       "   'Chair next to girl with curly blond hair'],\n",
       "  'bbox_target': [412.39, 195.78, 118.77, 144.01]},\n",
       " 959: {'image_emb': tensor([[ 0.4705, -0.1448, -0.1661,  ...,  0.6030,  0.2252, -0.0457],\n",
       "          [ 0.3079, -0.1710, -0.3879,  ...,  0.3511,  0.1334, -0.1125],\n",
       "          [ 0.6172, -0.4226, -0.3306,  ...,  0.3162,  0.0741,  0.0737]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0524, -0.3564, -0.2140,  ..., -0.0591, -0.1460, -0.2045],\n",
       "          [ 0.1405,  0.0289, -0.4666,  ..., -0.5186, -0.0995, -0.6475]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0534, 0.0853, 0.8613],\n",
       "          [0.0649, 0.7666, 0.1685]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0   56.855888   75.859650  295.633270  418.502563    0.941168     23  giraffe\n",
       "  1  242.934265  202.245544  410.624084  419.412476    0.940359     23  giraffe,\n",
       "  'caption': ['The fathest left giraffee.',\n",
       "   'A mother giraffe standing with her child.'],\n",
       "  'bbox_target': [55.22, 76.02, 234.18, 342.2]},\n",
       " 960: {'image_emb': tensor([[ 0.3442,  0.3772, -0.4858,  ...,  0.8955,  0.0896, -0.2708],\n",
       "          [ 0.0676, -0.0350, -0.0281,  ...,  0.7510,  0.1816, -0.1063],\n",
       "          [-0.1462,  0.4324, -0.1836,  ...,  0.6851, -0.0143, -0.2115],\n",
       "          [ 0.1062,  0.4792, -0.4685,  ...,  0.7183,  0.1144,  0.2607]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0913,  0.1418,  0.2030,  ..., -0.9116, -0.0423,  0.0731],\n",
       "          [-0.0084,  0.0190, -0.4739,  ...,  0.0217,  0.2457,  0.3696]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.5552e-02, 4.0161e-01, 3.2788e-01, 2.0508e-01],\n",
       "          [8.4814e-01, 2.2650e-06, 3.2425e-05, 1.5198e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class    name\n",
       "  0  129.431702   80.167221  309.586395  371.784698    0.941647      0  person\n",
       "  1  268.841888  302.957764  308.695160  327.690735    0.828208     65  remote\n",
       "  2  455.200775  286.900116  499.588287  373.433411    0.801351     56   chair\n",
       "  3    0.598806  339.432007  110.643456  373.940430    0.548467     57   couch,\n",
       "  'caption': ['a man', 'man behind woman in striped shirt'],\n",
       "  'bbox_target': [219.94, 71.37, 103.66, 284.83]},\n",
       " 961: {'image_emb': tensor([[ 0.0887,  0.2421, -0.2302,  ...,  0.8706, -0.3289, -0.0052],\n",
       "          [ 0.3489,  0.6069, -0.0779,  ...,  1.1855, -0.2634, -0.0768],\n",
       "          [ 0.1807,  0.3601, -0.3198,  ...,  1.0518,  0.1392, -0.2976],\n",
       "          [ 0.0123,  0.0326, -0.1387,  ...,  0.7305, -0.4133, -0.2069]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1095,  0.3193, -0.3721,  ..., -0.2571,  0.2400, -0.2588],\n",
       "          [ 0.1635,  0.0574, -0.2380,  ..., -0.3201,  0.0959, -0.2188]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.2139, 0.7705, 0.0067, 0.0091],\n",
       "          [0.4575, 0.1819, 0.3398, 0.0207]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin       ymin        xmax        ymax  confidence  class    name\n",
       "  0   32.867886  76.445648  245.149063  277.754944    0.940635      0  person\n",
       "  1  406.641052  97.122833  490.134949  278.561340    0.936906      0  person\n",
       "  2    0.628823   1.578856   52.743732  266.723053    0.804128     33    kite,\n",
       "  'caption': ['A woman in a long sleeved shirt with some string.',\n",
       "   'person on right'],\n",
       "  'bbox_target': [407.92, 96.32, 82.18, 184.68]},\n",
       " 962: {'image_emb': tensor([[-0.0621,  0.2942,  0.0320,  ...,  1.0391,  0.0604,  0.0250],\n",
       "          [-0.1844,  0.6025, -0.0575,  ...,  1.0518,  0.0351, -0.1877],\n",
       "          [ 0.1113,  0.1390,  0.1233,  ...,  0.8110,  0.0776,  0.1477],\n",
       "          ...,\n",
       "          [-0.2502, -0.0623, -0.1667,  ...,  1.1113, -0.0222, -0.0850],\n",
       "          [ 0.2859, -0.2299, -0.4744,  ...,  0.8896,  0.1041, -0.3875],\n",
       "          [-0.2639,  0.2137,  0.0781,  ...,  0.9253, -0.0037, -0.0373]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-1.1145e-01, -1.9446e-01, -1.2299e-01,  3.6597e-01,  2.0532e-01,\n",
       "            3.5547e-01, -1.2006e-01, -8.3643e-01,  6.2500e-02,  2.7319e-01,\n",
       "            4.0747e-01,  1.2512e-01,  3.2129e-01,  9.2651e-02, -1.9385e-01,\n",
       "           -1.8909e-01,  1.9653e-01,  2.0154e-01,  1.5833e-01,  1.7664e-01,\n",
       "            3.5107e-01,  3.5278e-01, -1.8225e-01,  1.2671e-01,  6.7566e-02,\n",
       "            2.8613e-01, -2.7515e-01,  8.7585e-02,  2.0886e-01,  9.2834e-02,\n",
       "           -3.7256e-01, -1.5393e-01,  7.2021e-02,  2.0557e-01, -6.1249e-02,\n",
       "            1.4722e-01, -8.8379e-02,  1.8542e-01, -3.4399e-01,  2.9883e-01,\n",
       "           -4.0820e-01,  4.0430e-01, -7.8735e-03, -2.1570e-01,  1.1719e-01,\n",
       "            2.5098e-01, -3.9282e-01,  1.2459e-02, -2.8735e-01, -3.7109e-01,\n",
       "            1.2695e-01, -5.1758e-01,  1.3184e-01, -1.8579e-01, -1.3623e-01,\n",
       "            2.0618e-01,  7.3608e-02,  9.5978e-03, -2.7124e-01,  3.3496e-01,\n",
       "           -1.1145e-01, -1.0718e-01,  1.3599e-01, -2.1912e-01,  8.5632e-02,\n",
       "            6.0425e-02,  2.6074e-01,  8.5156e-01, -2.6782e-01,  9.4177e-02,\n",
       "           -2.1692e-01,  6.4392e-02,  4.4019e-01, -3.8879e-02,  4.7791e-02,\n",
       "           -8.1604e-02, -6.5186e-02,  1.2964e-01, -1.5344e-01, -2.0660e-02,\n",
       "           -1.2439e-01,  1.2482e-01,  4.9463e-01, -2.8101e-01, -5.4474e-02,\n",
       "           -3.0472e-02,  1.2311e-01,  1.2927e-01, -6.8652e-01,  1.5503e-01,\n",
       "            2.0776e-01,  3.1030e-01, -1.3125e+00, -5.8447e-01, -2.3401e-01,\n",
       "           -2.4878e-01,  1.3611e-01,  2.4891e-03,  8.4033e-01,  1.3098e-01,\n",
       "            1.1121e-01, -4.8637e-04,  3.1543e-01, -2.2217e-01, -2.0190e-01,\n",
       "           -2.3413e-01, -7.9712e-02,  2.3877e-01, -6.9153e-02, -1.2903e-01,\n",
       "           -1.9507e-01,  4.3530e-01,  1.9910e-01, -8.3923e-02,  1.6333e-01,\n",
       "           -7.4524e-02,  2.0764e-01,  7.0496e-03,  2.3422e-02,  8.2703e-02,\n",
       "           -3.7811e-02, -2.3425e-01,  1.8445e-01, -1.1713e-01,  2.1896e-02,\n",
       "            1.2622e-01,  1.6492e-01,  3.9746e-01, -8.4229e-02,  1.5173e-01,\n",
       "            3.6401e-01,  1.0217e-01,  3.9160e-01,  4.0625e+00, -4.6802e-01,\n",
       "           -1.0535e-01, -3.5645e-01, -6.9519e-02,  1.9275e-01, -2.9028e-01,\n",
       "           -3.9093e-02,  7.8186e-02, -7.9150e-01,  5.7910e-01,  2.2424e-01,\n",
       "           -1.9421e-01,  1.1292e-01,  2.2510e-01,  1.9568e-01,  2.1680e-01,\n",
       "            6.8604e-02, -3.9307e-01,  2.9736e-01, -4.3579e-02, -4.9927e-02,\n",
       "           -1.4575e-01,  3.5797e-02, -4.4580e-01,  2.1497e-01,  2.4780e-01,\n",
       "            9.2346e-02,  4.4727e-01, -2.7295e-01,  2.0288e-01, -4.2578e-01,\n",
       "            6.6711e-02, -7.0267e-03, -1.2939e-01,  1.6943e-01, -6.4331e-02,\n",
       "            1.2115e-01, -5.3613e-01, -3.7506e-02,  1.8018e-01, -1.0382e-01,\n",
       "            3.3398e-01,  2.2644e-01,  8.1848e-02,  1.6125e-01,  4.5929e-02,\n",
       "           -2.9980e-01, -3.9404e-01, -2.3584e-01,  2.8149e-01, -1.6638e-01,\n",
       "           -5.2002e-01, -3.8696e-01, -1.8518e-01, -1.5857e-01,  4.0918e-01,\n",
       "            1.8115e-01, -2.7686e-01, -5.4199e-01,  5.3162e-02, -9.1003e-02,\n",
       "            1.4001e-01, -6.1890e-02,  1.1029e-01, -2.0422e-01,  3.4332e-02,\n",
       "           -1.2445e-01, -1.8579e-01,  2.5711e-02, -5.6519e-02,  1.0780e-02,\n",
       "            3.6957e-02,  1.6150e-01, -1.2549e-01,  3.4375e-01,  3.9331e-01,\n",
       "            4.3243e-02,  5.0391e-01,  1.4539e-01,  1.9202e-01, -2.8052e-01,\n",
       "           -1.8323e-01,  3.1372e-01, -1.7371e-01,  2.6489e-01, -2.1777e-01,\n",
       "           -2.3218e-01, -2.6367e-01, -2.7197e-01, -4.4385e-01, -2.2335e-03,\n",
       "            3.1982e-02,  8.4656e-02, -2.1948e-01,  2.3010e-01, -4.7424e-02,\n",
       "           -7.3120e-02, -1.3831e-01, -2.4170e-01, -2.1927e-02,  7.8552e-02,\n",
       "            2.8247e-01, -2.6993e-02,  5.3772e-02, -7.2403e-03, -9.6069e-02,\n",
       "            1.8628e-01, -2.7563e-01,  1.6589e-01,  4.2993e-01, -5.9131e-01,\n",
       "           -3.9001e-02, -1.1127e-01,  2.0459e-01,  1.7578e-01,  4.4525e-02,\n",
       "            4.3304e-02,  1.2286e-01,  8.4290e-02, -9.9564e-03, -2.0740e-01,\n",
       "           -6.8604e-02,  2.3303e-01,  1.3794e-01, -2.1088e-02,  5.3619e-02,\n",
       "           -3.5254e-01, -2.3572e-01,  1.1818e-02, -7.3120e-02,  5.5518e-01,\n",
       "            2.0117e-01, -1.4233e-01, -2.8711e-01,  4.0967e-01,  4.1284e-01,\n",
       "            1.4197e-01, -7.9834e-02, -3.2031e-01,  9.2896e-02, -9.0027e-02,\n",
       "            1.7834e-01,  6.8665e-02, -6.8909e-02, -1.3171e-01,  7.8613e-02,\n",
       "            2.8149e-01, -8.9905e-02,  2.4646e-01, -1.0870e-01, -1.6626e-01,\n",
       "            2.2522e-01, -3.2690e-01, -1.0948e-02,  6.6345e-02, -1.6052e-02,\n",
       "           -1.1139e-01,  5.1758e-01,  4.6729e-01, -1.6260e-01, -2.3694e-01,\n",
       "            1.6968e-01,  2.5978e-03,  1.6772e-01,  8.9478e-02, -1.9885e-01,\n",
       "           -4.5288e-01,  4.6899e-01,  3.1494e-01,  4.9774e-02, -1.2939e-01,\n",
       "           -4.3799e-01,  2.2046e-01,  1.2878e-01,  1.9092e-01, -3.0762e-01,\n",
       "            1.4819e-01,  5.0635e-01,  4.0586e+00,  7.0984e-02,  9.0759e-02,\n",
       "            5.6396e-01,  9.4849e-02, -1.7700e-01,  4.2212e-01, -1.1060e-01,\n",
       "            3.3374e-01, -1.3989e-01,  3.8770e-01,  2.1106e-01, -1.2109e-01,\n",
       "            2.0459e-01,  3.2251e-01, -4.7144e-01,  3.0060e-03, -2.1309e+00,\n",
       "            2.0447e-01,  8.4595e-02,  1.9556e-01, -1.7065e-01,  2.6718e-02,\n",
       "           -3.2074e-02, -1.6748e-01, -9.7290e-02,  3.6914e-01, -2.0264e-01,\n",
       "            1.2793e-01,  2.6703e-02,  3.6102e-02, -2.0117e-01,  1.3501e-01,\n",
       "           -2.6196e-01,  4.1406e-01, -3.6774e-02, -1.4868e-01,  1.1639e-01,\n",
       "            4.1724e-01,  3.5156e-02, -2.9199e-01,  1.3684e-01, -1.6895e-01,\n",
       "            3.1519e-01,  7.6637e-03,  5.9692e-02, -1.6162e-01, -6.7090e-01,\n",
       "           -3.2056e-01, -3.6646e-01,  2.1008e-01,  3.1250e-01,  4.1797e-01,\n",
       "           -1.9604e-01, -1.1572e-01,  3.4790e-01, -1.3586e-01, -1.0864e-02,\n",
       "           -2.6962e-02, -4.9023e-01,  7.5195e-01,  3.0811e-01, -9.4604e-02,\n",
       "           -3.6719e-01, -1.2988e-01,  8.7321e-05, -3.9185e-01, -2.5781e-01,\n",
       "           -2.4524e-01,  3.7695e-01, -1.8201e-01, -3.2764e-01,  1.0248e-01,\n",
       "           -1.2427e-01, -4.9286e-02, -2.7222e-01, -6.0449e-01, -6.6345e-02,\n",
       "            4.8637e-03, -1.4746e-01,  3.1958e-01,  2.9739e-02,  2.4689e-02,\n",
       "           -7.8613e-02,  2.9688e-01,  1.3416e-01, -2.8076e-01,  4.4214e-01,\n",
       "            2.9175e-02,  1.5112e-01,  2.9810e-01,  7.0343e-03, -3.6938e-01,\n",
       "           -3.5840e-01,  2.6904e-01, -4.5215e-01,  6.3049e-02, -1.1768e-01,\n",
       "            1.7456e-02, -1.1084e-01,  1.2756e-01,  7.1594e-02,  5.5573e-02,\n",
       "            2.2369e-02,  2.2729e-01,  6.0730e-02, -1.2985e-02, -3.6060e-01,\n",
       "           -8.2169e-03,  1.9971e-01,  1.6504e-01, -7.1289e-01,  1.6870e-01,\n",
       "            8.8806e-02,  2.9712e-01, -5.5664e-02,  1.8323e-01,  4.4238e-01,\n",
       "           -8.7509e-03, -2.2339e-01, -1.5637e-01,  1.1969e-01, -2.7661e-01,\n",
       "            1.7273e-01, -2.2925e-01,  9.2285e-02, -1.8387e-02,  4.2188e-01,\n",
       "           -1.8701e-01, -4.3164e-01, -5.2783e-01, -6.3110e-02, -5.0928e-01,\n",
       "           -3.0908e-01, -6.4941e-01,  5.9570e-02, -4.0918e-01,  1.3611e-01,\n",
       "           -5.5420e-02,  4.3416e-04, -1.0791e-01,  1.0162e-01, -8.6304e-02,\n",
       "            1.1298e-01,  1.2311e-01, -3.1543e-01, -2.9126e-01, -3.0688e-01,\n",
       "            3.5614e-02, -3.1421e-01,  2.2559e-01,  1.2671e-01,  3.1982e-01,\n",
       "            2.7985e-02, -2.8540e-01,  5.7259e-03,  1.1652e-01, -4.4769e-02,\n",
       "            5.4443e-01, -2.4988e-01,  2.9404e-02, -4.6021e-02,  4.8004e-02,\n",
       "            4.6924e-01,  2.3422e-02,  6.5796e-02,  3.3594e-01,  1.5869e-01,\n",
       "           -1.7615e-01, -4.7437e-01, -1.3550e-01,  1.5930e-02, -3.6108e-01,\n",
       "            2.6367e-01,  1.9104e-01, -2.0126e-02, -4.1162e-01, -3.3105e-01,\n",
       "            1.4209e-01,  1.0431e-01,  1.4580e-02,  3.6499e-01,  3.6743e-01,\n",
       "            1.2646e-01, -1.4297e-02,  2.5439e-01,  1.3770e-01,  3.9764e-02,\n",
       "           -4.8431e-02, -1.9153e-01,  9.1858e-03,  5.2734e-01,  8.4180e-01,\n",
       "           -3.5339e-02, -1.7273e-01, -3.0762e-01, -2.2522e-01,  1.6907e-01,\n",
       "            3.4399e-01, -7.6782e-02]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.4629e-01, 2.0492e-04, 2.2507e-04, 4.4751e-04, 7.2250e-03, 8.7643e-04,\n",
       "           4.9162e-04, 4.6654e-03, 2.5620e-02, 1.3649e-04, 1.3924e-02]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    18.135712  129.769577  277.955170  470.648376    0.948603      0   \n",
       "  1   404.911194  215.425018  636.221985  418.077118    0.944066     63   \n",
       "  2   258.797302  282.576569  293.764526  322.966400    0.915913     41   \n",
       "  3   426.038055  253.140991  465.071259  289.883972    0.898791     41   \n",
       "  4   280.160553  324.409393  437.299713  422.338287    0.879782     73   \n",
       "  5   338.835327  240.327789  359.745605  309.516571    0.861681     39   \n",
       "  6   388.637573  134.143005  482.653564  186.973633    0.833959     62   \n",
       "  7   138.531509  225.162872  640.000000  474.734863    0.814478     60   \n",
       "  8     0.393536  149.220795   42.131435  337.010895    0.776780     72   \n",
       "  9   285.472229  266.495209  316.240601  287.521759    0.739323     65   \n",
       "  10   30.287712  143.368713  168.637573  313.790833    0.638565     72   \n",
       "  11   21.135986  356.657135  118.611588  455.434143    0.554907     56   \n",
       "  12  243.053421  227.919556  271.341431  291.751343    0.449953     39   \n",
       "  13   50.450638  127.220886   86.049072  144.693848    0.447091     45   \n",
       "  14   96.813484  112.928482  106.130424  139.268936    0.440989     39   \n",
       "  15   27.969963  143.276413   99.160545  234.508972    0.384971     72   \n",
       "  16  278.625275  325.339661  435.647980  424.120361    0.358694     60   \n",
       "  17  200.525223   32.274040  228.523575   45.321892    0.353407     45   \n",
       "  18  434.291138  318.513916  584.103882  393.468384    0.351804     66   \n",
       "  19  213.795807  163.585419  257.090210  182.474396    0.323813     68   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         laptop  \n",
       "  2            cup  \n",
       "  3            cup  \n",
       "  4           book  \n",
       "  5         bottle  \n",
       "  6             tv  \n",
       "  7   dining table  \n",
       "  8   refrigerator  \n",
       "  9         remote  \n",
       "  10  refrigerator  \n",
       "  11         chair  \n",
       "  12        bottle  \n",
       "  13          bowl  \n",
       "  14        bottle  \n",
       "  15  refrigerator  \n",
       "  16  dining table  \n",
       "  17          bowl  \n",
       "  18      keyboard  \n",
       "  19     microwave  ,\n",
       "  'caption': ['Refrigerator behind white shirted man.'],\n",
       "  'bbox_target': [36.67, 147.78, 108.95, 163.95]},\n",
       " 963: {'image_emb': tensor([[-0.1609,  0.2837,  0.2216,  ...,  0.6919, -0.0756, -0.3926],\n",
       "          [-0.2737,  0.3953, -0.3074,  ...,  1.2783, -0.0687, -0.1925],\n",
       "          [-0.1708,  0.2397,  0.2352,  ...,  0.6260, -0.0444, -0.4048]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0091,  0.2988, -0.0649,  ...,  0.2561, -0.1313, -0.0526],\n",
       "          [-0.4165,  0.1030, -0.4370,  ...,  0.2954, -0.6719, -0.0954]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.1562e-01, 1.1921e-07, 4.8438e-01],\n",
       "          [4.7437e-01, 3.6011e-02, 4.8950e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    0.727524    0.356018  450.000000  638.317688    0.901109      0   \n",
       "  1  272.370361  306.004974  341.426697  361.398651    0.817803     67   \n",
       "  2    0.054581  199.638550   63.177994  379.097473    0.633757      0   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1  cell phone  \n",
       "  2      person  ,\n",
       "  'caption': ['A man wearing gold sunglasses talking on a cell phone',\n",
       "   'man holding cell phone'],\n",
       "  'bbox_target': [0.0, 0.0, 447.28, 632.81]},\n",
       " 964: {'image_emb': tensor([[-0.2947,  0.3044, -0.1949,  ...,  0.9307, -0.2888,  0.3018],\n",
       "          [-0.1116,  0.3472, -0.5444,  ...,  0.9648, -0.0472,  0.2078],\n",
       "          [-0.4534,  0.1908, -0.5737,  ...,  0.6899,  0.0359,  0.3020],\n",
       "          [-0.1033,  0.1707, -0.2703,  ...,  0.9756,  0.0697,  0.1691],\n",
       "          [-0.4724, -0.1448, -0.2163,  ...,  0.6685,  0.0130,  0.2654]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1948,  0.1854, -0.3831,  ..., -0.3645, -0.1057,  0.0211],\n",
       "          [-0.2827,  0.3093, -0.1228,  ...,  0.0513, -0.0250, -0.3035]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.7790e-03, 7.8271e-01, 2.0740e-01, 2.2590e-05, 7.0953e-03],\n",
       "          [7.1838e-02, 4.5410e-01, 4.6851e-01, 4.1723e-07, 5.3711e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  145.932281  189.914001  281.234558  508.735535    0.941288      0   \n",
       "  1  330.617126  137.632202  459.192322  508.149048    0.935481      0   \n",
       "  2  379.761871  318.978790  500.416107  508.023071    0.921715     38   \n",
       "  3  158.235229  444.056335  189.903015  509.534363    0.728535     38   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2  tennis racket  \n",
       "  3  tennis racket  ,\n",
       "  'caption': ['A woman in a red top.',\n",
       "   'Woman wearing a red short-sleeved shirt and a white tennis skort.'],\n",
       "  'bbox_target': [327.78, 145.63, 136.38, 356.42]},\n",
       " 965: {'image_emb': tensor([[ 0.5249, -0.0568, -0.4446,  ...,  0.7554,  0.0454, -0.2131],\n",
       "          [ 0.1236, -0.0059, -0.2098,  ...,  0.5811, -0.1033, -0.1372]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2129,  0.1898,  0.1190,  ..., -0.2627, -0.1477, -0.0556],\n",
       "          [ 0.1307, -0.0814,  0.1414,  ...,  0.1826, -0.4915,  0.0186]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.8687, 0.1312],\n",
       "          [0.3774, 0.6226]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  148.371826   76.615356  588.058960  292.835510    0.864856      8   boat\n",
       "  1  551.090637  194.407593  640.000000  261.697937    0.658546      8   boat\n",
       "  2    0.311298  110.922638   42.656200  260.391266    0.640452     56  chair\n",
       "  3    0.737017  111.821198   43.102219  261.334930    0.410714      8   boat,\n",
       "  'caption': ['A blue and red boat with the characters NN200 printed on the side',\n",
       "   'A blue boat on a beach.'],\n",
       "  'bbox_target': [172.18, 97.29, 412.49, 194.89]},\n",
       " 966: {'image_emb': tensor([[-0.1840,  0.0574, -0.2242,  ...,  0.4817,  0.1041, -0.0021],\n",
       "          [-0.3328, -0.0574,  0.0240,  ...,  0.6533,  0.5337,  0.0504],\n",
       "          [-0.3303,  0.1533, -0.2325,  ...,  0.4380,  0.1779, -0.0821],\n",
       "          [-0.1632, -0.1924, -0.1205,  ...,  0.6372,  0.0181,  0.1326]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1991, -0.1142, -0.2395,  ...,  0.0213, -0.0023,  0.0331],\n",
       "          [-0.3091, -0.3184, -0.0260,  ...,  0.0051,  0.0788,  0.3828]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3545, 0.3032, 0.2218, 0.1206],\n",
       "          [0.1411, 0.1305, 0.1959, 0.5327]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0   82.990051  121.263153  402.765533  375.145416    0.944389     22  zebra\n",
       "  1  383.249084  202.364685  626.868652  330.273376    0.929808     22  zebra\n",
       "  2    0.154266   84.994186  235.063675  226.117554    0.901680     22  zebra,\n",
       "  'caption': ['a zebra on the right, behind the rock',\n",
       "   'The farthest right zebra.'],\n",
       "  'bbox_target': [385.02, 207.98, 244.93, 130.63]},\n",
       " 967: {'image_emb': tensor([[-0.3853,  0.4077,  0.0571,  ...,  1.2119, -0.2715,  0.0415],\n",
       "          [ 0.0881,  0.3694, -0.1902,  ...,  0.5132,  0.1650, -0.0327],\n",
       "          [-0.4512,  0.6245, -0.1979,  ...,  0.8213, -0.3330, -0.0910],\n",
       "          [ 0.0387,  0.2678, -0.2399,  ...,  0.8945, -0.0290,  0.4939],\n",
       "          [ 0.0681,  0.3093, -0.0863,  ...,  0.6890, -0.1499,  0.6040],\n",
       "          [ 0.1221,  0.4575, -0.0330,  ...,  0.6177, -0.1519,  0.1885]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-2.7661e-01, -2.7441e-01, -1.9873e-01,  1.0400e-01, -1.3733e-01,\n",
       "           -1.2024e-01, -4.6844e-03,  4.9591e-02,  8.4326e-01,  3.8843e-01,\n",
       "            3.0835e-01, -2.8882e-01,  5.6494e-01,  1.1115e-01,  2.3877e-01,\n",
       "            1.0577e-01, -7.8491e-02,  1.4990e-01,  9.7168e-02,  3.2373e-01,\n",
       "            4.9731e-01,  2.0706e-02, -1.0089e-01, -5.3662e-01, -3.8062e-01,\n",
       "           -1.2671e-01, -2.1838e-01, -2.6709e-01, -3.2715e-01,  1.2512e-01,\n",
       "           -3.3179e-01,  4.6509e-01,  3.3569e-01,  2.9395e-01, -3.4058e-01,\n",
       "           -3.4473e-01, -2.3767e-01, -2.7197e-01, -1.1542e-01, -2.2791e-01,\n",
       "            6.1829e-02,  3.1250e-02,  9.0714e-03, -3.7402e-01,  4.4873e-01,\n",
       "           -3.2196e-02,  8.7952e-02,  1.0687e-01, -4.1077e-02, -2.4280e-01,\n",
       "           -1.5149e-01,  2.0203e-01,  2.4304e-01, -3.0298e-01, -1.9531e-01,\n",
       "            4.2358e-01, -3.7061e-01,  9.2957e-02,  2.1814e-01,  1.2741e-02,\n",
       "           -1.2964e-01, -1.3306e-01,  3.6841e-01,  2.4768e-01, -7.1960e-02,\n",
       "           -3.0566e-01,  5.3650e-02, -1.9135e-02,  7.8430e-02, -5.6091e-02,\n",
       "            1.5613e-01, -1.7517e-01,  4.5654e-01,  1.9458e-01, -1.6284e-01,\n",
       "           -8.0566e-02, -5.1178e-02,  4.8615e-02, -6.5430e-02, -1.1786e-01,\n",
       "           -1.6675e-01, -3.0469e-01,  3.2227e-01,  5.6427e-02, -1.2048e-01,\n",
       "           -2.9321e-01,  1.7664e-01, -9.1736e-02, -1.5369e-01,  2.1472e-01,\n",
       "            5.7190e-02,  1.3257e-01, -8.2227e-01,  3.9642e-02, -2.8613e-01,\n",
       "           -7.1838e-02,  3.5815e-01, -3.2373e-01,  2.4866e-01, -3.3960e-01,\n",
       "           -2.7661e-01,  3.7988e-01, -2.0972e-01, -4.1699e-01, -2.8491e-01,\n",
       "           -2.8931e-01, -6.9971e-01,  2.5415e-01,  4.3030e-02, -1.7147e-03,\n",
       "           -1.2488e-01, -7.6465e-01,  7.2815e-02,  5.5389e-02, -6.4575e-02,\n",
       "           -7.6355e-02,  1.9153e-01, -6.5613e-02, -5.9033e-01,  3.9575e-01,\n",
       "            3.3252e-01, -2.7808e-01, -3.9111e-01, -1.7651e-01,  4.1992e-02,\n",
       "           -3.1958e-01, -2.9346e-01, -2.9541e-01,  4.5227e-02, -6.0974e-02,\n",
       "            2.3303e-01, -1.2878e-01, -2.4988e-01,  3.8730e+00,  1.3989e-01,\n",
       "            2.1057e-02,  2.8149e-01, -3.9990e-01, -8.1604e-02, -9.5947e-02,\n",
       "           -1.4490e-01,  4.9530e-02, -2.1216e-01,  2.1289e-01, -2.4963e-01,\n",
       "           -1.0468e-02,  1.8091e-01, -7.5146e-01,  1.3672e-01, -3.4741e-01,\n",
       "           -5.1318e-01,  6.0699e-02,  4.0771e-01,  2.1252e-01,  2.2888e-01,\n",
       "           -2.5009e-02, -1.2671e-01, -3.9282e-01,  6.8665e-02, -3.5583e-02,\n",
       "           -1.0535e-01,  2.2430e-02, -5.6641e-01,  9.4727e-02,  2.7075e-01,\n",
       "            1.4038e-01,  2.0740e-01,  1.0968e-01,  3.2153e-01, -4.5972e-01,\n",
       "           -1.4209e-01, -3.3960e-01,  4.6722e-02,  3.8892e-01,  2.9053e-01,\n",
       "            1.1841e-01, -9.4849e-02, -8.6304e-02,  2.4133e-01,  9.5276e-02,\n",
       "           -4.0527e-01, -3.1958e-01, -3.4882e-02, -4.2651e-01, -2.7148e-01,\n",
       "           -3.7427e-01,  2.1265e-01, -1.0155e-02, -1.6528e-01,  5.0621e-03,\n",
       "           -1.2372e-01,  5.6641e-01,  1.8494e-01, -2.6465e-01,  1.8494e-01,\n",
       "           -3.8428e-01, -1.2030e-01,  6.7041e-01, -8.3984e-02, -1.9507e-01,\n",
       "            3.9575e-01,  1.4331e-01,  8.5083e-02,  3.4961e-01,  1.0046e-01,\n",
       "           -1.8323e-01,  1.2756e-01, -1.7285e-01, -1.6235e-01,  5.1514e-02,\n",
       "            1.1322e-01,  2.6001e-01,  4.1333e-01,  1.3623e-01, -4.2188e-01,\n",
       "            3.1860e-02,  1.2610e-01, -3.6011e-01,  2.5757e-01, -8.2825e-02,\n",
       "            3.6621e-01, -5.5615e-01,  1.8945e-01,  6.5308e-02,  7.1960e-02,\n",
       "            1.8176e-01,  6.0425e-02,  2.0044e-01,  4.9585e-01,  6.7627e-02,\n",
       "           -2.9932e-01,  2.9810e-01,  2.1973e-01,  5.8228e-02,  1.5344e-01,\n",
       "           -1.3428e-01, -2.1387e-01, -3.0396e-01, -2.4487e-01, -2.5635e-01,\n",
       "            7.1716e-02, -2.2424e-01,  3.7866e-01,  6.8726e-02, -2.7441e-01,\n",
       "            1.0684e+00, -2.2607e-01,  2.4658e-01, -5.6836e-01,  4.7803e-01,\n",
       "           -3.7500e-01, -3.8261e-03, -2.5464e-01, -3.6426e-01, -4.7778e-01,\n",
       "           -2.4414e-01, -2.7441e-01, -5.3558e-02,  1.8530e-01,  7.1655e-02,\n",
       "           -2.3633e-01, -2.0471e-01, -1.5930e-01,  1.5405e-01,  3.7793e-01,\n",
       "           -2.1790e-01,  1.7004e-01,  6.8237e-02,  9.2163e-02,  6.2744e-02,\n",
       "           -2.0789e-01,  2.6685e-01,  1.6675e-01, -2.0789e-01, -2.0398e-01,\n",
       "            3.7720e-01, -1.2054e-01,  6.5857e-02, -2.5122e-01,  1.6113e-01,\n",
       "            2.3730e-01,  2.7490e-01,  1.4160e-01,  3.5669e-01, -8.3679e-02,\n",
       "            1.9910e-01,  1.9397e-01, -3.3594e-01,  4.1595e-02, -3.5400e-01,\n",
       "            9.3750e-02, -2.8625e-02,  2.8467e-01,  5.6641e-01, -1.3879e-01,\n",
       "            2.0471e-01, -2.0361e-01,  2.6831e-01,  6.0822e-02,  9.0759e-02,\n",
       "           -4.8413e-01,  1.6187e-01,  3.3813e-01,  2.2571e-01,  3.0518e-01,\n",
       "           -3.3154e-01, -7.6660e-02,  5.0635e-01, -2.4658e-01, -2.9175e-02,\n",
       "           -8.6548e-02,  6.7188e-01,  3.8809e+00, -7.7515e-02,  2.4719e-01,\n",
       "            2.5854e-01,  1.7273e-01,  2.5366e-01,  4.3579e-01,  4.8926e-01,\n",
       "           -2.4194e-01, -4.5074e-02,  3.6865e-02, -1.3794e-01, -6.1670e-01,\n",
       "           -2.5635e-01,  5.2429e-02, -1.5045e-02, -1.0303e-01, -8.4180e-01,\n",
       "            1.7505e-01, -9.6313e-02, -1.1426e-01, -6.4941e-01, -1.4172e-01,\n",
       "           -1.3403e-01,  1.4664e-02,  2.1692e-01, -4.5166e-02,  1.6943e-01,\n",
       "            6.0730e-02,  1.2878e-01,  2.7979e-01, -2.3303e-01,  4.6289e-01,\n",
       "            8.1970e-02,  1.6345e-01,  6.7139e-02,  1.3013e-01, -2.7930e-01,\n",
       "            4.5972e-01, -7.3975e-02,  4.3945e-02,  8.6731e-02, -3.8525e-01,\n",
       "           -5.4492e-01, -3.3496e-01,  7.7026e-02,  1.0834e-01,  2.6489e-01,\n",
       "           -3.2007e-01, -1.5906e-01, -3.4668e-02, -1.9336e-01, -4.1016e-01,\n",
       "            7.2632e-02,  3.3374e-01,  2.3773e-02, -1.9153e-01,  5.9204e-02,\n",
       "            2.1399e-01, -2.1332e-02, -6.6895e-02, -1.3525e-01, -1.6553e-01,\n",
       "           -2.0117e-01, -6.6895e-02, -4.2664e-02, -6.4453e-02, -4.4312e-02,\n",
       "           -3.1104e-01, -2.7173e-01,  2.9639e-01, -8.4106e-02, -3.2593e-01,\n",
       "            7.1655e-02,  2.5708e-01,  8.3313e-02, -1.7896e-01,  1.2341e-01,\n",
       "           -8.4766e-01,  3.7012e-01, -1.1011e-01,  1.6870e-01, -1.4075e-01,\n",
       "            8.7036e-02, -2.1887e-01, -8.1421e-02,  1.1279e-01,  5.1807e-01,\n",
       "            1.2909e-02, -2.7294e-03, -5.4657e-02, -8.7891e-02, -1.7566e-01,\n",
       "            2.6807e-01, -6.9092e-01, -2.9126e-01, -6.9885e-02,  1.6870e-01,\n",
       "            2.0337e-01, -4.2749e-01, -3.7524e-01,  6.2805e-02,  5.9113e-02,\n",
       "           -3.8599e-01, -3.5254e-01, -1.4807e-01,  1.6626e-01, -5.0537e-01,\n",
       "            2.4597e-01,  1.5442e-01,  7.2670e-03, -1.8591e-01,  8.2825e-02,\n",
       "           -2.6001e-01, -2.5879e-01,  6.3110e-02, -4.5090e-03,  2.0288e-01,\n",
       "           -2.4011e-01, -2.9556e-02, -4.4653e-01, -5.9863e-01, -7.6355e-02,\n",
       "           -1.2915e-01, -3.3789e-01, -2.5073e-01, -1.3342e-01, -2.1533e-01,\n",
       "           -1.5063e-01, -2.1436e-01,  2.1072e-02, -2.1570e-01,  2.5391e-01,\n",
       "           -6.1531e-03, -5.6787e-01,  4.0747e-01,  1.1224e-01, -9.3567e-02,\n",
       "           -3.4204e-01,  1.7957e-01, -1.4258e-01, -3.8971e-02, -3.7354e-01,\n",
       "           -9.3018e-02,  8.7830e-02,  1.5869e-02, -8.7402e-02, -1.1395e-01,\n",
       "            2.6929e-01,  2.4353e-01, -2.6196e-01,  1.8970e-01, -4.4434e-02,\n",
       "            3.3496e-01, -1.5710e-01, -2.0764e-01, -1.0382e-01,  4.6082e-02,\n",
       "            2.3120e-01,  7.1472e-02,  3.6304e-01,  9.0149e-02,  4.0680e-02,\n",
       "            1.3623e-01,  3.3447e-01, -2.3727e-02,  3.9014e-01,  4.9194e-01,\n",
       "           -1.1700e-01, -8.6426e-01,  1.0852e-01,  2.2705e-01, -1.8933e-01,\n",
       "           -4.0527e-01,  5.2490e-01, -5.3802e-02, -4.8431e-02, -9.7046e-02,\n",
       "            5.7251e-02, -1.0364e-01, -5.9021e-02,  1.2031e+00,  4.3384e-01,\n",
       "            1.8188e-02,  1.4551e-01, -9.3872e-02, -2.3608e-01,  1.9373e-01,\n",
       "            5.0098e-01,  3.5400e-02, -8.3351e-04,  2.0459e-01,  8.2031e-01,\n",
       "            1.4246e-01, -1.1633e-01,  3.9276e-02,  3.2983e-01,  1.6260e-01,\n",
       "            3.3276e-01,  5.4352e-02]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.3591e-03, 9.2387e-06, 3.9787e-03, 2.3486e-01, 1.3599e-01, 6.1865e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   115.446945  254.632935  436.518005  477.881348    0.936506     56   \n",
       "  1   533.935181  295.673859  575.874634  328.117035    0.915798     64   \n",
       "  2   249.902542  227.152863  426.508484  278.612091    0.907492     66   \n",
       "  3   162.075348   76.247192  298.104523  202.666626    0.868150     62   \n",
       "  4   299.003235   97.968887  418.772766  213.393677    0.848878     62   \n",
       "  5    39.504486  104.862762  169.692886  233.511627    0.636556     62   \n",
       "  6   435.761749  258.819855  472.292328  291.267365    0.631931     64   \n",
       "  7    39.158859  105.576843  218.600876  258.485779    0.502851     63   \n",
       "  8   411.445038  105.412994  534.619873  227.753174    0.458637     62   \n",
       "  9     0.463115    0.000000   27.430458   27.047241    0.447815     73   \n",
       "  10   53.802181    3.459602  156.613983   25.648186    0.284949     73   \n",
       "  \n",
       "          name  \n",
       "  0      chair  \n",
       "  1      mouse  \n",
       "  2   keyboard  \n",
       "  3         tv  \n",
       "  4         tv  \n",
       "  5         tv  \n",
       "  6      mouse  \n",
       "  7     laptop  \n",
       "  8         tv  \n",
       "  9       book  \n",
       "  10      book  ,\n",
       "  'caption': ['a black computer monitor second from the left of four monitors'],\n",
       "  'bbox_target': [165.63, 76.51, 129.92, 124.96]},\n",
       " 968: {'image_emb': tensor([[ 0.3435, -0.3276, -0.2457,  ...,  0.3281, -0.0243, -0.3596],\n",
       "          [ 0.4153, -0.3481, -0.0815,  ...,  0.3240,  0.0275, -0.1219]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0834, -0.1117, -0.2343,  ...,  0.1355, -0.2512, -0.0677],\n",
       "          [ 0.1578, -0.1829, -0.3401,  ..., -0.1674,  0.1216, -0.2224]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3345, 0.6655],\n",
       "          [0.6582, 0.3416]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  120.393822  163.729126  375.808014  433.464539    0.900489     23  giraffe,\n",
       "  'caption': ['A LARGE GIRAFEE STANDING BEHIND A WOODEN FENCE.',\n",
       "   'A giraffe looking over a post'],\n",
       "  'bbox_target': [119.15, 163.58, 256.85, 273.36]},\n",
       " 969: {'image_emb': tensor([[-0.2209, -0.0608, -0.1548,  ...,  0.8169,  0.1958, -0.3545],\n",
       "          [ 0.0299,  0.3857,  0.0201,  ...,  1.1992,  0.0497, -0.1804],\n",
       "          [-0.1237,  0.3901, -0.4834,  ...,  1.3213, -0.2666, -0.2487],\n",
       "          ...,\n",
       "          [ 0.0381,  0.0786, -0.2754,  ...,  0.8511, -0.0825, -0.1665],\n",
       "          [ 0.0726,  0.1239, -0.3438,  ...,  0.9224, -0.1876, -0.1936],\n",
       "          [-0.3022, -0.2152, -0.1608,  ...,  0.5967,  0.0748, -0.4678]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1154,  0.1708,  0.1642,  ...,  0.7349, -0.2031, -0.4270],\n",
       "          [ 0.1112,  0.1285, -0.2175,  ..., -0.0762,  0.0829, -0.2698]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7700e-02, 1.1429e-02, 8.1848e-02, 8.1024e-03, 2.7847e-02, 2.3723e-04,\n",
       "           8.5303e-01],\n",
       "          [2.1210e-02, 1.1002e-02, 1.6689e-06, 8.8770e-01, 7.1526e-07, 2.9504e-05,\n",
       "           8.0017e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  274.359253    3.123398  639.695068  420.547119    0.925169      0   \n",
       "  1    0.806152   61.445023  289.283752  420.303589    0.902591      0   \n",
       "  2  265.865631  340.758911  325.170319  389.192444    0.867495     67   \n",
       "  3  206.053345    0.000000  390.122437  345.877441    0.780245      0   \n",
       "  4  302.423157   45.541428  334.796143   88.496597    0.744942     67   \n",
       "  5  430.192474  176.763672  463.744171  423.206726    0.728885     27   \n",
       "  6  619.668640   15.101707  639.971375  176.065521    0.458090      0   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2  cell phone  \n",
       "  3      person  \n",
       "  4  cell phone  \n",
       "  5         tie  \n",
       "  6      person  ,\n",
       "  'caption': ['man yelling at elderly man while holding cell phone',\n",
       "   'man wearing black coat and red plaid scarf'],\n",
       "  'bbox_target': [205.01, 0.0, 200.71, 343.39]},\n",
       " 970: {'image_emb': tensor([[ 0.5508,  0.3833, -0.5449,  ...,  1.0195, -0.4280, -0.4631],\n",
       "          [ 0.0138,  0.2554, -0.1655,  ...,  1.0840, -0.2896, -0.2061],\n",
       "          [ 0.1620,  0.4734, -0.0231,  ...,  0.7822, -0.5381, -0.5371],\n",
       "          [-0.2166,  0.4712, -0.0059,  ...,  0.9238, -0.3223, -0.6011],\n",
       "          [ 0.7554,  0.5264, -0.6812,  ...,  0.9531, -0.3669, -0.3496]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0309, -0.0385, -0.1554,  ...,  0.4495, -0.1870, -0.3928],\n",
       "          [-0.1233, -0.0602,  0.1075,  ...,  0.1304, -0.0255, -0.5869]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.0967e-03, 9.9756e-01, 9.0933e-04, 1.4162e-04, 4.5729e-04],\n",
       "          [1.1978e-02, 9.0771e-01, 3.9886e-02, 3.4668e-02, 5.6572e-03]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0    1.552063   19.495331  625.523621  478.070190    0.841284     69   oven\n",
       "  1   26.159897  296.186493  349.093933  476.491699    0.829822     55   cake\n",
       "  2  364.837494  100.427307  537.577515  185.291016    0.813697     55   cake\n",
       "  3  357.785706  148.249405  532.214417  215.518494    0.789510     55   cake\n",
       "  4  355.344299  178.710724  527.302185  244.098785    0.689646     55   cake\n",
       "  5   20.312485  239.301239  238.471970  280.209320    0.629870     43  knife,\n",
       "  'caption': ['Brownies in a pyrex baking tray on the stove.',\n",
       "   'A brownie in a pan that just came out of the oven'],\n",
       "  'bbox_target': [25.83, 296.83, 325.02, 180.8]},\n",
       " 971: {'image_emb': tensor([[-0.2084, -0.0792, -0.3086,  ...,  0.9170,  0.0141,  0.0931],\n",
       "          [-0.1671,  0.3269, -0.0292,  ...,  1.1768, -0.2314,  0.3682],\n",
       "          [-0.1798,  0.2583, -0.2583,  ...,  1.4111,  0.2764,  0.1941],\n",
       "          [-0.4988,  0.1722, -0.1298,  ...,  0.9673, -0.1005,  0.0908],\n",
       "          [-0.4531, -0.2444, -0.1537,  ...,  0.4822, -0.0196,  0.2428]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0179, -0.1794, -0.1562,  ...,  0.3601, -0.2345, -0.2341],\n",
       "          [-0.3147, -0.2910, -0.0656,  ..., -0.3857, -0.4126, -0.3516]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[2.0117e-01, 4.1284e-01, 1.4842e-05, 2.1881e-02, 3.6426e-01],\n",
       "          [6.5186e-01, 3.3301e-01, 1.1325e-06, 1.4633e-02, 3.4404e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   33.867371   72.594650  210.277405  390.978210    0.948507      0   \n",
       "  1  205.799561  123.853577  296.019135  279.321106    0.937724      0   \n",
       "  2  532.816406  237.108185  607.485107  392.750397    0.865021     62   \n",
       "  3  158.863953  183.875641  350.324432  392.410797    0.785842     59   \n",
       "  4  421.183655  327.758240  485.580261  391.518677    0.404282     24   \n",
       "  5  480.839874  265.390015  540.979614  305.319275    0.384176     63   \n",
       "  6  184.821396  286.514038  209.426987  301.170715    0.258386     67   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2          tv  \n",
       "  3         bed  \n",
       "  4    backpack  \n",
       "  5      laptop  \n",
       "  6  cell phone  ,\n",
       "  'caption': ['a guy who is wearing a white shirt that is kneeling on a bed watching tv',\n",
       "   'A man on his knees in a white shirt.'],\n",
       "  'bbox_target': [208.47, 124.75, 91.59, 152.4]},\n",
       " 972: {'image_emb': tensor([[-0.2800,  0.4353,  0.3328,  ...,  0.9351,  0.1996,  0.3206],\n",
       "          [-0.2037,  0.1890, -0.3730,  ...,  1.2197, -0.2913,  0.3140],\n",
       "          [-0.3572,  0.2507, -0.0561,  ...,  1.5508, -0.0586, -0.0837],\n",
       "          [-0.2041,  0.3853, -0.2617,  ...,  1.5410, -0.1829, -0.1942],\n",
       "          [ 0.0515, -0.1460, -0.1552,  ...,  0.7354, -0.0852,  0.2573]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2065, -0.0746, -0.4170,  ..., -0.1477, -0.3293, -0.1750],\n",
       "          [ 0.1022, -0.1934, -0.4590,  ..., -0.2896, -0.3542, -0.0651]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[4.7588e-04, 3.2127e-05, 1.1108e-01, 8.8770e-01, 6.8188e-04],\n",
       "          [8.3590e-04, 4.3929e-05, 4.0674e-01, 5.9180e-01, 7.8487e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0    0.525171    1.066548  189.919235  330.352478    0.937374      0   person\n",
       "  1  110.944153  121.090958  358.679504  330.717102    0.927875     16      dog\n",
       "  2  186.731949   38.432747  310.705231  127.554657    0.881087     26  handbag\n",
       "  3  368.254639   41.930431  499.491211  125.745537    0.762672     26  handbag\n",
       "  4  171.947067    1.995915  499.119476  273.477570    0.524708     13    bench,\n",
       "  'caption': ['A bag sitting alone on a bench.',\n",
       "   'the bag on the right of the bench'],\n",
       "  'bbox_target': [369.72, 38.88, 130.28, 88.74]},\n",
       " 973: {'image_emb': tensor([[-0.1095,  0.6699, -0.2883,  ...,  1.2432, -0.1337, -0.2705],\n",
       "          [ 0.2974,  0.3203,  0.0391,  ...,  0.8857, -0.0742,  0.2493],\n",
       "          [ 0.1735,  0.0983,  0.0593,  ...,  0.6650,  0.5435, -0.1288],\n",
       "          [ 0.1816, -0.1099,  0.1248,  ...,  0.5552,  0.2830, -0.1973]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 6.7261e-02,  9.3262e-02, -2.6929e-01,  1.9336e-01, -3.0029e-01,\n",
       "           -1.8884e-01, -3.9404e-01, -1.1943e+00, -1.2042e-01, -1.4514e-01,\n",
       "           -1.0199e-01,  7.7576e-02,  1.6394e-01,  2.3352e-01, -1.5427e-02,\n",
       "            2.4147e-03, -8.7097e-02,  1.2007e-03, -1.7896e-01,  1.0040e-01,\n",
       "            4.1431e-01,  4.7412e-01, -6.0333e-02,  2.3828e-01,  1.7444e-01,\n",
       "           -4.2993e-01, -2.8442e-01,  6.0760e-02, -2.7344e-01,  2.0813e-01,\n",
       "            4.2053e-02, -2.9443e-01, -8.5693e-02,  1.3098e-01,  3.6255e-01,\n",
       "           -2.9102e-01,  1.8103e-01, -3.3643e-01, -1.0071e-02,  9.9335e-03,\n",
       "            2.7271e-01, -9.9854e-02, -2.7295e-01, -3.2495e-01,  3.1250e-01,\n",
       "            1.5283e-01, -3.2983e-01,  2.9419e-01, -2.5903e-01,  5.8380e-02,\n",
       "           -4.8798e-02, -2.6413e-02,  1.6235e-01,  4.4037e-02, -3.3618e-01,\n",
       "           -4.1382e-02, -2.3865e-02,  3.0640e-01, -3.4253e-01, -2.2607e-01,\n",
       "            5.3436e-02, -1.2952e-01, -3.4473e-01, -9.9365e-02, -1.0773e-02,\n",
       "            1.4160e-01,  1.9873e-01,  6.6797e-01,  1.1395e-01,  2.9956e-01,\n",
       "            9.2545e-03,  3.1204e-02,  2.4280e-01,  2.6611e-01,  1.6205e-02,\n",
       "           -1.5833e-01,  4.2603e-01,  2.2839e-01, -5.9174e-02,  3.9307e-02,\n",
       "           -7.2510e-01, -2.3120e-01, -4.3640e-02,  4.4983e-02, -1.8103e-01,\n",
       "            6.8848e-02, -2.4255e-01, -1.3135e-01,  2.2049e-02,  7.1228e-02,\n",
       "           -2.5436e-02,  9.7900e-02, -1.5391e+00,  4.0222e-02,  1.3440e-01,\n",
       "           -2.8418e-01,  2.3743e-01, -1.0883e-01,  4.3604e-01,  7.7148e-02,\n",
       "           -3.6499e-01, -1.1328e-01,  1.6516e-01, -1.4978e-01,  2.5098e-01,\n",
       "            5.1660e-01, -1.7456e-01,  6.6345e-02, -1.1383e-02,  6.4331e-02,\n",
       "            4.1016e-01,  3.5889e-01,  2.1484e-01, -2.6428e-02, -2.2559e-01,\n",
       "            6.2354e-01,  4.7943e-02,  1.5778e-02, -2.3035e-01,  1.3501e-01,\n",
       "            9.1370e-02, -2.8906e-01, -2.3157e-01,  1.9495e-01,  3.5303e-01,\n",
       "            1.5784e-01,  1.2469e-01,  2.2900e-01, -3.9215e-02, -1.3318e-01,\n",
       "            2.2729e-01, -2.9272e-01,  8.4076e-03,  5.0234e+00, -5.1221e-01,\n",
       "           -1.3257e-01, -3.3496e-01, -1.5808e-01, -1.9019e-01,  4.9286e-02,\n",
       "           -3.2397e-01,  1.1420e-01, -4.7070e-01,  2.6172e-01, -1.0278e-01,\n",
       "           -5.4077e-02, -5.8807e-02,  4.2725e-01, -2.8979e-01, -1.3256e-03,\n",
       "            2.8247e-01, -2.4524e-01,  4.9438e-01,  1.2537e-01, -8.6182e-02,\n",
       "           -9.5032e-02, -5.8350e-02, -1.1517e-01, -2.2095e-01,  7.8674e-02,\n",
       "            3.5815e-01, -7.0618e-02,  3.1348e-01, -7.9163e-02, -3.4210e-02,\n",
       "           -4.5074e-02, -1.0602e-01, -2.3514e-02,  5.3192e-02,  1.1798e-01,\n",
       "           -1.6870e-01, -2.6489e-01,  2.1045e-01, -1.7554e-01, -3.3228e-01,\n",
       "           -1.7371e-01, -2.2668e-01, -1.6211e-01, -4.2407e-01,  1.5503e-01,\n",
       "            1.1615e-01,  1.6711e-01,  4.3164e-01, -2.2815e-01, -2.0959e-01,\n",
       "           -1.7004e-01, -2.8540e-01, -2.3999e-01, -1.3892e-01,  2.9639e-01,\n",
       "            2.6880e-01, -2.6123e-01, -7.9880e-03,  2.1558e-01, -3.1519e-01,\n",
       "           -1.7725e-01, -3.6572e-01, -1.6333e-01, -1.9714e-01, -2.3376e-01,\n",
       "            1.2598e-01,  2.1606e-01, -3.5107e-01, -4.6936e-02, -1.1383e-02,\n",
       "            3.1470e-01, -9.1675e-02,  1.9836e-01,  2.5806e-01,  7.8491e-02,\n",
       "           -4.0137e-01,  3.7085e-01,  6.0120e-02,  2.2717e-01,  5.6152e-02,\n",
       "            1.3794e-02,  6.7566e-02, -2.1118e-01, -5.7007e-02, -3.3057e-01,\n",
       "            5.8655e-02, -4.1797e-01, -3.2007e-01, -2.9102e-01, -2.1362e-01,\n",
       "            3.2910e-01,  1.4392e-01, -3.8965e-01, -1.5552e-01, -2.8149e-01,\n",
       "           -2.5024e-01,  2.6709e-01, -2.0309e-02, -8.7341e-02,  1.2500e-01,\n",
       "            1.2503e-03, -1.0077e-01, -2.7124e-01,  1.5466e-01, -1.2152e-01,\n",
       "            1.6516e-01,  1.0944e-01,  3.9355e-01,  1.7990e-02, -5.3564e-01,\n",
       "            4.9774e-02, -1.3245e-01, -9.1171e-04,  4.3488e-02,  1.2915e-01,\n",
       "            5.2795e-02, -2.7026e-01,  1.8689e-01,  1.4636e-01,  2.1973e-02,\n",
       "           -9.4360e-02,  2.3938e-01,  8.4045e-02,  1.7554e-01,  1.0992e-01,\n",
       "           -5.0244e-01,  1.7603e-01, -2.1591e-02,  5.5664e-02,  3.4351e-01,\n",
       "           -1.7859e-01,  9.4360e-02,  1.6052e-01,  1.5833e-01,  3.6377e-01,\n",
       "            6.0760e-02,  1.1407e-01,  4.7559e-01, -1.8433e-01, -2.3914e-01,\n",
       "            2.1069e-01, -5.8197e-02,  4.2676e-01,  3.2227e-01, -4.6120e-03,\n",
       "           -2.8003e-01,  1.9165e-01, -7.4463e-02,  5.2637e-01,  1.2108e-02,\n",
       "            5.6244e-02,  1.6785e-01,  1.8127e-01,  3.9014e-01, -3.9673e-02,\n",
       "           -1.2793e-01,  6.0394e-02,  3.9581e-02, -1.2793e-01, -3.1799e-02,\n",
       "           -3.7524e-01, -4.2896e-01, -1.5674e-01, -2.0605e-01,  1.8396e-01,\n",
       "           -2.4704e-02, -5.2979e-02,  3.5767e-01,  2.1838e-01,  3.0542e-01,\n",
       "           -2.4939e-01,  1.9104e-01, -1.6937e-02,  1.8420e-01, -1.0236e-01,\n",
       "           -1.9263e-01,  7.7576e-02,  5.0156e+00,  3.2812e-01, -1.4671e-02,\n",
       "            5.0586e-01,  1.5564e-01, -5.7739e-02, -9.2102e-02,  8.3008e-01,\n",
       "           -2.4622e-01, -3.6157e-01,  2.2876e-01,  7.8857e-02, -3.7158e-01,\n",
       "           -6.1707e-02, -2.5977e-01, -1.4172e-01,  7.7454e-02, -2.6328e+00,\n",
       "           -1.8042e-01,  1.1127e-01,  3.8452e-01,  3.2471e-01,  4.3526e-03,\n",
       "           -2.2864e-01, -3.8757e-02,  2.3633e-01,  3.0688e-01,  1.5515e-01,\n",
       "           -1.6711e-01,  3.2788e-01, -1.8958e-01, -1.4771e-01,  2.5098e-01,\n",
       "            1.9666e-01,  2.0288e-01,  2.1484e-01,  1.3147e-01,  1.1298e-01,\n",
       "            7.6855e-01,  1.4880e-01, -5.4565e-02,  2.4976e-01, -1.9821e-02,\n",
       "            4.7192e-01, -1.5137e-01,  1.8204e-02,  3.8849e-02, -1.1194e-01,\n",
       "           -2.3773e-02,  1.1243e-01, -3.7933e-02,  3.2202e-01,  3.9624e-01,\n",
       "            7.0801e-02, -1.3208e-01,  2.0947e-01, -2.0813e-01,  4.1309e-01,\n",
       "           -1.5039e-01, -2.5421e-02,  4.0625e-01,  5.7159e-02,  1.6272e-01,\n",
       "           -5.8990e-02, -4.2206e-02,  1.8005e-02,  4.0234e-01,  4.1962e-02,\n",
       "            3.4375e-01,  1.5308e-01, -2.1362e-01, -7.7783e-01, -9.4727e-02,\n",
       "            3.3594e-01, -8.3069e-02, -2.7319e-01, -3.7085e-01, -7.8003e-02,\n",
       "           -1.3037e-01, -4.7424e-02,  5.8594e-02, -5.1331e-02, -1.4209e-01,\n",
       "           -1.7773e-01,  9.2077e-04, -2.9370e-01,  9.7885e-03,  4.9243e-01,\n",
       "            1.3245e-01,  1.6211e-01, -5.5542e-02, -8.0933e-02,  2.3041e-02,\n",
       "           -1.6846e-02,  1.9360e-01, -1.4563e-01, -3.0737e-01, -2.0462e-02,\n",
       "            1.0571e-01,  1.8542e-01,  1.7932e-01, -4.6582e-01,  3.0823e-02,\n",
       "           -2.6685e-01,  3.3838e-01,  2.8613e-01, -5.5267e-02, -4.3106e-03,\n",
       "           -1.4172e-01, -2.6703e-02,  3.1030e-01, -6.8896e-01,  8.1543e-02,\n",
       "           -7.0679e-02, -2.3145e-01, -3.0542e-01,  4.5752e-01,  2.4597e-01,\n",
       "           -1.7586e-03, -1.8384e-01, -1.3635e-01,  2.7344e-01, -1.9943e-02,\n",
       "            4.3164e-01, -4.0924e-02, -7.4524e-02,  1.0760e-01, -2.2278e-01,\n",
       "            8.7952e-02,  1.1444e-01, -2.9565e-01,  1.2408e-01, -5.3955e-01,\n",
       "            1.2463e-01, -3.3838e-01,  1.3049e-01, -8.5022e-02,  2.1289e-01,\n",
       "           -7.2998e-01,  3.3521e-01, -2.5732e-01,  4.2871e-01,  9.4727e-02,\n",
       "            9.2041e-02,  2.0374e-01,  1.5417e-01, -1.3049e-01,  1.1023e-01,\n",
       "            1.1566e-01,  1.6516e-01,  1.1646e-01,  6.3515e-03,  7.8003e-02,\n",
       "            1.2274e-01, -2.7979e-01,  1.5366e-02, -3.5034e-02, -1.0809e-01,\n",
       "            3.4326e-01,  2.2437e-01,  3.4741e-01,  4.6143e-02,  3.4961e-01,\n",
       "            2.4475e-01, -6.1249e-02,  9.0088e-02,  1.5845e-01,  3.2397e-01,\n",
       "           -2.0422e-01, -5.4004e-01, -4.3896e-01, -5.4199e-02, -7.7148e-01,\n",
       "            2.5879e-02,  3.3984e-01,  4.5898e-01,  1.6345e-01, -4.8608e-01,\n",
       "            4.0576e-01,  2.0410e-01,  4.8242e-01,  4.4824e-01, -1.0986e-01,\n",
       "           -1.0248e-01, -2.1005e-04,  3.6865e-02,  2.9980e-01,  1.1725e-01,\n",
       "            1.7847e-01, -2.3474e-01,  1.2720e-01,  2.4890e-01,  3.9185e-02,\n",
       "           -3.6987e-01, -3.3276e-01,  1.7395e-01, -4.0063e-01,  6.1035e-03,\n",
       "           -2.6733e-01,  2.9224e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.7266, 0.0895, 0.1499, 0.0340]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  310.973267  102.420013  455.038086  635.563232    0.906107      0   \n",
       "  1   76.405487  222.867676  322.812286  636.736389    0.899902     31   \n",
       "  2    3.341934   35.063934  447.934692  638.559570    0.898033      0   \n",
       "  3   19.528122  138.265594  121.479202  306.116730    0.674522      0   \n",
       "  4    0.000000  141.103577  121.622345  573.696777    0.561601      0   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1  snowboard  \n",
       "  2     person  \n",
       "  3     person  \n",
       "  4     person  ,\n",
       "  'caption': ['The guy with blue pants.'],\n",
       "  'bbox_target': [312.06, 101.54, 144.94, 501.92]},\n",
       " 974: {'image_emb': tensor([[ 5.7800e-02,  2.5122e-01, -3.9331e-01,  ...,  1.2363e+00,\n",
       "            2.4097e-01, -2.5940e-04],\n",
       "          [-1.5955e-01,  4.5166e-01, -1.2866e-01,  ...,  1.2090e+00,\n",
       "            2.8979e-01,  1.7969e-01],\n",
       "          [-2.4341e-01,  3.1665e-01, -3.7506e-02,  ...,  7.0215e-01,\n",
       "            1.2830e-01,  3.4668e-02],\n",
       "          [-3.0273e-01,  3.6157e-01,  3.9746e-01,  ...,  3.4180e-01,\n",
       "           -3.1030e-01, -1.8530e-01],\n",
       "          [-6.2073e-02,  4.1504e-01,  7.3059e-02,  ...,  9.5068e-01,\n",
       "            5.7983e-03, -2.6855e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3120,  0.2433, -0.4680,  ..., -0.2600, -0.1072, -0.3840],\n",
       "          [ 0.1986,  0.4900, -0.6040,  ..., -0.2429,  0.0147, -0.0690]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.6907e-02, 9.8291e-01, 8.4698e-05, 1.0252e-05, 6.6161e-06],\n",
       "          [5.2738e-04, 9.9951e-01, 2.2650e-06, 5.9605e-08, 5.9605e-08]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin       ymin        xmax        ymax  confidence  class  \\\n",
       "  0    49.211300  63.541626  130.468430  213.489624    0.904292     16   \n",
       "  1    99.480156  70.472748  246.173462  213.931305    0.836299      0   \n",
       "  2   416.351837  82.328278  563.013733  213.437042    0.749428      0   \n",
       "  3   363.804291   0.000000  613.041809  211.604279    0.734163     59   \n",
       "  4   239.924347  29.681244  294.197296  204.388641    0.648366     56   \n",
       "  5   320.390381   0.000000  387.355835  212.507843    0.590450     56   \n",
       "  6     0.730869   0.000000   70.266571  211.027679    0.560382     56   \n",
       "  7    48.909096  41.944473   97.202843  130.187927    0.433056     56   \n",
       "  8    58.207397   0.000000  302.033020  208.596771    0.391262     59   \n",
       "  9     0.164646   0.000000   69.411301  212.096313    0.365440     60   \n",
       "  10  553.365723  43.674957  608.026855  208.988495    0.289370     56   \n",
       "  11   49.872978  43.199402   97.018761  116.030701    0.275338     16   \n",
       "  \n",
       "              name  \n",
       "  0            dog  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3            bed  \n",
       "  4          chair  \n",
       "  5          chair  \n",
       "  6          chair  \n",
       "  7          chair  \n",
       "  8            bed  \n",
       "  9   dining table  \n",
       "  10         chair  \n",
       "  11           dog  ,\n",
       "  'caption': ['A man snuggling his dog',\n",
       "   'A man with a black shirt holding up a dog'],\n",
       "  'bbox_target': [101.5, 72.83, 142.51, 141.17]},\n",
       " 975: {'image_emb': tensor([[ 0.1536,  0.1892, -0.2510,  ...,  0.9775,  0.3735,  0.0345],\n",
       "          [ 0.4080, -0.3174,  0.0071,  ...,  0.7026,  0.3530, -0.1511],\n",
       "          [ 0.1865,  0.3528, -0.0082,  ...,  0.4578, -0.2118, -0.0057],\n",
       "          [ 0.4512, -0.0242,  0.0139,  ...,  0.4795,  0.3486, -0.4700],\n",
       "          [ 0.3911, -0.0067,  0.0042,  ...,  0.5220,  0.0564, -0.4294]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2773, -0.1624, -0.1097,  ..., -0.0127, -0.1969, -0.4763],\n",
       "          [ 0.1362, -0.4465, -0.4895,  ...,  0.0464, -0.2156, -0.4229]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0097, 0.0119, 0.0209, 0.0676, 0.8896],\n",
       "          [0.0056, 0.0133, 0.0496, 0.0253, 0.9062]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  322.811066  152.149597  416.821259  333.944885    0.921200     18  sheep\n",
       "  1  230.051315  140.270859  352.161133  326.709106    0.899793     18  sheep\n",
       "  2  418.027679  221.571320  548.686096  361.358856    0.882044     16    dog\n",
       "  3  116.967628  159.894348  295.260620  328.691101    0.881781     18  sheep,\n",
       "  'caption': ['the sheep closest to the dog', 'lamb on right in front of dog'],\n",
       "  'bbox_target': [322.54, 153.02, 91.63, 178.68]},\n",
       " 976: {'image_emb': tensor([[-0.2537, -0.0273,  0.0989,  ...,  1.2783,  0.0893, -0.4834],\n",
       "          [ 0.0184,  0.1261, -0.2007,  ...,  1.1924,  0.3818, -0.3325],\n",
       "          [ 0.1453, -0.0588, -0.0543,  ...,  0.9404,  0.0792, -0.2305],\n",
       "          [ 0.6006,  0.1949,  0.1417,  ...,  0.4412, -0.1089, -0.0331]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0957, -0.0340,  0.2021,  ..., -0.0254, -0.3730, -0.2554],\n",
       "          [ 0.1573,  0.1722,  0.1429,  ...,  0.2125, -0.1398, -0.2474]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.5906e-04, 1.7345e-05, 8.0762e-01, 1.9189e-01],\n",
       "          [9.1314e-05, 6.1393e-06, 9.6484e-01, 3.5156e-02]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   345.106689   57.239426  380.972351  106.827652    0.756227     56   \n",
       "  1   239.949631   60.215378  272.750153   99.278152    0.750284     56   \n",
       "  2     1.184349  114.736588  202.960785  218.929626    0.732950      8   \n",
       "  3   191.959503   10.901375  616.796631  214.222595    0.693888      8   \n",
       "  4   621.805542   59.564911  640.000000  119.944672    0.684263     56   \n",
       "  5   496.268768   53.094055  532.259583  117.756958    0.668988     56   \n",
       "  6   303.818207   58.564743  328.163116  104.813797    0.668201     56   \n",
       "  7   521.010620   54.927856  553.106445  115.104156    0.657990     56   \n",
       "  8   285.475983   59.831024  310.818817  104.310028    0.653754     56   \n",
       "  9   137.120209   54.618317  164.591431  110.357025    0.587552     56   \n",
       "  10   88.427399   57.933334  123.605560  112.600235    0.575953     56   \n",
       "  11  554.785706   65.900940  589.108826  119.268280    0.507684     56   \n",
       "  12  325.477631   56.254196  348.860382  105.496689    0.501867     56   \n",
       "  13  421.728973   50.696976  460.235077  112.110092    0.436659     56   \n",
       "  14  390.184875   52.982971  424.758850  117.083221    0.396467     56   \n",
       "  15    0.010136   55.476761   39.665810  112.739365    0.369220     56   \n",
       "  16   40.915684   61.321640   74.918427  112.873642    0.346749     56   \n",
       "  17  560.445984   59.752029  624.870300  120.549301    0.341817     60   \n",
       "  18   55.165333   62.162796   84.632553  112.080612    0.326188     56   \n",
       "  19  159.824646   56.191940  178.947571  109.300491    0.312359     56   \n",
       "  20  198.409668    0.000000  601.498291  226.303955    0.306102     60   \n",
       "  21  267.106232   49.782364  286.385895   63.264938    0.295996     56   \n",
       "  22  266.744934   51.105743  287.022278   98.462738    0.278157     56   \n",
       "  23  367.242737   57.266663  421.299927  118.471741    0.274424     60   \n",
       "  24  286.271606   27.222824  503.843750  154.534058    0.272617     60   \n",
       "  25  451.335907   51.915405  494.721039  109.157104    0.268660     56   \n",
       "  \n",
       "              name  \n",
       "  0          chair  \n",
       "  1          chair  \n",
       "  2           boat  \n",
       "  3           boat  \n",
       "  4          chair  \n",
       "  5          chair  \n",
       "  6          chair  \n",
       "  7          chair  \n",
       "  8          chair  \n",
       "  9          chair  \n",
       "  10         chair  \n",
       "  11         chair  \n",
       "  12         chair  \n",
       "  13         chair  \n",
       "  14         chair  \n",
       "  15         chair  \n",
       "  16         chair  \n",
       "  17  dining table  \n",
       "  18         chair  \n",
       "  19         chair  \n",
       "  20  dining table  \n",
       "  21         chair  \n",
       "  22         chair  \n",
       "  23  dining table  \n",
       "  24  dining table  \n",
       "  25         chair  ,\n",
       "  'caption': ['the blue and white boat',\n",
       "   'A blue and white boat with a red life preserver on board.'],\n",
       "  'bbox_target': [190.73, 0.0, 420.67, 216.36]},\n",
       " 977: {'image_emb': tensor([[ 0.0854, -0.2759, -0.0973,  ...,  0.4131,  0.1796,  0.1963],\n",
       "          [ 0.2480,  0.1692, -0.3667,  ...,  0.5479,  0.3689,  0.0538],\n",
       "          [ 0.0671,  0.2661, -0.2190,  ...,  1.0156,  0.1014, -0.3757],\n",
       "          ...,\n",
       "          [ 0.1797, -0.0630, -0.2489,  ...,  0.2135,  0.3645,  0.3455],\n",
       "          [-0.0418,  0.2468, -0.1560,  ...,  1.1289,  0.3083, -0.2448],\n",
       "          [ 0.0411, -0.3499, -0.0961,  ...,  0.5581,  0.2666,  0.2766]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2581,  0.2432, -0.0966,  ...,  0.1125, -0.2778, -0.1160],\n",
       "          [-0.1394,  0.0375, -0.2302,  ...,  0.3345,  0.2280, -0.2131]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[7.2837e-05, 7.8223e-01, 5.4359e-03, 2.1045e-01, 1.6842e-03, 4.0889e-05,\n",
       "           3.1161e-04],\n",
       "          [1.0610e-05, 1.0000e+00, 7.1526e-07, 7.7486e-07, 6.8545e-06, 3.7551e-06,\n",
       "           1.5676e-05]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0    0.305389   54.165619  437.776581  476.582886    0.946140     20  elephant\n",
       "  1  353.492676  202.508728  496.712524  476.890259    0.919296      0    person\n",
       "  2  528.118530  251.542786  576.979492  451.616882    0.897759      0    person\n",
       "  3   71.496223    0.066437  190.076477  138.929779    0.867205      0    person\n",
       "  4  331.085571  132.476181  620.252502  412.501404    0.850346     20  elephant\n",
       "  5  563.969360  175.095154  639.213867  375.719727    0.850150     20  elephant,\n",
       "  'caption': ['A girl with blonde hair in an orange leigh.',\n",
       "   'A girl with an orange lei stands next to an elephant.'],\n",
       "  'bbox_target': [357.49, 202.03, 138.92, 277.89]},\n",
       " 978: {'image_emb': tensor([[ 0.1031,  0.4392, -0.5586,  ...,  1.2842,  0.0909, -0.2137],\n",
       "          [-0.1589,  0.1189, -0.3799,  ...,  0.9189,  0.0196, -0.0304],\n",
       "          [ 0.1351,  0.3953, -0.3635,  ...,  0.9043,  0.1000, -0.0158],\n",
       "          [ 0.0562, -0.1133, -0.3369,  ...,  1.1963,  0.1801, -0.0726],\n",
       "          [ 0.3989,  0.3462, -0.0718,  ...,  0.3201,  0.1262,  0.0354]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1134,  0.0472, -0.5801,  ..., -0.5088,  0.4075,  0.2769],\n",
       "          [-0.0915, -0.2549, -0.2081,  ...,  0.2573,  0.2578, -0.1724]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7078e-01, 3.3960e-01, 2.8038e-03, 2.4116e-04, 4.8657e-01],\n",
       "          [9.6863e-02, 7.8613e-01, 1.2123e-02, 4.2796e-04, 1.0474e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  279.690186  156.673950  416.706360  568.920410    0.951304      0   \n",
       "  1   97.038513  390.395447  177.829773  578.137878    0.937969      0   \n",
       "  2   17.812607  287.009644  107.964828  425.282593    0.922986      0   \n",
       "  3   71.915298  398.288208  107.735153  427.332214    0.747225     36   \n",
       "  4  300.221710  553.793030  349.555939  571.158020    0.263959     39   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2      person  \n",
       "  3  skateboard  \n",
       "  4      bottle  ,\n",
       "  'caption': [\"The little boy whose shirt matches the man's.\",\n",
       "   'A little boy standing in the grass wearing a plaid shirt'],\n",
       "  'bbox_target': [102.57, 389.71, 78.02, 187.81]},\n",
       " 979: {'image_emb': tensor([[-0.0392, -0.1054, -0.1204,  ...,  0.4058,  0.4258,  0.1704],\n",
       "          [-0.2072,  0.0927, -0.0604,  ...,  0.5645,  0.1953,  0.2161],\n",
       "          [-0.0564, -0.1370, -0.1582,  ...,  1.3965,  0.3381,  0.1198],\n",
       "          [-0.0309, -0.0546, -0.1542,  ...,  0.7261,  0.2133,  0.0248],\n",
       "          [ 0.1696,  0.0396, -0.1776,  ...,  0.4658,  0.4700,  0.1814],\n",
       "          [ 0.0997,  0.2240, -0.2881,  ...,  0.6436,  0.3528,  0.0536]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0596,  0.0074, -0.2234,  ..., -0.0974, -0.3240,  0.0983],\n",
       "          [-0.0854,  0.1705, -0.0892,  ...,  0.6948, -0.1058,  0.0910]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.3399e-04, 6.5470e-04, 1.9968e-04, 9.9658e-01, 1.9550e-03, 3.3951e-04],\n",
       "          [1.1325e-06, 7.5000e-01, 1.1921e-07, 8.2016e-03, 9.2926e-03, 2.3242e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  113.114258    0.659958  479.184570  298.499084    0.931820      5   \n",
       "  1    0.143066  101.141571  480.000000  639.086426    0.870652      3   \n",
       "  2  432.356812  133.076874  479.985596  306.910370    0.845146      0   \n",
       "  3  262.800903  179.860031  449.923645  322.005005    0.782140      0   \n",
       "  4    1.051331   22.686905  238.889038  301.235657    0.769891      0   \n",
       "  \n",
       "           name  \n",
       "  0         bus  \n",
       "  1  motorcycle  \n",
       "  2      person  \n",
       "  3      person  \n",
       "  4      person  ,\n",
       "  'caption': ['a woman posing with bluish hair',\n",
       "   'A girl with glasses and long black hair in a sidecar on a motorcycle.'],\n",
       "  'bbox_target': [260.14, 178.91, 195.31, 144.98]},\n",
       " 980: {'image_emb': tensor([[ 0.1379,  0.1245,  0.2440,  ...,  0.6548,  0.1220, -0.0989],\n",
       "          [-0.3408,  0.2913, -0.4407,  ...,  0.9072,  0.1703, -0.4727],\n",
       "          [-0.3386,  0.2495, -0.1459,  ...,  0.7935, -0.0396,  0.2114],\n",
       "          [-0.1071,  0.2466,  0.0066,  ...,  0.2932, -0.0044, -0.1113],\n",
       "          [-0.1926,  0.2360, -0.0088,  ...,  1.3564, -0.3005, -0.4133],\n",
       "          [-0.2771,  0.2441, -0.0162,  ...,  0.1749, -0.1526, -0.1901]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0855,  0.0865,  0.1001,  ...,  0.2678, -0.2622, -0.4436],\n",
       "          [ 0.2141,  0.2881,  0.2126,  ...,  0.0885, -0.1041, -0.7134]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.9394e-02, 1.9592e-01, 7.0267e-03, 7.2083e-02, 3.3021e-05, 7.0557e-01],\n",
       "          [5.6000e-02, 5.2612e-02, 1.1444e-03, 1.7517e-01, 4.8280e-06, 7.1484e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   207.346191   67.411591  542.797058  314.941498    0.903690     53   \n",
       "  1     0.492760    4.430550  227.555359  245.475586    0.783183     60   \n",
       "  2     0.372131  162.502350   81.848206  363.879242    0.774015     56   \n",
       "  3    24.434845   76.613281  638.587524  419.410828    0.756779     60   \n",
       "  4   514.590454    0.763901  577.272339  175.916443    0.754166     39   \n",
       "  5    14.693184    7.259720   57.863808   63.441605    0.686593     41   \n",
       "  6     0.066213   44.816971   25.553982   67.857254    0.650689     42   \n",
       "  7   554.627563  205.615509  640.000000  325.242523    0.620620     49   \n",
       "  8    49.205368   52.241165   69.353355   84.985886    0.588828     43   \n",
       "  9   142.766708   99.628387  249.900436  152.249512    0.553016     56   \n",
       "  10   34.735985    0.000000   70.921577   38.050751    0.526953     41   \n",
       "  11   90.657028   15.019211  116.296509   37.324875    0.507129     42   \n",
       "  12  195.016571   68.778549  546.483887  321.743225    0.414591     60   \n",
       "  13  358.008087    0.000000  426.236115   70.819626    0.350471     56   \n",
       "  14   48.842781   52.592957   69.423531   85.253784    0.314831     42   \n",
       "  15  484.245453    0.000000  546.035461  120.215134    0.302535     39   \n",
       "  16  123.021851   15.499138  171.729462   46.258606    0.288125     43   \n",
       "  \n",
       "              name  \n",
       "  0          pizza  \n",
       "  1   dining table  \n",
       "  2          chair  \n",
       "  3   dining table  \n",
       "  4         bottle  \n",
       "  5            cup  \n",
       "  6           fork  \n",
       "  7         orange  \n",
       "  8          knife  \n",
       "  9          chair  \n",
       "  10           cup  \n",
       "  11          fork  \n",
       "  12  dining table  \n",
       "  13         chair  \n",
       "  14          fork  \n",
       "  15        bottle  \n",
       "  16         knife  ,\n",
       "  'caption': ['A table covered with a veggie pizza and has fruits and vegetables on the side',\n",
       "   'A table with vegetables and a pizza on it.'],\n",
       "  'bbox_target': [28.85, 62.03, 611.15, 328.91]},\n",
       " 981: {'image_emb': tensor([[-0.3062,  0.4302,  0.2057,  ...,  1.0576, -0.0626, -0.1128],\n",
       "          [ 0.1790,  0.4314, -0.0169,  ...,  0.8340, -0.1127,  0.2340],\n",
       "          [-0.0030,  0.7993,  0.1627,  ...,  0.7739, -0.2842,  0.0249],\n",
       "          ...,\n",
       "          [-0.2159, -0.1844, -0.0222,  ...,  1.2510,  0.1495,  0.0043],\n",
       "          [-0.0933,  0.6323,  0.1273,  ...,  0.9570,  0.1000,  0.3179],\n",
       "          [ 0.3032,  0.6133,  0.1329,  ...,  0.6636,  0.0295,  0.1226]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1501, -0.3953, -0.1671,  ..., -0.0061,  0.3628,  0.0202],\n",
       "          [-0.4822, -0.0494,  0.0214,  ..., -0.4180,  0.3186,  0.0989]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0382, 0.1879, 0.0116, 0.0047, 0.0052, 0.0211, 0.7314],\n",
       "          [0.0336, 0.6738, 0.0058, 0.0053, 0.0034, 0.0220, 0.2559]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class      name\n",
       "  0  213.959793  207.024200  385.666229  303.871338    0.927033     66  keyboard\n",
       "  1  281.334625   75.391998  434.269653  229.678589    0.918506     62        tv\n",
       "  2  114.272545  135.016983  225.505066  237.510941    0.917837     63    laptop\n",
       "  3  364.722778  291.653259  407.479431  330.438904    0.897720     64     mouse\n",
       "  4  445.605743   91.969513  499.817841  312.696899    0.784975     62        tv\n",
       "  5    0.349700  154.763153  158.521851  320.463837    0.766447     56     chair\n",
       "  6  126.368591  198.676254  218.390610  218.356369    0.450281     66  keyboard\n",
       "  7    0.922304  270.254791  133.645874  372.177032    0.450190     56     chair,\n",
       "  'caption': ['The farthest monitor on the right',\n",
       "   'The monitor that is turned off to the far right'],\n",
       "  'bbox_target': [445.91, 81.9, 53.84, 245.57]},\n",
       " 982: {'image_emb': tensor([[ 0.0687,  0.2443, -0.5459,  ...,  0.7588,  0.4109, -0.3152],\n",
       "          [-0.1794,  0.6030,  0.1760,  ...,  1.2031,  0.3054,  0.0236],\n",
       "          [-0.1459,  0.6294, -0.1917,  ...,  0.9639,  0.2261, -0.1639],\n",
       "          [-0.2271,  0.3894, -0.1484,  ...,  0.5371,  0.3022, -0.0259]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1276,  0.2466, -0.2236,  ...,  0.0630,  0.2083, -0.2668],\n",
       "          [-0.0088,  0.1202, -0.3779,  ...,  0.4404,  0.4934, -0.1357]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.9565, 0.0221, 0.0085, 0.0130],\n",
       "          [0.9404, 0.0015, 0.0167, 0.0413]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  254.455750  166.227112  359.054565  373.342407    0.920360     75   \n",
       "  1   92.453247  228.208160  194.133789  380.799774    0.915357     75   \n",
       "  2  219.976624  281.851807  302.260193  398.375610    0.879009     75   \n",
       "  3  201.737457   11.588837  399.484344  375.358734    0.302748     58   \n",
       "  4    0.000000  341.360016  482.916656  420.313843    0.286145     60   \n",
       "  \n",
       "             name  \n",
       "  0          vase  \n",
       "  1          vase  \n",
       "  2          vase  \n",
       "  3  potted plant  \n",
       "  4  dining table  ,\n",
       "  'caption': ['A vase containing one yellow flower.',\n",
       "   'single yellow flower in a clear round vase'],\n",
       "  'bbox_target': [219.69, 211.56, 87.87, 188.93]},\n",
       " 983: {'image_emb': tensor([[ 0.2271,  0.3699, -0.3455,  ...,  1.0596,  0.2593, -0.1108],\n",
       "          [-0.0256,  0.3215, -0.0786,  ...,  1.0693, -0.1066,  0.2341],\n",
       "          [-0.1360,  0.2344,  0.1328,  ...,  0.8179,  0.1652,  0.0060],\n",
       "          ...,\n",
       "          [-0.0378,  0.2854, -0.2107,  ...,  0.9727,  0.1202, -0.0290],\n",
       "          [ 0.1525, -0.1016, -0.4277,  ...,  0.9404,  0.1171, -0.1799],\n",
       "          [-0.1517,  0.3035, -0.4058,  ...,  1.1523,  0.1169, -0.0394]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2096,  0.5757, -0.2411,  ..., -0.0556, -0.1666, -0.2131],\n",
       "          [-0.5044,  0.2258,  0.0665,  ..., -0.3115, -0.0506, -0.2225]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0262, 0.0188, 0.0438, 0.0793, 0.2642, 0.0537, 0.1603, 0.0679, 0.0393,\n",
       "           0.1172, 0.0210, 0.1084],\n",
       "          [0.0186, 0.0062, 0.0573, 0.0807, 0.0250, 0.0082, 0.6973, 0.0046, 0.0051,\n",
       "           0.0041, 0.0012, 0.0915]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    95.274078    0.526779  276.850433  229.059647    0.921266      0   \n",
       "  1   378.163635    0.237167  451.142029  146.101166    0.910766      0   \n",
       "  2   280.952393   13.700333  377.651794  148.389755    0.895577      0   \n",
       "  3   255.771271  251.167847  295.311005  289.450562    0.889577     41   \n",
       "  4   235.346191  196.729843  262.680969  237.884445    0.888470     41   \n",
       "  5    34.803650    1.031342  219.852783  300.119141    0.887848      0   \n",
       "  6   194.281769    0.036613  343.505463  184.188080    0.880167      0   \n",
       "  7   192.378571  232.589355  227.539154  274.922394    0.876770     41   \n",
       "  8     0.208527    1.256760   71.454498  336.432556    0.845717      0   \n",
       "  9   315.015320  160.403015  335.251465  189.284424    0.845357     41   \n",
       "  10  341.566193  155.674301  358.871307  182.155716    0.784886     41   \n",
       "  11  406.892792  189.703720  479.283142  256.569641    0.694676      0   \n",
       "  12    0.654434  211.800323  478.214050  636.250854    0.666616     60   \n",
       "  13  266.984436   91.045517  309.148071  122.116852    0.596648     45   \n",
       "  14  158.602814  213.504150  479.200195  355.788696    0.562871     60   \n",
       "  15  374.538696  206.373718  406.388428  276.665741    0.486830     39   \n",
       "  16  277.447693  155.236053  480.000000  220.968445    0.480235     60   \n",
       "  17  374.082184  207.548767  406.036896  276.912079    0.431265     41   \n",
       "  18  454.314331  132.713211  476.733032  173.275955    0.328699     41   \n",
       "  19  429.029572  155.521408  455.298828  199.867813    0.319386     39   \n",
       "  20    2.352341  205.988663  118.144730  333.264160    0.270454      0   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1         person  \n",
       "  2         person  \n",
       "  3            cup  \n",
       "  4            cup  \n",
       "  5         person  \n",
       "  6         person  \n",
       "  7            cup  \n",
       "  8         person  \n",
       "  9            cup  \n",
       "  10           cup  \n",
       "  11        person  \n",
       "  12  dining table  \n",
       "  13          bowl  \n",
       "  14  dining table  \n",
       "  15        bottle  \n",
       "  16  dining table  \n",
       "  17           cup  \n",
       "  18           cup  \n",
       "  19        bottle  \n",
       "  20        person  ,\n",
       "  'caption': ['brown color table in the front side of the image',\n",
       "   'A bown tray that is kept empty'],\n",
       "  'bbox_target': [0.0, 509.12, 480.0, 122.25]},\n",
       " 984: {'image_emb': tensor([[-0.4565,  0.0854, -0.1785,  ...,  0.5493,  0.0110, -0.5654],\n",
       "          [-0.1636, -0.1810,  0.0846,  ...,  0.7446, -0.1809, -0.3010],\n",
       "          [-0.1923,  0.1302, -0.3950,  ...,  0.4817, -0.1818, -0.6851],\n",
       "          [-0.0994,  0.0148, -0.5337,  ...,  1.2158,  0.0531,  0.0237],\n",
       "          [-0.6719, -0.0890, -0.4775,  ...,  0.9062, -0.0974,  0.0028],\n",
       "          [-0.0081, -0.1156, -0.1412,  ...,  0.5762, -0.1653, -0.3293]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2305,  0.0729, -0.1321,  ...,  0.3970, -0.0710, -0.5859],\n",
       "          [-0.2141,  0.2130, -0.2161,  ...,  0.0731,  0.3018, -0.5273]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.3208, 0.1613, 0.0575, 0.0128, 0.0294, 0.4182],\n",
       "          [0.1570, 0.0397, 0.6016, 0.0356, 0.1343, 0.0319]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   522.052734  119.851089  623.127808  311.727722    0.923281     39   \n",
       "  1   214.515472   66.658371  379.019318  403.260254    0.916094     15   \n",
       "  2   431.406830  150.877365  518.215576  311.543640    0.910852     39   \n",
       "  3   395.096771  202.042542  443.390839  311.189331    0.866621     39   \n",
       "  4   325.540283  161.376678  396.590332  312.047028    0.797838     39   \n",
       "  5   185.197433    2.903168  636.314270  474.286987    0.619438     72   \n",
       "  6   212.386810   56.031189  382.556610  395.681091    0.612629     72   \n",
       "  7   260.910461   94.109955  462.617065  378.300873    0.517492     39   \n",
       "  8   448.195007  360.664520  513.478638  434.166199    0.498340     39   \n",
       "  9     1.205452    0.000000  209.671387  472.445496    0.476813     72   \n",
       "  10  519.593018  358.728027  578.095947  418.088013    0.450279     39   \n",
       "  \n",
       "              name  \n",
       "  0         bottle  \n",
       "  1            cat  \n",
       "  2         bottle  \n",
       "  3         bottle  \n",
       "  4         bottle  \n",
       "  5   refrigerator  \n",
       "  6   refrigerator  \n",
       "  7         bottle  \n",
       "  8         bottle  \n",
       "  9   refrigerator  \n",
       "  10        bottle  ,\n",
       "  'caption': ['A half full bottle of spaghetti sauce next to the whipping cream.',\n",
       "   'A almost empty bottle of ragu sauce.'],\n",
       "  'bbox_target': [522.74, 118.58, 99.22, 193.08]},\n",
       " 985: {'image_emb': tensor([[-0.2142,  0.4265,  0.1482,  ...,  0.5967, -0.0032,  0.1223],\n",
       "          [-0.7480,  0.2255, -0.0997,  ...,  0.7520,  0.0698,  0.0187],\n",
       "          [ 0.0354,  0.2891, -0.2512,  ...,  0.8789,  0.2091, -0.4387],\n",
       "          [ 0.0969, -0.1525, -0.0487,  ...,  0.8247, -0.2042,  0.1412],\n",
       "          [ 0.2595, -0.0153, -0.3264,  ...,  0.8965,  0.0154, -0.0963],\n",
       "          [-0.7100,  0.0506,  0.0182,  ...,  0.4084, -0.0190,  0.0265]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1804, -0.0523, -0.0506,  ..., -0.0353,  0.2554,  0.0491],\n",
       "          [-0.2114, -0.1776,  0.0191,  ...,  0.1335,  0.1600,  0.0082]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.2321e-03, 1.4900e-02, 4.4922e-01, 9.9850e-04, 1.3647e-03, 5.2539e-01],\n",
       "          [4.8071e-01, 2.5085e-02, 3.1030e-01, 3.3092e-04, 1.0681e-03, 1.8250e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   420.729248   97.803360  635.719604  470.649536    0.936688      0   \n",
       "  1   152.473724  185.250732  357.793121  400.914673    0.935950      0   \n",
       "  2   601.208801   83.836884  626.991394  132.854828    0.857144      0   \n",
       "  3    70.127335  389.892365  418.405701  429.881317    0.834709     31   \n",
       "  4   559.508423   92.998642  578.572021  130.997513    0.739071      0   \n",
       "  5   545.644897   98.398041  558.418091  121.665558    0.603182      0   \n",
       "  6   313.556519  329.138947  585.148315  445.517456    0.594153     30   \n",
       "  7   444.460938   88.988129  455.635803  100.939301    0.460790      0   \n",
       "  8   584.812805  330.052032  639.818176  378.120697    0.449826     30   \n",
       "  9    41.099808    0.114151   81.171906   70.198654    0.419641      0   \n",
       "  10    7.787457    0.000000   36.324062   74.097809    0.360064      0   \n",
       "  \n",
       "           name  \n",
       "  0      person  \n",
       "  1      person  \n",
       "  2      person  \n",
       "  3   snowboard  \n",
       "  4      person  \n",
       "  5      person  \n",
       "  6        skis  \n",
       "  7      person  \n",
       "  8        skis  \n",
       "  9      person  \n",
       "  10     person  ,\n",
       "  'caption': ['the red ski that the man in the blue jacket is on.',\n",
       "   'The ski that the man standing up with the blue jacket is wearing on his right foot'],\n",
       "  'bbox_target': [308.61, 326.81, 251.65, 103.66]},\n",
       " 986: {'image_emb': tensor([[-0.1343,  0.2803,  0.4819,  ...,  0.9727,  0.3167, -0.1233],\n",
       "          [ 0.2734,  0.3574,  0.1270,  ...,  0.7280,  0.2915, -0.1299],\n",
       "          [-0.0305,  0.2133,  0.0363,  ...,  0.8853,  0.3020, -0.3228],\n",
       "          [-0.1008,  0.1146,  0.3206,  ...,  0.6416,  0.0623, -0.2957]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1919,  0.0081, -0.2275,  ...,  0.1923,  0.0890, -0.4883],\n",
       "          [ 0.2559, -0.0449, -0.2020,  ...,  0.3342,  0.3689, -0.3899]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.7119, 0.1144, 0.1516, 0.0222],\n",
       "          [0.3179, 0.3660, 0.3081, 0.0082]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  438.780182  182.036072  555.149536  315.519104    0.933466      0   \n",
       "  1  372.862549  222.844757  462.699951  299.889191    0.910125      0   \n",
       "  2  303.616791  263.893433  637.305176  330.325806    0.734607     37   \n",
       "  3  222.432602  200.683746  228.829117  212.196564    0.436153      0   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2  surfboard  \n",
       "  3     person  ,\n",
       "  'caption': ['A woman in a visor on a surfboard', 'woman on surfboard'],\n",
       "  'bbox_target': [439.08, 185.16, 116.05, 128.1]},\n",
       " 987: {'image_emb': tensor([[ 0.3804, -0.1589, -0.5410,  ...,  0.4465,  0.2195,  0.1558],\n",
       "          [ 0.0115, -0.2830, -0.0289,  ...,  0.0432, -0.1630,  0.1996],\n",
       "          [ 0.1987, -0.0057, -0.3574,  ...,  0.5679, -0.0140, -0.0158],\n",
       "          [-0.1985, -0.1354, -0.1425,  ...,  0.2020, -0.3433,  0.2776]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2134, -0.2261, -0.1637,  ...,  0.5132, -0.1965, -0.3674],\n",
       "          [ 0.2169, -0.1946, -0.3191,  ...,  0.5039, -0.6519, -0.1001]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[1.7441e-02, 5.6006e-01, 5.2989e-05, 4.2261e-01],\n",
       "          [4.9896e-03, 9.2188e-01, 1.4246e-05, 7.3303e-02]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class     name\n",
       "  0  309.445465  275.475494  347.211731  338.651031    0.862428     23  giraffe\n",
       "  1  153.588364  237.733032  252.659149  376.389465    0.835210     23  giraffe\n",
       "  2  284.281433  264.677551  319.394745  355.702942    0.799522     23  giraffe\n",
       "  3  169.029861  212.594986  208.903931  267.429688    0.335635     23  giraffe,\n",
       "  'caption': ['The giraffe that appears taller,  on the left side, munching on a leaf.',\n",
       "   'the girrafe eating from a tree behind another giraffe'],\n",
       "  'bbox_target': [164.69, 211.31, 88.83, 140.01]},\n",
       " 988: {'image_emb': tensor([[-0.2385,  0.2913,  0.1758,  ...,  0.8394, -0.1000,  0.2766],\n",
       "          [-0.0014,  0.4780,  0.0513,  ...,  0.6182, -0.1787, -0.1228],\n",
       "          [-0.1477,  0.4358,  0.0884,  ...,  0.4456, -0.1144,  0.0571]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-2.1973e-01,  1.7993e-01,  1.6577e-01,  3.1952e-02, -3.8110e-01,\n",
       "           -1.7053e-01, -1.0663e-01, -8.8916e-01, -4.1199e-02, -1.0455e-01,\n",
       "           -1.4221e-02, -7.2449e-02, -2.4731e-01,  1.0033e-02,  7.0007e-02,\n",
       "            8.3557e-02,  1.2793e-01, -6.6223e-02, -3.2440e-02,  1.8506e-01,\n",
       "            2.5366e-01, -8.1909e-02, -2.4146e-01,  6.2207e-01,  2.8540e-01,\n",
       "            1.5515e-01,  1.1511e-01,  1.1652e-01,  1.8713e-01, -1.7102e-01,\n",
       "           -3.5278e-01, -9.9426e-02,  8.9661e-02, -8.5388e-02, -2.0850e-01,\n",
       "           -3.8116e-02,  1.8909e-01,  4.3060e-02,  5.2393e-01,  1.7310e-01,\n",
       "           -3.9429e-01, -2.6929e-01, -2.1021e-01, -1.9666e-01,  1.3196e-01,\n",
       "           -1.0223e-01,  1.5002e-01,  2.3865e-01, -3.1934e-01,  2.2974e-01,\n",
       "           -3.1104e-01, -3.9844e-01, -2.5244e-01,  2.0544e-01, -3.9673e-01,\n",
       "           -5.4840e-02,  3.3496e-01, -8.2336e-02,  4.2725e-01,  8.5083e-02,\n",
       "            2.2144e-01,  1.7493e-01, -1.5759e-01,  1.5274e-02, -3.1021e-02,\n",
       "           -3.1934e-01, -3.1421e-01,  1.1017e-01,  2.0691e-01, -2.5610e-01,\n",
       "           -4.7241e-01,  1.4526e-01, -4.9805e-02, -2.2668e-01,  9.0271e-02,\n",
       "           -2.0300e-01, -1.8018e-01, -9.5581e-02, -1.6675e-01,  1.4697e-01,\n",
       "            3.0566e-01, -8.8562e-02, -3.2300e-01,  5.4395e-01, -5.8807e-02,\n",
       "           -2.5073e-01,  1.4069e-02,  4.8633e-01,  1.7493e-01, -1.3550e-01,\n",
       "            1.5656e-02,  1.6980e-01, -1.6523e+00,  9.6130e-02, -3.5718e-01,\n",
       "            5.1025e-01,  2.8052e-01, -3.6743e-01, -7.3486e-02, -1.4490e-01,\n",
       "           -9.0149e-02,  1.9617e-01,  2.9858e-01, -1.6357e-02, -1.0693e-01,\n",
       "            1.7932e-01, -7.4585e-02,  1.6772e-01,  1.1414e-01, -7.4524e-02,\n",
       "           -9.3323e-02,  4.0527e-01, -6.1035e-02, -4.5142e-01, -3.1934e-01,\n",
       "            2.7374e-02, -1.9653e-01,  6.6101e-02, -1.3330e-01, -3.8135e-01,\n",
       "            2.1838e-01, -2.0020e-01,  4.0918e-01, -6.8359e-02, -7.8003e-02,\n",
       "           -1.2036e-01, -2.7661e-01,  5.7861e-02,  2.6660e-01,  8.6426e-02,\n",
       "            8.8379e-02,  2.4866e-01, -2.2314e-01,  5.6484e+00, -6.7017e-02,\n",
       "           -3.3350e-01, -3.0249e-01, -2.2778e-01, -2.1130e-01, -9.6558e-02,\n",
       "            1.3062e-01,  6.7078e-02, -3.3643e-01, -2.0532e-01, -4.5142e-01,\n",
       "           -1.8079e-01, -1.1237e-01,  7.6843e-02, -1.5465e-02,  4.0137e-01,\n",
       "           -7.0129e-02,  2.6221e-01,  3.8354e-01,  3.0371e-01, -2.7539e-01,\n",
       "            1.1719e-01,  2.4597e-01, -1.8274e-01, -6.2347e-02,  7.0984e-02,\n",
       "            1.2854e-01,  8.8135e-02, -5.1636e-02,  3.8672e-01,  1.4404e-01,\n",
       "            3.8605e-02,  4.9512e-01, -4.1779e-02,  6.8298e-02,  3.6201e-03,\n",
       "           -1.9238e-01, -1.4050e-01,  1.8997e-02,  5.6976e-02, -3.9258e-01,\n",
       "            1.8661e-02, -4.2749e-01, -3.6255e-01,  4.0405e-01,  1.0779e-01,\n",
       "           -1.2500e-01,  1.0651e-01,  3.4399e-01, -7.0557e-02, -2.1753e-01,\n",
       "           -4.6436e-01,  1.4244e-02, -4.2163e-01,  2.3108e-01, -1.5454e-01,\n",
       "            7.0801e-02, -1.1053e-01,  3.4180e-01, -1.8567e-01,  2.8052e-01,\n",
       "            7.1838e-02, -2.2446e-02, -1.5251e-02,  1.0687e-01, -3.6279e-01,\n",
       "            4.8096e-01, -1.6064e-01,  2.0325e-01,  1.6626e-01,  3.8086e-01,\n",
       "            8.7158e-02,  1.0437e-01, -1.3550e-01,  3.2532e-02, -2.0447e-01,\n",
       "            3.9886e-02,  1.2830e-01, -1.0735e-02, -5.2344e-01,  1.0626e-01,\n",
       "            2.3364e-01,  7.8430e-02, -5.2051e-01,  2.2087e-03, -1.4832e-01,\n",
       "            3.0243e-02,  5.5542e-02, -1.2213e-01,  1.0022e-01, -3.4473e-01,\n",
       "            5.5054e-02,  3.1519e-01, -1.4746e-01, -4.3365e-02,  1.2903e-01,\n",
       "           -1.7627e-01,  5.5267e-02,  2.9199e-01, -8.3984e-02, -2.9358e-02,\n",
       "           -3.1885e-01,  1.2976e-01, -8.8196e-02, -1.2524e-01, -1.9324e-01,\n",
       "            7.1228e-02,  2.8183e-02, -2.0569e-01, -6.2500e-02, -6.2207e-01,\n",
       "            3.8910e-02,  1.8845e-02, -1.5271e-01, -1.1627e-01,  3.5498e-01,\n",
       "           -1.2238e-01,  1.5173e-01,  1.3623e-01, -6.7749e-02, -2.4780e-01,\n",
       "           -3.8086e-01, -8.8120e-03, -1.0297e-01,  2.2034e-01,  5.1514e-02,\n",
       "            1.2903e-01,  1.9495e-01,  2.0251e-01,  6.1920e-02,  5.8105e-02,\n",
       "           -2.4445e-02, -1.6833e-01,  4.2090e-01, -5.4785e-01,  1.7700e-01,\n",
       "           -2.1167e-01,  1.8823e-01, -1.7700e-01, -1.7871e-01, -2.6367e-01,\n",
       "           -8.4106e-02, -2.3438e-01,  1.9409e-01,  8.6594e-03,  7.7148e-02,\n",
       "           -3.4698e-02,  4.0869e-01,  2.3547e-01,  3.6719e-01,  7.8186e-02,\n",
       "            2.2375e-01,  1.4075e-01,  3.9673e-01,  2.0203e-01, -3.5889e-02,\n",
       "           -3.5596e-01,  1.8726e-01,  2.3193e-02, -1.3708e-01,  3.3386e-02,\n",
       "            1.8347e-01, -7.1436e-01, -2.5488e-01, -2.7319e-01, -2.1912e-02,\n",
       "           -4.6045e-01,  3.3051e-02, -1.8665e-01,  2.8442e-01,  2.7802e-02,\n",
       "            1.0864e-01,  2.3755e-01,  5.1056e-02,  6.3232e-02, -1.4612e-01,\n",
       "           -2.5879e-01, -3.7659e-02,  5.6367e+00, -2.3315e-01, -1.3855e-01,\n",
       "           -2.6245e-03,  1.1981e-01,  2.1497e-01,  2.2339e-01,  6.3037e-01,\n",
       "           -3.3081e-01,  3.2642e-01, -6.4575e-02, -2.7588e-01, -2.7344e-01,\n",
       "            1.9751e-01,  1.6492e-01, -2.9980e-01, -2.8717e-02, -2.8574e+00,\n",
       "            1.3708e-01, -9.6191e-02,  6.8018e-01,  1.8494e-01, -1.1969e-01,\n",
       "           -4.2407e-01,  1.0168e-01,  2.6782e-01, -1.3135e-01,  1.6675e-01,\n",
       "           -4.5728e-01,  3.1952e-02,  1.2083e-03, -3.9575e-01,  4.6448e-02,\n",
       "           -6.2561e-02, -8.2031e-02,  1.4136e-01,  1.3611e-01,  7.9956e-02,\n",
       "            1.8005e-01,  3.0200e-01,  4.3823e-01,  3.8745e-01, -7.1960e-02,\n",
       "           -5.0232e-02, -1.0822e-01,  5.4565e-02,  2.5497e-02,  2.1350e-01,\n",
       "           -2.7075e-01, -1.2711e-02,  2.9126e-01,  4.2773e-01, -3.0200e-01,\n",
       "           -5.7800e-02, -2.7267e-02,  4.4019e-01, -1.2274e-01,  1.3220e-01,\n",
       "           -4.5874e-01,  7.6477e-02, -2.6929e-01, -1.2708e-01, -1.0071e-03,\n",
       "            4.8553e-02, -1.7004e-01,  9.5825e-02,  4.1473e-02, -9.8877e-03,\n",
       "            1.6833e-01,  7.0457e-03,  1.7578e-01,  9.6313e-02,  1.5839e-02,\n",
       "            6.6406e-01,  6.6772e-02, -6.0944e-02,  2.4756e-01, -3.7158e-01,\n",
       "           -6.9678e-01, -2.6672e-02,  5.1231e-03,  2.4097e-01, -2.6172e-01,\n",
       "           -7.6790e-03,  9.8633e-02, -1.4844e-01, -1.8335e-01,  2.5122e-01,\n",
       "            1.9775e-01,  7.2517e-03, -6.2439e-02, -1.0614e-01,  3.2074e-02,\n",
       "            2.3083e-01,  8.1543e-02, -8.0078e-02, -1.5918e-01,  2.8778e-02,\n",
       "            3.7671e-01, -3.1812e-01,  1.0931e-01, -1.1511e-01,  2.0679e-01,\n",
       "           -9.8633e-02, -1.2805e-01,  3.0249e-01,  2.5928e-01, -1.1230e-02,\n",
       "            4.7314e-01,  1.6895e-01,  3.9032e-02, -5.4749e-02,  9.5093e-02,\n",
       "           -5.2460e-02, -1.0535e-01, -1.0992e-01,  7.4219e-02,  2.1875e-01,\n",
       "           -2.7222e-01,  3.2007e-01, -2.7954e-01, -4.1382e-02,  2.4963e-01,\n",
       "            9.2773e-02,  2.5192e-02, -3.1006e-01,  1.3123e-01,  9.4604e-03,\n",
       "            1.2659e-01, -8.5938e-02,  2.0850e-01,  1.8481e-01, -7.8247e-02,\n",
       "            1.5430e-01, -2.8979e-01, -6.7383e-02, -2.6392e-01, -5.8350e-01,\n",
       "            5.6348e-01, -6.7810e-02, -1.0883e-01, -4.5996e-01,  4.9957e-02,\n",
       "            8.1665e-02, -1.3138e-02,  2.7026e-01,  4.0430e-01, -4.2603e-01,\n",
       "            7.8613e-02,  1.1225e-03,  1.2439e-01,  1.1078e-01,  2.1338e-01,\n",
       "            1.6882e-01,  6.2805e-02, -1.0333e-01, -5.1453e-02, -1.5784e-01,\n",
       "            5.8936e-01, -7.2021e-02,  1.5796e-01, -8.2092e-02, -9.6497e-02,\n",
       "            2.0056e-01, -1.4880e-01,  1.1023e-01,  1.4185e-01,  3.4229e-01,\n",
       "            2.5732e-01, -7.9980e-01,  1.6650e-01, -2.2485e-01, -1.6638e-01,\n",
       "           -2.7710e-01,  1.1133e-01, -1.1322e-01,  2.9761e-01, -1.5454e-01,\n",
       "            2.9590e-01,  5.3253e-02,  6.3354e-02,  5.5420e-01,  2.3352e-01,\n",
       "           -2.3315e-02,  2.9492e-01,  1.9763e-01,  6.8481e-02,  3.0884e-01,\n",
       "            3.2764e-01, -3.0493e-01,  3.7329e-01,  2.5586e-01,  1.1444e-01,\n",
       "            3.3325e-02, -3.1567e-01,  1.7859e-01, -3.8025e-02, -3.2544e-01,\n",
       "           -6.4850e-03, -3.3862e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.0325, 0.8242, 0.1432]], dtype=torch.float16),\n",
       "  'df_boxes':         xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   0.802904  168.670059  230.135696  284.977264    0.848637     45   \n",
       "  1   2.956289    1.483065  241.779373  175.704193    0.801455     49   \n",
       "  2   0.089085   33.606415  109.544304  182.484177    0.597709     49   \n",
       "  3   0.000000    4.132462  331.237152  491.712097    0.390103     60   \n",
       "  4  53.229923    0.525564  185.023666  104.728790    0.282912     49   \n",
       "  5  90.066620  140.884262  168.406769  176.816376    0.256279     49   \n",
       "  \n",
       "             name  \n",
       "  0          bowl  \n",
       "  1        orange  \n",
       "  2        orange  \n",
       "  3  dining table  \n",
       "  4        orange  \n",
       "  5        orange  ,\n",
       "  'caption': ['Orange all the way to left.'],\n",
       "  'bbox_target': [3.53, 34.12, 108.23, 149.41]},\n",
       " 989: {'image_emb': tensor([[-2.0142e-01,  3.0615e-01, -3.8501e-01,  ...,  8.5791e-01,\n",
       "            8.8928e-02,  4.2908e-02],\n",
       "          [ 8.7952e-02, -1.2347e-01, -3.8818e-01,  ...,  7.0752e-01,\n",
       "            2.7051e-01, -1.1063e-02],\n",
       "          [-1.6736e-01, -5.3314e-02, -6.0352e-01,  ...,  2.0496e-01,\n",
       "            2.4658e-01, -4.3152e-02],\n",
       "          [ 2.8979e-01,  2.5610e-01, -5.0781e-01,  ...,  8.1006e-01,\n",
       "            2.9980e-01, -1.3477e-01],\n",
       "          [ 5.3930e-04,  1.2646e-01, -4.4312e-01,  ...,  3.8037e-01,\n",
       "            2.0288e-01, -5.5084e-02],\n",
       "          [-2.9810e-01, -1.8579e-01, -2.7490e-01,  ..., -5.5237e-02,\n",
       "           -1.3977e-01,  2.1423e-01]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.2693,  0.4668, -0.2979,  ..., -0.4434,  0.2294, -0.0672],\n",
       "          [-0.1685, -0.1133, -0.4534,  ..., -0.6997,  0.1925,  0.2610],\n",
       "          [-0.0675, -0.0490, -0.2233,  ..., -0.0647,  0.0261, -0.1418]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.6328e-01, 5.3497e-02, 8.0322e-02, 0.0000e+00, 2.9240e-03, 3.8564e-05],\n",
       "          [1.2524e-01, 3.6816e-01, 2.0642e-01, 3.9024e-03, 3.1174e-02, 2.6514e-01],\n",
       "          [4.5068e-01, 2.6074e-01, 2.6904e-01, 1.1963e-04, 1.8311e-02, 8.9788e-04]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  114.069969   65.184540  222.258331  320.991547    0.943765      0   \n",
       "  1    6.650234   93.965332  133.806427  412.818665    0.940067      0   \n",
       "  2  381.892792   86.543823  560.032410  391.491699    0.929602      0   \n",
       "  3  384.260376  162.438782  431.952271  208.638733    0.922701     32   \n",
       "  4  292.097107   82.734238  411.482361  387.859253    0.921258      0   \n",
       "  \n",
       "            name  \n",
       "  0       person  \n",
       "  1       person  \n",
       "  2       person  \n",
       "  3  sports ball  \n",
       "  4       person  ,\n",
       "  'caption': ['A person in a red shirt and black pants in mid-stride',\n",
       "   'Guy in red in the back.',\n",
       "   'man with red shirt and white shoes'],\n",
       "  'bbox_target': [113.55, 62.0, 108.54, 260.71]},\n",
       " 990: {'image_emb': tensor([[-0.1331,  0.3735, -0.5625,  ...,  1.0859,  0.1510, -0.2108],\n",
       "          [ 0.1085,  0.3435, -0.1897,  ...,  1.3750, -0.2009, -0.3376],\n",
       "          [ 0.1088,  0.2480, -0.4148,  ...,  0.6597,  0.0602, -0.2471],\n",
       "          ...,\n",
       "          [-0.0284, -0.0343, -0.1731,  ...,  0.9014,  0.0409, -0.1720],\n",
       "          [ 0.0360, -0.1039, -0.2328,  ...,  0.6475,  0.3071, -0.2146],\n",
       "          [-0.1863,  0.1718, -0.4636,  ...,  0.4819,  0.3884, -0.2096]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.4231,  0.2759, -0.4885,  ..., -0.0432,  0.5063, -0.2208],\n",
       "          [ 0.3818,  0.0704, -0.6318,  ..., -0.0235,  0.4170, -0.3125]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[5.7465e-02, 1.3173e-04, 1.4126e-05, 7.1645e-05, 1.0586e-04, 4.4107e-06,\n",
       "           3.0339e-05, 1.8287e-04, 9.4189e-01],\n",
       "          [1.6858e-01, 2.5344e-04, 3.3569e-04, 1.7524e-05, 4.0114e-05, 4.7088e-06,\n",
       "           3.7074e-05, 1.0834e-03, 8.2959e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   162.276093  235.534622  265.288055  541.133545    0.918106      0   \n",
       "  1    71.590622  242.705414  144.500534  485.148712    0.897012      0   \n",
       "  2   252.121857  235.457733  296.219940  441.367584    0.862686      0   \n",
       "  3   136.535126  240.066605  193.494354  469.826843    0.854184      0   \n",
       "  4    70.113525  279.395386  125.129395  382.494385    0.846182     26   \n",
       "  5   308.129578    0.495369  403.678772  162.899109    0.820914      9   \n",
       "  6   383.805267  271.903900  404.021027  296.587006    0.726063      2   \n",
       "  7    69.322403  209.814667  153.791321  251.176208    0.723712     25   \n",
       "  8   113.932648  201.798538  282.299103  291.888916    0.667742     25   \n",
       "  9   301.562927  267.344269  344.134521  286.966278    0.660803      2   \n",
       "  10  380.056458  235.275024  425.000000  492.283325    0.617104      0   \n",
       "  11  338.131104  212.798981  424.273071  277.457733    0.589491     25   \n",
       "  12   78.927872  275.341187  100.297226  290.091370    0.552976      2   \n",
       "  13  385.944855  276.607391  424.594788  350.010468    0.520686     26   \n",
       "  14  288.026550  297.158539  299.018738  321.686493    0.455118     26   \n",
       "  15  304.378845  255.535400  384.486084  286.035828    0.362478      5   \n",
       "  16  380.537231  346.524780  422.018005  410.706055    0.261979     26   \n",
       "  \n",
       "               name  \n",
       "  0          person  \n",
       "  1          person  \n",
       "  2          person  \n",
       "  3          person  \n",
       "  4         handbag  \n",
       "  5   traffic light  \n",
       "  6             car  \n",
       "  7        umbrella  \n",
       "  8        umbrella  \n",
       "  9             car  \n",
       "  10         person  \n",
       "  11       umbrella  \n",
       "  12            car  \n",
       "  13        handbag  \n",
       "  14        handbag  \n",
       "  15            bus  \n",
       "  16        handbag  ,\n",
       "  'caption': ['The black umbrella that the woman in yellow is holding.',\n",
       "   'The umbrella over the lady in the yellow dress'],\n",
       "  'bbox_target': [103.55, 188.4, 182.65, 80.54]},\n",
       " 991: {'image_emb': tensor([[ 0.0967,  0.6387, -0.2123,  ...,  0.8052, -0.0981,  0.2245],\n",
       "          [-0.2705,  0.5405,  0.0756,  ...,  0.8838,  0.1770,  0.0908],\n",
       "          [-0.3752,  0.7515,  0.0720,  ...,  0.8696,  0.1594,  0.1611],\n",
       "          [ 0.0487,  0.7925, -0.0928,  ...,  0.5557,  0.0832,  0.1857]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.3806,  0.5171,  0.2150,  ...,  0.1753, -0.4758, -0.0264],\n",
       "          [-0.2769, -0.0055,  0.3691,  ...,  0.0675, -0.2140,  0.2465]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.5835, 0.0081, 0.0011, 0.4072],\n",
       "          [0.3560, 0.0467, 0.0283, 0.5688]], dtype=torch.float16),\n",
       "  'df_boxes':         xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0  22.215576  201.331009  479.100220  606.589355    0.822039     45   bowl\n",
       "  1   0.000000    0.123375  391.186737  234.713104    0.783024     47  apple\n",
       "  2   0.000000   51.020065  190.893372  236.762589    0.710931     47  apple,\n",
       "  'caption': ['A pile of cooked apples in a bowl.', 'The sliced apple food.'],\n",
       "  'bbox_target': [84.78, 267.64, 369.04, 279.27]},\n",
       " 992: {'image_emb': tensor([[-0.0058, -0.1199,  0.2096,  ...,  0.5054, -0.1019, -0.2159],\n",
       "          [-0.2668, -0.4478, -0.0983,  ...,  0.8867, -0.1815,  0.0553],\n",
       "          [-0.0956, -0.1315,  0.0529,  ...,  0.5615,  0.0107, -0.2927],\n",
       "          [ 0.2915, -0.3486,  0.1010,  ...,  0.0776, -0.0348, -0.2438]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1486, -0.1183, -0.1874,  ...,  0.0898, -0.1718, -0.8071],\n",
       "          [ 0.2140,  0.1680, -0.1201,  ..., -0.1173,  0.0950, -0.6084]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1104, 0.0283, 0.5967, 0.2646],\n",
       "          [0.1389, 0.1625, 0.6841, 0.0146]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class   name\n",
       "  0   13.062347  100.330734  221.849030  353.555359    0.928633     18  sheep\n",
       "  1  243.815430   61.315735  472.552795  186.655579    0.925015     18  sheep\n",
       "  2  191.406540  117.390747  328.763062  321.051575    0.839065     18  sheep,\n",
       "  'caption': ['White lamb on left standing on top of big rock.',\n",
       "   'A sheep on a rock.'],\n",
       "  'bbox_target': [18.58, 99.87, 218.32, 257.81]},\n",
       " 993: {'image_emb': tensor([[ 3.2990e-02,  1.2201e-01, -3.2196e-02,  ...,  1.0283e+00,\n",
       "            1.8420e-01,  1.2109e-01],\n",
       "          [-1.8176e-01,  3.8110e-01, -1.9128e-01,  ...,  8.8477e-01,\n",
       "           -9.6619e-02, -1.2054e-01],\n",
       "          [-5.1611e-01,  4.7656e-01,  3.6836e-05,  ...,  1.1562e+00,\n",
       "           -2.4765e-02, -1.7419e-01],\n",
       "          ...,\n",
       "          [-1.1487e-01,  4.9536e-01, -3.1787e-01,  ...,  1.2598e+00,\n",
       "           -2.1838e-01, -1.4978e-01],\n",
       "          [ 6.9885e-02,  3.2837e-01, -9.1125e-02,  ...,  1.2764e+00,\n",
       "            8.2336e-02, -6.4453e-02],\n",
       "          [ 2.1851e-01,  9.8022e-02, -3.7207e-01,  ...,  5.3125e-01,\n",
       "            3.2056e-01,  1.3100e-02]], dtype=torch.float16),\n",
       "  'text_emb': tensor([[-4.4189e-02, -7.5455e-03,  5.3711e-01,  ...,  7.9150e-01,\n",
       "            5.6458e-04, -4.4287e-01],\n",
       "          [ 6.9702e-02,  1.4868e-01,  7.4768e-02,  ...,  7.1533e-02,\n",
       "            7.8201e-04, -4.3140e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.0442e-04, 1.1520e-03, 1.7762e-05, 2.9251e-02, 1.9407e-04, 3.0065e-04,\n",
       "           8.9600e-01, 1.0133e-05, 9.0599e-06, 7.2388e-02],\n",
       "          [5.6028e-05, 6.2132e-04, 1.4758e-04, 5.6494e-01, 2.1577e-05, 1.2291e-02,\n",
       "           4.1992e-01, 3.1948e-05, 3.5048e-05, 1.8272e-03]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   136.604736   93.497253  371.671478  289.251587    0.936224      0   \n",
       "  1    67.090569  312.697815  238.010437  437.725647    0.933150     45   \n",
       "  2    98.230621  268.455811  221.440918  321.910889    0.908642     45   \n",
       "  3   284.416260  367.031677  467.793518  458.025818    0.903643     53   \n",
       "  4   359.070679    2.041122  638.348755  356.502014    0.884556      0   \n",
       "  5   288.820221  309.192657  425.160553  366.531647    0.865562     53   \n",
       "  6     0.000000  236.355713  626.946411  473.491211    0.807708     60   \n",
       "  7    57.552635  135.373398  206.847534  217.791809    0.788390     68   \n",
       "  8   356.121979   11.549484  455.112823  284.560730    0.728214     72   \n",
       "  9    15.717949  271.527069  101.768669  336.495331    0.694536     45   \n",
       "  10  207.573151  149.040848  229.007446  177.151855    0.430303     43   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1           bowl  \n",
       "  2           bowl  \n",
       "  3          pizza  \n",
       "  4         person  \n",
       "  5          pizza  \n",
       "  6   dining table  \n",
       "  7      microwave  \n",
       "  8   refrigerator  \n",
       "  9           bowl  \n",
       "  10         knife  ,\n",
       "  'caption': ['A homemade pizza must include mozzarella cheese!',\n",
       "   'Pizza with cheese on top.'],\n",
       "  'bbox_target': [284.76, 358.11, 192.0, 106.79]},\n",
       " 994: {'image_emb': tensor([[-0.3694,  0.4438,  0.0392,  ...,  0.6709, -0.0352,  0.2866],\n",
       "          [ 0.2095,  0.2433,  0.0564,  ...,  1.0605,  0.1631, -0.3843],\n",
       "          [-0.0021,  0.3574, -0.4250,  ...,  1.4434,  0.0854, -0.0764],\n",
       "          ...,\n",
       "          [-0.1626,  0.2979, -0.0914,  ...,  1.1660, -0.0422, -0.1132],\n",
       "          [-0.0485,  0.4099, -0.2184,  ...,  1.4023,  0.1672, -0.0911],\n",
       "          [-0.0042,  0.3267,  0.2318,  ...,  0.6792,  0.1522, -0.2078]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.1901,  0.0352, -0.0927,  ..., -0.2278, -0.2556, -0.5039],\n",
       "          [ 0.0120,  0.0877, -0.0126,  ..., -0.1379, -0.0671, -0.6099]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.2187e-06, 6.0455e-02, 1.1652e-01, 1.4062e-01, 4.4678e-01, 2.1803e-04,\n",
       "           4.6432e-05, 2.3547e-01],\n",
       "          [1.9670e-06, 7.7881e-02, 4.7974e-02, 1.8677e-01, 5.3223e-01, 2.3282e-04,\n",
       "           5.7042e-05, 1.5491e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   465.301514    0.855896  639.517700  170.030884    0.930245      0   \n",
       "  1   347.632263  176.204559  564.349854  269.509552    0.857905     53   \n",
       "  2    62.580643  279.354279  412.907288  425.832886    0.852553     53   \n",
       "  3   405.276794  190.522583  639.389343  296.880859    0.843681     53   \n",
       "  4   139.904358  179.137054  350.490997  279.701813    0.828911     53   \n",
       "  5   463.520325  296.019867  522.179993  431.221863    0.800611     42   \n",
       "  6   439.504211  297.657288  639.762756  409.426086    0.746569     43   \n",
       "  7    83.600914    0.280762  303.925903  172.006409    0.696112     26   \n",
       "  8     0.243348   12.996445   21.083862  253.301880    0.430154     41   \n",
       "  9   440.091064  292.907654  638.487427  418.088989    0.415785     42   \n",
       "  10    3.860474  161.581726  633.495117  427.385437    0.394442     60   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1          pizza  \n",
       "  2          pizza  \n",
       "  3          pizza  \n",
       "  4          pizza  \n",
       "  5           fork  \n",
       "  6          knife  \n",
       "  7        handbag  \n",
       "  8            cup  \n",
       "  9           fork  \n",
       "  10  dining table  ,\n",
       "  'caption': ['A piece of pizza between two other pieces',\n",
       "   'middle piece of pizza on top'],\n",
       "  'bbox_target': [353.37, 178.62, 207.74, 90.29]},\n",
       " 995: {'image_emb': tensor([[-0.4475, -0.0032,  0.1913,  ...,  0.8037,  0.0889, -0.7383],\n",
       "          [-0.7764, -0.0331, -0.0454,  ...,  0.8135,  0.0919, -0.7114],\n",
       "          [-0.2754,  0.5601, -0.1761,  ...,  0.8945, -0.3088, -0.3308],\n",
       "          [-0.3989, -0.0532,  0.1041,  ...,  0.8550,  0.2302, -0.7173]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.2157,  0.0042, -0.1587,  ...,  0.5566, -0.1737, -0.3232],\n",
       "          [-0.4343, -0.1958,  0.2289,  ...,  0.3013, -0.1760, -0.5781]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[6.5674e-01, 2.3178e-02, 3.4642e-04, 3.2007e-01],\n",
       "          [7.7295e-01, 6.5842e-03, 2.2411e-03, 2.1802e-01]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0   93.503265   73.243469  391.868134  615.532349    0.946491      0   \n",
       "  1    8.747917   22.772545  143.393372  201.153625    0.861164     38   \n",
       "  2  298.697144   89.206024  389.540588  381.949188    0.823368      0   \n",
       "  3  158.933716  101.730148  227.050537  158.275620    0.554556      0   \n",
       "  4  297.981201    0.180677  376.796631   46.381767    0.475507      0   \n",
       "  \n",
       "              name  \n",
       "  0         person  \n",
       "  1  tennis racket  \n",
       "  2         person  \n",
       "  3         person  \n",
       "  4         person  ,\n",
       "  'caption': ['A man watching a tennis match in a blue shirt and orange shorts.',\n",
       "   'A non active tennis play wearing a blue shirt and orange bottoms.'],\n",
       "  'bbox_target': [296.3, 85.61, 69.08, 265.01]},\n",
       " 996: {'image_emb': tensor([[ 0.0930,  0.5947, -0.1584,  ...,  1.1885,  0.1615,  0.1605],\n",
       "          [-0.0900,  0.6211, -0.2935,  ...,  1.0586,  0.1177,  0.1011],\n",
       "          [-0.0415,  0.3665, -0.0571,  ...,  1.1807,  0.1841,  0.1296],\n",
       "          [ 0.1162,  0.5981, -0.1230,  ...,  1.1113,  0.2776,  0.1339],\n",
       "          [-0.0431,  0.3999, -0.0889,  ...,  0.6821, -0.1908,  0.1232]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-1.6748e-01,  1.8713e-01,  2.2614e-02, -2.4707e-01, -3.2739e-01,\n",
       "           -2.2729e-01, -4.1064e-01, -8.3545e-01,  1.3110e-01, -3.5205e-01,\n",
       "           -7.7942e-02,  1.4978e-01, -2.9663e-01, -4.7607e-02,  2.8198e-01,\n",
       "           -3.0054e-01, -2.2607e-01, -2.0764e-01,  2.9419e-02,  3.4027e-02,\n",
       "            1.0895e-01,  3.0957e-01,  1.3391e-01,  3.3301e-01, -2.0911e-01,\n",
       "           -3.6768e-01, -2.3083e-01,  2.4512e-01,  3.7183e-01,  2.6294e-01,\n",
       "            1.4478e-01, -7.2205e-02, -9.1125e-02, -2.6398e-02, -2.0398e-01,\n",
       "            2.2791e-01,  2.3730e-01,  2.6514e-01, -1.7371e-01,  2.9443e-01,\n",
       "            1.7746e-02,  1.5747e-01, -1.1255e-01,  2.7466e-01, -1.9885e-01,\n",
       "           -2.7710e-01,  2.5131e-02, -1.3196e-01, -9.9854e-02,  1.1523e-01,\n",
       "            5.9906e-02, -8.0444e-02, -4.8798e-02, -2.0374e-01, -5.2051e-01,\n",
       "            1.6162e-01, -5.8167e-02, -2.4756e-01,  3.9868e-01, -1.7710e-03,\n",
       "           -3.8208e-01,  3.6011e-01,  7.4829e-02, -7.7515e-02, -3.3398e-01,\n",
       "            3.0762e-02,  3.9337e-02,  8.7402e-02, -2.7173e-01,  3.5083e-01,\n",
       "            2.2498e-01, -2.5732e-01, -4.4060e-03, -2.4872e-02,  7.5195e-01,\n",
       "           -1.5662e-01,  4.4556e-02,  1.8335e-01,  5.9143e-02, -1.0628e-02,\n",
       "            1.3330e-01,  1.9421e-01,  1.8262e-01, -8.1055e-02, -1.1157e-01,\n",
       "            1.9177e-01,  8.0933e-02, -2.1912e-02, -3.2043e-02, -1.2915e-01,\n",
       "           -1.5234e-01, -7.3181e-02, -1.1572e+00,  5.0195e-01, -3.0762e-01,\n",
       "            2.4597e-01, -4.8615e-02, -9.0454e-02,  1.7761e-01, -4.8096e-02,\n",
       "            1.0931e-01, -1.7053e-01,  1.2708e-01, -2.1338e-01, -6.9466e-03,\n",
       "           -1.7944e-02, -2.5293e-01,  4.7455e-02,  1.5906e-01,  4.7534e-01,\n",
       "           -7.2083e-02, -1.1481e-01,  1.8204e-02, -1.1011e-01,  4.2023e-02,\n",
       "            1.2793e-01,  1.7297e-01, -4.1162e-01, -5.4248e-01,  9.1492e-02,\n",
       "           -1.9580e-01, -4.7559e-01,  2.4048e-02,  1.9116e-01,  9.1248e-02,\n",
       "            5.1709e-01,  3.1860e-01, -3.5889e-02, -1.5491e-01, -5.3925e-02,\n",
       "           -4.2432e-01, -2.5366e-01,  1.2354e-01,  4.6289e+00, -4.7168e-01,\n",
       "            1.7920e-01, -7.0984e-02, -3.5522e-01, -2.5610e-01, -1.0559e-01,\n",
       "            1.8753e-02,  6.4697e-01, -6.2695e-01, -5.9326e-02, -3.3569e-01,\n",
       "           -1.5967e-01, -1.5869e-01, -2.9541e-01, -3.4326e-01,  8.6914e-02,\n",
       "           -2.8149e-01, -6.6309e-01,  1.3318e-01,  2.6025e-01, -1.7102e-01,\n",
       "           -1.7786e-01,  7.5378e-02,  1.8494e-01,  8.4961e-02,  9.7961e-02,\n",
       "            1.6565e-01, -2.2827e-02, -2.1521e-01,  5.6244e-02,  4.1064e-01,\n",
       "           -2.5635e-01,  1.3794e-01, -2.5439e-01,  1.2537e-01,  7.5317e-02,\n",
       "            2.5317e-01, -2.4147e-03,  1.4490e-01,  7.0374e-02, -2.1924e-01,\n",
       "            1.9250e-01, -1.0925e-01, -1.5588e-01,  3.3984e-01,  9.2651e-02,\n",
       "            1.8347e-01,  1.6052e-01,  2.7661e-01, -2.2388e-01, -1.0034e-01,\n",
       "           -2.9565e-01,  4.2419e-02,  2.7563e-01, -3.4131e-01,  1.7554e-01,\n",
       "           -8.9905e-02, -7.9012e-04, -4.4360e-01,  8.5327e-02, -1.8030e-01,\n",
       "            2.1387e-01,  3.4033e-01,  9.5276e-02, -2.7420e-02, -1.5759e-01,\n",
       "            1.0211e-01,  1.2598e-01,  1.9104e-01, -1.0706e-01, -9.6252e-02,\n",
       "            1.2646e-01,  2.1436e-01, -3.2422e-01, -3.4088e-02,  1.8237e-01,\n",
       "            3.0823e-02, -1.6443e-01,  1.6040e-01, -3.1519e-01, -3.6438e-02,\n",
       "            9.1431e-02, -1.6602e-01, -4.8511e-01,  2.1826e-01,  4.6906e-02,\n",
       "            1.0767e-01,  3.4204e-01, -1.8408e-01,  5.0098e-01, -3.7354e-01,\n",
       "           -4.5776e-01, -2.9419e-01, -1.9312e-01, -3.5913e-01, -4.2114e-01,\n",
       "           -3.9526e-01,  5.8289e-02,  1.2030e-01,  2.6636e-01,  1.8628e-01,\n",
       "            3.9868e-01,  1.5405e-01, -2.5220e-01,  1.7773e-01, -3.1616e-02,\n",
       "            8.2031e-02,  4.4769e-02,  1.1310e-01, -2.9938e-02, -4.2065e-01,\n",
       "            1.8628e-01, -2.3401e-01,  4.7144e-01,  3.9648e-01,  1.3660e-01,\n",
       "            1.9861e-01, -3.5059e-01,  5.2734e-01,  2.8229e-02,  8.7646e-02,\n",
       "           -2.6215e-02, -3.4399e-01, -2.1558e-01,  5.0000e-01,  6.8604e-01,\n",
       "           -4.3701e-01,  2.0569e-01,  2.1399e-01, -1.2122e-01,  3.6475e-01,\n",
       "           -2.6758e-01,  3.6230e-01,  8.5678e-03, -2.1765e-01,  3.2007e-01,\n",
       "           -3.2764e-01,  3.9746e-01,  2.8467e-01,  6.2842e-01, -2.2675e-02,\n",
       "           -1.2573e-01, -9.1675e-02,  1.0675e-01, -2.6733e-01,  2.7173e-01,\n",
       "           -1.4136e-01,  5.9601e-02, -3.1543e-01, -6.3721e-02, -1.6479e-02,\n",
       "           -9.6558e-02,  1.4514e-01,  5.1709e-01,  2.6343e-01,  2.2656e-01,\n",
       "           -1.7688e-01,  2.0630e-01,  1.9714e-01,  6.6895e-02, -1.2073e-01,\n",
       "            5.5328e-02, -6.1670e-01, -3.6157e-01,  6.6589e-02,  2.2302e-01,\n",
       "           -4.4922e-01, -1.2924e-02,  2.1985e-01, -2.3865e-01,  2.2815e-01,\n",
       "           -5.2979e-01,  6.5125e-02, -7.9956e-03, -1.4372e-03, -2.1851e-01,\n",
       "           -4.8401e-02,  3.5986e-01,  4.6211e+00, -4.6204e-02,  1.4917e-01,\n",
       "            1.2329e-01,  3.8721e-01, -2.5928e-01,  1.1975e-01,  5.7031e-01,\n",
       "           -5.0873e-02, -1.2901e-02,  2.1069e-01, -4.1724e-01, -2.2278e-01,\n",
       "            1.3245e-01, -1.8530e-01, -1.0980e-01, -6.2561e-02, -1.7676e+00,\n",
       "            6.7322e-02, -4.1406e-01,  4.1138e-01, -1.2512e-01, -1.4124e-01,\n",
       "           -3.9368e-02,  1.5198e-02, -4.3610e-02, -1.0284e-01, -2.2858e-02,\n",
       "            2.8152e-02,  3.9771e-01,  1.4697e-01, -2.3499e-01,  1.6357e-01,\n",
       "            4.6069e-01,  4.1943e-01,  8.6853e-02, -2.4133e-01, -3.3643e-01,\n",
       "            2.4316e-01,  2.4182e-01,  2.7344e-01,  3.5327e-01, -4.6448e-02,\n",
       "           -8.0261e-02, -2.5464e-01,  1.2286e-01,  2.0496e-01,  9.3628e-02,\n",
       "            3.9001e-02, -3.4521e-01,  5.1611e-01, -1.8091e-01,  2.4734e-02,\n",
       "           -1.4801e-02,  2.1741e-01,  3.0347e-01,  7.2693e-02, -5.4199e-01,\n",
       "           -4.9463e-01, -8.4412e-02, -1.8176e-01, -4.4434e-02,  1.6101e-01,\n",
       "            1.3818e-01, -3.5156e-01,  7.2571e-02,  4.1919e-01,  3.9502e-01,\n",
       "           -2.1484e-02,  8.2812e-01,  2.7515e-01,  5.7068e-02,  5.1910e-02,\n",
       "           -2.1375e-01,  2.8641e-02, -3.7183e-01,  3.3539e-02, -3.9380e-01,\n",
       "           -8.3008e-01, -2.1454e-02,  4.3970e-01, -1.2830e-01, -1.5161e-01,\n",
       "           -3.6987e-01, -2.5757e-01, -1.3611e-01,  3.4131e-01,  3.1616e-01,\n",
       "            1.7932e-01, -2.2430e-02, -1.1090e-01, -1.5906e-01, -7.5000e-01,\n",
       "            2.7710e-01,  2.1277e-01,  2.4414e-01,  4.5459e-01,  1.5894e-01,\n",
       "            2.0813e-01, -4.7028e-02, -4.0918e-01, -1.4343e-01, -6.2683e-02,\n",
       "           -2.0239e-01,  4.2139e-01,  1.2524e-01, -8.0383e-02, -2.1912e-02,\n",
       "            1.2732e-01, -3.0664e-01, -1.9974e-02, -4.0430e-01,  2.6270e-01,\n",
       "            1.8692e-02,  7.8308e-02, -2.9614e-01,  4.0820e-01,  1.0547e-01,\n",
       "           -1.7163e-01, -6.7871e-02, -1.5503e-01,  2.2729e-01,  3.1052e-02,\n",
       "           -3.2324e-01, -3.8403e-01,  3.0347e-01, -1.9312e-01, -1.0516e-01,\n",
       "           -4.8389e-01, -8.3923e-02, -2.6904e-01, -3.4912e-02, -1.7078e-01,\n",
       "            8.5449e-02,  1.5332e-01, -2.8534e-02, -1.8600e-02,  1.5881e-01,\n",
       "            3.5205e-01,  2.8052e-01, -1.3599e-01,  2.2803e-01,  6.7993e-02,\n",
       "           -2.9639e-01, -8.7524e-02, -2.5806e-01, -1.0547e-01, -6.0352e-01,\n",
       "            8.3496e-02, -3.8403e-01, -4.0070e-02, -2.4866e-01,  3.3862e-01,\n",
       "            1.6833e-01, -6.0944e-02, -8.1177e-03, -3.3618e-01, -1.9604e-01,\n",
       "           -6.6589e-02,  2.3389e-01, -1.6443e-01, -5.0049e-02, -2.4207e-01,\n",
       "            2.6050e-01, -2.9282e-02,  1.1694e-01, -5.5762e-01, -1.2962e-02,\n",
       "           -5.4810e-02, -2.9175e-01,  3.3691e-01, -3.6804e-02, -2.3730e-01,\n",
       "            1.2741e-02, -2.3914e-01, -3.4106e-01,  1.9153e-01, -1.9507e-01,\n",
       "           -2.9834e-01,  2.4036e-01, -7.5623e-02,  4.0796e-01, -1.4026e-01,\n",
       "           -1.7810e-01, -3.1323e-01, -3.6896e-02, -1.2988e-01, -5.7793e-03,\n",
       "           -2.4719e-01, -3.4741e-01,  4.8950e-02,  7.5623e-02,  1.2299e-01,\n",
       "            2.1637e-02,  3.2520e-01,  9.3262e-02, -1.8356e-02,  3.3374e-01,\n",
       "           -2.2675e-02, -4.0015e-01]], dtype=torch.float16),\n",
       "  'text_similarity': tensor([[0.1603, 0.1137, 0.0039, 0.0147, 0.7075]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  367.401306  121.623871  490.744202  346.482452    0.904510     10   \n",
       "  1  250.354401  176.698059  388.775482  440.522583    0.901077     10   \n",
       "  2  504.448608   75.112915  567.855713  235.518860    0.862718     10   \n",
       "  3  452.507141   77.141296  542.307068  272.907471    0.834320     10   \n",
       "  4  251.892685   25.451019  308.936523  107.109833    0.268629     75   \n",
       "  \n",
       "             name  \n",
       "  0  fire hydrant  \n",
       "  1  fire hydrant  \n",
       "  2  fire hydrant  \n",
       "  3  fire hydrant  \n",
       "  4          vase  ,\n",
       "  'caption': ['Third fire hydrant from left.'],\n",
       "  'bbox_target': [449.87, 77.49, 92.55, 193.72]},\n",
       " 997: {'image_emb': tensor([[-0.0710,  0.2722, -0.1825,  ...,  1.2148,  0.0038,  0.2625],\n",
       "          [-0.1770, -0.0497, -0.4678,  ...,  1.1553,  0.2200, -0.2639],\n",
       "          [-0.1588,  0.4072, -0.2654,  ...,  0.9868,  0.1682,  0.0451],\n",
       "          [ 0.1366,  0.0464, -0.6411,  ...,  0.8394,  0.0679,  0.3999],\n",
       "          [ 0.3333,  0.2771, -0.3613,  ...,  0.2054,  0.3335, -0.0155]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[ 0.0233, -0.1433,  0.0182,  ..., -0.1384,  0.0585,  0.2244],\n",
       "          [-0.3745,  0.0707, -0.4089,  ..., -0.5352,  0.0141, -0.2686]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[8.8871e-05, 1.4901e-06, 8.9407e-07, 1.0890e-04, 1.0000e+00],\n",
       "          [1.9153e-01, 2.6172e-01, 5.6610e-02, 3.7720e-02, 4.5239e-01]],\n",
       "         dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  352.685730  137.263657  439.463623  378.289429    0.937879      0   \n",
       "  1   99.372643   40.365005  188.231567  369.344971    0.930283      0   \n",
       "  2  223.593582  104.938278  272.065918  292.777954    0.891750      0   \n",
       "  3  142.325516   88.723770  179.441208  156.889954    0.741052     27   \n",
       "  4  347.332642  185.798126  374.601074  253.806122    0.470457     58   \n",
       "  5  252.752457  134.427567  270.886078  182.246521    0.314262     27   \n",
       "  6  351.841309  185.886780  374.560364  217.361511    0.283825     58   \n",
       "  \n",
       "             name  \n",
       "  0        person  \n",
       "  1        person  \n",
       "  2        person  \n",
       "  3           tie  \n",
       "  4  potted plant  \n",
       "  5           tie  \n",
       "  6  potted plant  ,\n",
       "  'caption': ['A NAVY MAN STANDING IN FRONT OF HILLARY CLINTON',\n",
       "   'the man on the left in white standing closest to camera in the right hand picture'],\n",
       "  'bbox_target': [101.73, 36.31, 96.03, 330.87]},\n",
       " 998: {'image_emb': tensor([[ 0.1473, -0.1577,  0.0161,  ...,  1.0771,  0.3809,  0.4077],\n",
       "          [ 0.0356, -0.1556,  0.3154,  ...,  0.7905,  0.4661,  0.3127],\n",
       "          [ 0.0616, -0.2246,  0.2944,  ...,  0.4443,  0.4448, -0.0525]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.0597, -0.5430, -0.1200,  ...,  0.3875,  0.1230,  0.0778],\n",
       "          [ 0.2407, -0.1112, -0.1541,  ...,  0.1890,  0.0320,  0.0074]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[9.9756e-01, 2.1820e-03, 2.4867e-04],\n",
       "          [9.9072e-01, 6.9962e-03, 2.2354e-03]], dtype=torch.float16),\n",
       "  'df_boxes':          xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0  155.759155   16.409851  368.500244  305.271973    0.878708      0   \n",
       "  1   85.010468  145.779953  608.332031  509.129639    0.870218      0   \n",
       "  2    5.136322  269.998962  640.000000  507.456665    0.468803     59   \n",
       "  3  453.504669   21.156502  463.559235   57.191475    0.292287     11   \n",
       "  \n",
       "          name  \n",
       "  0     person  \n",
       "  1     person  \n",
       "  2        bed  \n",
       "  3  stop sign  ,\n",
       "  'caption': ['A little blond girl holding a US two dollar bill.',\n",
       "   'A girl in a blue shirt holding a $2 bill.'],\n",
       "  'bbox_target': [158.78, 16.11, 220.91, 288.79]},\n",
       " 999: {'image_emb': tensor([[ 0.1375,  0.1573,  0.2024,  ...,  0.1726,  0.4495,  0.6636],\n",
       "          [ 0.3369,  0.3950,  0.2747,  ...,  0.2302,  0.6431,  0.3325],\n",
       "          [ 0.1122,  0.2229, -0.1750,  ...,  0.4177, -0.0945, -0.1886],\n",
       "          ...,\n",
       "          [ 0.2227,  0.1412, -0.2910,  ...,  0.2013,  0.1187, -0.0651],\n",
       "          [ 0.3401,  0.1108, -0.1761,  ...,  0.5562, -0.0487, -0.1361],\n",
       "          [ 0.0417,  0.2642,  0.0563,  ...,  0.6602,  0.2986,  0.7104]],\n",
       "         dtype=torch.float16),\n",
       "  'text_emb': tensor([[-0.1858,  0.1072,  0.1421,  ...,  0.3521, -0.0841, -0.1050],\n",
       "          [ 0.0094,  0.3574, -0.0352,  ...,  0.2228,  0.0856,  0.2445]],\n",
       "         dtype=torch.float16),\n",
       "  'text_similarity': tensor([[3.9795e-01, 5.9717e-01, 2.0146e-05, 1.0133e-06, 1.3351e-04, 4.1723e-07,\n",
       "           4.2677e-05, 3.4571e-06, 5.3644e-06, 4.6310e-03],\n",
       "          [3.4277e-01, 1.2610e-01, 1.1921e-07, 5.9605e-08, 3.1590e-06, 0.0000e+00,\n",
       "           7.8082e-06, 4.1723e-07, 3.5763e-07, 5.3125e-01]], dtype=torch.float16),\n",
       "  'df_boxes':           xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "  0    91.638748  148.627716  315.098267  510.366608    0.903858      0   \n",
       "  1    87.958496  254.874329  318.384399  640.000000    0.886117      3   \n",
       "  2   351.955719   62.417709  378.663055  124.255875    0.854473      0   \n",
       "  3   267.705933   56.880352  299.822815  131.681656    0.853695      0   \n",
       "  4   136.941544   54.826630  168.895462  132.468323    0.842252      0   \n",
       "  5   386.479340   70.560730  407.512665  135.394470    0.826405      0   \n",
       "  6   422.728180  248.117599  479.602478  637.262390    0.820238      0   \n",
       "  7   328.610962   59.390717  350.447144  114.521759    0.820209      0   \n",
       "  8   218.036377   76.426300  238.749268  125.770279    0.802856      0   \n",
       "  9   155.382980   85.290787  197.065369  142.534927    0.695030      3   \n",
       "  10  162.712601   51.513489  182.478729   95.773163    0.664540      0   \n",
       "  11  125.742630   57.551632  142.123825   94.652008    0.637710      0   \n",
       "  12  239.756683   83.098778  261.716949  125.784126    0.564377      0   \n",
       "  13  469.380554   94.370972  480.000000  134.233856    0.492872      0   \n",
       "  \n",
       "            name  \n",
       "  0       person  \n",
       "  1   motorcycle  \n",
       "  2       person  \n",
       "  3       person  \n",
       "  4       person  \n",
       "  5       person  \n",
       "  6       person  \n",
       "  7       person  \n",
       "  8       person  \n",
       "  9   motorcycle  \n",
       "  10      person  \n",
       "  11      person  \n",
       "  12      person  \n",
       "  13      person  ,\n",
       "  'caption': ['The motorcycle the man is sitting on.',\n",
       "   'A motorcycle being driven by a man wearing a green shirt.'],\n",
       "  'bbox_target': [86.29, 256.1, 234.43, 376.8]},\n",
       " ...}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('yolov5l6+clip/1_dictionary_full_train.p', 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n",
    "    \n",
    "dictionary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASELINE with yolov5l6\n",
    "\n",
    "In this assessment, we are analyzing the baseline performance. The YOLOv5l6 model was utilized to locate the boxes, while the corresponding text for these boxes was computed in advance using CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 42226/42226 [00:07<00:00, 5824.72it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_the_best_matching_box(file_name):\n",
    "    box_target = []\n",
    "    box_pred = []\n",
    "    \n",
    "    with open(file_name, 'rb') as f:\n",
    "        dictionary = pickle.load(f)\n",
    "        \n",
    "    for sample in tqdm(dictionary.keys()):\n",
    "        \n",
    "        try:\n",
    "            # get the sample\n",
    "            element = dictionary[sample]\n",
    "            \n",
    "            for sim, cap in zip(element['text_similarity'],element['caption']):\n",
    "                \n",
    "                # get the max similarity index\n",
    "                max_sim_index = torch.argmax(sim).numpy()\n",
    "                \n",
    "                # gete the  bbox_target\n",
    "                box_pred.append(element['df_boxes'].loc[max_sim_index][:4].values.astype('int'))\n",
    "                box_target.append(element['bbox_target'])\n",
    "        except:\n",
    "            KeyError: print('error in sample:', sample)\n",
    "            \n",
    "    box_pred = torch.stack([torch.Tensor(p) for p in box_pred])\n",
    "    box_target = torch.tensor(box_target)\n",
    "    \n",
    "    # convert the box_pred to x1, y1, x2, y2\n",
    "    box_target[:, 2] = box_target[:, 0] + box_target[:, 2]\n",
    "    box_target[:, 3] = box_target[:, 1] + box_target[:, 3]\n",
    "        \n",
    "    return box_pred, box_target\n",
    "\n",
    "# 'yolov5l6+clip/1_dictionary_full_train.p'\n",
    "box_pred, box_target = load_the_best_matching_box('yolov5l6+clip/1_dictionary_full_test.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[374.,  68., 510., 262.],\n",
       "         [374.,  68., 510., 262.],\n",
       "         [426., 107., 639., 237.],\n",
       "         ...,\n",
       "         [199., 271., 333., 471.],\n",
       "         [225.,   0., 330., 127.],\n",
       "         [ 61.,   0., 239.,  82.]]),\n",
       " tensor([[374.3100,  65.0600, 510.3500, 267.0000],\n",
       "         [374.3100,  65.0600, 510.3500, 267.0000],\n",
       "         [ 93.9500,  83.2900, 598.5600, 373.8600],\n",
       "         ...,\n",
       "         [348.5800, 230.9100, 480.0000, 471.1400],\n",
       "         [212.8200,   0.0000, 355.9000, 132.4100],\n",
       "         [212.8200,   0.0000, 355.9000, 132.4100]]))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box_pred,box_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assert same dimensionality True\n"
     ]
    }
   ],
   "source": [
    "print('assert same dimensionality',len(box_pred) == len(box_target))\n",
    "\n",
    "# convert the lists to tensors\n",
    "# torchvision.ops.box_iou(box_target, box_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([74895, 4]), torch.Size([74895, 4]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box_target.shape,  box_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6099380016326904\n"
     ]
    }
   ],
   "source": [
    "mean_iou = []\n",
    "\n",
    "def iou(box1, box2):\n",
    "    # get the coordinates of bounding boxes\n",
    "    b1_x1, b1_y1, b1_x2, b1_y2 = box1\n",
    "    b2_x1, b2_y1, b2_x2, b2_y2 = box2\n",
    "    \n",
    "    # get the corrdinates of the intersection rectangle\n",
    "    inter_rect_x1 =  max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 =  max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 =  min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 =  min(b1_y2, b2_y2)\n",
    "    \n",
    "    # Intersection area\n",
    "    inter_area = max(inter_rect_x2 - inter_rect_x1 + 1, 0) * max(inter_rect_y2 - inter_rect_y1 + 1, 0)\n",
    "    \n",
    "    # Union Area\n",
    "    b1_area = (b1_x2 - b1_x1 + 1)*(b1_y2 - b1_y1 + 1)\n",
    "    b2_area = (b2_x2 - b2_x1 + 1)*(b2_y2 - b2_y1 + 1)\n",
    "    \n",
    "    iou = inter_area / (b1_area + b2_area - inter_area)\n",
    "    \n",
    "    return iou\n",
    "    \n",
    "for b_t, b_p in zip(box_target, box_pred):\n",
    "    \n",
    "    # get the iou\n",
    "    mean_iou.append(iou(b_t, b_p))\n",
    "    \n",
    "mean_iou_m = torch.stack(mean_iou).mean().numpy()\n",
    "\n",
    "print(mean_iou_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum number of boxes found:  27\n",
      "mean number of boxes found:  5.586103921958989\n",
      "std number of boxes found:  2.7619218371402146\n",
      "count max number of boxes found:  0\n",
      "count of number of boxes found:\n",
      "[25, 201, 794, 1047, 954, 637, 449, 290, 206, 132, 86, 49, 52, 31, 30, 9, 17, 3, 2, 0, 1, 0, 4, 2, 0, 1, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "file_name = 'yolov5l6+clip/1_dictionary_full_test.p'\n",
    "\n",
    "with open(file_name, 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n",
    "\n",
    "print('maximum number of boxes found: ',max([len(dictionary[sample]['image_emb']) for sample in dictionary.keys()]))\n",
    "print('mean number of boxes found: ',np.array([len(dictionary[sample]['image_emb']) for sample in dictionary.keys()]).mean())\n",
    "print('std number of boxes found: ',np.array([len(dictionary[sample]['image_emb']) for sample in dictionary.keys()]).std())\n",
    "print('count max number of boxes found: ',[len(dictionary[sample]['image_emb']) for sample in dictionary.keys()].count(32))\n",
    "print('count of number of boxes found:')\n",
    "print([[len(dictionary[sample]['image_emb']) for sample in dictionary.keys()].count(xx+1) for xx in range(32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum number of boxes found:  32\n",
      "mean number of boxes found:  5.56635722066973\n",
      "std number of boxes found:  2.7148657554979914\n",
      "count max number of boxes found:  3\n",
      "count of number of boxes found:\n",
      "[248, 1577, 6883, 8627, 7880, 5624, 3697, 2583, 1698, 1039, 776, 498, 341, 283, 144, 98, 56, 50, 36, 28, 17, 14, 9, 6, 2, 9, 0, 0, 0, 0, 0, 3]\n"
     ]
    }
   ],
   "source": [
    "file_name = 'yolov5l6+clip/1_dictionary_full_train.p'\n",
    "\n",
    "with open(file_name, 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n",
    "\n",
    "print('maximum number of boxes found: ',max([len(dictionary[sample]['image_emb']) for sample in dictionary.keys()]))\n",
    "print('mean number of boxes found: ',np.array([len(dictionary[sample]['image_emb']) for sample in dictionary.keys()]).mean())\n",
    "print('std number of boxes found: ',np.array([len(dictionary[sample]['image_emb']) for sample in dictionary.keys()]).std())\n",
    "print('count max number of boxes found: ',[len(dictionary[sample]['image_emb']) for sample in dictionary.keys()].count(32))\n",
    "print('count of number of boxes found:')\n",
    "print([[len(dictionary[sample]['image_emb']) for sample in dictionary.keys()].count(xx+1) for xx in range(32)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3163cfb8aa3549ad3f5400bc3427ee7a4002d2a0d6d7ead52f641c6a7636395"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
