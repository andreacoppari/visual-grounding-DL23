{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, d_model//4)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model//4)\n",
    "        self.linear2 = nn.Linear(d_model//4,  d_model//16)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model//16)\n",
    "        self.linear3 = nn.Linear(d_model//16, d_model//32)\n",
    "        self.layer_norm3 = nn.LayerNorm(d_model//32)\n",
    "        self.linear4 = nn.Linear(d_model//32, 4)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x = x.to(device)\n",
    "        x = x.cuda()\n",
    "\n",
    "        x = self.layer_norm1(self.dropout(self.linear1(x)))\n",
    "        x = self.layer_norm2(self.dropout(self.linear2(x)))\n",
    "        x = self.layer_norm3(self.dropout(self.linear3(x)))\n",
    "        x = self.linear4(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # embedding matching\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # feedforward\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        # activation\n",
    "        self.activation = nn.GELU()\n",
    "        \n",
    "    def forward(self, text_emb, box_emb):\n",
    "\n",
    "        # https://arxiv.org/pdf/2002.04745.pdf\n",
    "        # here we propose the original transformer encoder layer\n",
    "        # however, we designed the architecture in this way\n",
    "        # as the authors of the paper did to improve the convergence\n",
    "\n",
    "        # text_emb = text_emb.to(device)\n",
    "        # box_emb = box_emb.to(device)\n",
    "\n",
    "        text_emb = text_emb.cuda()\n",
    "        box_emb = box_emb.cuda()\n",
    "\n",
    "\n",
    "        # # Add & Norm\n",
    "        # text_emb = text_emb + self.dropout1(text_emb)\n",
    "        # text_emb = self.norm1(text_emb)\n",
    "\n",
    "        # box_emb = box_emb + self.dropout1(box_emb)\n",
    "        # box_emb = self.norm1(box_emb)\n",
    "\n",
    "        # print(text_emb.shape, box_emb.shape)\n",
    "\n",
    "        # embedding matching\n",
    "        x , _ = self.self_attn(box_emb, text_emb, box_emb)\n",
    "\n",
    "        # print(x.shape, box_emb.shape)\n",
    "        \n",
    "        # Add & Norm\n",
    "        x = box_emb + self.dropout1(x)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # feedforward\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "\n",
    "        x = box_emb + self.dropout2(x)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.encoder_block = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "        # self.transformer_encoder = nn.TransformerEncoder(self.encoder_block, num_layers)\n",
    "\n",
    "        self.transformer_encoder = [TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout).to(device).type(torch.float32) for _ in range(num_layers)]\n",
    "\n",
    "\n",
    "    def forward(self, text_emb, box_emb):\n",
    "\n",
    "        # # matching between the text and the first box\n",
    "        # x0 = self.transformer_encoder(text_emb, box_emb[:,0,:].unsqueeze(1))\n",
    "        # # matching between the text and the second box\n",
    "        # x1 = self.transformer_encoder(text_emb, box_emb[:,1,:].unsqueeze(1))\n",
    "        \n",
    "        # # concatenate the two boxes\n",
    "        # # shape: (batch_size, 2, d_model)\n",
    "        # x = torch.cat([x0, x1], axis=1)\n",
    "\n",
    "\n",
    "        # text_emb = text_emb.to(device)\n",
    "        # box_emb = box_emb.to(device)\n",
    "\n",
    "        text_emb = text_emb.cuda()\n",
    "        box_emb = box_emb.cuda()\n",
    "        \n",
    "        x0 = text_emb.to(device)\n",
    "        x1 = text_emb.to(device)\n",
    "\n",
    "        for layer in self.transformer_encoder:\n",
    "            # matching between the text and the first box\n",
    "            x0 = layer(x0, box_emb[:,0,:].unsqueeze(1))\n",
    "            # matching between the text and the second box\n",
    "            x1 = layer(x1, box_emb[:,1,:].unsqueeze(1))\n",
    "\n",
    "        # concatenate the two boxes\n",
    "        # shape: (batch_size, 2, d_model)\n",
    "        x = torch.cat([x0, x1], axis=1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "class BoxRegressor(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transformer_encoder = TransformerEncoder(d_model, nhead, num_layers, dim_feedforward, dropout).to(device).type(torch.float32)\n",
    "        self.mlp_regressor = MLP(1034, dropout).to(device).type(torch.float32)\n",
    "        self.flatten = nn.Flatten(start_dim=1).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, text_encoding, box_encoding, box_coords):\n",
    "\n",
    "        # text_encoding = text_encoding.to(device)\n",
    "        # box_encoding = box_encoding.to(device)\n",
    "        # box_coords = box_coords.to(device)\n",
    "\n",
    "        text_encoding = text_encoding.cuda()\n",
    "        box_encoding = box_encoding.cuda()\n",
    "        box_coords = box_coords.cuda()\n",
    "\n",
    "        # compute the similarity matrix between the text and the boxes encoding\n",
    "        similarity_matrix = torch.bmm(text_encoding.permute(0, 2, 1), box_encoding)\n",
    "\n",
    "        # get the index of the top two boxes with the highest score\n",
    "        top2_indices = torch.topk(similarity_matrix, k=2, dim=-1).indices.squeeze(1)\n",
    "        top2 = torch.topk(similarity_matrix, k=2, dim=-1).indices.squeeze(1)\n",
    "\n",
    "\n",
    "        # permute the dimensions to get the top two boxes\n",
    "        box_encoding = box_encoding.permute(0, 2, 1)\n",
    "        # get the top two boxes\n",
    "        top2_boxes = box_encoding[torch.arange(box_encoding.shape[0]).unsqueeze(1), top2_indices]\n",
    "        # print(top2_boxes.shape)\n",
    "\n",
    "        # get the top two boxes coordinates\n",
    "        top2_boxes_coords = box_coords[torch.arange(box_encoding.shape[0]).unsqueeze(1), top2_indices]\n",
    "        # print(top2_boxes_coords.shape)\n",
    "\n",
    "        top2_boxes = top2_boxes.to(device)\n",
    "\n",
    "        # compute the matching between the text and the top two boxes\n",
    "        out_matching = self.transformer_encoder(text_encoding.permute(0, 2, 1), top2_boxes)\n",
    "\n",
    "        # concatenate the matching score with the top two boxes coordinates\n",
    "        matching_score = torch.cat([top2.unsqueeze(2), top2_boxes_coords, out_matching], axis=-1)\n",
    "\n",
    "        return self.mlp_regressor(self.flatten(matching_score))\n",
    "    \n",
    "\n",
    "\n",
    "# def SUM_MSE_loss(pred, target):\n",
    "#     return (pred - target).pow(2).sum(axis=-1).mean()\n",
    "\n",
    "from torch.nn import HuberLoss\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "# class Net(pl.LightningModule):\n",
    "#     def __init__(self, d_model, nhead, num_layers, dim_feedforward, dropout=0.1):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.box_regressor = BoxRegressor(d_model, nhead, num_layers, dim_feedforward, dropout).to(device)\n",
    "#         self.loss = MSELoss()\n",
    "\n",
    "#     def forward(self, text_encoding, box_encoding, box_coords):\n",
    "#         return self.box_regressor(text_encoding, box_encoding, box_coords)\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         # training_step defined the train loop.\n",
    "#         # It is independent of forward\n",
    "#         text_encoding, box_encoding, box_coords, labels = batch\n",
    "\n",
    "#         # text_encoding = text_encoding.to(device)\n",
    "#         # box_encoding = box_encoding.to(device)\n",
    "#         # box_coords = box_coords.to(device)\n",
    "#         # labels = labels.to(device)\n",
    "\n",
    "#         text_encoding = text_encoding.cuda()\n",
    "#         box_encoding = box_encoding.cuda()\n",
    "#         box_coords = box_coords.cuda()\n",
    "#         labels = labels.cuda()\n",
    "\n",
    "#         self = self.to(device)\n",
    "\n",
    "#         out = self(text_encoding, box_encoding, box_coords)\n",
    "\n",
    "#         # print(F.mse_loss(1/(self.forward(text_encoding, box_encoding, box_coords).squeeze(1)+1),  1/(labels+1)))\n",
    "\n",
    "#         # out = out.squeeze(1)\n",
    "#         # loss = F.mse_loss(1/(out+1), 1/(labels+1))\n",
    "#         loss = self.loss(out, labels)/labels.shape[0]\n",
    "#         # loss = SUM_MSE_loss(out, labels)\n",
    "\n",
    "#         print(loss)\n",
    "\n",
    "#         # Logging to TensorBoard by default\n",
    "#         self.log('train_loss', loss)\n",
    "\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "\n",
    "#         text_encoding, box_encoding, box_coords, labels = batch\n",
    "\n",
    "#         # text_encoding = text_encoding.to(device)\n",
    "#         # box_encoding = box_encoding.to(device)\n",
    "#         # box_coords = box_coords.to(device)\n",
    "#         # labels = labels.to(device)\n",
    "\n",
    "#         text_encoding = text_encoding.cuda()\n",
    "#         box_encoding = box_encoding.cuda()\n",
    "#         box_coords = box_coords.cuda()\n",
    "#         labels = labels.cuda()\n",
    "\n",
    "#         out = self(text_encoding, box_encoding, box_coords)\n",
    "\n",
    "#         print(out.shape)\n",
    "\n",
    "#         # out = out.squeeze(1)\n",
    "#         # loss = F.mse_loss(1/(out+1), 1/(labels+1))\n",
    "#         loss = self.loss(out, labels)/labels.shape[0]\n",
    "#         # loss = SUM_MSE_loss(out, labels)\n",
    "\n",
    "#         # Logging to TensorBoard by default\n",
    "#         self.log('val_loss', loss)\n",
    "\n",
    "#         return loss\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = torch.optim.AdamW(self.parameters(), lr=5e-4)\n",
    "#         scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "#         return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.box_regressor = BoxRegressor(d_model, nhead, num_layers, dim_feedforward, dropout).to(device)\n",
    "        self.loss = MSELoss()\n",
    "\n",
    "    def forward(self, text_encoding, box_encoding, box_coords):\n",
    "        return self.box_regressor(text_encoding, box_encoding, box_coords)\n",
    "\n",
    "\n",
    "def training(model, train_loader, val_loader, optimizer, criterion = MSELoss(), device = 'cuda', epochs = 10):\n",
    "\n",
    "    sample = 0.0\n",
    "    cum_loss = 0.0\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        for batch_idx, (text_encoding, box_encoding, box_coords, labels) in enumerate(train_loader):\n",
    "\n",
    "            text_encoding = text_encoding.cuda()\n",
    "            box_encoding = box_encoding.cuda()\n",
    "            box_coords = box_coords.cuda()\n",
    "\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            output = model(text_encoding, box_encoding, box_coords)\n",
    "\n",
    "            output = output.squeeze(1)\n",
    "            labels = labels.squeeze(1)\n",
    "\n",
    "            # print(output - labels)\n",
    "            # print(output)\n",
    "            # print(output.shape, labels.shape)\n",
    "\n",
    "            loss = criterion(output, 1/labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            sample += len(text_encoding)\n",
    "            cum_loss += loss.item()\n",
    "\n",
    "        test_fn(model, val_loader, MSELoss(), device)\n",
    "\n",
    "        print(f'Train Epoch: {e} Loss: {cum_loss/sample}')    \n",
    "\n",
    "\n",
    "def test_fn(model, test_loader, criterion = MSELoss(), device = 'cuda'):\n",
    "\n",
    "    sample = 0.0\n",
    "    cum_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (text_encoding, box_encoding, box_coords, labels) in enumerate(test_loader):\n",
    "\n",
    "            text_encoding = text_encoding.cuda()\n",
    "            box_encoding = box_encoding.cuda()\n",
    "            box_coords = box_coords.cuda()\n",
    "\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            output = model(text_encoding, box_encoding, box_coords)\n",
    "\n",
    "            output = output.squeeze(1)\n",
    "            labels = labels.squeeze(1)\n",
    "\n",
    "            loss = criterion(output, 1/labels)\n",
    "\n",
    "            print(loss)\n",
    "            \n",
    "            sample += len(text_encoding)\n",
    "            cum_loss += loss.item()\n",
    "\n",
    "        print(f'Test Loss: {cum_loss/sample}')      \n",
    "        \n",
    "\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(device)\n",
    "\n",
    "# text_encoding = torch.randn(10000, 512, 1).to(device)\n",
    "# box_encoding = torch.randn(10000, 512, 10).to(device)\n",
    "# box_coords = torch.randn(10000, 10, 4).to(device)\n",
    "# target_boxes = torch.randn(10000, 1, 4).to(device)\n",
    "\n",
    "\n",
    "# # get dataset\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# batch_size = 256\n",
    "\n",
    "# dataset = TensorDataset(text_encoding, box_encoding, box_coords, target_boxes)\n",
    "# train_loader = DataLoader(dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "\n",
    "# # init model\n",
    "# model = Net(512, 8, 2, 2048, 0.1).cuda()\n",
    "\n",
    "# print(model)\n",
    "# print('number of parameter: ',sum(p.numel() for p in model.parameters() if p.requires_grad)/1000000.0, 'M')\n",
    "\n",
    "# # most basic trainer, uses good defaults\n",
    "# trainer = pl.Trainer(accelerator='auto', max_epochs=10)\n",
    "\n",
    "# # train the model\n",
    "# trainer.fit(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (box_regressor): BoxRegressor(\n",
      "    (transformer_encoder): TransformerEncoder()\n",
      "    (mlp_regressor): MLP(\n",
      "      (linear1): Linear(in_features=1034, out_features=258, bias=True)\n",
      "      (layer_norm1): LayerNorm((258,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear2): Linear(in_features=258, out_features=64, bias=True)\n",
      "      (layer_norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear3): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (layer_norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear4): Linear(in_features=32, out_features=4, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (loss): MSELoss()\n",
      ")\n",
      "number of parameter:  0.286526 M\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "Test Loss: nan\n",
      "Train Epoch: 0 Loss: nan\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "Test Loss: nan\n",
      "Train Epoch: 1 Loss: nan\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "Test Loss: nan\n",
      "Train Epoch: 2 Loss: nan\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "Test Loss: nan\n",
      "Train Epoch: 3 Loss: nan\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "tensor(nan, device='cuda:0')\n",
      "Test Loss: nan\n",
      "Train Epoch: 4 Loss: nan\n"
     ]
    }
   ],
   "source": [
    "# init model\n",
    "model = Net(512, 1, 1, 10, 0.1).type(torch.float32).cuda()\n",
    "\n",
    "print(model)\n",
    "print('number of parameter: ',sum(p.numel() for p in model.parameters() if p.requires_grad)/1000000.0, 'M')\n",
    "\n",
    "\n",
    "del dataset_test, dataset_train, dataset_val\n",
    "\n",
    "dataset_test = TensorDataset(text_encoding_test.type(torch.float32), box_encoding_test.type(torch.float32), box_coords_test.type(torch.float32), target_boxes_test.type(torch.float32))\n",
    "dataset_val = TensorDataset(text_encoding_val.type(torch.float32), box_encoding_val.type(torch.float32), box_coords_val.type(torch.float32), target_boxes_val.type(torch.float32))\n",
    "# dataset_train = TensorDataset(text_encoding_train, box_encoding_train, box_coords_train, target_boxes_train)\n",
    "\n",
    "\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "training(model, test_loader, val_loader, torch.optim.AdamW(model.parameters(), lr=5e-4), MSELoss(), device, 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "\n",
    "import pickle\n",
    "\n",
    "# load test dataset\n",
    "file_name = './data/yolo_v8x/yolo_v8x_1_dictionary_full_test.p'\n",
    "with open(file_name, 'rb') as f:\n",
    "    data_test = pickle.load(f)\n",
    "\n",
    "# load val dataset\n",
    "file_name = './data/yolo_v8x/yolo_v8x_1_dictionary_full_val.p'\n",
    "with open(file_name, 'rb') as f:\n",
    "    data_val = pickle.load(f)\n",
    "\n",
    "# load train dataset\n",
    "file_name = './data/yolo_v8x/yolo_v8x_1_dictionary_full_train.p'\n",
    "with open(file_name, 'rb') as f:\n",
    "    data_train = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "# text_encoding = torch.randn(10000, 512, 1).to(device)\n",
    "# box_encoding = torch.randn(10000, 512, 10).to(device)\n",
    "# box_coords = torch.randn(10000, 10, 4).to(device)\n",
    "# target_boxes = torch.randn(10000, 1, 4).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5023/5023 [00:10<00:00, 467.57it/s]\n",
      "100%|██████████| 2573/2573 [00:05<00:00, 478.34it/s]\n",
      "100%|██████████| 42226/42226 [01:24<00:00, 502.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_data(full_data):\n",
    "\n",
    "    text_encoding, box_encoding, box_coords, target_boxes = [], [], [], []\n",
    "\n",
    "    for idx in tqdm(list(full_data)):\n",
    "        # for _ in range(data['image_emb'].shape[0]):\n",
    "        for idx_text in range(full_data[idx]['text_emb'].shape[0]):\n",
    "            \n",
    "            # number of available crops\n",
    "            number_of_crop = min(full_data[idx]['image_emb'].shape[0], len(full_data[idx]['df_boxes']))\n",
    "\n",
    "            if number_of_crop == 0:\n",
    "                break\n",
    "\n",
    "            # shape: (number of samples, 512, 1)\n",
    "            text_encoding.append(full_data[idx]['text_emb'][idx_text].unsqueeze(1))\n",
    "\n",
    "            number_of_crop = min(full_data[idx]['image_emb'].shape[0], len(full_data[idx]['df_boxes']))\n",
    "\n",
    "            # shape: (number of samples, 512, number of crop embeddings)\n",
    "            box_encoding.append(full_data[idx]['image_emb'][:number_of_crop,:].permute(1, 0))\n",
    "\n",
    "            # shape: (number of samples, number of boxes, 4)\n",
    "            box_coords.append(torch.stack([torch.tensor(full_data[idx]['df_boxes'].iloc[i][:4]).type(torch.float16) \n",
    "                                                    for i in range(number_of_crop)]))\n",
    "            \n",
    "            # shape: (number of samples, 1, 4)\n",
    "            target_boxes.append(torch.tensor(full_data[idx]['bbox_target']).type(torch.float16).unsqueeze(0))\n",
    "\n",
    "    return torch.stack(text_encoding), torch.stack([torch.nn.functional.pad(b.permute(1, 0), (0, 0, 0, 48 - b.shape[1])).permute(1, 0) for b in box_encoding]), torch.stack([torch.nn.functional.pad(b, (0, 0, 0, 48 - b.shape[0])) for b in box_coords]), torch.stack(target_boxes)\n",
    "\n",
    "\n",
    "\n",
    "text_encoding_test, box_encoding_test, box_coords_test, target_boxes_test = get_data(data_test)\n",
    "text_encoding_val, box_encoding_val, box_coords_val, target_boxes_val = get_data(data_val)\n",
    "text_encoding_train, box_encoding_train, box_coords_train, target_boxes_train = get_data(data_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_norm_rescale(box_target):\n",
    "    \"\"\" Rescale the box_target \n",
    "    Args:\n",
    "        box_target: (number of samples, 1, 4)\n",
    "\n",
    "    Returns:\n",
    "        box_target: (number of samples, 1, 4)\n",
    "\n",
    "    \"\"\"\n",
    "    # convert the box_pred to x1, y1, x2, y2\n",
    "    box_target[:, 0, 2] = box_target[:, 0, 0] + box_target[:, 0, 2]\n",
    "    box_target[:, 0, 3] = box_target[:, 0, 1] + box_target[:, 0, 3]\n",
    "\n",
    "    return box_target\n",
    "\n",
    "# box rescaling\n",
    "target_boxes_test = box_norm_rescale(target_boxes_test)\n",
    "target_boxes_val = box_norm_rescale(target_boxes_val)\n",
    "target_boxes_train = box_norm_rescale(target_boxes_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type         | Params\n",
      "-----------------------------------------------\n",
      "0 | box_regressor | BoxRegressor | 286 K \n",
      "1 | loss          | MSELoss      | 0     \n",
      "-----------------------------------------------\n",
      "286 K     Trainable params\n",
      "0         Non-trainable params\n",
      "286 K     Total params\n",
      "1.146     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (box_regressor): BoxRegressor(\n",
      "    (transformer_encoder): TransformerEncoder()\n",
      "    (mlp_regressor): MLP(\n",
      "      (linear1): Linear(in_features=1034, out_features=258, bias=True)\n",
      "      (layer_norm1): LayerNorm((258,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear2): Linear(in_features=258, out_features=64, bias=True)\n",
      "      (layer_norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear3): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (layer_norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear4): Linear(in_features=32, out_features=4, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (loss): MSELoss()\n",
      ")\n",
      "number of parameter:  0.286526 M\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]torch.Size([256, 4])\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 61.85it/s]torch.Size([256, 4])\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/rickbook/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 1, 4])) that is different to the input size (torch.Size([256, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/315 [00:00<?, ?it/s] tensor(inf, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   0%|          | 1/315 [00:00<00:10, 29.01it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   1%|          | 2/315 [00:00<00:10, 30.63it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   1%|          | 3/315 [00:00<00:09, 32.55it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   1%|▏         | 4/315 [00:00<00:09, 32.37it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   2%|▏         | 5/315 [00:00<00:09, 32.48it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   2%|▏         | 6/315 [00:00<00:09, 33.05it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   2%|▏         | 7/315 [00:00<00:09, 33.54it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   3%|▎         | 8/315 [00:00<00:09, 33.64it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   3%|▎         | 9/315 [00:00<00:08, 34.00it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   3%|▎         | 10/315 [00:00<00:08, 33.94it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   3%|▎         | 11/315 [00:00<00:08, 34.14it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   4%|▍         | 12/315 [00:00<00:08, 34.69it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   4%|▍         | 13/315 [00:00<00:08, 35.12it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   4%|▍         | 14/315 [00:00<00:08, 35.40it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   5%|▍         | 15/315 [00:00<00:08, 35.71it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   5%|▌         | 16/315 [00:00<00:08, 35.96it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   5%|▌         | 17/315 [00:00<00:08, 36.12it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   6%|▌         | 18/315 [00:00<00:08, 36.26it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   6%|▌         | 19/315 [00:00<00:08, 36.53it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   6%|▋         | 20/315 [00:00<00:08, 36.66it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   7%|▋         | 21/315 [00:00<00:07, 36.94it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   7%|▋         | 22/315 [00:00<00:07, 36.99it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   7%|▋         | 23/315 [00:00<00:07, 37.24it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   8%|▊         | 24/315 [00:00<00:07, 37.51it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   8%|▊         | 25/315 [00:00<00:07, 37.93it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   8%|▊         | 26/315 [00:00<00:07, 37.96it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   9%|▊         | 27/315 [00:00<00:07, 38.07it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   9%|▉         | 28/315 [00:00<00:07, 38.24it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:   9%|▉         | 29/315 [00:00<00:07, 38.42it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  10%|▉         | 30/315 [00:00<00:07, 38.57it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  10%|▉         | 31/315 [00:00<00:07, 38.64it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  10%|█         | 32/315 [00:00<00:07, 37.52it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  10%|█         | 33/315 [00:00<00:07, 37.30it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  11%|█         | 34/315 [00:00<00:07, 37.18it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  11%|█         | 35/315 [00:00<00:07, 36.97it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  11%|█▏        | 36/315 [00:00<00:07, 36.79it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  12%|█▏        | 37/315 [00:01<00:07, 36.95it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  12%|█▏        | 38/315 [00:01<00:07, 37.08it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  12%|█▏        | 39/315 [00:01<00:07, 37.21it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  13%|█▎        | 40/315 [00:01<00:07, 37.32it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  13%|█▎        | 41/315 [00:01<00:07, 37.43it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  13%|█▎        | 42/315 [00:01<00:07, 37.50it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  14%|█▎        | 43/315 [00:01<00:07, 37.52it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  14%|█▍        | 44/315 [00:01<00:07, 37.43it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  14%|█▍        | 45/315 [00:01<00:07, 37.39it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  15%|█▍        | 46/315 [00:01<00:07, 37.15it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  15%|█▍        | 47/315 [00:01<00:07, 37.16it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  15%|█▌        | 48/315 [00:01<00:07, 37.32it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  16%|█▌        | 49/315 [00:01<00:07, 37.51it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  16%|█▌        | 50/315 [00:01<00:07, 37.53it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  16%|█▌        | 51/315 [00:01<00:07, 37.60it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  17%|█▋        | 52/315 [00:01<00:06, 37.64it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  17%|█▋        | 53/315 [00:01<00:06, 37.54it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  17%|█▋        | 54/315 [00:01<00:06, 37.62it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  17%|█▋        | 55/315 [00:01<00:06, 37.63it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  18%|█▊        | 56/315 [00:01<00:06, 37.69it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  18%|█▊        | 57/315 [00:01<00:06, 37.74it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  18%|█▊        | 58/315 [00:01<00:06, 37.79it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  19%|█▊        | 59/315 [00:01<00:06, 37.87it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  19%|█▉        | 60/315 [00:01<00:06, 37.77it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  19%|█▉        | 61/315 [00:01<00:06, 37.85it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  20%|█▉        | 62/315 [00:01<00:06, 37.94it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  20%|██        | 63/315 [00:01<00:06, 38.04it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  20%|██        | 64/315 [00:01<00:06, 38.02it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  21%|██        | 65/315 [00:01<00:06, 38.02it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  21%|██        | 66/315 [00:01<00:06, 38.04it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  21%|██▏       | 67/315 [00:01<00:06, 38.09it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  22%|██▏       | 68/315 [00:01<00:06, 37.92it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  22%|██▏       | 69/315 [00:01<00:06, 37.68it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  22%|██▏       | 70/315 [00:01<00:06, 37.54it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  23%|██▎       | 71/315 [00:01<00:06, 37.53it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  23%|██▎       | 72/315 [00:01<00:06, 37.47it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  23%|██▎       | 73/315 [00:01<00:06, 37.31it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  23%|██▎       | 74/315 [00:01<00:06, 37.30it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  24%|██▍       | 75/315 [00:02<00:06, 37.22it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  24%|██▍       | 76/315 [00:02<00:06, 37.30it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  24%|██▍       | 77/315 [00:02<00:06, 37.34it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  25%|██▍       | 78/315 [00:02<00:06, 37.23it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  25%|██▌       | 79/315 [00:02<00:06, 36.97it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  25%|██▌       | 80/315 [00:02<00:06, 36.78it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  26%|██▌       | 81/315 [00:02<00:06, 36.75it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  26%|██▌       | 82/315 [00:02<00:06, 36.70it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  26%|██▋       | 83/315 [00:02<00:06, 36.75it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  27%|██▋       | 84/315 [00:02<00:06, 36.73it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  27%|██▋       | 85/315 [00:02<00:06, 36.77it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  27%|██▋       | 86/315 [00:02<00:06, 36.82it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  28%|██▊       | 87/315 [00:02<00:06, 36.87it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  28%|██▊       | 88/315 [00:02<00:06, 36.85it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  28%|██▊       | 89/315 [00:02<00:06, 36.83it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  29%|██▊       | 90/315 [00:02<00:06, 36.77it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  29%|██▉       | 91/315 [00:02<00:06, 36.71it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  29%|██▉       | 92/315 [00:02<00:06, 36.67it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  30%|██▉       | 93/315 [00:02<00:06, 36.62it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  30%|██▉       | 94/315 [00:02<00:06, 36.60it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  30%|███       | 95/315 [00:02<00:06, 36.58it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  30%|███       | 96/315 [00:02<00:05, 36.63it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  31%|███       | 97/315 [00:02<00:05, 36.64it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  31%|███       | 98/315 [00:02<00:05, 36.65it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  31%|███▏      | 99/315 [00:02<00:05, 36.64it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  32%|███▏      | 100/315 [00:02<00:05, 36.57it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  32%|███▏      | 101/315 [00:02<00:05, 36.60it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  32%|███▏      | 102/315 [00:02<00:05, 36.49it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  33%|███▎      | 103/315 [00:02<00:05, 36.46it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  33%|███▎      | 104/315 [00:02<00:05, 36.27it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  33%|███▎      | 105/315 [00:02<00:05, 36.20it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  34%|███▎      | 106/315 [00:02<00:05, 36.06it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  34%|███▍      | 107/315 [00:02<00:05, 35.99it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  34%|███▍      | 108/315 [00:03<00:05, 35.99it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  35%|███▍      | 109/315 [00:03<00:05, 35.99it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  35%|███▍      | 110/315 [00:03<00:05, 36.08it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  35%|███▌      | 111/315 [00:03<00:05, 36.12it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  36%|███▌      | 112/315 [00:03<00:05, 36.13it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  36%|███▌      | 113/315 [00:03<00:05, 36.10it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  36%|███▌      | 114/315 [00:03<00:05, 36.13it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  37%|███▋      | 115/315 [00:03<00:05, 36.09it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  37%|███▋      | 116/315 [00:03<00:05, 35.97it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  37%|███▋      | 117/315 [00:03<00:05, 35.89it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  37%|███▋      | 118/315 [00:03<00:05, 35.93it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  38%|███▊      | 119/315 [00:03<00:05, 35.92it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  38%|███▊      | 120/315 [00:03<00:05, 35.93it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  38%|███▊      | 121/315 [00:03<00:05, 35.93it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  39%|███▊      | 122/315 [00:03<00:05, 35.93it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  39%|███▉      | 123/315 [00:03<00:05, 35.79it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  39%|███▉      | 124/315 [00:03<00:05, 35.58it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  40%|███▉      | 125/315 [00:03<00:05, 35.54it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  40%|████      | 126/315 [00:03<00:05, 35.52it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  40%|████      | 127/315 [00:03<00:05, 35.56it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  41%|████      | 128/315 [00:03<00:05, 35.60it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  41%|████      | 129/315 [00:03<00:05, 35.66it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  41%|████▏     | 130/315 [00:03<00:05, 35.70it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  42%|████▏     | 131/315 [00:03<00:05, 35.74it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  42%|████▏     | 132/315 [00:03<00:05, 35.76it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  42%|████▏     | 133/315 [00:03<00:05, 35.75it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  43%|████▎     | 134/315 [00:03<00:05, 35.78it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  43%|████▎     | 135/315 [00:03<00:05, 35.80it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  43%|████▎     | 136/315 [00:03<00:04, 35.81it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  43%|████▎     | 137/315 [00:03<00:04, 35.85it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  44%|████▍     | 138/315 [00:03<00:04, 35.81it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  44%|████▍     | 139/315 [00:03<00:04, 35.74it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  44%|████▍     | 140/315 [00:03<00:04, 35.75it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  45%|████▍     | 141/315 [00:03<00:04, 35.78it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  45%|████▌     | 142/315 [00:03<00:04, 35.80it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  45%|████▌     | 143/315 [00:03<00:04, 35.81it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  46%|████▌     | 144/315 [00:04<00:04, 35.82it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  46%|████▌     | 145/315 [00:04<00:04, 35.86it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  46%|████▋     | 146/315 [00:04<00:04, 35.82it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  47%|████▋     | 147/315 [00:04<00:04, 35.86it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  47%|████▋     | 148/315 [00:04<00:04, 35.86it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  47%|████▋     | 149/315 [00:04<00:04, 35.87it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  48%|████▊     | 150/315 [00:04<00:04, 35.90it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  48%|████▊     | 151/315 [00:04<00:04, 35.89it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  48%|████▊     | 152/315 [00:04<00:04, 35.90it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  49%|████▊     | 153/315 [00:04<00:04, 35.93it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  49%|████▉     | 154/315 [00:04<00:04, 35.92it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  49%|████▉     | 155/315 [00:04<00:04, 35.92it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  50%|████▉     | 156/315 [00:04<00:04, 35.94it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  50%|████▉     | 157/315 [00:04<00:04, 35.94it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  50%|█████     | 158/315 [00:04<00:04, 35.92it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  50%|█████     | 159/315 [00:04<00:04, 35.95it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  51%|█████     | 160/315 [00:04<00:04, 35.94it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  51%|█████     | 161/315 [00:04<00:04, 35.91it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  51%|█████▏    | 162/315 [00:04<00:04, 35.89it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  52%|█████▏    | 163/315 [00:04<00:04, 35.90it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  52%|█████▏    | 164/315 [00:04<00:04, 35.88it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  52%|█████▏    | 165/315 [00:04<00:04, 35.89it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  53%|█████▎    | 166/315 [00:04<00:04, 35.90it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  53%|█████▎    | 167/315 [00:04<00:04, 35.87it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  53%|█████▎    | 168/315 [00:04<00:04, 35.89it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  54%|█████▎    | 169/315 [00:04<00:04, 35.90it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  54%|█████▍    | 170/315 [00:04<00:04, 35.91it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  54%|█████▍    | 171/315 [00:04<00:04, 35.84it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  55%|█████▍    | 172/315 [00:04<00:03, 35.82it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  55%|█████▍    | 173/315 [00:04<00:03, 35.82it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  55%|█████▌    | 174/315 [00:04<00:03, 35.74it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  56%|█████▌    | 175/315 [00:04<00:03, 35.63it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  56%|█████▌    | 176/315 [00:04<00:03, 35.65it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  56%|█████▌    | 177/315 [00:04<00:03, 35.67it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  57%|█████▋    | 178/315 [00:04<00:03, 35.66it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  57%|█████▋    | 179/315 [00:05<00:03, 35.63it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  57%|█████▋    | 180/315 [00:05<00:03, 35.66it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  57%|█████▋    | 181/315 [00:05<00:03, 35.69it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  58%|█████▊    | 182/315 [00:05<00:03, 35.66it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  58%|█████▊    | 183/315 [00:05<00:03, 35.65it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  58%|█████▊    | 184/315 [00:05<00:03, 35.66it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  59%|█████▊    | 185/315 [00:05<00:03, 35.68it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  59%|█████▉    | 186/315 [00:05<00:03, 35.70it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  59%|█████▉    | 187/315 [00:05<00:03, 35.67it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  60%|█████▉    | 188/315 [00:05<00:03, 35.69it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  60%|██████    | 189/315 [00:05<00:03, 35.70it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  60%|██████    | 190/315 [00:05<00:03, 35.73it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  61%|██████    | 191/315 [00:05<00:03, 35.70it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  61%|██████    | 192/315 [00:05<00:03, 35.72it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  61%|██████▏   | 193/315 [00:05<00:03, 35.75it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  62%|██████▏   | 194/315 [00:05<00:03, 35.76it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  62%|██████▏   | 195/315 [00:05<00:03, 35.79it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  62%|██████▏   | 196/315 [00:05<00:03, 35.81it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  63%|██████▎   | 197/315 [00:05<00:03, 35.82it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  63%|██████▎   | 198/315 [00:05<00:03, 35.84it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  63%|██████▎   | 199/315 [00:05<00:03, 35.83it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  63%|██████▎   | 200/315 [00:05<00:03, 35.83it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  64%|██████▍   | 201/315 [00:05<00:03, 35.82it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  64%|██████▍   | 202/315 [00:05<00:03, 35.80it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  64%|██████▍   | 203/315 [00:05<00:03, 35.81it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  65%|██████▍   | 204/315 [00:05<00:03, 35.73it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  65%|██████▌   | 205/315 [00:05<00:03, 35.73it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  65%|██████▌   | 206/315 [00:05<00:03, 35.73it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  66%|██████▌   | 207/315 [00:05<00:03, 35.77it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  66%|██████▌   | 208/315 [00:05<00:02, 35.81it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  66%|██████▋   | 209/315 [00:05<00:02, 35.83it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  67%|██████▋   | 210/315 [00:05<00:02, 35.86it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  67%|██████▋   | 211/315 [00:05<00:02, 35.74it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  67%|██████▋   | 212/315 [00:05<00:02, 35.76it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  68%|██████▊   | 213/315 [00:05<00:02, 35.78it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  68%|██████▊   | 214/315 [00:05<00:02, 35.78it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  68%|██████▊   | 215/315 [00:06<00:02, 35.78it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  69%|██████▊   | 216/315 [00:06<00:02, 35.79it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  69%|██████▉   | 217/315 [00:06<00:02, 35.79it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  69%|██████▉   | 218/315 [00:06<00:02, 35.82it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  70%|██████▉   | 219/315 [00:06<00:02, 35.80it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  70%|██████▉   | 220/315 [00:06<00:02, 35.83it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  70%|███████   | 221/315 [00:06<00:02, 35.84it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  70%|███████   | 222/315 [00:06<00:02, 35.87it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  71%|███████   | 223/315 [00:06<00:02, 35.88it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  71%|███████   | 224/315 [00:06<00:02, 35.86it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  71%|███████▏  | 225/315 [00:06<00:02, 35.88it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  72%|███████▏  | 226/315 [00:06<00:02, 35.88it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  72%|███████▏  | 227/315 [00:06<00:02, 35.81it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  72%|███████▏  | 228/315 [00:06<00:02, 35.79it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  73%|███████▎  | 229/315 [00:06<00:02, 35.71it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  73%|███████▎  | 230/315 [00:06<00:02, 35.65it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  73%|███████▎  | 231/315 [00:06<00:02, 35.54it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  74%|███████▎  | 232/315 [00:06<00:02, 35.51it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  74%|███████▍  | 233/315 [00:06<00:02, 35.49it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  74%|███████▍  | 234/315 [00:06<00:02, 35.51it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  75%|███████▍  | 235/315 [00:06<00:02, 35.49it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  75%|███████▍  | 236/315 [00:06<00:02, 35.50it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  75%|███████▌  | 237/315 [00:06<00:02, 35.49it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  76%|███████▌  | 238/315 [00:06<00:02, 35.48it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  76%|███████▌  | 239/315 [00:06<00:02, 35.46it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  76%|███████▌  | 240/315 [00:06<00:02, 35.42it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  77%|███████▋  | 241/315 [00:06<00:02, 35.40it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  77%|███████▋  | 242/315 [00:06<00:02, 35.38it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  77%|███████▋  | 243/315 [00:06<00:02, 35.38it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  77%|███████▋  | 244/315 [00:06<00:02, 35.32it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  78%|███████▊  | 245/315 [00:06<00:01, 35.27it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  78%|███████▊  | 246/315 [00:06<00:01, 35.27it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  78%|███████▊  | 247/315 [00:07<00:01, 35.27it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  79%|███████▊  | 248/315 [00:07<00:01, 35.27it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  79%|███████▉  | 249/315 [00:07<00:01, 35.22it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  79%|███████▉  | 250/315 [00:07<00:01, 35.20it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  80%|███████▉  | 251/315 [00:07<00:01, 35.20it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  80%|████████  | 252/315 [00:07<00:01, 35.17it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  80%|████████  | 253/315 [00:07<00:01, 35.14it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  81%|████████  | 254/315 [00:07<00:01, 35.17it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  81%|████████  | 255/315 [00:07<00:01, 35.20it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  81%|████████▏ | 256/315 [00:07<00:01, 35.20it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  82%|████████▏ | 257/315 [00:07<00:01, 35.21it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  82%|████████▏ | 258/315 [00:07<00:01, 35.24it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  82%|████████▏ | 259/315 [00:07<00:01, 35.26it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  83%|████████▎ | 260/315 [00:07<00:01, 35.27it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  83%|████████▎ | 261/315 [00:07<00:01, 35.22it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  83%|████████▎ | 262/315 [00:07<00:01, 35.24it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  83%|████████▎ | 263/315 [00:07<00:01, 35.27it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  84%|████████▍ | 264/315 [00:07<00:01, 35.28it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  84%|████████▍ | 265/315 [00:07<00:01, 35.30it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  84%|████████▍ | 266/315 [00:07<00:01, 35.32it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  85%|████████▍ | 267/315 [00:07<00:01, 35.34it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  85%|████████▌ | 268/315 [00:07<00:01, 35.35it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  85%|████████▌ | 269/315 [00:07<00:01, 35.35it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  86%|████████▌ | 270/315 [00:07<00:01, 35.35it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  86%|████████▌ | 271/315 [00:07<00:01, 35.37it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  86%|████████▋ | 272/315 [00:07<00:01, 35.38it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  87%|████████▋ | 273/315 [00:07<00:01, 35.38it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  87%|████████▋ | 274/315 [00:07<00:01, 35.39it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  87%|████████▋ | 275/315 [00:07<00:01, 35.40it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  88%|████████▊ | 276/315 [00:07<00:01, 35.41it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  88%|████████▊ | 277/315 [00:07<00:01, 35.40it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  88%|████████▊ | 278/315 [00:07<00:01, 35.36it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  89%|████████▊ | 279/315 [00:07<00:01, 35.29it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  89%|████████▉ | 280/315 [00:07<00:00, 35.22it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  89%|████████▉ | 281/315 [00:07<00:00, 35.22it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  90%|████████▉ | 282/315 [00:08<00:00, 35.20it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  90%|████████▉ | 283/315 [00:08<00:00, 35.21it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  90%|█████████ | 284/315 [00:08<00:00, 35.23it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  90%|█████████ | 285/315 [00:08<00:00, 35.25it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  91%|█████████ | 286/315 [00:08<00:00, 35.25it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  91%|█████████ | 287/315 [00:08<00:00, 35.27it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  91%|█████████▏| 288/315 [00:08<00:00, 35.29it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  92%|█████████▏| 289/315 [00:08<00:00, 35.30it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  92%|█████████▏| 290/315 [00:08<00:00, 35.29it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  92%|█████████▏| 291/315 [00:08<00:00, 35.30it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  93%|█████████▎| 292/315 [00:08<00:00, 35.30it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  93%|█████████▎| 293/315 [00:08<00:00, 35.26it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  93%|█████████▎| 294/315 [00:08<00:00, 35.26it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  94%|█████████▎| 295/315 [00:08<00:00, 35.26it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  94%|█████████▍| 296/315 [00:08<00:00, 35.27it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  94%|█████████▍| 297/315 [00:08<00:00, 35.28it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  95%|█████████▍| 298/315 [00:08<00:00, 35.27it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  95%|█████████▍| 299/315 [00:08<00:00, 35.26it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  95%|█████████▌| 300/315 [00:08<00:00, 35.26it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  96%|█████████▌| 301/315 [00:08<00:00, 35.25it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  96%|█████████▌| 302/315 [00:08<00:00, 35.23it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  96%|█████████▌| 303/315 [00:08<00:00, 35.22it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  97%|█████████▋| 304/315 [00:08<00:00, 35.19it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  97%|█████████▋| 305/315 [00:08<00:00, 35.19it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  97%|█████████▋| 306/315 [00:08<00:00, 35.19it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  97%|█████████▋| 307/315 [00:08<00:00, 35.22it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  98%|█████████▊| 308/315 [00:08<00:00, 35.22it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  98%|█████████▊| 309/315 [00:08<00:00, 35.22it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  98%|█████████▊| 310/315 [00:08<00:00, 35.20it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  99%|█████████▊| 311/315 [00:08<00:00, 35.20it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  99%|█████████▉| 312/315 [00:08<00:00, 35.21it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0:  99%|█████████▉| 313/315 [00:08<00:00, 35.19it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0: 100%|█████████▉| 314/315 [00:08<00:00, 35.14it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 0: 100%|██████████| 315/315 [00:08<00:00, 35.11it/s, v_num=53]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rickbook/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([126, 1, 4])) that is different to the input size (torch.Size([126, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 4])\n",
      "torch.Size([256, 4])\n",
      "torch.Size([256, 4])\n",
      "torch.Size([256, 4])\n",
      "torch.Size([256, 4])\n",
      "torch.Size([256, 4])\n",
      "torch.Size([256, 4])\n",
      "torch.Size([256, 4])\n",
      "torch.Size([256, 4])\n",
      "torch.Size([256, 4])\n",
      "torch.Size([256, 4])\n",
      "torch.Size([256, 4])\n",
      "torch.Size([256, 4])\n",
      "torch.Size([256, 4])\n",
      "torch.Size([256, 4])\n",
      "torch.Size([256, 4])\n",
      "torch.Size([256, 4])\n",
      "torch.Size([256, 4])\n",
      "torch.Size([256, 4])\n",
      "torch.Size([32, 4])\n",
      "Epoch 1:   0%|          | 0/315 [00:00<?, ?it/s, v_num=53]          tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   0%|          | 1/315 [00:00<00:11, 26.99it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   1%|          | 2/315 [00:00<00:10, 29.01it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   1%|          | 3/315 [00:00<00:10, 30.38it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   1%|▏         | 4/315 [00:00<00:09, 31.86it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   2%|▏         | 5/315 [00:00<00:09, 32.57it/s, v_num=53]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rickbook/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32, 1, 4])) that is different to the input size (torch.Size([32, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   2%|▏         | 6/315 [00:00<00:09, 33.08it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   2%|▏         | 7/315 [00:00<00:09, 33.36it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   3%|▎         | 8/315 [00:00<00:09, 33.18it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   3%|▎         | 9/315 [00:00<00:09, 33.55it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   3%|▎         | 10/315 [00:00<00:09, 33.30it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   3%|▎         | 11/315 [00:00<00:09, 33.59it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   4%|▍         | 12/315 [00:00<00:08, 33.91it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   4%|▍         | 13/315 [00:00<00:08, 34.06it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   4%|▍         | 14/315 [00:00<00:08, 34.17it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   5%|▍         | 15/315 [00:00<00:08, 34.30it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   5%|▌         | 16/315 [00:00<00:08, 33.28it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   5%|▌         | 17/315 [00:00<00:09, 32.77it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   6%|▌         | 18/315 [00:00<00:08, 33.07it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   6%|▌         | 19/315 [00:00<00:08, 33.18it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   6%|▋         | 20/315 [00:00<00:08, 33.05it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   7%|▋         | 21/315 [00:00<00:08, 33.58it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   7%|▋         | 22/315 [00:00<00:08, 33.83it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   7%|▋         | 23/315 [00:00<00:08, 34.15it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   8%|▊         | 24/315 [00:00<00:08, 34.50it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   8%|▊         | 25/315 [00:00<00:08, 34.59it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   8%|▊         | 26/315 [00:00<00:08, 34.75it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   9%|▊         | 27/315 [00:00<00:08, 34.48it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   9%|▉         | 28/315 [00:00<00:08, 34.46it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:   9%|▉         | 29/315 [00:00<00:08, 34.25it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:  10%|▉         | 30/315 [00:00<00:08, 34.34it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "Epoch 1:  10%|▉         | 31/315 [00:00<00:08, 34.45it/s, v_num=53]tensor(nan, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "# dataloaders\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# text_encoding_test = text_encoding_test.cpu().type(torch.float32)\n",
    "# box_encoding_test = box_encoding_test.cpu().type(torch.float32)\n",
    "# box_coords_test = box_coords_test.cpu().type(torch.float32)\n",
    "\n",
    "# text_encoding_val = text_encoding_val.cpu().type(torch.float32)\n",
    "# box_encoding_val = box_encoding_val.cpu().type(torch.float32)\n",
    "# box_coords_val = box_coords_val.cpu().type(torch.float32)\n",
    "\n",
    "# text_encoding_train = text_encoding_train.cpu().type(torch.float32)\n",
    "# box_encoding_train = box_encoding_train.cpu().type(torch.float32)\n",
    "# box_coords_train = box_coords_train.cpu().type(torch.float32)\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "dataset_test = TensorDataset(text_encoding_test, box_encoding_test, box_coords_test, target_boxes_test)\n",
    "dataset_val = TensorDataset(text_encoding_val, box_encoding_val, box_coords_val, target_boxes_val)\n",
    "dataset_train = TensorDataset(text_encoding_train, box_encoding_train, box_coords_train, target_boxes_train)\n",
    "\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# init model\n",
    "model = Net(512, 2, 1, 128, 0.1).type(torch.float16).cuda()\n",
    "\n",
    "print(model)\n",
    "print('number of parameter: ',sum(p.numel() for p in model.parameters() if p.requires_grad)/1000000.0, 'M')\n",
    "\n",
    "# most basic trainer, uses good defaults\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=3, verbose=True, mode=\"min\")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(accelerator='auto', max_epochs=5)#, callbacks=[early_stop_callback])\n",
    "\n",
    "# train the model\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# test the model\n",
    "# trainer.test(test_dataloaders=test_loader)\n",
    "\n",
    "# max([b.shape[0] for b in box_coords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 1 6.852848301124163 4.310411508820321\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(max([b.shape[0] for b in box_coords]), min([b.shape[0] for b in box_coords]), np.array([b.shape[0] for b in box_coords]).mean(), np.array([b.shape[0] for b in box_coords]).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_encoding = torch.randn(10000, 512, 1).to(device)\n",
    "# box_encoding = torch.randn(10000, 512, 10).to(device)\n",
    "# box_coords = torch.randn(10000, 10, 4).to(device)\n",
    "# target_boxes = torch.randn(10000, 1, 4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-43e57e843e5394dd\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-43e57e843e5394dd\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3163cfb8aa3549ad3f5400bc3427ee7a4002d2a0d6d7ead52f641c6a7636395"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
