{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/home/rickbook/document/dl/PreProcessing/data/1_dictionary_full_train.p', 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 512])\n",
      "torch.Size([2, 512])\n",
      "['Two woman one in black eatting and the other has a white shirt at the desk', 'Woman in white shirt looking down at laptop computer.']\n",
      "tensor([[1.3977e-01, 4.0894e-03, 7.6408e-03, 7.3242e-01, 3.3069e-04, 1.1584e-01],\n",
      "        [9.4434e-01, 3.4393e-02, 1.0658e-02, 2.7239e-05, 5.2512e-05, 1.0658e-02]],\n",
      "       dtype=torch.float16)\n",
      "         xmin        ymin        xmax        ymax  confidence  class   \n",
      "0    0.465897   41.905228  233.664261  466.485474    0.942153      0  \\\n",
      "1  374.536560    2.168396  639.489319  476.299683    0.928562      0   \n",
      "2  194.285980  211.953125  559.413086  449.904907    0.921412     63   \n",
      "3  135.091156   35.643280  309.594635  224.878021    0.907532      0   \n",
      "4  261.507874  424.752625  363.576904  479.412903    0.880423     39   \n",
      "5  220.025009  171.333633  288.526001  249.269989    0.662977     73   \n",
      "6  460.298340  422.794983  486.305847  454.070251    0.497339     74   \n",
      "7  342.408447  232.942444  399.530701  251.802063    0.260358     67   \n",
      "\n",
      "         name  \n",
      "0      person  \n",
      "1      person  \n",
      "2      laptop  \n",
      "3      person  \n",
      "4      bottle  \n",
      "5        book  \n",
      "6       clock  \n",
      "7  cell phone  \n",
      "[0.0, 45.95, 238.92, 408.64]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['image_emb', 'text_emb', 'text_similarity', 'df_boxes', 'caption', 'bbox_target'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (num_boxes, dim_embedding)\n",
    "print(dictionary[0]['image_emb'].shape)\n",
    "# (number of target caption, dim_embedding)\n",
    "print(dictionary[0]['text_emb'].shape)\n",
    "# (target captions)\n",
    "print(dictionary[0]['caption'])\n",
    "# (text_similarity, target captions)\n",
    "print(dictionary[0]['text_similarity'])\n",
    "# ()\n",
    "print(dictionary[0]['df_boxes'])\n",
    "# ()\n",
    "print(dictionary[0]['bbox_target'])\n",
    "\n",
    "dictionary[0].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
