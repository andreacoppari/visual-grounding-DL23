{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "a_maps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../extractCOCO/data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# with open('../extractCOCO/data_4000-8000.pkl', 'rb') as f:\n",
    "#     data = pickle.load(f)\n",
    "\n",
    "# with open('../extractCOCO/data_8000-12000.pkl', 'rb') as f:\n",
    "#     data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for key in data.keys():\n",
    "    target.append(torch.Tensor(data[key]['boxe']).to(torch.float32))\n",
    "    a_maps.append(data[key]['attn_map'])\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7636"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_a_maps = sum([1 for i in a_maps for j in i if j is not None])\n",
    "num_a_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_maps = torch.Tensor(num_a_maps, a_maps[0][0].shape[0], a_maps[0][0].shape[1])\n",
    "t_targets = torch.Tensor(num_a_maps, 4)\n",
    "\n",
    "for i, (a_map, t) in enumerate(zip(a_maps, target)):\n",
    "    for a in a_map:\n",
    "        t_targets[i] = t\n",
    "        t_maps[i] = a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bach_size = 32\n",
    "\n",
    "data = torch.utils.data.TensorDataset(t_maps, t_targets)\n",
    "\n",
    "train, test = torch.utils.data.random_split(data, [int(0.8*len(data)), len(data)-int(0.8*len(data))])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=bach_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=bach_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_box(output, target):\n",
    "    loss = torch.sum(torch.mean((output - target)**2, dim=1))\n",
    "    return loss\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, dim_emb, nhead: int = 2, dim_feedforward: int = 10, dropout: float =0.1, activation = nn.GELU):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        self.attn = nn.MultiheadAttention(dim_emb, nhead, dropout=dropout)\n",
    "        \n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(dim_emb, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, dim_emb)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim_emb)\n",
    "        self.norm2 = nn.LayerNorm(dim_emb)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = activation()\n",
    "\n",
    "    def forward(self, data, src_mask=None, src_key_padding_mask=None):\n",
    "\n",
    "        # MultiHeadAttention\n",
    "        x, attn = self.attn(data, data, data, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        # add & norm\n",
    "        x = data + self.dropout1(x)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Implementation of Feedforward model\n",
    "        x1 = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        \n",
    "        # add & norm\n",
    "        x = x + self.dropout2(x1)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim_emb, dim_feedforward = 10, dropout=0.1, activation = nn.GELU):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(dim_emb, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, dim_emb)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = activation()\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(data))))\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim_emb, \n",
    "        ch_out_conv1d: int = 1, \n",
    "        dim_box: int = 4, \n",
    "        nhead: int = 1, \n",
    "        dim_feedforward: int = 10, \n",
    "        dropout: float = 0.1, \n",
    "        kernel_conv1d: int = 3,\n",
    "        stride_conv1d: int = 1,\n",
    "        activation = nn.GELU\n",
    "    ):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv2d = nn.Conv2d(1, ch_out_conv1d, kernel_conv1d, stride=2)\n",
    "        dim_emb = 255\n",
    "        self.encoder_layer = TransformerEncoderLayer(dim_emb, nhead, dim_feedforward, dropout, activation)\n",
    "        self.mlp = MLP(dim_emb, dim_feedforward, dropout, activation)\n",
    "        self.conv1d = nn.Conv1d(dim_emb, ch_out_conv1d, kernel_conv1d, stride=stride_conv1d)\n",
    "        self.flatten = nn.Flatten()\n",
    "        # with cnn2d 253\n",
    "        # without 510\n",
    "        # self.linear = nn.Linear(510, dim_feedforward*2)\n",
    "        # self.linear2 = nn.Linear(dim_feedforward*2, dim_box)\n",
    "\n",
    "        self.linear = nn.Linear(253, dim_box)\n",
    "\n",
    "\n",
    "        self.activation = activation()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        data = data.reshape(data.shape[0], 1, data.shape[1], data.shape[2])\n",
    "        x = self.conv2d(data)\n",
    "        x = x.reshape(x.shape[0], x.shape[2], x.shape[3])\n",
    "        # x = data\n",
    "        x = self.encoder_layer(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.flatten(x)\n",
    "        # print(x.shape)\n",
    "        # mlp\n",
    "        # x = self.linear2(self.activation(self.linear(x)))\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "#  loss nn.CrossEntropyLoss(reduction='mean')\n",
    "def training(model, train_loader, optimizer, criterion = mse_box, device = 'cuda', epochs = 10):\n",
    "\n",
    "    sample = 0.0\n",
    "    cum_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, target.to(torch.float32))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            sample += len(data)\n",
    "            cum_loss += loss.item()\n",
    "\n",
    "        print(f'Train Epoch: {e} Loss: {cum_loss/sample}')    \n",
    "\n",
    "\n",
    "def test_fn(model, test_loader, criterion = nn.CrossEntropyLoss(), device = 'cuda'):\n",
    "\n",
    "    sample = 0.0\n",
    "    cum_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            sample += len(data)\n",
    "            cum_loss += loss.item()\n",
    "\n",
    "        print(f'Test Loss: {cum_loss/sample}')       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 Loss: 1574908.619781434\n",
      "Train Epoch: 1 Loss: 1574248.648309594\n",
      "Train Epoch: 2 Loss: 1574005.2780523493\n",
      "Train Epoch: 3 Loss: 1573881.9299445911\n",
      "Train Epoch: 4 Loss: 1573521.4473794613\n",
      "Train Epoch: 5 Loss: 1572078.1378400596\n",
      "Train Epoch: 6 Loss: 1570877.7323635863\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m net \u001b[39m=\u001b[39m Net(\u001b[39m512\u001b[39m)\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m----> 3\u001b[0m training(net, train_loader, optimizer \u001b[39m=\u001b[39;49m optim\u001b[39m.\u001b[39;49mAdamW(net\u001b[39m.\u001b[39;49mparameters(), lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m))\n",
      "Cell \u001b[0;32mIn[16], line 125\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(model, train_loader, optimizer, criterion, device, epochs)\u001b[0m\n\u001b[1;32m    122\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    124\u001b[0m     sample \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data)\n\u001b[0;32m--> 125\u001b[0m     cum_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m    127\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrain Epoch: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m Loss: \u001b[39m\u001b[39m{\u001b[39;00mcum_loss\u001b[39m/\u001b[39msample\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# with cnn2d\n",
    "# head = 1\n",
    "net = Net(512).cuda()\n",
    "\n",
    "training(net, train_loader, optimizer = optim.AdamW(net.parameters(), lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 Loss: 1574442.5332351015\n",
      "Train Epoch: 1 Loss: 1574049.551567872\n",
      "Train Epoch: 2 Loss: 1570313.8399989426\n",
      "Train Epoch: 3 Loss: 1568347.3311237567\n",
      "Train Epoch: 4 Loss: 1567160.905685167\n",
      "Train Epoch: 5 Loss: 1566364.3843064425\n",
      "Train Epoch: 6 Loss: 1565790.2630347202\n",
      "Train Epoch: 7 Loss: 1565357.357050308\n",
      "Train Epoch: 8 Loss: 1565017.6816413356\n",
      "Train Epoch: 9 Loss: 1564735.8094123485\n"
     ]
    }
   ],
   "source": [
    "# without cnn2d\n",
    "# head = 1\n",
    "net = Net(512).cuda()\n",
    "\n",
    "training(net, train_loader, optimizer = optim.AdamW(net.parameters(), lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 Loss: 1575357.3728614112\n",
      "Train Epoch: 1 Loss: 1569566.9609323838\n",
      "Train Epoch: 2 Loss: 1567202.7041158453\n",
      "Train Epoch: 3 Loss: 1565983.2081504278\n",
      "Train Epoch: 4 Loss: 1565178.9855691306\n",
      "Train Epoch: 5 Loss: 1564572.787003213\n",
      "Train Epoch: 6 Loss: 1564086.8453031445\n",
      "Train Epoch: 7 Loss: 1563692.671444916\n",
      "Train Epoch: 8 Loss: 1563372.216937998\n",
      "Train Epoch: 9 Loss: 1563103.5590490955\n"
     ]
    }
   ],
   "source": [
    "# without cnn2d\n",
    "# increasing the number of neurons in the mlp and head = 2\n",
    "\n",
    "net = Net(512, dim_feedforward = 100, nhead = 2).cuda()\n",
    "\n",
    "training(net, train_loader, optimizer = optim.AdamW(net.parameters(), lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 Loss: 1573897.554539129\n",
      "Train Epoch: 1 Loss: 1573765.478110163\n",
      "Train Epoch: 2 Loss: 1573449.9140386241\n",
      "Train Epoch: 3 Loss: 1573503.5356781066\n",
      "Train Epoch: 4 Loss: 1573538.4815610675\n",
      "Train Epoch: 5 Loss: 1573444.1662044995\n",
      "Train Epoch: 6 Loss: 1573469.2474579592\n",
      "Train Epoch: 7 Loss: 1572854.5262744557\n",
      "Train Epoch: 8 Loss: 1571978.421947196\n",
      "Train Epoch: 9 Loss: 1571429.2642144524\n"
     ]
    }
   ],
   "source": [
    "# without cnn2d\n",
    "# increasing the number of neurons in the mlp and head = 2\n",
    "\n",
    "net = Net(512, dim_feedforward = 10, nhead = 3).cuda()\n",
    "\n",
    "training(net, train_loader, optimizer = optim.AdamW(net.parameters(), lr=0.001))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
