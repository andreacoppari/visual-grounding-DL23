{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class GraphAttentionModulation_LINEAR(nn.Module):\n",
    "    def __init__(self, dim_embedding, max_num_boxes) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_boxes = max_num_boxes\n",
    "\n",
    "        self.W = nn.Bilinear(max_num_boxes, dim_embedding, dim_embedding)\n",
    "\n",
    "        self.W1 = nn.Linear(dim_embedding, dim_embedding)\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(dim_embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # compute the clip score\n",
    "        # x: (batch_size, n_boxes, dim_embedding)\n",
    "        # c_s: (batch_size, n_boxes, n_boxes)\n",
    "        # the clip score is the similarity between each pair of boxes\n",
    "        # c_s[i, j] is the similarity between box i and box j\n",
    "        # the similarity will be used to compute the attention score\n",
    "        # considering the similarity between each pair of boxes\n",
    "        # as the link in between the boxes\n",
    "\n",
    "        # normalize the embedding\n",
    "        x /= x.norm(dim = -1, keepdim = True)\n",
    "        c_s = torch.bmm(x, x.permute(0, 2, 1)) \n",
    "\n",
    "        # find a transformation of the embedding\n",
    "        # based on the similarity between each pair of boxes\n",
    "        # c_s W^T -> attention_score\n",
    "        # attention_score: (batch_size, n_boxes, n_boxes)\n",
    "        # attention_score Embedding -> (batch_size, n_boxes, dim_embedding)\n",
    "        x = self.W(c_s, x)\n",
    "        del c_s\n",
    "\n",
    "        # apply a non-linear transformation\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        # apply a layer normalization\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        # apply a linear transformation \n",
    "        x = self.W1(x)\n",
    "\n",
    "        # apply a non-linear transformation\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        # apply a layer normalization\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "##############################################################################\n",
    "##############################################################################\n",
    "\n",
    "class GraphBoxRegressor(nn.Module):\n",
    "    def __init__(self, dim_embedding = 512, max_num_boxes = 48) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.max_num_boxes = max_num_boxes\n",
    "\n",
    "        self.gam = GraphAttentionModulation_LINEAR(dim_embedding, max_num_boxes)\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=1, end_dim=- 1)\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(dim_embedding * max_num_boxes + max_num_boxes * 4, dim_embedding),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(dim_embedding),\n",
    "            nn.Linear(dim_embedding, dim_embedding//2),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(dim_embedding//2),\n",
    "            nn.Linear(dim_embedding//2, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, boxes):\n",
    "\n",
    "        # apply the graph attention modulation\n",
    "        # x: (batch_size, n_boxes, dim_embedding)\n",
    "        x = self.gam(x)\n",
    "\n",
    "        # concatenate the boxes to the embedding\n",
    "        # x: (batch_size, n_boxes, dim_embedding + 4)\n",
    "        x = torch.cat([x, boxes], dim = -1)\n",
    "\n",
    "        # flatten the embedding\n",
    "        # x: (batch_size, n_boxes * dim_embedding)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # apply the regressor\n",
    "        # x: (batch_size, 4)\n",
    "        x = self.regressor(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "##############################################################################\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "class GraphBoxRegressorLightning(pl.LightningModule):\n",
    "    def __init__(self, dim_embedding = 512, max_num_boxes = 48) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.max_num_boxes = max_num_boxes\n",
    "\n",
    "        self.gam_l = GraphBoxRegressor(dim_embedding, max_num_boxes)\n",
    "\n",
    "\n",
    "        self.MSE = nn.MSELoss()\n",
    "        self.MAE = nn.L1Loss()\n",
    "        self.HUBER = nn.SmoothL1Loss()  \n",
    "\n",
    "        # GENERALIZED_BOX_IOU_LOSS https://arxiv.org/abs/1902.09630\n",
    "        self.generalized_box_iou_loss = torchvision.ops.generalized_box_iou_loss\n",
    "        # DISTANCE_BOX_IOU_LOSS https://arxiv.org/abs/1911.08287\n",
    "        self.distance_box_iou_loss = torchvision.ops.distance_box_iou_loss\n",
    "        # COMPLETE_BOX_IOU_LOSS https://arxiv.org/abs/1911.08287\n",
    "        self.complete_box_iou_loss = torchvision.ops.complete_box_iou_loss\n",
    "\n",
    "\n",
    "    def forward(self, x, boxes):\n",
    "        return self.gam_l(x, boxes)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        x, boxes, target = batch\n",
    "\n",
    "        pred = self(x, boxes)\n",
    "\n",
    "        # keep track of the losses\n",
    "\n",
    "        mse_loss = self.MSE(pred, target)\n",
    "\n",
    "        mae_loss = self.MAE(pred, target)\n",
    "\n",
    "        huber_loss = self.HUBER(pred, target)\n",
    "\n",
    "        g_box_iou_loss = self.generalized_box_iou_loss(pred, target, reduction = 'mean')\n",
    "\n",
    "        d_box_iou_loss = self.distance_box_iou_loss(pred, target, reduction = 'mean')\n",
    "\n",
    "        c_box_iou_loss = self.complete_box_iou_loss(pred, target, reduction = 'mean')\n",
    "\n",
    "        self.log('train_mse_loss', mse_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
    "        self.log('train_mae_loss', mae_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
    "        self.log('train_huber_loss', huber_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
    "        self.log('train_g_box_iou_loss', g_box_iou_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
    "        self.log('train_d_box_iou_loss', d_box_iou_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
    "        self.log('train_c_box_iou_loss', c_box_iou_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
    "\n",
    "        return g_box_iou_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "            \n",
    "        x, boxes, target = batch\n",
    "\n",
    "        pred = self(x, boxes)\n",
    "\n",
    "        # keep track of the losses\n",
    "\n",
    "        mse_loss = self.MSE(pred, target)\n",
    "\n",
    "        mae_loss = self.MAE(pred, target)\n",
    "\n",
    "        huber_loss = self.HUBER(pred, target)\n",
    "\n",
    "        g_box_iou_loss = self.generalized_box_iou_loss(pred, target, reduction = 'mean')\n",
    "\n",
    "        d_box_iou_loss = self.distance_box_iou_loss(pred, target, reduction = 'mean')\n",
    "\n",
    "        c_box_iou_loss = self.complete_box_iou_loss(pred, target, reduction = 'mean')\n",
    "\n",
    "        self.log('val_mse_loss', mse_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
    "        self.log('val_mae_loss', mae_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
    "        self.log('val_huber_loss', huber_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
    "        self.log('val_g_box_iou_loss', g_box_iou_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
    "        self.log('val_d_box_iou_loss', d_box_iou_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
    "        self.log('val_c_box_iou_loss', c_box_iou_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
    "\n",
    "        return g_box_iou_loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr = 1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 10)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the dataloaders\n",
    "\n",
    "X = torch.randn(1000, 48, 512)\n",
    "boxes = torch.randn(1000, 48, 4)\n",
    "target = boxes[:, 0,:]\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_size = int(len(X)*0.8)\n",
    "val_size = len(X) - train_size\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(X, boxes, target)\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | gam_l | GraphBoxRegressor | 25.7 M\n",
      "1 | MSE   | MSELoss           | 0     \n",
      "2 | MAE   | L1Loss            | 0     \n",
      "3 | HUBER | SmoothL1Loss      | 0     \n",
      "--------------------------------------------\n",
      "25.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "25.7 M    Total params\n",
      "102.651   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 13/13 [00:01<00:00,  8.24it/s, v_num=5, train_mse_loss_step=8.900, train_mae_loss_step=2.760, train_huber_loss_step=2.270, train_g_box_iou_loss_step=0.993, train_d_box_iou_loss_step=0.995, train_c_box_iou_loss_step=1.320, val_mse_loss_step=9.420, val_mae_loss_step=2.960, val_huber_loss_step=2.460, val_g_box_iou_loss_step=0.994, val_d_box_iou_loss_step=1.010, val_c_box_iou_loss_step=1.520, val_mse_loss_epoch=9.880, val_mae_loss_epoch=2.960, val_huber_loss_epoch=2.460, val_g_box_iou_loss_epoch=0.998, val_d_box_iou_loss_epoch=1.010, val_c_box_iou_loss_epoch=1.300, train_mse_loss_epoch=9.990, train_mae_loss_epoch=2.970, train_huber_loss_epoch=2.480, train_g_box_iou_loss_epoch=0.998, train_d_box_iou_loss_epoch=1.010, train_c_box_iou_loss_epoch=1.320]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 13/13 [00:02<00:00,  6.45it/s, v_num=5, train_mse_loss_step=8.900, train_mae_loss_step=2.760, train_huber_loss_step=2.270, train_g_box_iou_loss_step=0.993, train_d_box_iou_loss_step=0.995, train_c_box_iou_loss_step=1.320, val_mse_loss_step=9.420, val_mae_loss_step=2.960, val_huber_loss_step=2.460, val_g_box_iou_loss_step=0.994, val_d_box_iou_loss_step=1.010, val_c_box_iou_loss_step=1.520, val_mse_loss_epoch=9.880, val_mae_loss_epoch=2.960, val_huber_loss_epoch=2.460, val_g_box_iou_loss_epoch=0.998, val_d_box_iou_loss_epoch=1.010, val_c_box_iou_loss_epoch=1.300, train_mse_loss_epoch=9.990, train_mae_loss_epoch=2.970, train_huber_loss_epoch=2.480, train_g_box_iou_loss_epoch=0.998, train_d_box_iou_loss_epoch=1.010, train_c_box_iou_loss_epoch=1.320]\n"
     ]
    }
   ],
   "source": [
    "# set up the model\n",
    "\n",
    "model = GraphBoxRegressorLightning().cuda()\n",
    "\n",
    "# early stopping\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# early_stopping = EarlyStopping('val_g_box_iou_loss', patience = 3, mode = 'min')\n",
    "# checkpoint = ModelCheckpoint('checkpoints', monitor = 'val_g_box_iou_loss', save_top_k = 1, mode = 'min')\n",
    "\n",
    "# set up the trainer\n",
    "\n",
    "trainer = pl.Trainer(accelerator='auto', max_epochs = 10)\n",
    "\n",
    "# train the model\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 100, 100])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "torch.bmm(torch.randn(64,100, 512), torch.randn(64, 512, 100)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
