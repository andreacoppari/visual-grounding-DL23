{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore sentece denoising\n",
    "\n",
    "The last cell is the one which has the most interesting results. It shows the original sentence and the other versions of the same sentence. The result it seems promissing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "input_sentence = \"They were there to enjoy us and they were there to pray for us.\"\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained('eugenesiow/bart-paraphrase')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "tokenizer = BartTokenizer.from_pretrained('eugenesiow/bart-paraphrase')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riccardotedoldi/mambaforge/envs/ml/lib/python3.10/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  4.072089195251465\n",
      "['A purple elephant is flying in the sky, surrounded by pink clouds.', 'The Eiffel tower is tall in the middle of a dense forest.', 'A group of penguins is sunbathing on a sandy beach.', 'A rainbow-colored dinosaur is chasing a school bus on a city street.', 'A mermaid is swimming in a fish tank filled with colorful tropical fish.', 'A spaceship has landed on a snowy mountaintop, next to a cozy cabin.', 'A giant cupcake is floating in the ocean, attracting seagulls.', 'A giraffe is riding a bicycle in a crowded amusement park.', 'A waterfall flows through a desert landscape with cacti and sand dunes.']\n"
     ]
    }
   ],
   "source": [
    "input_sentence = [\n",
    "    \"A purple elephant is flying in the sky, surrounded by pink clouds.\",\n",
    "    \"The Eiffel Tower is standing tall in the middle of a dense forest.\",\n",
    "    \"A group of penguins is sunbathing on a sandy beach.\",\n",
    "    \"A rainbow-colored dinosaur is chasing a school bus on a city street.\",\n",
    "    \"A mermaid is swimming in a fish tank filled with colorful tropical fish.\",\n",
    "    \"A spaceship is landing on a snowy mountaintop, next to a cozy cabin.\",\n",
    "    \"A giant cupcake is floating in the ocean, attracting seagulls.\",\n",
    "    \"A giraffe is riding a bicycle in a crowded amusement park.\",\n",
    "    \"A waterfall is flowing through a desert landscape with cacti and sand dunes.\"\n",
    "]\n",
    "\n",
    "batch = tokenizer(input_sentence, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "translated = model.generate(**batch)\n",
    "generated_sentences = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken in seconds: \", end - start)\n",
    "\n",
    "print(generated_sentences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text-Simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: filelock in /home/rickbook/.local/lib/python3.10/site-packages (from transformers) (3.10.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/rickbook/.local/lib/python3.10/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: requests in /home/rickbook/.local/lib/python3.10/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/rickbook/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: fsspec in /home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/rickbook/.local/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/rickbook/.local/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/rickbook/.local/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/rickbook/.local/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 243/243 [00:00<00:00, 377kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 2.53MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 61.4MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 3.22MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 108/108 [00:00<00:00, 375kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 893/893 [00:00<00:00, 1.14MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.44G/1.44G [00:35<00:00, 41.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"philippelaban/keep_it_simple\")\n",
    "kis_model = AutoModelForCausalLM.from_pretrained(\"philippelaban/keep_it_simple\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "A small capsule containing soil samples that was dropped from 136,700 miles in space by Japan's Hayabusa2 probe was successfully brought back to earth by the Outback on December 6. The precise timing of the mission thrilled many in Japan, who said they took pride in its success.\n",
      "----\n",
      "A small capsule containing soil samples that was dropped from 136,700 miles, Japan's Hayabusa2 space probe, landed as planned on December 6. The mission was intended to test the limits of the country's space program, said many in Japan, who said they took pride in its success.\n",
      "----\n",
      "A small capsule containing samples of asteroid soil that was dropped from 136,700 miles over the space of a few days earlier this year by Japan's Hayabusa2 probe. The extremely high precision required to carry out the mission thrilled many in Japan, who said they took pride in its success.\n",
      "----\n",
      "A small capsule containing soil samples that Japan dropped from 136,700 miles over the past two years was successfully launched by Hayabusa2, the country's top space exploration vehicle.\n",
      "----\n",
      "A small capsule containing soil samples that was dropped from 136,700 miles in space by Japan's Hayabusa2 probe was successfully brought back to earth by the Outback on December 6. The precise timing of the mission thrilled many in Japan, who said they took pride in its success.\n",
      "----\n",
      "A small capsule containing soil samples that was dropped from 136,700 miles, Japan's Hayabusa2 space probe, landed as planned in the Outback on December 6. The mission was intended to test the precision of the country's space program, said many in Japan.\n",
      "----\n",
      "A small capsule containing samples of asteroid soil that was dropped from 136,700 miles over the past two years by Japan's Hayabusa2 space probe landed as planned in the Outback. The precise timing of the mission thrilled many in Japan, who said they took pride in its success.\n",
      "----\n",
      "A small capsule containing soil samples that was dropped from 136,700 miles in space by Japan's Hayabusa2 probe was intended for use by outback residents on December 6. But the mission was so precise that many in Japan took pride in its success, said the team.\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"\"\"A small capsule containing asteroid soil samples that was dropped from 136,700 miles in space by Japan's Hayabusa2 spacecraft landed as planned in the Australian Outback on December 6. The extremely high precision required to carry out the mission thrilled many in Japan, who said they took pride in its success.\"\"\"\n",
    "\n",
    "start_id = tokenizer.bos_token_id\n",
    "tokenized_paragraph = [(tokenizer.encode(text=paragraph) + [start_id])]\n",
    "input_ids = torch.LongTensor(tokenized_paragraph)\n",
    "\n",
    "output_ids = kis_model.generate(input_ids, max_length=150, num_beams=4, do_sample=True, num_return_sequences=8)\n",
    "output_ids = output_ids[:, input_ids.shape[1]:]\n",
    "output = tokenizer.batch_decode(output_ids)\n",
    "output = [o.replace(tokenizer.eos_token, \"\") for o in output]\n",
    "\n",
    "for o in output:\n",
    "    print(\"----\")\n",
    "    print(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, EncoderDecoderModel\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('mrm8488/bert-small2bert-small-finetuned-cnn_daily_mail-summarization')\n",
    "model = EncoderDecoderModel.from_pretrained('mrm8488/bert-small2bert-small-finetuned-cnn_daily_mail-summarization').to(device)\n",
    "\n",
    "\n",
    "def generate_summary(text):\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = tokenizer([text], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask, length_penalty=100.0, num_beams=4)\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a purple elephant is flying in the sky, surrounded by pink clouds. the elephant is surrounded by the pink clouds and is flying into the sky. it is also surrounded by yellow clouds, with pink clouds flying in sky. the elephants are flying through the sky for the first time in the world.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"A purple elephant is flying in the sky, surrounded by pink clouds.\"\n",
    "generate_summary(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel, GPT2Model\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)\n",
    "outputs = model(input_ids)\n",
    "last_hidden_states = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'could you rephrase A purple elephant is flying in the sky, surrounded by pink clouds.'},\n",
       " {'generated_text': 'could you rephrase A purple elephant is flying in the sky, surrounded by pink clouds.\\n\\n\\nBut you can never know the real name of'},\n",
       " {'generated_text': 'could you rephrase A purple elephant is flying in the sky, surrounded by pink clouds.\\n\\nYou must get in there and catch up (like'},\n",
       " {'generated_text': 'could you rephrase A purple elephant is flying in the sky, surrounded by pink clouds.\\nAs one can imagine, you could not walk, a'},\n",
       " {'generated_text': 'could you rephrase A purple elephant is flying in the sky, surrounded by pink clouds.'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='distilgpt2')\n",
    "set_seed(42)\n",
    "generator(\"could you rephrase A purple elephant is flying in the sky, surrounded by pink clouds.\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With this model we can do rephrasing of sentences\n",
    "\n",
    "The main idea is that we rephrase the sentences reducing the perplexity of the sentence. In this way we make easier to CLIP to understand which is the box which best match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A purple elephant is seen in the sky. Pink clouds surround the purple elephant.', 'A purple elephant is in the sky. Pink clouds surround the purple elephant.', 'A purple elephant is in the sky. Pink clouds surround a purple elephant.', 'A purple elephant is surrounded by pink clouds. A purple elephant is in the sky.', 'A purple elephant is seen in the sky. Pink clouds surround the elephant.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "checkpoint=\"unikei/t5-base-split-and-rephrase\"\n",
    "\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint)\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoint)\n",
    "\n",
    "complex_sentence = \"A purple elephant surrounded by pink clouds is fly in the sky.\"\n",
    "complex_tokenized = tokenizer(complex_sentence, \n",
    "                                 padding=\"max_length\", \n",
    "                                 truncation=True,\n",
    "                                 max_length=256, \n",
    "                                 return_tensors='pt')\n",
    "\n",
    "simple_tokenized = model.generate(complex_tokenized['input_ids'], attention_mask = complex_tokenized['attention_mask'], max_length=256, num_beams=5, num_return_sequences=5)\n",
    "simple_sentences = tokenizer.batch_decode(simple_tokenized, skip_special_tokens=True)\n",
    "print(simple_sentences)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3163cfb8aa3549ad3f5400bc3427ee7a4002d2a0d6d7ead52f641c6a7636395"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
